{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kvY2HmDexFj3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets, ensemble\n",
        "from catboost import CatBoostRegressor\n",
        "from tqdm import tqdm\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "import itertools\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>3.77</td>\n",
              "      <td>15.9</td>\n",
              "      <td>2.730798</td>\n",
              "      <td>12</td>\n",
              "      <td>42.01</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>3.048333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.72</td>\n",
              "      <td>24.5</td>\n",
              "      <td>4.289058</td>\n",
              "      <td>10</td>\n",
              "      <td>67.53</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>17.138611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>65.30</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>98.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06</td>\n",
              "      <td>B726632</td>\n",
              "      <td>10.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1490</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.31</td>\n",
              "      <td>22.1</td>\n",
              "      <td>4.693735</td>\n",
              "      <td>7</td>\n",
              "      <td>43.02</td>\n",
              "      <td>43.15</td>\n",
              "      <td>41.11</td>\n",
              "      <td>1169.853455</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>2.31</td>\n",
              "      <td>22.8</td>\n",
              "      <td>2.345875</td>\n",
              "      <td>14</td>\n",
              "      <td>90.45</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>96.030556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367436</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>61.25</td>\n",
              "      <td>62.21</td>\n",
              "      <td>55.70</td>\n",
              "      <td>1333.609109</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>65.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367437</th>\n",
              "      <td>TRAIN_367437</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 2:58</td>\n",
              "      <td>D847216</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>1280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.028558</td>\n",
              "      <td>11</td>\n",
              "      <td>105.37</td>\n",
              "      <td>109.34</td>\n",
              "      <td>104.69</td>\n",
              "      <td>1955.103846</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367438</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>3.36</td>\n",
              "      <td>31.7</td>\n",
              "      <td>2.557156</td>\n",
              "      <td>15</td>\n",
              "      <td>97.73</td>\n",
              "      <td>99.10</td>\n",
              "      <td>95.78</td>\n",
              "      <td>1601.291086</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.997500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367439</th>\n",
              "      <td>TRAIN_367439</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07</td>\n",
              "      <td>N211282</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2400</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.44</td>\n",
              "      <td>10.8</td>\n",
              "      <td>3.055715</td>\n",
              "      <td>19</td>\n",
              "      <td>49.75</td>\n",
              "      <td>50.08</td>\n",
              "      <td>47.02</td>\n",
              "      <td>1191.353331</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367440</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>19.8</td>\n",
              "      <td>3.177475</td>\n",
              "      <td>22</td>\n",
              "      <td>70.10</td>\n",
              "      <td>71.89</td>\n",
              "      <td>69.62</td>\n",
              "      <td>2115.046707</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>8.464167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>367441 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000003     JP   TMR7              Cargo   0.000000   \n",
              "4       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "367436  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "367437  TRAIN_367437     JP   QYY1             Tanker   0.000000   \n",
              "367438  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "367439  TRAIN_367439     JP   TMR7              Cargo   0.000000   \n",
              "367440  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  V_WIND  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...    3.77   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...   -6.72   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...    0.00   \n",
              "3       2020-09-18 22:06  B726632     10.0     33        1490  ...   -7.31   \n",
              "4       2022-08-13 12:57  D215135     30.0     10       27600  ...    2.31   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "367436  2017-11-11 22:23  J661243     40.0     13       93200  ...     NaN   \n",
              "367437   2022-04-29 2:58  D847216     10.0      9        1280  ...    0.87   \n",
              "367438   2022-07-14 7:58  Q635545     30.0      6       25000  ...    3.36   \n",
              "367439  2020-12-22 10:07  N211282     10.0      8        2400  ...   -2.44   \n",
              "367440  2021-06-04 14:54  V628821     40.0     10       87200  ...   -0.84   \n",
              "\n",
              "        AIR_TEMPERATURE        BN  ATA_LT   DUBAI   BRENT     WTI  \\\n",
              "0                  15.9  2.730798      12   42.01   43.16   40.96   \n",
              "1                  24.5  4.289058      10   67.53   64.55   59.34   \n",
              "2                   9.4  0.000000      14   65.30   66.39   56.94   \n",
              "3                  22.1  4.693735       7   43.02   43.15   41.11   \n",
              "4                  22.8  2.345875      14   90.45   93.65   88.11   \n",
              "...                 ...       ...     ...     ...     ...     ...   \n",
              "367436              NaN       NaN       6   61.25   62.21   55.70   \n",
              "367437             17.1  1.028558      11  105.37  109.34  104.69   \n",
              "367438             31.7  2.557156      15   97.73   99.10   95.78   \n",
              "367439             10.8  3.055715      19   49.75   50.08   47.02   \n",
              "367440             19.8  3.177475      22   70.10   71.89   69.62   \n",
              "\n",
              "            BDI_ADJ  PORT_SIZE    CI_HOUR  \n",
              "0       1407.668330   0.001660   3.048333  \n",
              "1       2089.046774   0.001614  17.138611  \n",
              "2        603.193047   0.001743  98.827500  \n",
              "3       1169.853455   0.000069   0.000000  \n",
              "4       1107.944894   0.000197  96.030556  \n",
              "...             ...        ...        ...  \n",
              "367436  1333.609109   0.000360  65.850000  \n",
              "367437  1955.103846   0.000552   0.000000  \n",
              "367438  1601.291086   0.002615   0.997500  \n",
              "367439  1191.353331   0.000069   0.000000  \n",
              "367440  2115.046707   0.001660   8.464167  \n",
              "\n",
              "[367441 rows x 27 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>ATA_datetime</th>\n",
              "      <th>port_daily_arrival_rate</th>\n",
              "      <th>port_daily_service_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>12</td>\n",
              "      <td>42.01</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>3.048333</td>\n",
              "      <td>2020-10-15 04:03:00</td>\n",
              "      <td>8.710324</td>\n",
              "      <td>0.028614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>10</td>\n",
              "      <td>67.53</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>17.138611</td>\n",
              "      <td>2019-09-17 02:55:00</td>\n",
              "      <td>7.542746</td>\n",
              "      <td>0.018029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>14</td>\n",
              "      <td>65.30</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>98.827500</td>\n",
              "      <td>2019-02-23 06:43:00</td>\n",
              "      <td>9.475826</td>\n",
              "      <td>0.023371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06</td>\n",
              "      <td>B726632</td>\n",
              "      <td>10.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1490</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>43.02</td>\n",
              "      <td>43.15</td>\n",
              "      <td>41.11</td>\n",
              "      <td>1169.853455</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06:00</td>\n",
              "      <td>2.185466</td>\n",
              "      <td>0.020537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>14</td>\n",
              "      <td>90.45</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>96.030556</td>\n",
              "      <td>2022-08-13 12:57:00</td>\n",
              "      <td>1.527931</td>\n",
              "      <td>0.012597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367436</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>61.25</td>\n",
              "      <td>62.21</td>\n",
              "      <td>55.70</td>\n",
              "      <td>1333.609109</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>65.850000</td>\n",
              "      <td>2017-11-11 22:23:00</td>\n",
              "      <td>2.746103</td>\n",
              "      <td>0.010488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367437</th>\n",
              "      <td>TRAIN_367437</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 2:58</td>\n",
              "      <td>D847216</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>1280</td>\n",
              "      <td>...</td>\n",
              "      <td>11</td>\n",
              "      <td>105.37</td>\n",
              "      <td>109.34</td>\n",
              "      <td>104.69</td>\n",
              "      <td>1955.103846</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 02:58:00</td>\n",
              "      <td>6.129946</td>\n",
              "      <td>0.051559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367438</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>15</td>\n",
              "      <td>97.73</td>\n",
              "      <td>99.10</td>\n",
              "      <td>95.78</td>\n",
              "      <td>1601.291086</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.997500</td>\n",
              "      <td>2022-07-14 07:58:00</td>\n",
              "      <td>13.495088</td>\n",
              "      <td>0.009215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367439</th>\n",
              "      <td>TRAIN_367439</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07</td>\n",
              "      <td>N211282</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2400</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>49.75</td>\n",
              "      <td>50.08</td>\n",
              "      <td>47.02</td>\n",
              "      <td>1191.353331</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07:00</td>\n",
              "      <td>2.185466</td>\n",
              "      <td>0.020537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367440</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>22</td>\n",
              "      <td>70.10</td>\n",
              "      <td>71.89</td>\n",
              "      <td>69.62</td>\n",
              "      <td>2115.046707</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>8.464167</td>\n",
              "      <td>2021-06-04 14:54:00</td>\n",
              "      <td>8.710324</td>\n",
              "      <td>0.028614</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>367441 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000003     JP   TMR7              Cargo   0.000000   \n",
              "4       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "367436  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "367437  TRAIN_367437     JP   QYY1             Tanker   0.000000   \n",
              "367438  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "367439  TRAIN_367439     JP   TMR7              Cargo   0.000000   \n",
              "367440  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  ATA_LT  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...      12   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...      10   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...      14   \n",
              "3       2020-09-18 22:06  B726632     10.0     33        1490  ...       7   \n",
              "4       2022-08-13 12:57  D215135     30.0     10       27600  ...      14   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "367436  2017-11-11 22:23  J661243     40.0     13       93200  ...       6   \n",
              "367437   2022-04-29 2:58  D847216     10.0      9        1280  ...      11   \n",
              "367438   2022-07-14 7:58  Q635545     30.0      6       25000  ...      15   \n",
              "367439  2020-12-22 10:07  N211282     10.0      8        2400  ...      19   \n",
              "367440  2021-06-04 14:54  V628821     40.0     10       87200  ...      22   \n",
              "\n",
              "         DUBAI   BRENT     WTI      BDI_ADJ PORT_SIZE    CI_HOUR  \\\n",
              "0        42.01   43.16   40.96  1407.668330  0.001660   3.048333   \n",
              "1        67.53   64.55   59.34  2089.046774  0.001614  17.138611   \n",
              "2        65.30   66.39   56.94   603.193047  0.001743  98.827500   \n",
              "3        43.02   43.15   41.11  1169.853455  0.000069   0.000000   \n",
              "4        90.45   93.65   88.11  1107.944894  0.000197  96.030556   \n",
              "...        ...     ...     ...          ...       ...        ...   \n",
              "367436   61.25   62.21   55.70  1333.609109  0.000360  65.850000   \n",
              "367437  105.37  109.34  104.69  1955.103846  0.000552   0.000000   \n",
              "367438   97.73   99.10   95.78  1601.291086  0.002615   0.997500   \n",
              "367439   49.75   50.08   47.02  1191.353331  0.000069   0.000000   \n",
              "367440   70.10   71.89   69.62  2115.046707  0.001660   8.464167   \n",
              "\n",
              "              ATA_datetime  port_daily_arrival_rate  port_daily_service_rate  \n",
              "0      2020-10-15 04:03:00                 8.710324                 0.028614  \n",
              "1      2019-09-17 02:55:00                 7.542746                 0.018029  \n",
              "2      2019-02-23 06:43:00                 9.475826                 0.023371  \n",
              "3      2020-09-18 22:06:00                 2.185466                 0.020537  \n",
              "4      2022-08-13 12:57:00                 1.527931                 0.012597  \n",
              "...                    ...                      ...                      ...  \n",
              "367436 2017-11-11 22:23:00                 2.746103                 0.010488  \n",
              "367437 2022-04-29 02:58:00                 6.129946                 0.051559  \n",
              "367438 2022-07-14 07:58:00                13.495088                 0.009215  \n",
              "367439 2020-12-22 10:07:00                 2.185466                 0.020537  \n",
              "367440 2021-06-04 14:54:00                 8.710324                 0.028614  \n",
              "\n",
              "[367441 rows x 30 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ATA를 datetime 형태로 변환\n",
        "train['ATA_datetime'] = pd.to_datetime(train['ATA'])\n",
        "\n",
        "# 항구별 일별 도착 선박 수 계산\n",
        "port_daily_arrivals = train.groupby(['ARI_PO', train['ATA_datetime'].dt.date]).size().reset_index(name='arrivals')\n",
        "\n",
        "# 항구별 일별 평균 도착률 계산\n",
        "port_daily_avg_arrival_rate = port_daily_arrivals.groupby('ARI_PO')['arrivals'].mean()\n",
        "\n",
        "# 전체 데이터의 평균 대기 시간을 계산하여 일별 서비스률 추정\n",
        "average_waiting_time = train['CI_HOUR'].mean()\n",
        "daily_service_rate = 1 / average_waiting_time\n",
        "\n",
        "# 데이터에 항구별 파생 변수 추가\n",
        "train['port_daily_arrival_rate'] = train.apply(lambda row: port_daily_avg_arrival_rate[row['ARI_PO']], axis=1)\n",
        "port_avg_waiting_time = train.groupby('ARI_PO')['CI_HOUR'].mean()\n",
        "port_daily_service_rate = 1 / port_avg_waiting_time\n",
        "train['port_daily_service_rate'] = train['ARI_PO'].map(port_daily_service_rate)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>ATA_datetime</th>\n",
              "      <th>port_daily_arrival_rate</th>\n",
              "      <th>port_daily_service_rate</th>\n",
              "      <th>port_dayofweek_arrival_rate</th>\n",
              "      <th>port_dayofweek_service_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>3.048333</td>\n",
              "      <td>2020-10-15 04:03:00</td>\n",
              "      <td>8.710324</td>\n",
              "      <td>0.028614</td>\n",
              "      <td>4807.0</td>\n",
              "      <td>0.039002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>17.138611</td>\n",
              "      <td>2019-09-17 02:55:00</td>\n",
              "      <td>7.542746</td>\n",
              "      <td>0.018029</td>\n",
              "      <td>3907.0</td>\n",
              "      <td>0.021508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>98.827500</td>\n",
              "      <td>2019-02-23 06:43:00</td>\n",
              "      <td>9.475826</td>\n",
              "      <td>0.023371</td>\n",
              "      <td>1331.0</td>\n",
              "      <td>0.009466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06</td>\n",
              "      <td>B726632</td>\n",
              "      <td>10.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1490</td>\n",
              "      <td>...</td>\n",
              "      <td>43.15</td>\n",
              "      <td>41.11</td>\n",
              "      <td>1169.853455</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06:00</td>\n",
              "      <td>2.185466</td>\n",
              "      <td>0.020537</td>\n",
              "      <td>430.0</td>\n",
              "      <td>0.034673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>96.030556</td>\n",
              "      <td>2022-08-13 12:57:00</td>\n",
              "      <td>1.527931</td>\n",
              "      <td>0.012597</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.006117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367436</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>62.21</td>\n",
              "      <td>55.70</td>\n",
              "      <td>1333.609109</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>65.850000</td>\n",
              "      <td>2017-11-11 22:23:00</td>\n",
              "      <td>2.746103</td>\n",
              "      <td>0.010488</td>\n",
              "      <td>764.0</td>\n",
              "      <td>0.007638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367437</th>\n",
              "      <td>TRAIN_367437</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 2:58</td>\n",
              "      <td>D847216</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>1280</td>\n",
              "      <td>...</td>\n",
              "      <td>109.34</td>\n",
              "      <td>104.69</td>\n",
              "      <td>1955.103846</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 02:58:00</td>\n",
              "      <td>6.129946</td>\n",
              "      <td>0.051559</td>\n",
              "      <td>3002.0</td>\n",
              "      <td>0.071653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367438</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>99.10</td>\n",
              "      <td>95.78</td>\n",
              "      <td>1601.291086</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.997500</td>\n",
              "      <td>2022-07-14 07:58:00</td>\n",
              "      <td>13.495088</td>\n",
              "      <td>0.009215</td>\n",
              "      <td>7030.0</td>\n",
              "      <td>0.010007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367439</th>\n",
              "      <td>TRAIN_367439</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07</td>\n",
              "      <td>N211282</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2400</td>\n",
              "      <td>...</td>\n",
              "      <td>50.08</td>\n",
              "      <td>47.02</td>\n",
              "      <td>1191.353331</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07:00</td>\n",
              "      <td>2.185466</td>\n",
              "      <td>0.020537</td>\n",
              "      <td>573.0</td>\n",
              "      <td>0.026080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367440</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>71.89</td>\n",
              "      <td>69.62</td>\n",
              "      <td>2115.046707</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>8.464167</td>\n",
              "      <td>2021-06-04 14:54:00</td>\n",
              "      <td>8.710324</td>\n",
              "      <td>0.028614</td>\n",
              "      <td>4128.0</td>\n",
              "      <td>0.033647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>367441 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000003     JP   TMR7              Cargo   0.000000   \n",
              "4       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "367436  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "367437  TRAIN_367437     JP   QYY1             Tanker   0.000000   \n",
              "367438  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "367439  TRAIN_367439     JP   TMR7              Cargo   0.000000   \n",
              "367440  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...   BRENT  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...   43.16   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...   64.55   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...   66.39   \n",
              "3       2020-09-18 22:06  B726632     10.0     33        1490  ...   43.15   \n",
              "4       2022-08-13 12:57  D215135     30.0     10       27600  ...   93.65   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "367436  2017-11-11 22:23  J661243     40.0     13       93200  ...   62.21   \n",
              "367437   2022-04-29 2:58  D847216     10.0      9        1280  ...  109.34   \n",
              "367438   2022-07-14 7:58  Q635545     30.0      6       25000  ...   99.10   \n",
              "367439  2020-12-22 10:07  N211282     10.0      8        2400  ...   50.08   \n",
              "367440  2021-06-04 14:54  V628821     40.0     10       87200  ...   71.89   \n",
              "\n",
              "           WTI      BDI_ADJ  PORT_SIZE    CI_HOUR        ATA_datetime  \\\n",
              "0        40.96  1407.668330   0.001660   3.048333 2020-10-15 04:03:00   \n",
              "1        59.34  2089.046774   0.001614  17.138611 2019-09-17 02:55:00   \n",
              "2        56.94   603.193047   0.001743  98.827500 2019-02-23 06:43:00   \n",
              "3        41.11  1169.853455   0.000069   0.000000 2020-09-18 22:06:00   \n",
              "4        88.11  1107.944894   0.000197  96.030556 2022-08-13 12:57:00   \n",
              "...        ...          ...        ...        ...                 ...   \n",
              "367436   55.70  1333.609109   0.000360  65.850000 2017-11-11 22:23:00   \n",
              "367437  104.69  1955.103846   0.000552   0.000000 2022-04-29 02:58:00   \n",
              "367438   95.78  1601.291086   0.002615   0.997500 2022-07-14 07:58:00   \n",
              "367439   47.02  1191.353331   0.000069   0.000000 2020-12-22 10:07:00   \n",
              "367440   69.62  2115.046707   0.001660   8.464167 2021-06-04 14:54:00   \n",
              "\n",
              "        port_daily_arrival_rate  port_daily_service_rate  \\\n",
              "0                      8.710324                 0.028614   \n",
              "1                      7.542746                 0.018029   \n",
              "2                      9.475826                 0.023371   \n",
              "3                      2.185466                 0.020537   \n",
              "4                      1.527931                 0.012597   \n",
              "...                         ...                      ...   \n",
              "367436                 2.746103                 0.010488   \n",
              "367437                 6.129946                 0.051559   \n",
              "367438                13.495088                 0.009215   \n",
              "367439                 2.185466                 0.020537   \n",
              "367440                 8.710324                 0.028614   \n",
              "\n",
              "        port_dayofweek_arrival_rate  port_dayofweek_service_rate  \n",
              "0                            4807.0                     0.039002  \n",
              "1                            3907.0                     0.021508  \n",
              "2                            1331.0                     0.009466  \n",
              "3                             430.0                     0.034673  \n",
              "4                             181.0                     0.006117  \n",
              "...                             ...                          ...  \n",
              "367436                        764.0                     0.007638  \n",
              "367437                       3002.0                     0.071653  \n",
              "367438                       7030.0                     0.010007  \n",
              "367439                        573.0                     0.026080  \n",
              "367440                       4128.0                     0.033647  \n",
              "\n",
              "[367441 rows x 32 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 항구 및 요일별 일별 도착 선박 수 계산\n",
        "port_dayofweek_daily_arrivals = train.groupby(['ARI_PO', train['ATA_datetime'].dt.dayofweek]).size().reset_index(name='arrivals')\n",
        "\n",
        "# 항구 및 요일별 일별 평균 도착률 계산\n",
        "port_dayofweek_avg_arrival_rate = port_dayofweek_daily_arrivals.groupby(['ARI_PO', 'ATA_datetime'])['arrivals'].mean()\n",
        "\n",
        "# 항구 및 요일별 평균 대기 시간 계산\n",
        "port_dayofweek_avg_waiting_time = train.groupby(['ARI_PO', train['ATA_datetime'].dt.dayofweek])['CI_HOUR'].mean()\n",
        "\n",
        "# 항구 및 요일별 서비스률 추정 (가정: 하루에 처리 가능한 선박 수는 1/평균 대기 시간)\n",
        "port_dayofweek_service_rate = 1 / port_dayofweek_avg_waiting_time\n",
        "\n",
        "# 데이터에 항구 및 요일별 파생 변수 추가\n",
        "train['port_dayofweek_arrival_rate'] = train.apply(lambda row: port_dayofweek_avg_arrival_rate[(row['ARI_PO'], row['ATA_datetime'].dayofweek)], axis=1)\n",
        "train['port_dayofweek_service_rate'] = train.apply(lambda row: port_dayofweek_service_rate[(row['ARI_PO'], row['ATA_datetime'].dayofweek)], axis=1)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 항구 및 요일별 rho (도착률/서비스률) 계산\n",
        "port_dayofweek_rho = train['port_dayofweek_arrival_rate'] / train['port_dayofweek_service_rate']\n",
        "\n",
        "# 시스템이 특정 인원(예: 5인 이상)일 확률\n",
        "n = 5\n",
        "P_n = (1 - port_dayofweek_rho) * (port_dayofweek_rho ** n)\n",
        "\n",
        "# 시스템이 바쁜 확률\n",
        "P_busy = port_dayofweek_rho\n",
        "\n",
        "# 데이터에 파생 변수 추가\n",
        "train['P_n'] = P_n\n",
        "train['P_busy'] = P_busy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 요일별 일별 도착 선박 수 계산\n",
        "dayofweek_daily_arrivals = train.groupby(train['ATA_datetime'].dt.dayofweek).size().reset_index(name='arrivals')\n",
        "\n",
        "# 요일별 일별 평균 도착률 계산\n",
        "dayofweek_avg_arrival_rate = dayofweek_daily_arrivals.set_index('ATA_datetime')['arrivals'] / len(train['ATA_datetime'].dt.date.unique())\n",
        "\n",
        "# 요일별 평균 대기 시간 계산\n",
        "dayofweek_avg_waiting_time = train.groupby(train['ATA_datetime'].dt.dayofweek)['CI_HOUR'].mean()\n",
        "\n",
        "# 요일별 서비스률 추정 (가정: 하루에 처리 가능한 선박 수는 1/평균 대기 시간)\n",
        "dayofweek_service_rate = 1 / dayofweek_avg_waiting_time\n",
        "\n",
        "# 요일별 rho (도착률/서비스률) 계산\n",
        "dayofweek_rho = dayofweek_avg_arrival_rate / dayofweek_service_rate\n",
        "\n",
        "# 시스템이 특정 인원(예: 5인 이상)일 확률\n",
        "P_n_dayofweek = (1 - dayofweek_rho) * (dayofweek_rho ** n)\n",
        "\n",
        "# 시스템이 바쁜 확률\n",
        "P_busy_dayofweek = dayofweek_rho\n",
        "\n",
        "# 데이터에 요일별 파생 변수 추가\n",
        "train['P_n_dayofweek'] = train['ATA_datetime'].dt.dayofweek.map(P_n_dayofweek)\n",
        "train['P_busy_dayofweek'] = train['ATA_datetime'].dt.dayofweek.map(P_busy_dayofweek)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>ATA_datetime</th>\n",
              "      <th>port_daily_arrival_rate</th>\n",
              "      <th>port_daily_service_rate</th>\n",
              "      <th>port_dayofweek_arrival_rate</th>\n",
              "      <th>port_dayofweek_service_rate</th>\n",
              "      <th>P_n</th>\n",
              "      <th>P_busy</th>\n",
              "      <th>P_n_dayofweek</th>\n",
              "      <th>P_busy_dayofweek</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>3.048333</td>\n",
              "      <td>2020-10-15 04:03:00</td>\n",
              "      <td>8.710324</td>\n",
              "      <td>0.028614</td>\n",
              "      <td>4807.0</td>\n",
              "      <td>0.039002</td>\n",
              "      <td>-3.505444e+30</td>\n",
              "      <td>123251.183611</td>\n",
              "      <td>-1.351226e+18</td>\n",
              "      <td>1051.615274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>17.138611</td>\n",
              "      <td>2019-09-17 02:55:00</td>\n",
              "      <td>7.542746</td>\n",
              "      <td>0.018029</td>\n",
              "      <td>3907.0</td>\n",
              "      <td>0.021508</td>\n",
              "      <td>-3.592642e+31</td>\n",
              "      <td>181650.275277</td>\n",
              "      <td>-1.529832e+18</td>\n",
              "      <td>1073.597276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>98.827500</td>\n",
              "      <td>2019-02-23 06:43:00</td>\n",
              "      <td>9.475826</td>\n",
              "      <td>0.023371</td>\n",
              "      <td>1331.0</td>\n",
              "      <td>0.009466</td>\n",
              "      <td>-7.727559e+30</td>\n",
              "      <td>140607.201668</td>\n",
              "      <td>-9.844177e+17</td>\n",
              "      <td>997.552656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>96.030556</td>\n",
              "      <td>2022-08-13 12:57:00</td>\n",
              "      <td>1.527931</td>\n",
              "      <td>0.012597</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.006117</td>\n",
              "      <td>-6.711144e+26</td>\n",
              "      <td>29589.334999</td>\n",
              "      <td>-9.844177e+17</td>\n",
              "      <td>997.552656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000005</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>81.435335</td>\n",
              "      <td>2015-09-08 14:24</td>\n",
              "      <td>Z156413</td>\n",
              "      <td>30.0</td>\n",
              "      <td>22</td>\n",
              "      <td>18100</td>\n",
              "      <td>...</td>\n",
              "      <td>42.078056</td>\n",
              "      <td>2015-09-08 14:24:00</td>\n",
              "      <td>9.475826</td>\n",
              "      <td>0.023371</td>\n",
              "      <td>5093.0</td>\n",
              "      <td>0.028268</td>\n",
              "      <td>-3.420287e+31</td>\n",
              "      <td>180167.930277</td>\n",
              "      <td>-1.529832e+18</td>\n",
              "      <td>1073.597276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220050</th>\n",
              "      <td>TRAIN_367433</td>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.199074</td>\n",
              "      <td>2022-03-23 8:35</td>\n",
              "      <td>Y242521</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2</td>\n",
              "      <td>63500</td>\n",
              "      <td>...</td>\n",
              "      <td>53.400833</td>\n",
              "      <td>2022-03-23 08:35:00</td>\n",
              "      <td>1.750577</td>\n",
              "      <td>0.015647</td>\n",
              "      <td>544.0</td>\n",
              "      <td>0.021243</td>\n",
              "      <td>-2.820130e+26</td>\n",
              "      <td>25608.316389</td>\n",
              "      <td>-1.412431e+18</td>\n",
              "      <td>1059.407138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220051</th>\n",
              "      <td>TRAIN_367434</td>\n",
              "      <td>CN</td>\n",
              "      <td>QQW1</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>55.408765</td>\n",
              "      <td>2022-06-16 14:27</td>\n",
              "      <td>D236761</td>\n",
              "      <td>30.0</td>\n",
              "      <td>16</td>\n",
              "      <td>26500</td>\n",
              "      <td>...</td>\n",
              "      <td>83.960833</td>\n",
              "      <td>2022-06-16 14:27:00</td>\n",
              "      <td>2.826051</td>\n",
              "      <td>0.023430</td>\n",
              "      <td>1213.0</td>\n",
              "      <td>0.028873</td>\n",
              "      <td>-5.498079e+27</td>\n",
              "      <td>42011.696667</td>\n",
              "      <td>-1.351226e+18</td>\n",
              "      <td>1051.615274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220052</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>65.850000</td>\n",
              "      <td>2017-11-11 22:23:00</td>\n",
              "      <td>2.746103</td>\n",
              "      <td>0.010488</td>\n",
              "      <td>764.0</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>-1.001397e+30</td>\n",
              "      <td>100023.444167</td>\n",
              "      <td>-9.844177e+17</td>\n",
              "      <td>997.552656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220053</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.997500</td>\n",
              "      <td>2022-07-14 07:58:00</td>\n",
              "      <td>13.495088</td>\n",
              "      <td>0.009215</td>\n",
              "      <td>7030.0</td>\n",
              "      <td>0.010007</td>\n",
              "      <td>-1.201726e+35</td>\n",
              "      <td>702480.641106</td>\n",
              "      <td>-1.351226e+18</td>\n",
              "      <td>1051.615274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220054</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>8.464167</td>\n",
              "      <td>2021-06-04 14:54:00</td>\n",
              "      <td>8.710324</td>\n",
              "      <td>0.028614</td>\n",
              "      <td>4128.0</td>\n",
              "      <td>0.033647</td>\n",
              "      <td>-3.410154e+30</td>\n",
              "      <td>122686.355001</td>\n",
              "      <td>-1.003302e+18</td>\n",
              "      <td>1000.716253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220055 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "4       TRAIN_000005     CN   NGG6          Container  81.435335   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "220050  TRAIN_367433     IN   UJM2               Bulk  30.199074   \n",
              "220051  TRAIN_367434     CN   QQW1               Bulk  55.408765   \n",
              "220052  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "220053  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "220054  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...    CI_HOUR  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...   3.048333   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...  17.138611   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...  98.827500   \n",
              "3       2022-08-13 12:57  D215135     30.0     10       27600  ...  96.030556   \n",
              "4       2015-09-08 14:24  Z156413     30.0     22       18100  ...  42.078056   \n",
              "...                  ...      ...      ...    ...         ...  ...        ...   \n",
              "220050   2022-03-23 8:35  Y242521     30.0      2       63500  ...  53.400833   \n",
              "220051  2022-06-16 14:27  D236761     30.0     16       26500  ...  83.960833   \n",
              "220052  2017-11-11 22:23  J661243     40.0     13       93200  ...  65.850000   \n",
              "220053   2022-07-14 7:58  Q635545     30.0      6       25000  ...   0.997500   \n",
              "220054  2021-06-04 14:54  V628821     40.0     10       87200  ...   8.464167   \n",
              "\n",
              "              ATA_datetime  port_daily_arrival_rate  port_daily_service_rate  \\\n",
              "0      2020-10-15 04:03:00                 8.710324                 0.028614   \n",
              "1      2019-09-17 02:55:00                 7.542746                 0.018029   \n",
              "2      2019-02-23 06:43:00                 9.475826                 0.023371   \n",
              "3      2022-08-13 12:57:00                 1.527931                 0.012597   \n",
              "4      2015-09-08 14:24:00                 9.475826                 0.023371   \n",
              "...                    ...                      ...                      ...   \n",
              "220050 2022-03-23 08:35:00                 1.750577                 0.015647   \n",
              "220051 2022-06-16 14:27:00                 2.826051                 0.023430   \n",
              "220052 2017-11-11 22:23:00                 2.746103                 0.010488   \n",
              "220053 2022-07-14 07:58:00                13.495088                 0.009215   \n",
              "220054 2021-06-04 14:54:00                 8.710324                 0.028614   \n",
              "\n",
              "       port_dayofweek_arrival_rate port_dayofweek_service_rate           P_n  \\\n",
              "0                           4807.0                    0.039002 -3.505444e+30   \n",
              "1                           3907.0                    0.021508 -3.592642e+31   \n",
              "2                           1331.0                    0.009466 -7.727559e+30   \n",
              "3                            181.0                    0.006117 -6.711144e+26   \n",
              "4                           5093.0                    0.028268 -3.420287e+31   \n",
              "...                            ...                         ...           ...   \n",
              "220050                       544.0                    0.021243 -2.820130e+26   \n",
              "220051                      1213.0                    0.028873 -5.498079e+27   \n",
              "220052                       764.0                    0.007638 -1.001397e+30   \n",
              "220053                      7030.0                    0.010007 -1.201726e+35   \n",
              "220054                      4128.0                    0.033647 -3.410154e+30   \n",
              "\n",
              "               P_busy  P_n_dayofweek  P_busy_dayofweek  \n",
              "0       123251.183611  -1.351226e+18       1051.615274  \n",
              "1       181650.275277  -1.529832e+18       1073.597276  \n",
              "2       140607.201668  -9.844177e+17        997.552656  \n",
              "3        29589.334999  -9.844177e+17        997.552656  \n",
              "4       180167.930277  -1.529832e+18       1073.597276  \n",
              "...               ...            ...               ...  \n",
              "220050   25608.316389  -1.412431e+18       1059.407138  \n",
              "220051   42011.696667  -1.351226e+18       1051.615274  \n",
              "220052  100023.444167  -9.844177e+17        997.552656  \n",
              "220053  702480.641106  -1.351226e+18       1051.615274  \n",
              "220054  122686.355001  -1.003302e+18       1000.716253  \n",
              "\n",
              "[220055 rows x 36 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = train[train['DIST'] != 0]\n",
        "train = train.reset_index(drop = True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n# datetime 컬럼 처리\\ntrain['ATA'] = pd.to_datetime(train['ATA'])|\\ntest['ATA'] = pd.to_datetime(test['ATA'])\\n\\n# datetime을 여러 파생 변수로 변환\\nfor df in [train, test]:\\n    df['year'] = df['ATA'].dt.year\\n    df['month'] = df['ATA'].dt.month\\n    df['day'] = df['ATA'].dt.day\\n    df['hour'] = df['ATA'].dt.hour\\n    df['minute'] = df['ATA'].dt.minute\\n    df['weekday'] = df['ATA'].dt.weekday\\n\\n# datetime 컬럼 제거\\ntrain.drop(columns='ATA', inplace=True)\\ntest.drop(columns='ATA', inplace=True)\\n\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "# datetime 컬럼 처리\n",
        "train['ATA'] = pd.to_datetime(train['ATA'])|\n",
        "test['ATA'] = pd.to_datetime(test['ATA'])\n",
        "\n",
        "# datetime을 여러 파생 변수로 변환\n",
        "for df in [train, test]:\n",
        "    df['year'] = df['ATA'].dt.year\n",
        "    df['month'] = df['ATA'].dt.month\n",
        "    df['day'] = df['ATA'].dt.day\n",
        "    df['hour'] = df['ATA'].dt.hour\n",
        "    df['minute'] = df['ATA'].dt.minute\n",
        "    df['weekday'] = df['ATA'].dt.weekday\n",
        "\n",
        "# datetime 컬럼 제거\n",
        "train.drop(columns='ATA', inplace=True)\n",
        "test.drop(columns='ATA', inplace=True)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['SAMPLE_ID'],axis=1,inplace=True)\n",
        "test.drop(['SAMPLE_ID'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>rounded_hour</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022</td>\n",
              "      <td>8</td>\n",
              "      <td>27</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>6</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month  day  weekday  rounded_hour\n",
              "0  2022      8   27        5             8\n",
              "1  2022      3   27        6            21\n",
              "2  2023      1   18        2             2\n",
              "3  2016      8    2        1             1\n",
              "4  2023      1   24        1             0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['ATA'] = pd.to_datetime(train['ATA'])\n",
        "test['ATA'] = pd.to_datetime(test['ATA'])\n",
        "train['year'] = train['ATA'].dt.year\n",
        "train['month'] = train['ATA'].dt.month\n",
        "train['day'] = train['ATA'].dt.day\n",
        "train['weekday'] = train['ATA'].dt.dayofweek\n",
        "train['rounded_hour'] = (train['ATA'].dt.hour + (train['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "test['year'] = test['ATA'].dt.year\n",
        "test['month'] = test['ATA'].dt.month\n",
        "test['day'] = test['ATA'].dt.day\n",
        "test['weekday'] = test['ATA'].dt.dayofweek\n",
        "test['rounded_hour'] = (test['ATA'].dt.hour + (test['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "\n",
        "# sin, cos 변환 함수 정의\n",
        "def encode_cyclic_feature(data, column, max_val):\n",
        "    data[column + '_sin'] = np.sin(2 * np.pi * data[column] / max_val)\n",
        "    data[column + '_cos'] = np.cos(2 * np.pi * data[column] / max_val)\n",
        "    return data\n",
        "\n",
        "# 각 피처에 대해 sin, cos 변환 수행\n",
        "train = encode_cyclic_feature(train, 'month', 12)\n",
        "train = encode_cyclic_feature(train, 'day', 31)\n",
        "train = encode_cyclic_feature(train, 'weekday', 7)\n",
        "train = encode_cyclic_feature(train, 'rounded_hour', 24)\n",
        "test = encode_cyclic_feature(test, 'month', 12)\n",
        "test = encode_cyclic_feature(test, 'day', 31)\n",
        "test = encode_cyclic_feature(test, 'weekday', 7)\n",
        "test = encode_cyclic_feature(test, 'rounded_hour', 24)\n",
        "\n",
        "train.drop(['ATA'],axis=1,inplace=True)\n",
        "test.drop(['ATA'],axis=1,inplace=True)\n",
        "\n",
        "test[['year', 'month', 'day', 'weekday', 'rounded_hour']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>DEPTH</th>\n",
              "      <th>DRAUGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_enc_day</th>\n",
              "      <th>std_enc_day</th>\n",
              "      <th>mean_enc_weekday</th>\n",
              "      <th>std_enc_weekday</th>\n",
              "      <th>mean_enc_rounded_hour</th>\n",
              "      <th>std_enc_rounded_hour</th>\n",
              "      <th>DUBAI_BRENT</th>\n",
              "      <th>DUBAI_WTI</th>\n",
              "      <th>BRENT_WTI</th>\n",
              "      <th>DUBAI_BRENT_WTI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JP</td>\n",
              "      <td>HYG5</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>9.508139</td>\n",
              "      <td>R721438</td>\n",
              "      <td>20.0</td>\n",
              "      <td>9</td>\n",
              "      <td>5510</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>99.269587</td>\n",
              "      <td>204.832111</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>97.697088</td>\n",
              "      <td>201.667547</td>\n",
              "      <td>144.796982</td>\n",
              "      <td>152.612162</td>\n",
              "      <td>149.849234</td>\n",
              "      <td>223.629189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>24.186684</td>\n",
              "      <td>G185231</td>\n",
              "      <td>40.0</td>\n",
              "      <td>14</td>\n",
              "      <td>109000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>99.269587</td>\n",
              "      <td>204.832111</td>\n",
              "      <td>108.406656</td>\n",
              "      <td>207.392917</td>\n",
              "      <td>103.780640</td>\n",
              "      <td>207.373622</td>\n",
              "      <td>152.525524</td>\n",
              "      <td>159.688869</td>\n",
              "      <td>164.199700</td>\n",
              "      <td>238.207046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TW</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>P862781</td>\n",
              "      <td>20.0</td>\n",
              "      <td>14</td>\n",
              "      <td>13700</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>100.907119</td>\n",
              "      <td>203.251035</td>\n",
              "      <td>90.842366</td>\n",
              "      <td>202.567329</td>\n",
              "      <td>96.976710</td>\n",
              "      <td>199.178563</td>\n",
              "      <td>121.801723</td>\n",
              "      <td>133.968688</td>\n",
              "      <td>133.013721</td>\n",
              "      <td>194.392066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JP</td>\n",
              "      <td>HYG5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>B415437</td>\n",
              "      <td>10.0</td>\n",
              "      <td>18</td>\n",
              "      <td>2840</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>100.314370</td>\n",
              "      <td>196.641885</td>\n",
              "      <td>87.803141</td>\n",
              "      <td>195.561214</td>\n",
              "      <td>103.974608</td>\n",
              "      <td>210.640285</td>\n",
              "      <td>42.834313</td>\n",
              "      <td>69.968265</td>\n",
              "      <td>68.496124</td>\n",
              "      <td>90.649351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>19.834186</td>\n",
              "      <td>J412562</td>\n",
              "      <td>30.0</td>\n",
              "      <td>23</td>\n",
              "      <td>30700</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>103.141252</td>\n",
              "      <td>214.851043</td>\n",
              "      <td>87.803141</td>\n",
              "      <td>195.561214</td>\n",
              "      <td>101.085320</td>\n",
              "      <td>207.179760</td>\n",
              "      <td>123.017593</td>\n",
              "      <td>134.529014</td>\n",
              "      <td>134.475066</td>\n",
              "      <td>196.010837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>RU</td>\n",
              "      <td>ZME5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>1.625700</td>\n",
              "      <td>R355837</td>\n",
              "      <td>10.0</td>\n",
              "      <td>35</td>\n",
              "      <td>3120</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>99.227510</td>\n",
              "      <td>205.385334</td>\n",
              "      <td>85.508007</td>\n",
              "      <td>188.359476</td>\n",
              "      <td>101.961888</td>\n",
              "      <td>206.196032</td>\n",
              "      <td>105.576973</td>\n",
              "      <td>118.005274</td>\n",
              "      <td>117.057845</td>\n",
              "      <td>170.320046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>RU</td>\n",
              "      <td>ZME5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.642841</td>\n",
              "      <td>P438645</td>\n",
              "      <td>30.0</td>\n",
              "      <td>17</td>\n",
              "      <td>46400</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>100.799906</td>\n",
              "      <td>207.421689</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>109.006623</td>\n",
              "      <td>228.232254</td>\n",
              "      <td>88.379317</td>\n",
              "      <td>104.486010</td>\n",
              "      <td>103.028000</td>\n",
              "      <td>147.946664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>CN</td>\n",
              "      <td>RGT8</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>R763233</td>\n",
              "      <td>20.0</td>\n",
              "      <td>12</td>\n",
              "      <td>5280</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>105.413494</td>\n",
              "      <td>214.522677</td>\n",
              "      <td>98.936224</td>\n",
              "      <td>216.337699</td>\n",
              "      <td>109.034067</td>\n",
              "      <td>222.568626</td>\n",
              "      <td>63.478400</td>\n",
              "      <td>87.059985</td>\n",
              "      <td>85.486785</td>\n",
              "      <td>118.012585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>VN</td>\n",
              "      <td>ONW1</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>F587712</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>7020</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>108.108772</td>\n",
              "      <td>213.026799</td>\n",
              "      <td>90.842366</td>\n",
              "      <td>202.567329</td>\n",
              "      <td>105.699271</td>\n",
              "      <td>212.570578</td>\n",
              "      <td>143.019526</td>\n",
              "      <td>151.472391</td>\n",
              "      <td>152.162023</td>\n",
              "      <td>223.326970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>VN</td>\n",
              "      <td>ONW1</td>\n",
              "      <td>Container</td>\n",
              "      <td>3.247493</td>\n",
              "      <td>F587712</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>7020</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>101.490305</td>\n",
              "      <td>197.997678</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>101.036616</td>\n",
              "      <td>204.252692</td>\n",
              "      <td>121.652368</td>\n",
              "      <td>133.388063</td>\n",
              "      <td>134.312777</td>\n",
              "      <td>194.676604</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST       ID  BREADTH  BUILT  \\\n",
              "0          JP   HYG5             Tanker   9.508139  R721438     20.0      9   \n",
              "1          SG   GIW5          Container  24.186684  G185231     40.0     14   \n",
              "2          TW   EKP8              Cargo   0.000000  P862781     20.0     14   \n",
              "3          JP   HYG5               Bulk   0.000000  B415437     10.0     18   \n",
              "4          SG   GIW5          Container  19.834186  J412562     30.0     23   \n",
              "...       ...    ...                ...        ...      ...      ...    ...   \n",
              "244984     RU   ZME5               Bulk   1.625700  R355837     10.0     35   \n",
              "244985     RU   ZME5               Bulk  30.642841  P438645     30.0     17   \n",
              "244986     CN   RGT8              Cargo   0.000000  R763233     20.0     12   \n",
              "244987     VN   ONW1          Container   0.000000  F587712     20.0     27   \n",
              "244988     VN   ONW1          Container   3.247493  F587712     20.0     27   \n",
              "\n",
              "        DEADWEIGHT  DEPTH  DRAUGHT  ...  mean_enc_day  std_enc_day  \\\n",
              "0             5510   10.0     10.0  ...     99.269587   204.832111   \n",
              "1           109000   20.0     10.0  ...     99.269587   204.832111   \n",
              "2            13700   10.0     10.0  ...    100.907119   203.251035   \n",
              "3             2840   10.0     10.0  ...    100.314370   196.641885   \n",
              "4            30700   20.0     10.0  ...    103.141252   214.851043   \n",
              "...            ...    ...      ...  ...           ...          ...   \n",
              "244984        3120   10.0      0.0  ...     99.227510   205.385334   \n",
              "244985       46400   20.0     10.0  ...    100.799906   207.421689   \n",
              "244986        5280   10.0     10.0  ...    105.413494   214.522677   \n",
              "244987        7020   10.0     10.0  ...    108.108772   213.026799   \n",
              "244988        7020   10.0     10.0  ...    101.490305   197.997678   \n",
              "\n",
              "       mean_enc_weekday std_enc_weekday  mean_enc_rounded_hour  \\\n",
              "0            160.614133      236.913654              97.697088   \n",
              "1            108.406656      207.392917             103.780640   \n",
              "2             90.842366      202.567329              96.976710   \n",
              "3             87.803141      195.561214             103.974608   \n",
              "4             87.803141      195.561214             101.085320   \n",
              "...                 ...             ...                    ...   \n",
              "244984        85.508007      188.359476             101.961888   \n",
              "244985       160.614133      236.913654             109.006623   \n",
              "244986        98.936224      216.337699             109.034067   \n",
              "244987        90.842366      202.567329             105.699271   \n",
              "244988       160.614133      236.913654             101.036616   \n",
              "\n",
              "        std_enc_rounded_hour  DUBAI_BRENT   DUBAI_WTI   BRENT_WTI  \\\n",
              "0                 201.667547   144.796982  152.612162  149.849234   \n",
              "1                 207.373622   152.525524  159.688869  164.199700   \n",
              "2                 199.178563   121.801723  133.968688  133.013721   \n",
              "3                 210.640285    42.834313   69.968265   68.496124   \n",
              "4                 207.179760   123.017593  134.529014  134.475066   \n",
              "...                      ...          ...         ...         ...   \n",
              "244984            206.196032   105.576973  118.005274  117.057845   \n",
              "244985            228.232254    88.379317  104.486010  103.028000   \n",
              "244986            222.568626    63.478400   87.059985   85.486785   \n",
              "244987            212.570578   143.019526  151.472391  152.162023   \n",
              "244988            204.252692   121.652368  133.388063  134.312777   \n",
              "\n",
              "        DUBAI_BRENT_WTI  \n",
              "0            223.629189  \n",
              "1            238.207046  \n",
              "2            194.392066  \n",
              "3             90.649351  \n",
              "4            196.010837  \n",
              "...                 ...  \n",
              "244984       170.320046  \n",
              "244985       147.946664  \n",
              "244986       118.012585  \n",
              "244987       223.326970  \n",
              "244988       194.676604  \n",
              "\n",
              "[244989 rows x 55 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train.drop(['ID'],axis=1,inplace=True)\n",
        "#test.drop(['ID'],axis=1,inplace=True)\n",
        "#train['ID'] = train['ID'].str[0]\n",
        "#test['ID'] = test['ID'].str[0]\n",
        "\n",
        "\n",
        "features_to_encode = ['ARI_CO', 'ARI_PO', 'year', 'month', 'day', 'weekday', 'rounded_hour']\n",
        "\n",
        "for feature in features_to_encode:\n",
        "    # mean encoding\n",
        "    train[f'mean_enc_{feature}'] = train.groupby(feature)['CI_HOUR'].transform('mean')\n",
        "    # std encoding\n",
        "    train[f'std_enc_{feature}'] = train.groupby(feature)['CI_HOUR'].transform('std')\n",
        "\n",
        "# 결과의 상위 몇 개 행을 출력하여 확인\n",
        "mean_enc_maps = {feature: train.groupby(feature)['CI_HOUR'].mean().to_dict() for feature in features_to_encode}\n",
        "std_enc_maps = {feature: train.groupby(feature)['CI_HOUR'].std().to_dict() for feature in features_to_encode}\n",
        "\n",
        "for feature in features_to_encode:\n",
        "    test[f'mean_enc_{feature}'] = test[feature].map(mean_enc_maps[feature])\n",
        "    test[f'std_enc_{feature}'] = test[feature].map(std_enc_maps[feature])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ARI_PO, weekday, rounded_hour 조합에 따른 mean 및 std encoding\n",
        "'''\n",
        "grouped = train.groupby(['ARI_CO', 'weekday', 'rounded_hour'])['CI_HOUR']\n",
        "train['mean_enc_ARI_CO_weekday_hour'] = grouped.transform('mean')\n",
        "train['std_enc_ARI_CO_weekday_hour'] = grouped.transform('std')\n",
        "\n",
        "# 해당 encoding을 test 데이터에도 적용\n",
        "mean_enc_map = train.groupby(['ARI_CO', 'weekday', 'rounded_hour'])['CI_HOUR'].mean().to_dict()\n",
        "std_enc_map = train.groupby(['ARI_CO', 'weekday', 'rounded_hour'])['CI_HOUR'].std().to_dict()\n",
        "\n",
        "test['mean_enc_ARI_CO_weekday_hour'] = test.set_index(['ARI_CO', 'weekday', 'rounded_hour']).index.map(mean_enc_map).values\n",
        "test['std_enc_ARI_CO_weekday_hour'] = test.set_index(['ARI_CO', 'weekday', 'rounded_hour']).index.map(std_enc_map).values\n",
        "'''\n",
        "'''\n",
        "# 해당 encoding을 test 데이터에도 적용\n",
        "grouped = train.groupby(['ARI_CO', 'weekday'])['CI_HOUR']\n",
        "train['mean_enc_ARI_CO_weekday'] = grouped.transform('mean')\n",
        "train['std_enc_ARI_CO_weekday'] = grouped.transform('std')\n",
        "mean_enc_map = train.groupby(['ARI_CO', 'weekday'])['CI_HOUR'].mean().to_dict()\n",
        "std_enc_map = train.groupby(['ARI_CO', 'weekday'])['CI_HOUR'].std().to_dict()\n",
        "test['mean_enc_ARI_CO_weekday'] = test.set_index(['ARI_CO', 'weekday']).index.map(mean_enc_map).values\n",
        "test['std_enc_ARI_CO_weekday'] = test.set_index(['ARI_CO', 'weekday']).index.map(std_enc_map).values\n",
        "train['std_enc_ARI_CO_weekday'].fillna(0, inplace=True)\n",
        "test['std_enc_ARI_CO_weekday'].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# 해당 encoding을 test 데이터에도 적용\n",
        "ouped = train.groupby(['ARI_CO', 'month'])['CI_HOUR']\n",
        "train['mean_enc_ARI_CO_month'] = grouped.transform('mean')\n",
        "train['std_enc_ARI_CO_month'] = grouped.transform('std')\n",
        "mean_enc_map = train.groupby(['ARI_CO', 'month'])['CI_HOUR'].mean().to_dict()\n",
        "std_enc_map = train.groupby(['ARI_CO', 'month'])['CI_HOUR'].std().to_dict()\n",
        "test['mean_enc_ARI_CO_month'] = test.set_index(['ARI_CO', 'month']).index.map(mean_enc_map).values\n",
        "test['std_enc_ARI_CO_month'] = test.set_index(['ARI_CO', 'month']).index.map(std_enc_map).values\n",
        "train['std_enc_ARI_CO_month'].fillna(0, inplace=True)\n",
        "test['std_enc_ARI_CO_month'].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# 해당 encoding을 test 데이터에도 적용\n",
        "ouped = train.groupby(['ARI_CO', 'month', 'weekday'])['CI_HOUR']\n",
        "train['mean_enc_ARI_CO_month_weekday'] = grouped.transform('mean')\n",
        "train['std_enc_ARI_CO_month_weekday'] = grouped.transform('std')\n",
        "mean_enc_map = train.groupby(['ARI_CO', 'month', 'weekday'])['CI_HOUR'].mean().to_dict()\n",
        "std_enc_map = train.groupby(['ARI_CO', 'month', 'weekday'])['CI_HOUR'].std().to_dict()\n",
        "test['mean_enc_ARI_CO_month_weekday'] = test.set_index(['ARI_CO', 'month', 'weekday']).index.map(mean_enc_map).values\n",
        "test['std_enc_ARI_CO_month_weekday'] = test.set_index(['ARI_CO', 'month', 'weekday']).index.map(std_enc_map).values\n",
        "train['std_enc_ARI_CO_month_weekday'].fillna(0, inplace=True)\n",
        "test['std_enc_ARI_CO_month_weekday'].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# 해당 encoding을 test 데이터에도 적용\n",
        "ouped = train.groupby(['ARI_PO', 'month', 'weekday'])['CI_HOUR']\n",
        "train['mean_enc_ARI_PO_month_weekday'] = grouped.transform('mean')\n",
        "train['std_enc_ARI_PO_month_weekday'] = grouped.transform('std')\n",
        "mean_enc_map = train.groupby(['ARI_PO', 'month', 'weekday'])['CI_HOUR'].mean().to_dict()\n",
        "std_enc_map = train.groupby(['ARI_PO', 'month', 'weekday'])['CI_HOUR'].std().to_dict()\n",
        "test['mean_enc_ARI_PO_month_weekday'] = test.set_index(['ARI_PO', 'month', 'weekday']).index.map(mean_enc_map).values\n",
        "test['std_enc_ARI_PO_month_weekday'] = test.set_index(['ARI_PO', 'month', 'weekday']).index.map(std_enc_map).values\n",
        "train['std_enc_ARI_PO_month_weekday'].fillna(0, inplace=True)\n",
        "test['std_enc_ARI_PO_month_weekday'].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# 해당 encoding을 test 데이터에도 적용\n",
        "ouped = train.groupby(['ARI_PO', 'weekday'])['CI_HOUR']\n",
        "train['mean_enc_ARI_PO_month_weekday'] = grouped.transform('mean')\n",
        "train['std_enc_ARI_PO_month_weekday'] = grouped.transform('std')\n",
        "mean_enc_map = train.groupby(['ARI_PO', 'month', 'weekday'])['CI_HOUR'].mean().to_dict()\n",
        "std_enc_map = train.groupby(['ARI_PO', 'month', 'weekday'])['CI_HOUR'].std().to_dict()\n",
        "test['mean_enc_ARI_PO_month_weekday'] = test.set_index(['ARI_PO', 'month', 'weekday']).index.map(mean_enc_map).values\n",
        "test['std_enc_ARI_PO_month_weekday'] = test.set_index(['ARI_PO', 'month', 'weekday']).index.map(std_enc_map).values\n",
        "train['std_enc_ARI_PO_month_weekday'].fillna(0, inplace=True)\n",
        "test['std_enc_ARI_PO_month_weekday'].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# 해당 encoding을 test 데이터에도 적용\n",
        "ouped = train.groupby(['ARI_PO', 'month'])['CI_HOUR']\n",
        "train['mean_enc_ARI_PO_month_weekday'] = grouped.transform('mean')\n",
        "train['std_enc_ARI_PO_month_weekday'] = grouped.transform('std')\n",
        "mean_enc_map = train.groupby(['ARI_PO', 'month', 'weekday'])['CI_HOUR'].mean().to_dict()\n",
        "std_enc_map = train.groupby(['ARI_PO', 'month', 'weekday'])['CI_HOUR'].std().to_dict()\n",
        "test['mean_enc_ARI_PO_month_weekday'] = test.set_index(['ARI_PO', 'month', 'weekday']).index.map(mean_enc_map).values\n",
        "test['std_enc_ARI_PO_month_weekday'] = test.set_index(['ARI_PO', 'month', 'weekday']).index.map(std_enc_map).values\n",
        "train['std_enc_ARI_PO_month_weekday'].fillna(0, inplace=True)\n",
        "test['std_enc_ARI_PO_month_weekday'].fillna(0, inplace=True)\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "train['std_enc_ARI_CO_weekday_hour'].fillna(0, inplace=True)\n",
        "test['std_enc_ARI_CO_weekday_hour'].fillna(0, inplace=True)\n",
        "\n",
        "# train 데이터에서 ARI_CO와 weekday 조합에 따른 mean 값을 계산\n",
        "mean_enc_map_ari_co_weekday = train.groupby(['ARI_CO', 'weekday'])['CI_HOUR'].mean().to_dict()\n",
        "\n",
        "# test 데이터에서 결측값을 해당 mean 값으로 채워주기\n",
        "test['mean_enc_ARI_CO_weekday_hour'].fillna(test.set_index(['ARI_CO', 'weekday']).index.map(mean_enc_map_ari_co_weekday).values, inplace=True)\n",
        "# train 데이터에서 ARI_CO 조합에 따른 mean 값을 계산\n",
        "mean_enc_map_ari_co = train.groupby(['ARI_CO'])['CI_HOUR'].mean().to_dict()\n",
        "\n",
        "# test 데이터에서 결측값을 해당 mean 값으로 채워주기\n",
        "fill_values_ari_co = pd.Series(test['ARI_CO'].map(mean_enc_map_ari_co), index=test.index)\n",
        "test['mean_enc_ARI_CO_weekday_hour'].fillna(fill_values_ari_co, inplace=True)\n",
        "\n",
        "'''\n",
        "# 'DUBAI', 'BRENT', 'WTI',\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "features_to_scale = ['DUBAI', 'WTI', 'BRENT']\n",
        "scaler = MinMaxScaler(feature_range=(0, 100))\n",
        "train[features_to_scale] = scaler.fit_transform(train[features_to_scale])\n",
        "test[features_to_scale] = scaler.transform(test[features_to_scale])\n",
        "\n",
        "train['DUBAI_BRENT'] = train['DUBAI'] + train['BRENT']\n",
        "train['DUBAI_WTI'] = train['DUBAI'] + train['WTI']\n",
        "train['BRENT_WTI'] = train['BRENT'] + train['WTI']\n",
        "train['DUBAI_BRENT_WTI'] = train['DUBAI'] + train['BRENT'] + train['WTI']\n",
        "\n",
        "test['DUBAI_BRENT'] = test['DUBAI'] + test['BRENT']\n",
        "test['DUBAI_WTI'] = test['DUBAI'] + test['WTI']\n",
        "test['BRENT_WTI'] = test['BRENT'] + test['WTI']\n",
        "test['DUBAI_BRENT_WTI'] = test['DUBAI'] + test['BRENT'] + test['WTI']\n",
        "\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "train['DUBAI_BRENT'] = train['DUBAI'] * train['BRENT']\n",
        "train['DUBAI_WTI'] = train['DUBAI'] * train['WTI']\n",
        "train['BRENT_WTI'] = train['BRENT'] * train['WTI']\n",
        "train['DUBAI_BRENT_WTI'] = train['DUBAI'] * train['BRENT'] * train['WTI']\n",
        "\n",
        "test['DUBAI_BRENT'] = test['DUBAI'] * test['BRENT']\n",
        "test['DUBAI_WTI'] = test['DUBAI'] * test['WTI']\n",
        "test['BRENT_WTI'] = test['BRENT'] * test['WTI']\n",
        "test['DUBAI_BRENT_WTI'] = test['DUBAI'] * test['BRENT'] * test['WTI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_enc_day</th>\n",
              "      <th>std_enc_day</th>\n",
              "      <th>mean_enc_weekday</th>\n",
              "      <th>std_enc_weekday</th>\n",
              "      <th>mean_enc_rounded_hour</th>\n",
              "      <th>std_enc_rounded_hour</th>\n",
              "      <th>DUBAI_BRENT</th>\n",
              "      <th>DUBAI_WTI</th>\n",
              "      <th>BRENT_WTI</th>\n",
              "      <th>DUBAI_BRENT_WTI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [V_WIND, AIR_TEMPERATURE, BN, ATA_LT, DUBAI, BRENT, WTI, BDI_ADJ, PORT_SIZE, CI_HOUR, ATA_datetime, port_daily_arrival_rate, port_daily_service_rate, port_dayofweek_arrival_rate, port_dayofweek_service_rate, P_n, P_busy, P_n_dayofweek, P_busy_dayofweek, year, month, day, weekday, rounded_hour, month_sin, month_cos, day_sin, day_cos, weekday_sin, weekday_cos, rounded_hour_sin, rounded_hour_cos, mean_enc_ARI_CO, std_enc_ARI_CO, mean_enc_ARI_PO, std_enc_ARI_PO, mean_enc_year, std_enc_year, mean_enc_month, std_enc_month, mean_enc_day, std_enc_day, mean_enc_weekday, std_enc_weekday, mean_enc_rounded_hour, std_enc_rounded_hour, DUBAI_BRENT, DUBAI_WTI, BRENT_WTI, DUBAI_BRENT_WTI]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 50 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train[train['WTI'] < 0].iloc[:,15:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 244989 entries, 0 to 244988\n",
            "Data columns (total 55 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   ARI_CO                 244989 non-null  object \n",
            " 1   ARI_PO                 244989 non-null  object \n",
            " 2   SHIP_TYPE_CATEGORY     244989 non-null  object \n",
            " 3   DIST                   244989 non-null  float64\n",
            " 4   ID                     244989 non-null  object \n",
            " 5   BREADTH                244989 non-null  float64\n",
            " 6   BUILT                  244989 non-null  int64  \n",
            " 7   DEADWEIGHT             244989 non-null  int64  \n",
            " 8   DEPTH                  244989 non-null  float64\n",
            " 9   DRAUGHT                244989 non-null  float64\n",
            " 10  GT                     244989 non-null  int64  \n",
            " 11  LENGTH                 244989 non-null  float64\n",
            " 12  SHIPMANAGER            244989 non-null  object \n",
            " 13  FLAG                   244989 non-null  object \n",
            " 14  U_WIND                 143062 non-null  float64\n",
            " 15  V_WIND                 143062 non-null  float64\n",
            " 16  AIR_TEMPERATURE        142478 non-null  float64\n",
            " 17  BN                     143062 non-null  float64\n",
            " 18  ATA_LT                 244989 non-null  int64  \n",
            " 19  DUBAI                  244989 non-null  float64\n",
            " 20  BRENT                  244989 non-null  float64\n",
            " 21  WTI                    244989 non-null  float64\n",
            " 22  BDI_ADJ                244989 non-null  float64\n",
            " 23  PORT_SIZE              244989 non-null  float64\n",
            " 24  year                   244989 non-null  int32  \n",
            " 25  month                  244989 non-null  int32  \n",
            " 26  day                    244989 non-null  int32  \n",
            " 27  weekday                244989 non-null  int32  \n",
            " 28  rounded_hour           244989 non-null  int64  \n",
            " 29  month_sin              244989 non-null  float64\n",
            " 30  month_cos              244989 non-null  float64\n",
            " 31  day_sin                244989 non-null  float64\n",
            " 32  day_cos                244989 non-null  float64\n",
            " 33  weekday_sin            244989 non-null  float64\n",
            " 34  weekday_cos            244989 non-null  float64\n",
            " 35  rounded_hour_sin       244989 non-null  float64\n",
            " 36  rounded_hour_cos       244989 non-null  float64\n",
            " 37  mean_enc_ARI_CO        244989 non-null  float64\n",
            " 38  std_enc_ARI_CO         244989 non-null  float64\n",
            " 39  mean_enc_ARI_PO        244989 non-null  float64\n",
            " 40  std_enc_ARI_PO         244989 non-null  float64\n",
            " 41  mean_enc_year          244989 non-null  float64\n",
            " 42  std_enc_year           244989 non-null  float64\n",
            " 43  mean_enc_month         244989 non-null  float64\n",
            " 44  std_enc_month          244989 non-null  float64\n",
            " 45  mean_enc_day           244989 non-null  float64\n",
            " 46  std_enc_day            244989 non-null  float64\n",
            " 47  mean_enc_weekday       244989 non-null  float64\n",
            " 48  std_enc_weekday        244989 non-null  float64\n",
            " 49  mean_enc_rounded_hour  244989 non-null  float64\n",
            " 50  std_enc_rounded_hour   244989 non-null  float64\n",
            " 51  DUBAI_BRENT            244989 non-null  float64\n",
            " 52  DUBAI_WTI              244989 non-null  float64\n",
            " 53  BRENT_WTI              244989 non-null  float64\n",
            " 54  DUBAI_BRENT_WTI        244989 non-null  float64\n",
            "dtypes: float64(40), int32(4), int64(5), object(6)\n",
            "memory usage: 99.1+ MB\n"
          ]
        }
      ],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>DEPTH</th>\n",
              "      <th>DRAUGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>std_enc_day</th>\n",
              "      <th>mean_enc_weekday</th>\n",
              "      <th>std_enc_weekday</th>\n",
              "      <th>mean_enc_rounded_hour</th>\n",
              "      <th>std_enc_rounded_hour</th>\n",
              "      <th>DUBAI_BRENT</th>\n",
              "      <th>DUBAI_WTI</th>\n",
              "      <th>BRENT_WTI</th>\n",
              "      <th>DUBAI_BRENT_WTI</th>\n",
              "      <th>country_cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>203.933754</td>\n",
              "      <td>98.936224</td>\n",
              "      <td>216.337699</td>\n",
              "      <td>99.381610</td>\n",
              "      <td>201.303551</td>\n",
              "      <td>546.497995</td>\n",
              "      <td>1213.797997</td>\n",
              "      <td>1068.430966</td>\n",
              "      <td>26622.002999</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>212.879594</td>\n",
              "      <td>87.803141</td>\n",
              "      <td>195.561214</td>\n",
              "      <td>101.961888</td>\n",
              "      <td>206.196032</td>\n",
              "      <td>1965.969685</td>\n",
              "      <td>2839.214807</td>\n",
              "      <td>2501.630073</td>\n",
              "      <td>118167.780538</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>211.720962</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>105.699271</td>\n",
              "      <td>212.570578</td>\n",
              "      <td>1961.489783</td>\n",
              "      <td>2654.618480</td>\n",
              "      <td>2538.986826</td>\n",
              "      <td>114980.529854</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>212.673518</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>101.197570</td>\n",
              "      <td>203.194964</td>\n",
              "      <td>4602.285060</td>\n",
              "      <td>5243.917696</td>\n",
              "      <td>5331.314374</td>\n",
              "      <td>358700.380279</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>81.435335</td>\n",
              "      <td>Z156413</td>\n",
              "      <td>30.0</td>\n",
              "      <td>22</td>\n",
              "      <td>18100</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>207.421689</td>\n",
              "      <td>87.803141</td>\n",
              "      <td>195.561214</td>\n",
              "      <td>101.036616</td>\n",
              "      <td>204.252692</td>\n",
              "      <td>766.896608</td>\n",
              "      <td>1459.800265</td>\n",
              "      <td>1408.983864</td>\n",
              "      <td>39716.240979</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220050</th>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.199074</td>\n",
              "      <td>Y242521</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2</td>\n",
              "      <td>63500</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>211.720962</td>\n",
              "      <td>90.842366</td>\n",
              "      <td>202.567329</td>\n",
              "      <td>101.257372</td>\n",
              "      <td>205.632412</td>\n",
              "      <td>8026.135630</td>\n",
              "      <td>8083.967025</td>\n",
              "      <td>8758.887331</td>\n",
              "      <td>753858.756571</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220051</th>\n",
              "      <td>CN</td>\n",
              "      <td>QQW1</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>55.408765</td>\n",
              "      <td>D236761</td>\n",
              "      <td>30.0</td>\n",
              "      <td>16</td>\n",
              "      <td>26500</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>209.272356</td>\n",
              "      <td>98.936224</td>\n",
              "      <td>216.337699</td>\n",
              "      <td>101.036616</td>\n",
              "      <td>204.252692</td>\n",
              "      <td>7242.564844</td>\n",
              "      <td>7573.156942</td>\n",
              "      <td>7960.526239</td>\n",
              "      <td>660777.982621</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220052</th>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>217.267351</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>103.943272</td>\n",
              "      <td>210.636630</td>\n",
              "      <td>1647.472950</td>\n",
              "      <td>2414.901262</td>\n",
              "      <td>2283.132825</td>\n",
              "      <td>95306.917741</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220053</th>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>227.188738</td>\n",
              "      <td>98.936224</td>\n",
              "      <td>216.337699</td>\n",
              "      <td>97.697088</td>\n",
              "      <td>201.667547</td>\n",
              "      <td>5407.236199</td>\n",
              "      <td>6090.300616</td>\n",
              "      <td>6071.319832</td>\n",
              "      <td>447145.218708</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220054</th>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>208.796355</td>\n",
              "      <td>129.240971</td>\n",
              "      <td>239.623764</td>\n",
              "      <td>108.974336</td>\n",
              "      <td>226.314867</td>\n",
              "      <td>2393.813882</td>\n",
              "      <td>3289.629006</td>\n",
              "      <td>3215.938930</td>\n",
              "      <td>159137.506244</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220055 rows × 66 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST       ID  BREADTH  BUILT  \\\n",
              "0          CN   EKP8               Bulk  30.736578  Z517571     30.0     28   \n",
              "1          CN   EUC8          Container  63.220425  U467618     30.0     15   \n",
              "2          CN   NGG6          Container  90.427421  V378315     50.0      7   \n",
              "3          RU   NNC2          Container   8.813725  D215135     30.0     10   \n",
              "4          CN   NGG6          Container  81.435335  Z156413     30.0     22   \n",
              "...       ...    ...                ...        ...      ...      ...    ...   \n",
              "220050     IN   UJM2               Bulk  30.199074  Y242521     30.0      2   \n",
              "220051     CN   QQW1               Bulk  55.408765  D236761     30.0     16   \n",
              "220052     CN   YRT6               Bulk  59.018184  J661243     40.0     13   \n",
              "220053     SG   GIW5          Container   1.768630  Q635545     30.0      6   \n",
              "220054     CN   EKP8               Bulk  32.152412  V628821     40.0     10   \n",
              "\n",
              "        DEADWEIGHT  DEPTH  DRAUGHT  ...  std_enc_day  mean_enc_weekday  \\\n",
              "0            73100   20.0     10.0  ...   203.933754         98.936224   \n",
              "1            37900   20.0     10.0  ...   212.879594         87.803141   \n",
              "2           115000   20.0     10.0  ...   211.720962        160.614133   \n",
              "3            27600   10.0     10.0  ...   212.673518        160.614133   \n",
              "4            18100   10.0     10.0  ...   207.421689         87.803141   \n",
              "...            ...    ...      ...  ...          ...               ...   \n",
              "220050       63500   20.0     10.0  ...   211.720962         90.842366   \n",
              "220051       26500   10.0     10.0  ...   209.272356         98.936224   \n",
              "220052       93200   20.0     10.0  ...   217.267351        160.614133   \n",
              "220053       25000   20.0     10.0  ...   227.188738         98.936224   \n",
              "220054       87200   20.0     10.0  ...   208.796355        129.240971   \n",
              "\n",
              "       std_enc_weekday mean_enc_rounded_hour  std_enc_rounded_hour  \\\n",
              "0           216.337699             99.381610            201.303551   \n",
              "1           195.561214            101.961888            206.196032   \n",
              "2           236.913654            105.699271            212.570578   \n",
              "3           236.913654            101.197570            203.194964   \n",
              "4           195.561214            101.036616            204.252692   \n",
              "...                ...                   ...                   ...   \n",
              "220050      202.567329            101.257372            205.632412   \n",
              "220051      216.337699            101.036616            204.252692   \n",
              "220052      236.913654            103.943272            210.636630   \n",
              "220053      216.337699             97.697088            201.667547   \n",
              "220054      239.623764            108.974336            226.314867   \n",
              "\n",
              "        DUBAI_BRENT    DUBAI_WTI    BRENT_WTI  DUBAI_BRENT_WTI  \\\n",
              "0        546.497995  1213.797997  1068.430966     26622.002999   \n",
              "1       1965.969685  2839.214807  2501.630073    118167.780538   \n",
              "2       1961.489783  2654.618480  2538.986826    114980.529854   \n",
              "3       4602.285060  5243.917696  5331.314374    358700.380279   \n",
              "4        766.896608  1459.800265  1408.983864     39716.240979   \n",
              "...             ...          ...          ...              ...   \n",
              "220050  8026.135630  8083.967025  8758.887331    753858.756571   \n",
              "220051  7242.564844  7573.156942  7960.526239    660777.982621   \n",
              "220052  1647.472950  2414.901262  2283.132825     95306.917741   \n",
              "220053  5407.236199  6090.300616  6071.319832    447145.218708   \n",
              "220054  2393.813882  3289.629006  3215.938930    159137.506244   \n",
              "\n",
              "        country_cluster  \n",
              "0                     5  \n",
              "1                     5  \n",
              "2                     5  \n",
              "3                     5  \n",
              "4                     5  \n",
              "...                 ...  \n",
              "220050                0  \n",
              "220051                5  \n",
              "220052                5  \n",
              "220053                0  \n",
              "220054                5  \n",
              "\n",
              "[220055 rows x 66 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 각 국가의 중심 위도와 경도\n",
        "countries_coordinates = {\n",
        "    'CN': (35.8617, 104.1954),\n",
        "    'JP': (36.2048, 138.2529),\n",
        "    'RU': (61.5240, 105.3188),\n",
        "    'AU': (-25.2744, 133.7751),\n",
        "    'SG': (1.3521, 103.8198),\n",
        "    'ZA': (-30.5595, 22.9375),\n",
        "    'KR': (35.9078, 127.7669),\n",
        "    'TW': (23.6978, 120.9605),\n",
        "    'TT': (10.6918, -61.2225),\n",
        "    'ID': (-0.7893, 113.9213),\n",
        "    'BR': (-14.2350, -51.9253),\n",
        "    'QA': (25.3548, 51.1839),\n",
        "    'LV': (56.8796, 24.6032),\n",
        "    'MZ': (-18.6657, 35.5296),\n",
        "    'US': (37.0902, -95.7129),\n",
        "    'IN': (20.5937, 78.9629),\n",
        "    'UA': (48.3794, 31.1656),\n",
        "    'CA': (56.1304, -106.3468),\n",
        "    'MY': (4.2105, 101.9758),\n",
        "    'PE': (-9.1900, -75.0152),\n",
        "    'VN': (14.0583, 108.2772),\n",
        "    'FI': (61.9241, 25.7482),\n",
        "    'CL': (-35.6751, -71.5430),\n",
        "    'VE': (6.4238, -66.5897),\n",
        "    'PH': (12.8797, 121.7740)\n",
        "}\n",
        "\n",
        "# 데이터 프레임 생성\n",
        "coordinates_df = pd.DataFrame(countries_coordinates).T\n",
        "coordinates_df.columns = ['Latitude', 'Longitude']\n",
        "\n",
        "# 최적의 클러스터 수를 찾기 위한 Elbow Method\n",
        "distortions = []\n",
        "K = range(1, 15)\n",
        "for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k)\n",
        "    kmeanModel.fit(coordinates_df)\n",
        "    distortions.append(kmeanModel.inertia_)\n",
        "\n",
        "optimal_clusters = distortions.index(min(distortions[2:5])) + 2  # 3 또는 4 클러스터를 선택\n",
        "\n",
        "# KMeans 클러스터링 수행\n",
        "kmeanModel = KMeans(n_clusters=optimal_clusters)\n",
        "kmeanModel.fit(coordinates_df)\n",
        "\n",
        "# 각 국가에 할당된 클러스터 레이블 출력\n",
        "countries_labels = dict(zip(countries_coordinates.keys(), kmeanModel.labels_))\n",
        "# train 및 test 데이터프레임에 ARI_CO 기반으로 클러스터링 레이블 추가\n",
        "train['country_cluster'] = train['ARI_CO'].map(countries_labels)\n",
        "test['country_cluster'] = test['ARI_CO'].map(countries_labels)\n",
        "\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "features_to_convert = ['SHIP_TYPE_CATEGORY', 'BREADTH', 'DEPTH', 'DRAUGHT', 'country_cluster']\n",
        "\n",
        "for col in features_to_convert:\n",
        "    train[col] = train[col].astype('object')\n",
        "    if col in test.columns:  # test 데이터셋에도 해당 컬럼이 있으면 같이 변환\n",
        "        test[col] = test[col].astype('object')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>DEPTH</th>\n",
              "      <th>DRAUGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_enc_weekday</th>\n",
              "      <th>std_enc_weekday</th>\n",
              "      <th>mean_enc_rounded_hour</th>\n",
              "      <th>std_enc_rounded_hour</th>\n",
              "      <th>DUBAI_BRENT</th>\n",
              "      <th>DUBAI_WTI</th>\n",
              "      <th>BRENT_WTI</th>\n",
              "      <th>DUBAI_BRENT_WTI</th>\n",
              "      <th>country_cluster</th>\n",
              "      <th>PORT_SIZE_Zone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JP</td>\n",
              "      <td>HYG5</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>9.508139</td>\n",
              "      <td>R721438</td>\n",
              "      <td>20.0</td>\n",
              "      <td>9</td>\n",
              "      <td>5510</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>97.697088</td>\n",
              "      <td>201.667547</td>\n",
              "      <td>5239.633033</td>\n",
              "      <td>5816.236668</td>\n",
              "      <td>5598.429005</td>\n",
              "      <td>413051.837352</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>24.186684</td>\n",
              "      <td>G185231</td>\n",
              "      <td>40.0</td>\n",
              "      <td>14</td>\n",
              "      <td>109000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>108.406656</td>\n",
              "      <td>207.392917</td>\n",
              "      <td>103.780640</td>\n",
              "      <td>207.373622</td>\n",
              "      <td>5810.921980</td>\n",
              "      <td>6341.062114</td>\n",
              "      <td>6727.556992</td>\n",
              "      <td>497888.641441</td>\n",
              "      <td>0</td>\n",
              "      <td>Zone_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TW</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>P862781</td>\n",
              "      <td>20.0</td>\n",
              "      <td>14</td>\n",
              "      <td>13700</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>90.842366</td>\n",
              "      <td>202.567329</td>\n",
              "      <td>96.976710</td>\n",
              "      <td>199.178563</td>\n",
              "      <td>3708.686947</td>\n",
              "      <td>4455.475123</td>\n",
              "      <td>4386.153708</td>\n",
              "      <td>269214.856738</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JP</td>\n",
              "      <td>HYG5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>B415437</td>\n",
              "      <td>10.0</td>\n",
              "      <td>18</td>\n",
              "      <td>2840</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>87.803141</td>\n",
              "      <td>195.561214</td>\n",
              "      <td>103.974608</td>\n",
              "      <td>210.640285</td>\n",
              "      <td>458.152798</td>\n",
              "      <td>1059.257390</td>\n",
              "      <td>988.866905</td>\n",
              "      <td>21906.593241</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>19.834186</td>\n",
              "      <td>J412562</td>\n",
              "      <td>30.0</td>\n",
              "      <td>23</td>\n",
              "      <td>30700</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>87.803141</td>\n",
              "      <td>195.561214</td>\n",
              "      <td>101.085320</td>\n",
              "      <td>207.179760</td>\n",
              "      <td>3783.331312</td>\n",
              "      <td>4491.695491</td>\n",
              "      <td>4487.757641</td>\n",
              "      <td>276157.624278</td>\n",
              "      <td>0</td>\n",
              "      <td>Zone_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>RU</td>\n",
              "      <td>ZME5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>1.625700</td>\n",
              "      <td>R355837</td>\n",
              "      <td>10.0</td>\n",
              "      <td>35</td>\n",
              "      <td>3120</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>85.508007</td>\n",
              "      <td>188.359476</td>\n",
              "      <td>101.961888</td>\n",
              "      <td>206.196032</td>\n",
              "      <td>2786.399884</td>\n",
              "      <td>3448.358543</td>\n",
              "      <td>3387.019126</td>\n",
              "      <td>180400.091658</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>RU</td>\n",
              "      <td>ZME5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.642841</td>\n",
              "      <td>P438645</td>\n",
              "      <td>30.0</td>\n",
              "      <td>17</td>\n",
              "      <td>46400</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>109.006623</td>\n",
              "      <td>228.232254</td>\n",
              "      <td>1952.194475</td>\n",
              "      <td>2675.685598</td>\n",
              "      <td>2588.835802</td>\n",
              "      <td>116287.044583</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>CN</td>\n",
              "      <td>RGT8</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>R763233</td>\n",
              "      <td>20.0</td>\n",
              "      <td>12</td>\n",
              "      <td>5280</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>98.936224</td>\n",
              "      <td>216.337699</td>\n",
              "      <td>109.034067</td>\n",
              "      <td>222.568626</td>\n",
              "      <td>1006.758088</td>\n",
              "      <td>1773.767995</td>\n",
              "      <td>1687.974807</td>\n",
              "      <td>54902.731390</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>VN</td>\n",
              "      <td>ONW1</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>F587712</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>7020</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>90.842366</td>\n",
              "      <td>202.567329</td>\n",
              "      <td>105.699271</td>\n",
              "      <td>212.570578</td>\n",
              "      <td>5113.527276</td>\n",
              "      <td>5715.074994</td>\n",
              "      <td>5770.457600</td>\n",
              "      <td>410654.307217</td>\n",
              "      <td>0</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>VN</td>\n",
              "      <td>ONW1</td>\n",
              "      <td>Container</td>\n",
              "      <td>3.247493</td>\n",
              "      <td>F587712</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>7020</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>160.614133</td>\n",
              "      <td>236.913654</td>\n",
              "      <td>101.036616</td>\n",
              "      <td>204.252692</td>\n",
              "      <td>3699.610908</td>\n",
              "      <td>4408.022364</td>\n",
              "      <td>4475.548898</td>\n",
              "      <td>270161.260229</td>\n",
              "      <td>0</td>\n",
              "      <td>Zone_1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 57 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST       ID BREADTH  BUILT  \\\n",
              "0          JP   HYG5             Tanker   9.508139  R721438    20.0      9   \n",
              "1          SG   GIW5          Container  24.186684  G185231    40.0     14   \n",
              "2          TW   EKP8              Cargo   0.000000  P862781    20.0     14   \n",
              "3          JP   HYG5               Bulk   0.000000  B415437    10.0     18   \n",
              "4          SG   GIW5          Container  19.834186  J412562    30.0     23   \n",
              "...       ...    ...                ...        ...      ...     ...    ...   \n",
              "244984     RU   ZME5               Bulk   1.625700  R355837    10.0     35   \n",
              "244985     RU   ZME5               Bulk  30.642841  P438645    30.0     17   \n",
              "244986     CN   RGT8              Cargo   0.000000  R763233    20.0     12   \n",
              "244987     VN   ONW1          Container   0.000000  F587712    20.0     27   \n",
              "244988     VN   ONW1          Container   3.247493  F587712    20.0     27   \n",
              "\n",
              "        DEADWEIGHT DEPTH DRAUGHT  ...  mean_enc_weekday  std_enc_weekday  \\\n",
              "0             5510  10.0    10.0  ...        160.614133       236.913654   \n",
              "1           109000  20.0    10.0  ...        108.406656       207.392917   \n",
              "2            13700  10.0    10.0  ...         90.842366       202.567329   \n",
              "3             2840  10.0    10.0  ...         87.803141       195.561214   \n",
              "4            30700  20.0    10.0  ...         87.803141       195.561214   \n",
              "...            ...   ...     ...  ...               ...              ...   \n",
              "244984        3120  10.0     0.0  ...         85.508007       188.359476   \n",
              "244985       46400  20.0    10.0  ...        160.614133       236.913654   \n",
              "244986        5280  10.0    10.0  ...         98.936224       216.337699   \n",
              "244987        7020  10.0    10.0  ...         90.842366       202.567329   \n",
              "244988        7020  10.0    10.0  ...        160.614133       236.913654   \n",
              "\n",
              "       mean_enc_rounded_hour std_enc_rounded_hour  DUBAI_BRENT    DUBAI_WTI  \\\n",
              "0                  97.697088           201.667547  5239.633033  5816.236668   \n",
              "1                 103.780640           207.373622  5810.921980  6341.062114   \n",
              "2                  96.976710           199.178563  3708.686947  4455.475123   \n",
              "3                 103.974608           210.640285   458.152798  1059.257390   \n",
              "4                 101.085320           207.179760  3783.331312  4491.695491   \n",
              "...                      ...                  ...          ...          ...   \n",
              "244984            101.961888           206.196032  2786.399884  3448.358543   \n",
              "244985            109.006623           228.232254  1952.194475  2675.685598   \n",
              "244986            109.034067           222.568626  1006.758088  1773.767995   \n",
              "244987            105.699271           212.570578  5113.527276  5715.074994   \n",
              "244988            101.036616           204.252692  3699.610908  4408.022364   \n",
              "\n",
              "          BRENT_WTI  DUBAI_BRENT_WTI  country_cluster  PORT_SIZE_Zone  \n",
              "0       5598.429005    413051.837352                5          Zone_1  \n",
              "1       6727.556992    497888.641441                0          Zone_4  \n",
              "2       4386.153708    269214.856738                5          Zone_1  \n",
              "3        988.866905     21906.593241                5          Zone_1  \n",
              "4       4487.757641    276157.624278                0          Zone_4  \n",
              "...             ...              ...              ...             ...  \n",
              "244984  3387.019126    180400.091658                5          Zone_1  \n",
              "244985  2588.835802    116287.044583                5          Zone_1  \n",
              "244986  1687.974807     54902.731390                5          Zone_1  \n",
              "244987  5770.457600    410654.307217                0          Zone_1  \n",
              "244988  4475.548898    270161.260229                0          Zone_1  \n",
              "\n",
              "[244989 rows x 57 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 지정된 구역에 따라 PORT_SIZE를 나눔\n",
        "bins = [0, 0.0007, 0.0015, 0.002, float('inf')]\n",
        "labels = ['Zone_1', 'Zone_2', 'Zone_3', 'Zone_4']\n",
        "\n",
        "#train['PORT_SIZE_Zone'] = pd.cut(train['PORT_SIZE'], bins=bins, labels=labels, include_lowest=True)\n",
        "#test['PORT_SIZE_Zone'] = pd.cut(test['PORT_SIZE'], bins=bins, labels=labels, include_lowest=True)\n",
        "train['PORT_SIZE_Zone'] = pd.cut(train['PORT_SIZE'], bins=bins, labels=labels, include_lowest=True).astype('object')\n",
        "test['PORT_SIZE_Zone'] = pd.cut(test['PORT_SIZE'], bins=bins, labels=labels, include_lowest=True).astype('object')\n",
        "\n",
        "#train.drop(['PORT_SIZE'],axis=1,inplace=True)\n",
        "#test.drop(['PORT_SIZE'],axis=1,inplace=True)\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['U_WIND', 'V_WIND', 'AIR_TEMPERATURE','BN'],axis=1,inplace=True)\n",
        "test.drop(['U_WIND', 'V_WIND', 'AIR_TEMPERATURE','BN'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요없어보이는거\n",
        "train.drop(['SHIPMANAGER'],axis=1,inplace=True)\n",
        "test.drop(['SHIPMANAGER'],axis=1,inplace=True)\n",
        "train.drop(['BUILT'],axis=1,inplace=True)\n",
        "test.drop(['BUILT'],axis=1,inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 가정: 'hour'라는 이름의 시간 피처가 있다고 가정합니다.\n",
        "train['binned_hour'] = train['ATA_LT'] // 3\n",
        "test['binned_hour'] = test['ATA_LT'] // 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>DEPTH</th>\n",
              "      <th>DRAUGHT</th>\n",
              "      <th>GT</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_enc_rounded_hour</th>\n",
              "      <th>std_enc_rounded_hour</th>\n",
              "      <th>DUBAI_BRENT</th>\n",
              "      <th>DUBAI_WTI</th>\n",
              "      <th>BRENT_WTI</th>\n",
              "      <th>DUBAI_BRENT_WTI</th>\n",
              "      <th>country_cluster</th>\n",
              "      <th>PORT_SIZE_Zone</th>\n",
              "      <th>binned_hour</th>\n",
              "      <th>PO_Y_M_D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30004</th>\n",
              "      <td>CN</td>\n",
              "      <td>RGT8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>28.265609</td>\n",
              "      <td>G463522</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3680</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2000</td>\n",
              "      <td>...</td>\n",
              "      <td>109.034067</td>\n",
              "      <td>222.568626</td>\n",
              "      <td>998.883446</td>\n",
              "      <td>1780.818557</td>\n",
              "      <td>1602.067877</td>\n",
              "      <td>53383.580665</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>6</td>\n",
              "      <td>RGT8_6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56395</th>\n",
              "      <td>CN</td>\n",
              "      <td>RGT8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>29.401089</td>\n",
              "      <td>I325466</td>\n",
              "      <td>20.0</td>\n",
              "      <td>8000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5900</td>\n",
              "      <td>...</td>\n",
              "      <td>101.961888</td>\n",
              "      <td>206.196032</td>\n",
              "      <td>747.889345</td>\n",
              "      <td>1476.909227</td>\n",
              "      <td>1414.639566</td>\n",
              "      <td>39529.240980</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>3</td>\n",
              "      <td>RGT8_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78042</th>\n",
              "      <td>CN</td>\n",
              "      <td>RGT8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>31.162879</td>\n",
              "      <td>E215754</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2840</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2000</td>\n",
              "      <td>...</td>\n",
              "      <td>101.085320</td>\n",
              "      <td>207.179760</td>\n",
              "      <td>652.324338</td>\n",
              "      <td>1292.212773</td>\n",
              "      <td>1267.197895</td>\n",
              "      <td>32682.933283</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>2</td>\n",
              "      <td>RGT8_6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91897</th>\n",
              "      <td>ID</td>\n",
              "      <td>DIN2</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>F284788</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5360</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3270</td>\n",
              "      <td>...</td>\n",
              "      <td>105.699271</td>\n",
              "      <td>212.570578</td>\n",
              "      <td>7063.937083</td>\n",
              "      <td>7535.218577</td>\n",
              "      <td>7825.564354</td>\n",
              "      <td>645400.313638</td>\n",
              "      <td>0</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>4</td>\n",
              "      <td>DIN2_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110874</th>\n",
              "      <td>KR</td>\n",
              "      <td>RKA2</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>3.505856</td>\n",
              "      <td>L262821</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2980</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2990</td>\n",
              "      <td>...</td>\n",
              "      <td>96.976710</td>\n",
              "      <td>199.178563</td>\n",
              "      <td>4724.924598</td>\n",
              "      <td>5231.493423</td>\n",
              "      <td>5468.940262</td>\n",
              "      <td>367673.113497</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>3</td>\n",
              "      <td>RKA2_5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152470</th>\n",
              "      <td>ID</td>\n",
              "      <td>DIN2</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>P578812</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1210</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2420</td>\n",
              "      <td>...</td>\n",
              "      <td>101.197570</td>\n",
              "      <td>203.194964</td>\n",
              "      <td>7967.818830</td>\n",
              "      <td>8365.230941</td>\n",
              "      <td>8963.134876</td>\n",
              "      <td>772927.320910</td>\n",
              "      <td>0</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>6</td>\n",
              "      <td>DIN2_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213725</th>\n",
              "      <td>CN</td>\n",
              "      <td>RGT8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>A165342</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2600</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1980</td>\n",
              "      <td>...</td>\n",
              "      <td>109.034067</td>\n",
              "      <td>222.568626</td>\n",
              "      <td>1099.800765</td>\n",
              "      <td>1829.868911</td>\n",
              "      <td>1672.726558</td>\n",
              "      <td>58020.233770</td>\n",
              "      <td>5</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>6</td>\n",
              "      <td>RGT8_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>VN</td>\n",
              "      <td>ONW1</td>\n",
              "      <td>Container</td>\n",
              "      <td>3.247493</td>\n",
              "      <td>F587712</td>\n",
              "      <td>20.0</td>\n",
              "      <td>7020</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4910</td>\n",
              "      <td>...</td>\n",
              "      <td>101.036616</td>\n",
              "      <td>204.252692</td>\n",
              "      <td>3699.610908</td>\n",
              "      <td>4408.022364</td>\n",
              "      <td>4475.548898</td>\n",
              "      <td>270161.260229</td>\n",
              "      <td>0</td>\n",
              "      <td>Zone_1</td>\n",
              "      <td>7</td>\n",
              "      <td>ONW1_5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 53 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST       ID BREADTH  \\\n",
              "30004      CN   RGT8               Bulk  28.265609  G463522    10.0   \n",
              "56395      CN   RGT8               Bulk  29.401089  I325466    20.0   \n",
              "78042      CN   RGT8               Bulk  31.162879  E215754    10.0   \n",
              "91897      ID   DIN2             Tanker   0.000000  F284788    20.0   \n",
              "110874     KR   RKA2             Tanker   3.505856  L262821    10.0   \n",
              "152470     ID   DIN2             Tanker   0.000000  P578812    20.0   \n",
              "213725     CN   RGT8               Bulk   0.000000  A165342    10.0   \n",
              "244988     VN   ONW1          Container   3.247493  F587712    20.0   \n",
              "\n",
              "        DEADWEIGHT DEPTH DRAUGHT    GT  ...  mean_enc_rounded_hour  \\\n",
              "30004         3680  10.0    10.0  2000  ...             109.034067   \n",
              "56395         8000  10.0    10.0  5900  ...             101.961888   \n",
              "78042         2840  10.0    10.0  2000  ...             101.085320   \n",
              "91897         5360  10.0    10.0  3270  ...             105.699271   \n",
              "110874        2980  10.0    10.0  2990  ...              96.976710   \n",
              "152470        1210  10.0     0.0  2420  ...             101.197570   \n",
              "213725        2600  10.0     0.0  1980  ...             109.034067   \n",
              "244988        7020  10.0    10.0  4910  ...             101.036616   \n",
              "\n",
              "       std_enc_rounded_hour  DUBAI_BRENT    DUBAI_WTI    BRENT_WTI  \\\n",
              "30004            222.568626   998.883446  1780.818557  1602.067877   \n",
              "56395            206.196032   747.889345  1476.909227  1414.639566   \n",
              "78042            207.179760   652.324338  1292.212773  1267.197895   \n",
              "91897            212.570578  7063.937083  7535.218577  7825.564354   \n",
              "110874           199.178563  4724.924598  5231.493423  5468.940262   \n",
              "152470           203.194964  7967.818830  8365.230941  8963.134876   \n",
              "213725           222.568626  1099.800765  1829.868911  1672.726558   \n",
              "244988           204.252692  3699.610908  4408.022364  4475.548898   \n",
              "\n",
              "        DUBAI_BRENT_WTI  country_cluster  PORT_SIZE_Zone  binned_hour  \\\n",
              "30004      53383.580665                5          Zone_1            6   \n",
              "56395      39529.240980                5          Zone_1            3   \n",
              "78042      32682.933283                5          Zone_1            2   \n",
              "91897     645400.313638                0          Zone_1            4   \n",
              "110874    367673.113497                5          Zone_1            3   \n",
              "152470    772927.320910                0          Zone_1            6   \n",
              "213725     58020.233770                5          Zone_1            6   \n",
              "244988    270161.260229                0          Zone_1            7   \n",
              "\n",
              "        PO_Y_M_D  \n",
              "30004     RGT8_6  \n",
              "56395     RGT8_4  \n",
              "78042     RGT8_6  \n",
              "91897     DIN2_4  \n",
              "110874    RKA2_5  \n",
              "152470    DIN2_4  \n",
              "213725    RGT8_4  \n",
              "244988    ONW1_5  \n",
              "\n",
              "[8 rows x 53 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['PO_Y_M_D'] = train['ARI_PO'].astype(str) + \"_\" + train['weekday'].astype(str)\n",
        "test['PO_Y_M_D'] = test['ARI_PO'].astype(str) + \"_\" + test['weekday'].astype(str)\n",
        "unique_in_test_only = set(test['PO_Y_M_D']) - set(train['PO_Y_M_D'])\n",
        "test_samples_unique = test[test['PO_Y_M_D'].isin(unique_in_test_only)]\n",
        "test_samples_unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train.drop(['WTI','DUBAI','BRENT'],axis=1,inplace=True)\n",
        "#test.drop(['WTI','DUBAI','BRENT'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['ATA_datetime'],axis=1,inplace=True)\n",
        "#test.drop(['ATA_datetime'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 220055 entries, 0 to 220054\n",
            "Data columns (total 62 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   ARI_CO                       220055 non-null  object \n",
            " 1   ARI_PO                       220055 non-null  object \n",
            " 2   SHIP_TYPE_CATEGORY           220055 non-null  object \n",
            " 3   DIST                         220055 non-null  float64\n",
            " 4   ID                           220055 non-null  object \n",
            " 5   BREADTH                      220055 non-null  object \n",
            " 6   DEADWEIGHT                   220055 non-null  int64  \n",
            " 7   DEPTH                        220055 non-null  object \n",
            " 8   DRAUGHT                      220055 non-null  object \n",
            " 9   GT                           220055 non-null  int64  \n",
            " 10  LENGTH                       220055 non-null  float64\n",
            " 11  FLAG                         220055 non-null  object \n",
            " 12  ATA_LT                       220055 non-null  int64  \n",
            " 13  DUBAI                        220055 non-null  float64\n",
            " 14  BRENT                        220055 non-null  float64\n",
            " 15  WTI                          220055 non-null  float64\n",
            " 16  BDI_ADJ                      220055 non-null  float64\n",
            " 17  PORT_SIZE                    220055 non-null  float64\n",
            " 18  CI_HOUR                      220055 non-null  float64\n",
            " 19  port_daily_arrival_rate      220055 non-null  float64\n",
            " 20  port_daily_service_rate      220055 non-null  float64\n",
            " 21  port_dayofweek_arrival_rate  220055 non-null  float64\n",
            " 22  port_dayofweek_service_rate  220055 non-null  float64\n",
            " 23  P_n                          220055 non-null  float64\n",
            " 24  P_busy                       220055 non-null  float64\n",
            " 25  P_n_dayofweek                220055 non-null  float64\n",
            " 26  P_busy_dayofweek             220055 non-null  float64\n",
            " 27  year                         220055 non-null  int32  \n",
            " 28  month                        220055 non-null  int32  \n",
            " 29  day                          220055 non-null  int32  \n",
            " 30  weekday                      220055 non-null  int32  \n",
            " 31  rounded_hour                 220055 non-null  int64  \n",
            " 32  month_sin                    220055 non-null  float64\n",
            " 33  month_cos                    220055 non-null  float64\n",
            " 34  day_sin                      220055 non-null  float64\n",
            " 35  day_cos                      220055 non-null  float64\n",
            " 36  weekday_sin                  220055 non-null  float64\n",
            " 37  weekday_cos                  220055 non-null  float64\n",
            " 38  rounded_hour_sin             220055 non-null  float64\n",
            " 39  rounded_hour_cos             220055 non-null  float64\n",
            " 40  mean_enc_ARI_CO              220055 non-null  float64\n",
            " 41  std_enc_ARI_CO               220055 non-null  float64\n",
            " 42  mean_enc_ARI_PO              220055 non-null  float64\n",
            " 43  std_enc_ARI_PO               220055 non-null  float64\n",
            " 44  mean_enc_year                220055 non-null  float64\n",
            " 45  std_enc_year                 220055 non-null  float64\n",
            " 46  mean_enc_month               220055 non-null  float64\n",
            " 47  std_enc_month                220055 non-null  float64\n",
            " 48  mean_enc_day                 220055 non-null  float64\n",
            " 49  std_enc_day                  220055 non-null  float64\n",
            " 50  mean_enc_weekday             220055 non-null  float64\n",
            " 51  std_enc_weekday              220055 non-null  float64\n",
            " 52  mean_enc_rounded_hour        220055 non-null  float64\n",
            " 53  std_enc_rounded_hour         220055 non-null  float64\n",
            " 54  DUBAI_BRENT                  220055 non-null  float64\n",
            " 55  DUBAI_WTI                    220055 non-null  float64\n",
            " 56  BRENT_WTI                    220055 non-null  float64\n",
            " 57  DUBAI_BRENT_WTI              220055 non-null  float64\n",
            " 58  country_cluster              220055 non-null  object \n",
            " 59  PORT_SIZE_Zone               220055 non-null  object \n",
            " 60  binned_hour                  220055 non-null  int64  \n",
            " 61  PO_Y_M_D                     220055 non-null  object \n",
            "dtypes: float64(42), int32(4), int64(5), object(11)\n",
            "memory usage: 100.7+ MB\n"
          ]
        }
      ],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = train.drop(['CI_HOUR'],axis=1)\n",
        "train_y = train[['CI_HOUR']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# ARI_CO + weekday 결합 열 생성\\ntrain[\\'fold_column\\'] = train[\\'ARI_CO\\'].astype(str) + \"_\" + train[\\'weekday\\'].astype(str)\\n\\n# Stratified 5-fold 생성\\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=313)\\n\\n# 각 폴드마다의 예측값과 MAE를 저장할 리스트\\npredictions = []\\nmae_scores = []\\n\\n# StratifiedKFold는 레이블 분포를 기반으로 하기 때문에, 너무 적은 빈도의 조합은 오류를 발생시킬 수 있습니다.\\nmin_samples = train[\\'fold_column\\'].value_counts().min()\\nfiltered_train = train[train[\\'fold_column\\'].map(train[\\'fold_column\\'].value_counts()) > min_samples]\\n\\nfor train_idx, val_idx in skf.split(filtered_train, filtered_train[\\'fold_column\\']):\\n    train_fold = filtered_train.iloc[train_idx].drop(columns=[\\'fold_column\\'])\\n    val_fold = filtered_train.iloc[val_idx].drop(columns=[\\'fold_column\\'])\\n    train_data = TabularDataset(train_fold)\\n    val_data = TabularDataset(val_fold)\\n    test_data = TabularDataset(test)\\n    \\n    # 각 폴드마다 모델 학습\\n    predictor = TabularPredictor(label=\\'CI_HOUR\\', eval_metric=\\'mean_absolute_error\\').fit(\\n        train_data, \\n        presets=\\'medium_quality\\',\\n        ag_args_fit={\\'num_gpus\\': 0},\\n        included_model_types=[\\'CAT\\']\\n    )\\n    \\n    # validation set에 대한 예측값 계산\\n    val_predictions = predictor.predict(val_data.drop(columns=\\'CI_HOUR\\'))\\n    \\n    # MAE 계산 후 저장\\n    mae = mean_absolute_error(val_data[\\'CI_HOUR\\'], val_predictions)\\n    mae_scores.append(mae)\\n    \\n    # test set에 대한 예측값 저장\\n    predictions.append(predictor.predict(test_data))\\n\\n# 예측값의 평균 계산\\nfinal_predictions = np.mean(predictions, axis=0)\\ny_pred = pd.DataFrame(final_predictions, columns=[\\'CI_HOUR\\'])\\nsubmission[\\'CI_HOUR\\'] = y_pred[\\'CI_HOUR\\']\\nmae_scores, np.mean(mae_scores)\\n'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "'''\n",
        "# ARI_CO + weekday 결합 열 생성\n",
        "train['fold_column'] = train['ARI_CO'].astype(str) + \"_\" + train['weekday'].astype(str)\n",
        "\n",
        "# Stratified 5-fold 생성\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=313)\n",
        "\n",
        "# 각 폴드마다의 예측값과 MAE를 저장할 리스트\n",
        "predictions = []\n",
        "mae_scores = []\n",
        "\n",
        "# StratifiedKFold는 레이블 분포를 기반으로 하기 때문에, 너무 적은 빈도의 조합은 오류를 발생시킬 수 있습니다.\n",
        "min_samples = train['fold_column'].value_counts().min()\n",
        "filtered_train = train[train['fold_column'].map(train['fold_column'].value_counts()) > min_samples]\n",
        "\n",
        "for train_idx, val_idx in skf.split(filtered_train, filtered_train['fold_column']):\n",
        "    train_fold = filtered_train.iloc[train_idx].drop(columns=['fold_column'])\n",
        "    val_fold = filtered_train.iloc[val_idx].drop(columns=['fold_column'])\n",
        "    train_data = TabularDataset(train_fold)\n",
        "    val_data = TabularDataset(val_fold)\n",
        "    test_data = TabularDataset(test)\n",
        "    \n",
        "    # 각 폴드마다 모델 학습\n",
        "    predictor = TabularPredictor(label='CI_HOUR', eval_metric='mean_absolute_error').fit(\n",
        "        train_data, \n",
        "        presets='medium_quality',\n",
        "        ag_args_fit={'num_gpus': 0},\n",
        "        included_model_types=['CAT']\n",
        "    )\n",
        "    \n",
        "    # validation set에 대한 예측값 계산\n",
        "    val_predictions = predictor.predict(val_data.drop(columns='CI_HOUR'))\n",
        "    \n",
        "    # MAE 계산 후 저장\n",
        "    mae = mean_absolute_error(val_data['CI_HOUR'], val_predictions)\n",
        "    mae_scores.append(mae)\n",
        "    \n",
        "    # test set에 대한 예측값 저장\n",
        "    predictions.append(predictor.predict(test_data))\n",
        "\n",
        "# 예측값의 평균 계산\n",
        "final_predictions = np.mean(predictions, axis=0)\n",
        "y_pred = pd.DataFrame(final_predictions, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred['CI_HOUR']\n",
        "mae_scores, np.mean(mae_scores)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231010_083649\\\"\n",
            "Presets specified: ['medium_quality']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (220055 samples, 212.13 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231010_083649\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   14.29 GB / 511.09 GB (2.8%)\n",
            "Train Data Rows:    220055\n",
            "Train Data Columns: 61\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.002777778, 103.30825, 210.47157)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    16240.81 MB\n",
            "\tTrain Data (Original)  Memory Usage: 210.37 MB (1.3% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 41 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) : 11 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 11 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', ...]\n",
            "\t\t('float', [])    : 41 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t1.7s = Fit runtime\n",
            "\t61 features in original data used to generate 61 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 87.37 MB (0.5% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.92s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.011360796164595215, Train Rows: 217555, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-33.9877\t = Validation score   (-mean_absolute_error)\n",
            "\t2319.69s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-33.9877\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 2322.42s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231010_083649\\\")\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"8 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 8 missing columns: ['port_daily_arrival_rate', 'port_daily_service_rate', 'port_dayofweek_arrival_rate', 'port_dayofweek_service_rate', 'P_n', 'P_busy', 'P_n_dayofweek', 'P_busy_dayofweek'] | 53 available columns: ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'DIST', 'ID', 'BREADTH', 'DEADWEIGHT', 'DEPTH', 'DRAUGHT', 'GT', 'LENGTH', 'FLAG', 'ATA_LT', 'DUBAI', 'BRENT', 'WTI', 'BDI_ADJ', 'PORT_SIZE', 'year', 'month', 'day', 'weekday', 'rounded_hour', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'weekday_sin', 'weekday_cos', 'rounded_hour_sin', 'rounded_hour_cos', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'mean_enc_year', 'std_enc_year', 'mean_enc_month', 'std_enc_month', 'mean_enc_day', 'std_enc_day', 'mean_enc_weekday', 'std_enc_weekday', 'mean_enc_rounded_hour', 'std_enc_rounded_hour', 'DUBAI_BRENT', 'DUBAI_WTI', 'BRENT_WTI', 'DUBAI_BRENT_WTI', 'country_cluster', 'PORT_SIZE_Zone', 'binned_hour', 'PO_Y_M_D']\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\features\\generators\\abstract.py:338\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_in:\n\u001b[0;32m    336\u001b[0m         \u001b[39m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[0;32m    337\u001b[0m         \u001b[39m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m         X \u001b[39m=\u001b[39m X[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_in]\n\u001b[0;32m    339\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['port_daily_arrival_rate', 'port_daily_service_rate', 'port_dayofweek_arrival_rate', 'port_dayofweek_service_rate', 'P_n', 'P_busy', 'P_n_dayofweek', 'P_busy_dayofweek'] not in index\"",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v2.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0})\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m predictor\u001b[39m.\u001b[39mleaderboard(train, silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_pred \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(test_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(y_pred, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mCI_HOUR\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m submission[\u001b[39m'\u001b[39m\u001b[39mCI_HOUR\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m y_pred[\u001b[39m'\u001b[39m\u001b[39mCI_HOUR\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1579\u001b[0m, in \u001b[0;36mTabularPredictor.predict\u001b[1;34m(self, data, model, as_pandas, transform_features, decision_threshold)\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[39mif\u001b[39;00m decision_threshold \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1578\u001b[0m     decision_threshold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_threshold\n\u001b[1;32m-> 1579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mpredict(X\u001b[39m=\u001b[39;49mdata, model\u001b[39m=\u001b[39;49mmodel, as_pandas\u001b[39m=\u001b[39;49mas_pandas, transform_features\u001b[39m=\u001b[39;49mtransform_features, decision_threshold\u001b[39m=\u001b[39;49mdecision_threshold)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:208\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict\u001b[1;34m(self, X, model, as_pandas, inverse_transform, transform_features, decision_threshold)\u001b[0m\n\u001b[0;32m    206\u001b[0m     decision_threshold \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m    207\u001b[0m X_index \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(X\u001b[39m.\u001b[39mindex) \u001b[39mif\u001b[39;00m as_pandas \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(\n\u001b[0;32m    209\u001b[0m     X\u001b[39m=\u001b[39;49mX, model\u001b[39m=\u001b[39;49mmodel, as_pandas\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, as_multiclass\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, inverse_transform\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, transform_features\u001b[39m=\u001b[39;49mtransform_features\n\u001b[0;32m    210\u001b[0m )\n\u001b[0;32m    211\u001b[0m problem_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_cleaner\u001b[39m.\u001b[39mproblem_type_transform \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type\n\u001b[0;32m    212\u001b[0m y_pred \u001b[39m=\u001b[39m get_pred_from_proba(y_pred_proba\u001b[39m=\u001b[39my_pred_proba, problem_type\u001b[39m=\u001b[39mproblem_type, decision_threshold\u001b[39m=\u001b[39mdecision_threshold)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:188\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict_proba\u001b[1;34m(self, X, model, as_pandas, as_multiclass, inverse_transform, transform_features)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mif\u001b[39;00m transform_features:\n\u001b[1;32m--> 188\u001b[0m         X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_features(X)\n\u001b[0;32m    189\u001b[0m     y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_trainer()\u001b[39m.\u001b[39mpredict_proba(X, model\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m    190\u001b[0m y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_process_predict_proba(\n\u001b[0;32m    191\u001b[0m     y_pred_proba\u001b[39m=\u001b[39my_pred_proba, as_pandas\u001b[39m=\u001b[39mas_pandas, index\u001b[39m=\u001b[39mX_index, as_multiclass\u001b[39m=\u001b[39mas_multiclass, inverse_transform\u001b[39m=\u001b[39minverse_transform\n\u001b[0;32m    192\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:462\u001b[0m, in \u001b[0;36mAbstractTabularLearner.transform_features\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform_features\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    461\u001b[0m     \u001b[39mfor\u001b[39;00m feature_generator \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_generators:\n\u001b[1;32m--> 462\u001b[0m         X \u001b[39m=\u001b[39m feature_generator\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[0;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\features\\generators\\abstract.py:344\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m X\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m    343\u001b[0m             missing_cols\u001b[39m.\u001b[39mappend(col)\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    345\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(missing_cols)\u001b[39m}\u001b[39;00m\u001b[39m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(missing_cols)\u001b[39m}\u001b[39;00m\u001b[39m missing columns: \u001b[39m\u001b[39m{\u001b[39;00mmissing_cols\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    347\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\u001b[39m}\u001b[39;00m\u001b[39m available columns: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_astype_generator:\n\u001b[0;32m    350\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_astype_generator\u001b[39m.\u001b[39mtransform(X)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"8 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 8 missing columns: ['port_daily_arrival_rate', 'port_daily_service_rate', 'port_dayofweek_arrival_rate', 'port_dayofweek_service_rate', 'P_n', 'P_busy', 'P_n_dayofweek', 'P_busy_dayofweek'] | 53 available columns: ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'DIST', 'ID', 'BREADTH', 'DEADWEIGHT', 'DEPTH', 'DRAUGHT', 'GT', 'LENGTH', 'FLAG', 'ATA_LT', 'DUBAI', 'BRENT', 'WTI', 'BDI_ADJ', 'PORT_SIZE', 'year', 'month', 'day', 'weekday', 'rounded_hour', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'weekday_sin', 'weekday_cos', 'rounded_hour_sin', 'rounded_hour_cos', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'mean_enc_year', 'std_enc_year', 'mean_enc_month', 'std_enc_month', 'mean_enc_day', 'std_enc_day', 'mean_enc_weekday', 'std_enc_weekday', 'mean_enc_rounded_hour', 'std_enc_rounded_hour', 'DUBAI_BRENT', 'DUBAI_WTI', 'BRENT_WTI', 'DUBAI_BRENT_WTI', 'country_cluster', 'PORT_SIZE_Zone', 'binned_hour', 'PO_Y_M_D']\""
          ]
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(train)\n",
        "test_data = TabularDataset(test)\n",
        "\n",
        "\n",
        "predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['CAT'])\n",
        "#predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0})\n",
        "\n",
        "predictor.leaderboard(train, silent=True)\n",
        "\n",
        "\n",
        "y_pred = predictor.predict(test_data)\n",
        "y_pred = pd.DataFrame(y_pred, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred['CI_HOUR']\n",
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pycaret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a3fa5_row9_col1, #T_a3fa5_row18_col1 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a3fa5\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a3fa5_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
              "      <th id=\"T_a3fa5_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a3fa5_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
              "      <td id=\"T_a3fa5_row0_col1\" class=\"data row0 col1\" >7777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_a3fa5_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
              "      <td id=\"T_a3fa5_row1_col1\" class=\"data row1 col1\" >CI_HOUR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_a3fa5_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
              "      <td id=\"T_a3fa5_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_a3fa5_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
              "      <td id=\"T_a3fa5_row3_col1\" class=\"data row3 col1\" >(220055, 54)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_a3fa5_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
              "      <td id=\"T_a3fa5_row4_col1\" class=\"data row4 col1\" >(220055, 99)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_a3fa5_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
              "      <td id=\"T_a3fa5_row5_col1\" class=\"data row5 col1\" >(154038, 99)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_a3fa5_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
              "      <td id=\"T_a3fa5_row6_col1\" class=\"data row6 col1\" >(66017, 99)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_a3fa5_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
              "      <td id=\"T_a3fa5_row7_col1\" class=\"data row7 col1\" >42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_a3fa5_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
              "      <td id=\"T_a3fa5_row8_col1\" class=\"data row8 col1\" >11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_a3fa5_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
              "      <td id=\"T_a3fa5_row9_col1\" class=\"data row9 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_a3fa5_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
              "      <td id=\"T_a3fa5_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_a3fa5_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
              "      <td id=\"T_a3fa5_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_a3fa5_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
              "      <td id=\"T_a3fa5_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_a3fa5_row13_col0\" class=\"data row13 col0\" >Maximum one-hot encoding</td>\n",
              "      <td id=\"T_a3fa5_row13_col1\" class=\"data row13 col1\" >25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_a3fa5_row14_col0\" class=\"data row14 col0\" >Encoding method</td>\n",
              "      <td id=\"T_a3fa5_row14_col1\" class=\"data row14 col1\" >TargetEncoder()</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_a3fa5_row15_col0\" class=\"data row15 col0\" >Fold Generator</td>\n",
              "      <td id=\"T_a3fa5_row15_col1\" class=\"data row15 col1\" >KFold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_a3fa5_row16_col0\" class=\"data row16 col0\" >Fold Number</td>\n",
              "      <td id=\"T_a3fa5_row16_col1\" class=\"data row16 col1\" >10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_a3fa5_row17_col0\" class=\"data row17 col0\" >CPU Jobs</td>\n",
              "      <td id=\"T_a3fa5_row17_col1\" class=\"data row17 col1\" >-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_a3fa5_row18_col0\" class=\"data row18 col0\" >Use GPU</td>\n",
              "      <td id=\"T_a3fa5_row18_col1\" class=\"data row18 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_a3fa5_row19_col0\" class=\"data row19 col0\" >Log Experiment</td>\n",
              "      <td id=\"T_a3fa5_row19_col1\" class=\"data row19 col1\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_a3fa5_row20_col0\" class=\"data row20 col0\" >Experiment Name</td>\n",
              "      <td id=\"T_a3fa5_row20_col1\" class=\"data row20 col1\" >reg-default-name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a3fa5_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_a3fa5_row21_col0\" class=\"data row21 col0\" >USI</td>\n",
              "      <td id=\"T_a3fa5_row21_col1\" class=\"data row21 col1\" >41bc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1f8d57b45e0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;,\n",
              "                                             &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;, &#x27;DUBAI&#x27;,\n",
              "                                             &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                                             &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;,\n",
              "                                             &#x27;rounded_h...\n",
              "                                             &#x27;country_cluster&#x27;,\n",
              "                                             &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                    transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                                    &#x27;BREADTH&#x27;,\n",
              "                                                                    &#x27;DEPTH&#x27;,\n",
              "                                                                    &#x27;DRAUGHT&#x27;,\n",
              "                                                                    &#x27;country_cluster&#x27;,\n",
              "                                                                    &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                             &#x27;PO_Y_M_D&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;,\n",
              "                                                                    &#x27;PO_Y_M_D&#x27;])))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;,\n",
              "                                             &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;, &#x27;DUBAI&#x27;,\n",
              "                                             &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                                             &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;,\n",
              "                                             &#x27;rounded_h...\n",
              "                                             &#x27;country_cluster&#x27;,\n",
              "                                             &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                    transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                                    &#x27;BREADTH&#x27;,\n",
              "                                                                    &#x27;DEPTH&#x27;,\n",
              "                                                                    &#x27;DRAUGHT&#x27;,\n",
              "                                                                    &#x27;country_cluster&#x27;,\n",
              "                                                                    &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                             &#x27;PO_Y_M_D&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;,\n",
              "                                                                    &#x27;PO_Y_M_D&#x27;])))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;,\n",
              "                            &#x27;DUBAI&#x27;, &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;, &#x27;PORT_SIZE&#x27;,\n",
              "                            &#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                            &#x27;month_sin&#x27;, &#x27;month_cos&#x27;, &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                            &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;, &#x27;rounded_hour_sin&#x27;,\n",
              "                            &#x27;rounded_hour_cos&#x27;, &#x27;mean_enc_ARI_CO&#x27;,\n",
              "                            &#x27;std_enc_ARI_CO&#x27;, &#x27;mean_enc_ARI_PO&#x27;,\n",
              "                            &#x27;std_enc_ARI_PO&#x27;, &#x27;mean_enc_year&#x27;, &#x27;std_enc_year&#x27;,\n",
              "                            &#x27;mean_enc_month&#x27;, ...],\n",
              "                   transformer=SimpleImputer())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;ARI_PO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;ID&#x27;,\n",
              "                            &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;, &#x27;DRAUGHT&#x27;, &#x27;FLAG&#x27;,\n",
              "                            &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;, &#x27;PO_Y_M_D&#x27;],\n",
              "                   transformer=SimpleImputer(strategy=&#x27;most_frequent&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                            &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                   transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                   &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                   &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                                                   &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;,\n",
              "                                                   &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                             handle_missing=&#x27;return_nan&#x27;,\n",
              "                                             use_cat_names=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                    &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                    &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rest_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;],\n",
              "                   transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                                   &#x27;PO_Y_M_D&#x27;]))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;])</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[('numerical_imputer',\n",
              "                 TransformerWrapper(include=['DIST', 'DEADWEIGHT', 'GT',\n",
              "                                             'LENGTH', 'ATA_LT', 'DUBAI',\n",
              "                                             'BRENT', 'WTI', 'BDI_ADJ',\n",
              "                                             'PORT_SIZE', 'year', 'month',\n",
              "                                             'day', 'weekday', 'rounded_hour',\n",
              "                                             'month_sin', 'month_cos',\n",
              "                                             'day_sin', 'day_cos',\n",
              "                                             'weekday_sin', 'weekday_cos',\n",
              "                                             'rounded_h...\n",
              "                                             'country_cluster',\n",
              "                                             'PORT_SIZE_Zone'],\n",
              "                                    transformer=OneHotEncoder(cols=['ARI_CO',\n",
              "                                                                    'SHIP_TYPE_CATEGORY',\n",
              "                                                                    'BREADTH',\n",
              "                                                                    'DEPTH',\n",
              "                                                                    'DRAUGHT',\n",
              "                                                                    'country_cluster',\n",
              "                                                                    'PORT_SIZE_Zone'],\n",
              "                                                              handle_missing='return_nan',\n",
              "                                                              use_cat_names=True))),\n",
              "                ('rest_encoding',\n",
              "                 TransformerWrapper(include=['ARI_PO', 'ID', 'FLAG',\n",
              "                                             'PO_Y_M_D'],\n",
              "                                    transformer=TargetEncoder(cols=['ARI_PO',\n",
              "                                                                    'ID',\n",
              "                                                                    'FLAG',\n",
              "                                                                    'PO_Y_M_D'])))])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pycaret.regression import *\n",
        "import category_encoders\n",
        "\n",
        "reg = setup(data=train, target='CI_HOUR', use_gpu=True, session_id=7777, encoding_method=category_encoders.target_encoder.TargetEncoder(smoothing=10))\n",
        "reg.pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_53367 th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_53367_row0_col0, #T_53367_row0_col2, #T_53367_row0_col3, #T_53367_row0_col4, #T_53367_row0_col6, #T_53367_row1_col0, #T_53367_row1_col1, #T_53367_row1_col5, #T_53367_row1_col6, #T_53367_row2_col0, #T_53367_row2_col1, #T_53367_row2_col2, #T_53367_row2_col3, #T_53367_row2_col4, #T_53367_row2_col5, #T_53367_row3_col0, #T_53367_row3_col1, #T_53367_row3_col2, #T_53367_row3_col3, #T_53367_row3_col4, #T_53367_row3_col5, #T_53367_row3_col6, #T_53367_row4_col0, #T_53367_row4_col1, #T_53367_row4_col2, #T_53367_row4_col3, #T_53367_row4_col4, #T_53367_row4_col5, #T_53367_row4_col6, #T_53367_row5_col0, #T_53367_row5_col1, #T_53367_row5_col2, #T_53367_row5_col3, #T_53367_row5_col4, #T_53367_row5_col5, #T_53367_row5_col6, #T_53367_row6_col0, #T_53367_row6_col1, #T_53367_row6_col2, #T_53367_row6_col3, #T_53367_row6_col4, #T_53367_row6_col5, #T_53367_row6_col6, #T_53367_row7_col0, #T_53367_row7_col1, #T_53367_row7_col2, #T_53367_row7_col3, #T_53367_row7_col4, #T_53367_row7_col5, #T_53367_row7_col6, #T_53367_row8_col0, #T_53367_row8_col1, #T_53367_row8_col2, #T_53367_row8_col3, #T_53367_row8_col4, #T_53367_row8_col5, #T_53367_row8_col6, #T_53367_row9_col0, #T_53367_row9_col1, #T_53367_row9_col2, #T_53367_row9_col3, #T_53367_row9_col4, #T_53367_row9_col5, #T_53367_row9_col6, #T_53367_row10_col0, #T_53367_row10_col1, #T_53367_row10_col2, #T_53367_row10_col3, #T_53367_row10_col4, #T_53367_row10_col5, #T_53367_row10_col6, #T_53367_row11_col0, #T_53367_row11_col1, #T_53367_row11_col2, #T_53367_row11_col3, #T_53367_row11_col4, #T_53367_row11_col5, #T_53367_row11_col6, #T_53367_row12_col0, #T_53367_row12_col1, #T_53367_row12_col2, #T_53367_row12_col3, #T_53367_row12_col4, #T_53367_row12_col5, #T_53367_row12_col6, #T_53367_row13_col0, #T_53367_row13_col1, #T_53367_row13_col2, #T_53367_row13_col3, #T_53367_row13_col4, #T_53367_row13_col5, #T_53367_row13_col6, #T_53367_row14_col0, #T_53367_row14_col1, #T_53367_row14_col2, #T_53367_row14_col3, #T_53367_row14_col4, #T_53367_row14_col5, #T_53367_row14_col6, #T_53367_row15_col0, #T_53367_row15_col1, #T_53367_row15_col2, #T_53367_row15_col3, #T_53367_row15_col4, #T_53367_row15_col5, #T_53367_row15_col6, #T_53367_row16_col0, #T_53367_row16_col1, #T_53367_row16_col2, #T_53367_row16_col3, #T_53367_row16_col4, #T_53367_row16_col5, #T_53367_row16_col6, #T_53367_row17_col0, #T_53367_row17_col1, #T_53367_row17_col2, #T_53367_row17_col3, #T_53367_row17_col4, #T_53367_row17_col5, #T_53367_row17_col6, #T_53367_row18_col0, #T_53367_row18_col1, #T_53367_row18_col2, #T_53367_row18_col3, #T_53367_row18_col4, #T_53367_row18_col5, #T_53367_row18_col6, #T_53367_row19_col0, #T_53367_row19_col1, #T_53367_row19_col2, #T_53367_row19_col3, #T_53367_row19_col4, #T_53367_row19_col5, #T_53367_row19_col6 {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_53367_row0_col1, #T_53367_row0_col5, #T_53367_row1_col2, #T_53367_row1_col3, #T_53367_row1_col4, #T_53367_row2_col6 {\n",
              "  text-align: left;\n",
              "  background-color: yellow;\n",
              "}\n",
              "#T_53367_row0_col7, #T_53367_row1_col7, #T_53367_row2_col7, #T_53367_row3_col7, #T_53367_row4_col7, #T_53367_row5_col7, #T_53367_row6_col7, #T_53367_row7_col7, #T_53367_row9_col7, #T_53367_row10_col7, #T_53367_row11_col7, #T_53367_row12_col7, #T_53367_row13_col7, #T_53367_row14_col7, #T_53367_row15_col7, #T_53367_row16_col7, #T_53367_row17_col7, #T_53367_row18_col7, #T_53367_row19_col7 {\n",
              "  text-align: left;\n",
              "  background-color: lightgrey;\n",
              "}\n",
              "#T_53367_row8_col7 {\n",
              "  text-align: left;\n",
              "  background-color: yellow;\n",
              "  background-color: lightgrey;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_53367\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_53367_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_53367_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
              "      <th id=\"T_53367_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
              "      <th id=\"T_53367_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
              "      <th id=\"T_53367_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
              "      <th id=\"T_53367_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
              "      <th id=\"T_53367_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
              "      <th id=\"T_53367_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row0\" class=\"row_heading level0 row0\" >xgboost</th>\n",
              "      <td id=\"T_53367_row0_col0\" class=\"data row0 col0\" >Extreme Gradient Boosting</td>\n",
              "      <td id=\"T_53367_row0_col1\" class=\"data row0 col1\" >77.2377</td>\n",
              "      <td id=\"T_53367_row0_col2\" class=\"data row0 col2\" >26911.8789</td>\n",
              "      <td id=\"T_53367_row0_col3\" class=\"data row0 col3\" >164.0356</td>\n",
              "      <td id=\"T_53367_row0_col4\" class=\"data row0 col4\" >0.3946</td>\n",
              "      <td id=\"T_53367_row0_col5\" class=\"data row0 col5\" >1.3282</td>\n",
              "      <td id=\"T_53367_row0_col6\" class=\"data row0 col6\" >7.1303</td>\n",
              "      <td id=\"T_53367_row0_col7\" class=\"data row0 col7\" >3.8350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row1\" class=\"row_heading level0 row1\" >catboost</th>\n",
              "      <td id=\"T_53367_row1_col0\" class=\"data row1 col0\" >CatBoost Regressor</td>\n",
              "      <td id=\"T_53367_row1_col1\" class=\"data row1 col1\" >78.0802</td>\n",
              "      <td id=\"T_53367_row1_col2\" class=\"data row1 col2\" >25976.1109</td>\n",
              "      <td id=\"T_53367_row1_col3\" class=\"data row1 col3\" >161.1658</td>\n",
              "      <td id=\"T_53367_row1_col4\" class=\"data row1 col4\" >0.4156</td>\n",
              "      <td id=\"T_53367_row1_col5\" class=\"data row1 col5\" >1.3420</td>\n",
              "      <td id=\"T_53367_row1_col6\" class=\"data row1 col6\" >7.0373</td>\n",
              "      <td id=\"T_53367_row1_col7\" class=\"data row1 col7\" >8.2230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row2\" class=\"row_heading level0 row2\" >huber</th>\n",
              "      <td id=\"T_53367_row2_col0\" class=\"data row2 col0\" >Huber Regressor</td>\n",
              "      <td id=\"T_53367_row2_col1\" class=\"data row2 col1\" >84.0494</td>\n",
              "      <td id=\"T_53367_row2_col2\" class=\"data row2 col2\" >45752.0485</td>\n",
              "      <td id=\"T_53367_row2_col3\" class=\"data row2 col3\" >213.8861</td>\n",
              "      <td id=\"T_53367_row2_col4\" class=\"data row2 col4\" >-0.0289</td>\n",
              "      <td id=\"T_53367_row2_col5\" class=\"data row2 col5\" >1.3456</td>\n",
              "      <td id=\"T_53367_row2_col6\" class=\"data row2 col6\" >4.1945</td>\n",
              "      <td id=\"T_53367_row2_col7\" class=\"data row2 col7\" >9.3980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row3\" class=\"row_heading level0 row3\" >lightgbm</th>\n",
              "      <td id=\"T_53367_row3_col0\" class=\"data row3 col0\" >Light Gradient Boosting Machine</td>\n",
              "      <td id=\"T_53367_row3_col1\" class=\"data row3 col1\" >91.3349</td>\n",
              "      <td id=\"T_53367_row3_col2\" class=\"data row3 col2\" >37766.2811</td>\n",
              "      <td id=\"T_53367_row3_col3\" class=\"data row3 col3\" >194.3288</td>\n",
              "      <td id=\"T_53367_row3_col4\" class=\"data row3 col4\" >0.1505</td>\n",
              "      <td id=\"T_53367_row3_col5\" class=\"data row3 col5\" >1.4488</td>\n",
              "      <td id=\"T_53367_row3_col6\" class=\"data row3 col6\" >8.2974</td>\n",
              "      <td id=\"T_53367_row3_col7\" class=\"data row3 col7\" >4.2230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row4\" class=\"row_heading level0 row4\" >et</th>\n",
              "      <td id=\"T_53367_row4_col0\" class=\"data row4 col0\" >Extra Trees Regressor</td>\n",
              "      <td id=\"T_53367_row4_col1\" class=\"data row4 col1\" >95.0498</td>\n",
              "      <td id=\"T_53367_row4_col2\" class=\"data row4 col2\" >40826.4559</td>\n",
              "      <td id=\"T_53367_row4_col3\" class=\"data row4 col3\" >202.0445</td>\n",
              "      <td id=\"T_53367_row4_col4\" class=\"data row4 col4\" >0.0816</td>\n",
              "      <td id=\"T_53367_row4_col5\" class=\"data row4 col5\" >1.4442</td>\n",
              "      <td id=\"T_53367_row4_col6\" class=\"data row4 col6\" >8.7758</td>\n",
              "      <td id=\"T_53367_row4_col7\" class=\"data row4 col7\" >52.8620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row5\" class=\"row_heading level0 row5\" >rf</th>\n",
              "      <td id=\"T_53367_row5_col0\" class=\"data row5 col0\" >Random Forest Regressor</td>\n",
              "      <td id=\"T_53367_row5_col1\" class=\"data row5 col1\" >96.0423</td>\n",
              "      <td id=\"T_53367_row5_col2\" class=\"data row5 col2\" >41073.9277</td>\n",
              "      <td id=\"T_53367_row5_col3\" class=\"data row5 col3\" >202.6578</td>\n",
              "      <td id=\"T_53367_row5_col4\" class=\"data row5 col4\" >0.0761</td>\n",
              "      <td id=\"T_53367_row5_col5\" class=\"data row5 col5\" >1.4503</td>\n",
              "      <td id=\"T_53367_row5_col6\" class=\"data row5 col6\" >9.5445</td>\n",
              "      <td id=\"T_53367_row5_col7\" class=\"data row5 col7\" >99.9780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row6\" class=\"row_heading level0 row6\" >gbr</th>\n",
              "      <td id=\"T_53367_row6_col0\" class=\"data row6 col0\" >Gradient Boosting Regressor</td>\n",
              "      <td id=\"T_53367_row6_col1\" class=\"data row6 col1\" >96.1261</td>\n",
              "      <td id=\"T_53367_row6_col2\" class=\"data row6 col2\" >40646.6879</td>\n",
              "      <td id=\"T_53367_row6_col3\" class=\"data row6 col3\" >201.6049</td>\n",
              "      <td id=\"T_53367_row6_col4\" class=\"data row6 col4\" >0.0857</td>\n",
              "      <td id=\"T_53367_row6_col5\" class=\"data row6 col5\" >1.4891</td>\n",
              "      <td id=\"T_53367_row6_col6\" class=\"data row6 col6\" >8.9363</td>\n",
              "      <td id=\"T_53367_row6_col7\" class=\"data row6 col7\" >79.1650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row7\" class=\"row_heading level0 row7\" >dummy</th>\n",
              "      <td id=\"T_53367_row7_col0\" class=\"data row7 col0\" >Dummy Regressor</td>\n",
              "      <td id=\"T_53367_row7_col1\" class=\"data row7 col1\" >110.1183</td>\n",
              "      <td id=\"T_53367_row7_col2\" class=\"data row7 col2\" >44468.0480</td>\n",
              "      <td id=\"T_53367_row7_col3\" class=\"data row7 col3\" >210.8644</td>\n",
              "      <td id=\"T_53367_row7_col4\" class=\"data row7 col4\" >-0.0000</td>\n",
              "      <td id=\"T_53367_row7_col5\" class=\"data row7 col5\" >1.7870</td>\n",
              "      <td id=\"T_53367_row7_col6\" class=\"data row7 col6\" >12.2057</td>\n",
              "      <td id=\"T_53367_row7_col7\" class=\"data row7 col7\" >2.5600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row8\" class=\"row_heading level0 row8\" >ridge</th>\n",
              "      <td id=\"T_53367_row8_col0\" class=\"data row8 col0\" >Ridge Regression</td>\n",
              "      <td id=\"T_53367_row8_col1\" class=\"data row8 col1\" >112.1675</td>\n",
              "      <td id=\"T_53367_row8_col2\" class=\"data row8 col2\" >42056.7184</td>\n",
              "      <td id=\"T_53367_row8_col3\" class=\"data row8 col3\" >205.0715</td>\n",
              "      <td id=\"T_53367_row8_col4\" class=\"data row8 col4\" >0.0541</td>\n",
              "      <td id=\"T_53367_row8_col5\" class=\"data row8 col5\" >1.7062</td>\n",
              "      <td id=\"T_53367_row8_col6\" class=\"data row8 col6\" >11.9721</td>\n",
              "      <td id=\"T_53367_row8_col7\" class=\"data row8 col7\" >2.4260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row9\" class=\"row_heading level0 row9\" >lr</th>\n",
              "      <td id=\"T_53367_row9_col0\" class=\"data row9 col0\" >Linear Regression</td>\n",
              "      <td id=\"T_53367_row9_col1\" class=\"data row9 col1\" >112.1713</td>\n",
              "      <td id=\"T_53367_row9_col2\" class=\"data row9 col2\" >42055.9042</td>\n",
              "      <td id=\"T_53367_row9_col3\" class=\"data row9 col3\" >205.0696</td>\n",
              "      <td id=\"T_53367_row9_col4\" class=\"data row9 col4\" >0.0541</td>\n",
              "      <td id=\"T_53367_row9_col5\" class=\"data row9 col5\" >1.7062</td>\n",
              "      <td id=\"T_53367_row9_col6\" class=\"data row9 col6\" >11.9704</td>\n",
              "      <td id=\"T_53367_row9_col7\" class=\"data row9 col7\" >3.4840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row10\" class=\"row_heading level0 row10\" >br</th>\n",
              "      <td id=\"T_53367_row10_col0\" class=\"data row10 col0\" >Bayesian Ridge</td>\n",
              "      <td id=\"T_53367_row10_col1\" class=\"data row10 col1\" >112.1820</td>\n",
              "      <td id=\"T_53367_row10_col2\" class=\"data row10 col2\" >42048.3666</td>\n",
              "      <td id=\"T_53367_row10_col3\" class=\"data row10 col3\" >205.0512</td>\n",
              "      <td id=\"T_53367_row10_col4\" class=\"data row10 col4\" >0.0542</td>\n",
              "      <td id=\"T_53367_row10_col5\" class=\"data row10 col5\" >1.7060</td>\n",
              "      <td id=\"T_53367_row10_col6\" class=\"data row10 col6\" >11.9725</td>\n",
              "      <td id=\"T_53367_row10_col7\" class=\"data row10 col7\" >3.4430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row11\" class=\"row_heading level0 row11\" >lasso</th>\n",
              "      <td id=\"T_53367_row11_col0\" class=\"data row11 col0\" >Lasso Regression</td>\n",
              "      <td id=\"T_53367_row11_col1\" class=\"data row11 col1\" >112.5026</td>\n",
              "      <td id=\"T_53367_row11_col2\" class=\"data row11 col2\" >42077.2569</td>\n",
              "      <td id=\"T_53367_row11_col3\" class=\"data row11 col3\" >205.1215</td>\n",
              "      <td id=\"T_53367_row11_col4\" class=\"data row11 col4\" >0.0536</td>\n",
              "      <td id=\"T_53367_row11_col5\" class=\"data row11 col5\" >1.7061</td>\n",
              "      <td id=\"T_53367_row11_col6\" class=\"data row11 col6\" >11.8887</td>\n",
              "      <td id=\"T_53367_row11_col7\" class=\"data row11 col7\" >15.1410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row12\" class=\"row_heading level0 row12\" >llar</th>\n",
              "      <td id=\"T_53367_row12_col0\" class=\"data row12 col0\" >Lasso Least Angle Regression</td>\n",
              "      <td id=\"T_53367_row12_col1\" class=\"data row12 col1\" >112.5028</td>\n",
              "      <td id=\"T_53367_row12_col2\" class=\"data row12 col2\" >42077.3329</td>\n",
              "      <td id=\"T_53367_row12_col3\" class=\"data row12 col3\" >205.1217</td>\n",
              "      <td id=\"T_53367_row12_col4\" class=\"data row12 col4\" >0.0536</td>\n",
              "      <td id=\"T_53367_row12_col5\" class=\"data row12 col5\" >1.7061</td>\n",
              "      <td id=\"T_53367_row12_col6\" class=\"data row12 col6\" >11.8889</td>\n",
              "      <td id=\"T_53367_row12_col7\" class=\"data row12 col7\" >2.8480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row13\" class=\"row_heading level0 row13\" >en</th>\n",
              "      <td id=\"T_53367_row13_col0\" class=\"data row13 col0\" >Elastic Net</td>\n",
              "      <td id=\"T_53367_row13_col1\" class=\"data row13 col1\" >112.7797</td>\n",
              "      <td id=\"T_53367_row13_col2\" class=\"data row13 col2\" >41990.1820</td>\n",
              "      <td id=\"T_53367_row13_col3\" class=\"data row13 col3\" >204.9093</td>\n",
              "      <td id=\"T_53367_row13_col4\" class=\"data row13 col4\" >0.0556</td>\n",
              "      <td id=\"T_53367_row13_col5\" class=\"data row13 col5\" >1.7084</td>\n",
              "      <td id=\"T_53367_row13_col6\" class=\"data row13 col6\" >11.9042</td>\n",
              "      <td id=\"T_53367_row13_col7\" class=\"data row13 col7\" >16.6350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row14\" class=\"row_heading level0 row14\" >omp</th>\n",
              "      <td id=\"T_53367_row14_col0\" class=\"data row14 col0\" >Orthogonal Matching Pursuit</td>\n",
              "      <td id=\"T_53367_row14_col1\" class=\"data row14 col1\" >112.9967</td>\n",
              "      <td id=\"T_53367_row14_col2\" class=\"data row14 col2\" >42084.5944</td>\n",
              "      <td id=\"T_53367_row14_col3\" class=\"data row14 col3\" >205.1395</td>\n",
              "      <td id=\"T_53367_row14_col4\" class=\"data row14 col4\" >0.0534</td>\n",
              "      <td id=\"T_53367_row14_col5\" class=\"data row14 col5\" >1.7091</td>\n",
              "      <td id=\"T_53367_row14_col6\" class=\"data row14 col6\" >11.8424</td>\n",
              "      <td id=\"T_53367_row14_col7\" class=\"data row14 col7\" >2.6530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row15\" class=\"row_heading level0 row15\" >knn</th>\n",
              "      <td id=\"T_53367_row15_col0\" class=\"data row15 col0\" >K Neighbors Regressor</td>\n",
              "      <td id=\"T_53367_row15_col1\" class=\"data row15 col1\" >114.7922</td>\n",
              "      <td id=\"T_53367_row15_col2\" class=\"data row15 col2\" >50546.6234</td>\n",
              "      <td id=\"T_53367_row15_col3\" class=\"data row15 col3\" >224.8175</td>\n",
              "      <td id=\"T_53367_row15_col4\" class=\"data row15 col4\" >-0.1369</td>\n",
              "      <td id=\"T_53367_row15_col5\" class=\"data row15 col5\" >1.7004</td>\n",
              "      <td id=\"T_53367_row15_col6\" class=\"data row15 col6\" >10.8136</td>\n",
              "      <td id=\"T_53367_row15_col7\" class=\"data row15 col7\" >5.9470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row16\" class=\"row_heading level0 row16\" >dt</th>\n",
              "      <td id=\"T_53367_row16_col0\" class=\"data row16 col0\" >Decision Tree Regressor</td>\n",
              "      <td id=\"T_53367_row16_col1\" class=\"data row16 col1\" >116.7298</td>\n",
              "      <td id=\"T_53367_row16_col2\" class=\"data row16 col2\" >68453.5513</td>\n",
              "      <td id=\"T_53367_row16_col3\" class=\"data row16 col3\" >261.5985</td>\n",
              "      <td id=\"T_53367_row16_col4\" class=\"data row16 col4\" >-0.5399</td>\n",
              "      <td id=\"T_53367_row16_col5\" class=\"data row16 col5\" >1.6252</td>\n",
              "      <td id=\"T_53367_row16_col6\" class=\"data row16 col6\" >9.3141</td>\n",
              "      <td id=\"T_53367_row16_col7\" class=\"data row16 col7\" >9.4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row17\" class=\"row_heading level0 row17\" >ada</th>\n",
              "      <td id=\"T_53367_row17_col0\" class=\"data row17 col0\" >AdaBoost Regressor</td>\n",
              "      <td id=\"T_53367_row17_col1\" class=\"data row17 col1\" >162.0218</td>\n",
              "      <td id=\"T_53367_row17_col2\" class=\"data row17 col2\" >72753.3292</td>\n",
              "      <td id=\"T_53367_row17_col3\" class=\"data row17 col3\" >269.6142</td>\n",
              "      <td id=\"T_53367_row17_col4\" class=\"data row17 col4\" >-0.6377</td>\n",
              "      <td id=\"T_53367_row17_col5\" class=\"data row17 col5\" >1.9853</td>\n",
              "      <td id=\"T_53367_row17_col6\" class=\"data row17 col6\" >21.7663</td>\n",
              "      <td id=\"T_53367_row17_col7\" class=\"data row17 col7\" >20.1560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row18\" class=\"row_heading level0 row18\" >par</th>\n",
              "      <td id=\"T_53367_row18_col0\" class=\"data row18 col0\" >Passive Aggressive Regressor</td>\n",
              "      <td id=\"T_53367_row18_col1\" class=\"data row18 col1\" >168.1471</td>\n",
              "      <td id=\"T_53367_row18_col2\" class=\"data row18 col2\" >86985.9743</td>\n",
              "      <td id=\"T_53367_row18_col3\" class=\"data row18 col3\" >278.3824</td>\n",
              "      <td id=\"T_53367_row18_col4\" class=\"data row18 col4\" >-0.9730</td>\n",
              "      <td id=\"T_53367_row18_col5\" class=\"data row18 col5\" >1.9158</td>\n",
              "      <td id=\"T_53367_row18_col6\" class=\"data row18 col6\" >16.5375</td>\n",
              "      <td id=\"T_53367_row18_col7\" class=\"data row18 col7\" >3.7460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53367_level0_row19\" class=\"row_heading level0 row19\" >lar</th>\n",
              "      <td id=\"T_53367_row19_col0\" class=\"data row19 col0\" >Least Angle Regression</td>\n",
              "      <td id=\"T_53367_row19_col1\" class=\"data row19 col1\" >470497716466708863166584389441655010523800901853440045864046875792666705840959686950146496710850618175616951176654307532948934370880124359839803794003503217988416032922993162243143101723052258924512020670346054712383781208064.0000</td>\n",
              "      <td id=\"T_53367_row19_col2\" class=\"data row19 col2\" >inf</td>\n",
              "      <td id=\"T_53367_row19_col3\" class=\"data row19 col3\" >inf</td>\n",
              "      <td id=\"T_53367_row19_col4\" class=\"data row19 col4\" >-inf</td>\n",
              "      <td id=\"T_53367_row19_col5\" class=\"data row19 col5\" >68.8854</td>\n",
              "      <td id=\"T_53367_row19_col6\" class=\"data row19 col6\" >58150931681669956342782104088623495658546750758598032494105430254618274180973181441422561040073099615188660621951599344706774759326277905678431473034352322537356670607656123243547606558880015581333960252485544179425685274624.0000</td>\n",
              "      <td id=\"T_53367_row19_col7\" class=\"data row19 col7\" >2.9420</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1f8c15281c0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=-1,\n",
              "             num_parallel_tree=None, random_state=313, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=-1,\n",
              "             num_parallel_tree=None, random_state=313, ...)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "XGBRegressor(base_score=None, booster='gbtree', callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=-1,\n",
              "             num_parallel_tree=None, random_state=313, ...)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best = compare_models(n_select=1, sort='MAE')\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ae1b5_row10_col0, #T_ae1b5_row10_col1, #T_ae1b5_row10_col2, #T_ae1b5_row10_col3, #T_ae1b5_row10_col4, #T_ae1b5_row10_col5 {\n",
              "  background: yellow;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ae1b5\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ae1b5_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
              "      <th id=\"T_ae1b5_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
              "      <th id=\"T_ae1b5_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
              "      <th id=\"T_ae1b5_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
              "      <th id=\"T_ae1b5_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
              "      <th id=\"T_ae1b5_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Fold</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_ae1b5_row0_col0\" class=\"data row0 col0\" >78.5710</td>\n",
              "      <td id=\"T_ae1b5_row0_col1\" class=\"data row0 col1\" >27194.3716</td>\n",
              "      <td id=\"T_ae1b5_row0_col2\" class=\"data row0 col2\" >164.9072</td>\n",
              "      <td id=\"T_ae1b5_row0_col3\" class=\"data row0 col3\" >0.4222</td>\n",
              "      <td id=\"T_ae1b5_row0_col4\" class=\"data row0 col4\" >1.3286</td>\n",
              "      <td id=\"T_ae1b5_row0_col5\" class=\"data row0 col5\" >6.2403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_ae1b5_row1_col0\" class=\"data row1 col0\" >79.4191</td>\n",
              "      <td id=\"T_ae1b5_row1_col1\" class=\"data row1 col1\" >27959.4764</td>\n",
              "      <td id=\"T_ae1b5_row1_col2\" class=\"data row1 col2\" >167.2109</td>\n",
              "      <td id=\"T_ae1b5_row1_col3\" class=\"data row1 col3\" >0.3765</td>\n",
              "      <td id=\"T_ae1b5_row1_col4\" class=\"data row1 col4\" >1.3506</td>\n",
              "      <td id=\"T_ae1b5_row1_col5\" class=\"data row1 col5\" >6.6535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_ae1b5_row2_col0\" class=\"data row2 col0\" >79.5921</td>\n",
              "      <td id=\"T_ae1b5_row2_col1\" class=\"data row2 col1\" >27532.2625</td>\n",
              "      <td id=\"T_ae1b5_row2_col2\" class=\"data row2 col2\" >165.9285</td>\n",
              "      <td id=\"T_ae1b5_row2_col3\" class=\"data row2 col3\" >0.4168</td>\n",
              "      <td id=\"T_ae1b5_row2_col4\" class=\"data row2 col4\" >1.3458</td>\n",
              "      <td id=\"T_ae1b5_row2_col5\" class=\"data row2 col5\" >7.4735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_ae1b5_row3_col0\" class=\"data row3 col0\" >77.0645</td>\n",
              "      <td id=\"T_ae1b5_row3_col1\" class=\"data row3 col1\" >25274.2824</td>\n",
              "      <td id=\"T_ae1b5_row3_col2\" class=\"data row3 col2\" >158.9789</td>\n",
              "      <td id=\"T_ae1b5_row3_col3\" class=\"data row3 col3\" >0.4093</td>\n",
              "      <td id=\"T_ae1b5_row3_col4\" class=\"data row3 col4\" >1.3534</td>\n",
              "      <td id=\"T_ae1b5_row3_col5\" class=\"data row3 col5\" >7.1385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_ae1b5_row4_col0\" class=\"data row4 col0\" >78.5889</td>\n",
              "      <td id=\"T_ae1b5_row4_col1\" class=\"data row4 col1\" >27516.9818</td>\n",
              "      <td id=\"T_ae1b5_row4_col2\" class=\"data row4 col2\" >165.8824</td>\n",
              "      <td id=\"T_ae1b5_row4_col3\" class=\"data row4 col3\" >0.3994</td>\n",
              "      <td id=\"T_ae1b5_row4_col4\" class=\"data row4 col4\" >1.3355</td>\n",
              "      <td id=\"T_ae1b5_row4_col5\" class=\"data row4 col5\" >7.4442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_ae1b5_row5_col0\" class=\"data row5 col0\" >79.9769</td>\n",
              "      <td id=\"T_ae1b5_row5_col1\" class=\"data row5 col1\" >28196.6505</td>\n",
              "      <td id=\"T_ae1b5_row5_col2\" class=\"data row5 col2\" >167.9186</td>\n",
              "      <td id=\"T_ae1b5_row5_col3\" class=\"data row5 col3\" >0.4079</td>\n",
              "      <td id=\"T_ae1b5_row5_col4\" class=\"data row5 col4\" >1.3314</td>\n",
              "      <td id=\"T_ae1b5_row5_col5\" class=\"data row5 col5\" >7.3918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_ae1b5_row6_col0\" class=\"data row6 col0\" >78.0806</td>\n",
              "      <td id=\"T_ae1b5_row6_col1\" class=\"data row6 col1\" >25307.2327</td>\n",
              "      <td id=\"T_ae1b5_row6_col2\" class=\"data row6 col2\" >159.0825</td>\n",
              "      <td id=\"T_ae1b5_row6_col3\" class=\"data row6 col3\" >0.4239</td>\n",
              "      <td id=\"T_ae1b5_row6_col4\" class=\"data row6 col4\" >1.3482</td>\n",
              "      <td id=\"T_ae1b5_row6_col5\" class=\"data row6 col5\" >6.6870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_ae1b5_row7_col0\" class=\"data row7 col0\" >75.8661</td>\n",
              "      <td id=\"T_ae1b5_row7_col1\" class=\"data row7 col1\" >24517.9706</td>\n",
              "      <td id=\"T_ae1b5_row7_col2\" class=\"data row7 col2\" >156.5822</td>\n",
              "      <td id=\"T_ae1b5_row7_col3\" class=\"data row7 col3\" >0.4344</td>\n",
              "      <td id=\"T_ae1b5_row7_col4\" class=\"data row7 col4\" >1.3338</td>\n",
              "      <td id=\"T_ae1b5_row7_col5\" class=\"data row7 col5\" >6.4500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_ae1b5_row8_col0\" class=\"data row8 col0\" >76.3029</td>\n",
              "      <td id=\"T_ae1b5_row8_col1\" class=\"data row8 col1\" >23725.4392</td>\n",
              "      <td id=\"T_ae1b5_row8_col2\" class=\"data row8 col2\" >154.0306</td>\n",
              "      <td id=\"T_ae1b5_row8_col3\" class=\"data row8 col3\" >0.4424</td>\n",
              "      <td id=\"T_ae1b5_row8_col4\" class=\"data row8 col4\" >1.3377</td>\n",
              "      <td id=\"T_ae1b5_row8_col5\" class=\"data row8 col5\" >6.4947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_ae1b5_row9_col0\" class=\"data row9 col0\" >77.1638</td>\n",
              "      <td id=\"T_ae1b5_row9_col1\" class=\"data row9 col1\" >24175.3958</td>\n",
              "      <td id=\"T_ae1b5_row9_col2\" class=\"data row9 col2\" >155.4844</td>\n",
              "      <td id=\"T_ae1b5_row9_col3\" class=\"data row9 col3\" >0.4146</td>\n",
              "      <td id=\"T_ae1b5_row9_col4\" class=\"data row9 col4\" >1.3604</td>\n",
              "      <td id=\"T_ae1b5_row9_col5\" class=\"data row9 col5\" >7.3339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
              "      <td id=\"T_ae1b5_row10_col0\" class=\"data row10 col0\" >78.0626</td>\n",
              "      <td id=\"T_ae1b5_row10_col1\" class=\"data row10 col1\" >26140.0063</td>\n",
              "      <td id=\"T_ae1b5_row10_col2\" class=\"data row10 col2\" >161.6006</td>\n",
              "      <td id=\"T_ae1b5_row10_col3\" class=\"data row10 col3\" >0.4147</td>\n",
              "      <td id=\"T_ae1b5_row10_col4\" class=\"data row10 col4\" >1.3425</td>\n",
              "      <td id=\"T_ae1b5_row10_col5\" class=\"data row10 col5\" >6.9307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ae1b5_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
              "      <td id=\"T_ae1b5_row11_col0\" class=\"data row11 col0\" >1.3455</td>\n",
              "      <td id=\"T_ae1b5_row11_col1\" class=\"data row11 col1\" >1620.1914</td>\n",
              "      <td id=\"T_ae1b5_row11_col2\" class=\"data row11 col2\" >5.0250</td>\n",
              "      <td id=\"T_ae1b5_row11_col3\" class=\"data row11 col3\" >0.0175</td>\n",
              "      <td id=\"T_ae1b5_row11_col4\" class=\"data row11 col4\" >0.0101</td>\n",
              "      <td id=\"T_ae1b5_row11_col5\" class=\"data row11 col5\" >0.4484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1f8d5f52fd0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cat = create_model(\"catboost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_d63ab_row10_col0, #T_d63ab_row10_col1, #T_d63ab_row10_col2, #T_d63ab_row10_col3, #T_d63ab_row10_col4, #T_d63ab_row10_col5 {\n",
              "  background: yellow;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_d63ab\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_d63ab_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
              "      <th id=\"T_d63ab_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
              "      <th id=\"T_d63ab_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
              "      <th id=\"T_d63ab_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
              "      <th id=\"T_d63ab_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
              "      <th id=\"T_d63ab_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Fold</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_d63ab_row0_col0\" class=\"data row0 col0\" >65.9738</td>\n",
              "      <td id=\"T_d63ab_row0_col1\" class=\"data row0 col1\" >19343.5614</td>\n",
              "      <td id=\"T_d63ab_row0_col2\" class=\"data row0 col2\" >139.0811</td>\n",
              "      <td id=\"T_d63ab_row0_col3\" class=\"data row0 col3\" >0.5890</td>\n",
              "      <td id=\"T_d63ab_row0_col4\" class=\"data row0 col4\" >1.2451</td>\n",
              "      <td id=\"T_d63ab_row0_col5\" class=\"data row0 col5\" >5.3571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_d63ab_row1_col0\" class=\"data row1 col0\" >66.4681</td>\n",
              "      <td id=\"T_d63ab_row1_col1\" class=\"data row1 col1\" >19241.4675</td>\n",
              "      <td id=\"T_d63ab_row1_col2\" class=\"data row1 col2\" >138.7136</td>\n",
              "      <td id=\"T_d63ab_row1_col3\" class=\"data row1 col3\" >0.5709</td>\n",
              "      <td id=\"T_d63ab_row1_col4\" class=\"data row1 col4\" >1.2778</td>\n",
              "      <td id=\"T_d63ab_row1_col5\" class=\"data row1 col5\" >5.8650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_d63ab_row2_col0\" class=\"data row2 col0\" >65.6116</td>\n",
              "      <td id=\"T_d63ab_row2_col1\" class=\"data row2 col1\" >19148.8556</td>\n",
              "      <td id=\"T_d63ab_row2_col2\" class=\"data row2 col2\" >138.3794</td>\n",
              "      <td id=\"T_d63ab_row2_col3\" class=\"data row2 col3\" >0.5944</td>\n",
              "      <td id=\"T_d63ab_row2_col4\" class=\"data row2 col4\" >1.2603</td>\n",
              "      <td id=\"T_d63ab_row2_col5\" class=\"data row2 col5\" >5.8653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_d63ab_row3_col0\" class=\"data row3 col0\" >66.0880</td>\n",
              "      <td id=\"T_d63ab_row3_col1\" class=\"data row3 col1\" >18530.2779</td>\n",
              "      <td id=\"T_d63ab_row3_col2\" class=\"data row3 col2\" >136.1260</td>\n",
              "      <td id=\"T_d63ab_row3_col3\" class=\"data row3 col3\" >0.5669</td>\n",
              "      <td id=\"T_d63ab_row3_col4\" class=\"data row3 col4\" >1.2882</td>\n",
              "      <td id=\"T_d63ab_row3_col5\" class=\"data row3 col5\" >5.9141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_d63ab_row4_col0\" class=\"data row4 col0\" >66.2902</td>\n",
              "      <td id=\"T_d63ab_row4_col1\" class=\"data row4 col1\" >19988.4980</td>\n",
              "      <td id=\"T_d63ab_row4_col2\" class=\"data row4 col2\" >141.3807</td>\n",
              "      <td id=\"T_d63ab_row4_col3\" class=\"data row4 col3\" >0.5637</td>\n",
              "      <td id=\"T_d63ab_row4_col4\" class=\"data row4 col4\" >1.2574</td>\n",
              "      <td id=\"T_d63ab_row4_col5\" class=\"data row4 col5\" >6.3566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_d63ab_row5_col0\" class=\"data row5 col0\" >68.6930</td>\n",
              "      <td id=\"T_d63ab_row5_col1\" class=\"data row5 col1\" >20331.2303</td>\n",
              "      <td id=\"T_d63ab_row5_col2\" class=\"data row5 col2\" >142.5876</td>\n",
              "      <td id=\"T_d63ab_row5_col3\" class=\"data row5 col3\" >0.5731</td>\n",
              "      <td id=\"T_d63ab_row5_col4\" class=\"data row5 col4\" >1.2665</td>\n",
              "      <td id=\"T_d63ab_row5_col5\" class=\"data row5 col5\" >6.4792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_d63ab_row6_col0\" class=\"data row6 col0\" >65.8405</td>\n",
              "      <td id=\"T_d63ab_row6_col1\" class=\"data row6 col1\" >18213.8855</td>\n",
              "      <td id=\"T_d63ab_row6_col2\" class=\"data row6 col2\" >134.9588</td>\n",
              "      <td id=\"T_d63ab_row6_col3\" class=\"data row6 col3\" >0.5854</td>\n",
              "      <td id=\"T_d63ab_row6_col4\" class=\"data row6 col4\" >1.2697</td>\n",
              "      <td id=\"T_d63ab_row6_col5\" class=\"data row6 col5\" >5.8856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_d63ab_row7_col0\" class=\"data row7 col0\" >63.3068</td>\n",
              "      <td id=\"T_d63ab_row7_col1\" class=\"data row7 col1\" >17203.9212</td>\n",
              "      <td id=\"T_d63ab_row7_col2\" class=\"data row7 col2\" >131.1637</td>\n",
              "      <td id=\"T_d63ab_row7_col3\" class=\"data row7 col3\" >0.6031</td>\n",
              "      <td id=\"T_d63ab_row7_col4\" class=\"data row7 col4\" >1.2486</td>\n",
              "      <td id=\"T_d63ab_row7_col5\" class=\"data row7 col5\" >5.1833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_d63ab_row8_col0\" class=\"data row8 col0\" >65.2960</td>\n",
              "      <td id=\"T_d63ab_row8_col1\" class=\"data row8 col1\" >17623.2784</td>\n",
              "      <td id=\"T_d63ab_row8_col2\" class=\"data row8 col2\" >132.7527</td>\n",
              "      <td id=\"T_d63ab_row8_col3\" class=\"data row8 col3\" >0.5858</td>\n",
              "      <td id=\"T_d63ab_row8_col4\" class=\"data row8 col4\" >1.2753</td>\n",
              "      <td id=\"T_d63ab_row8_col5\" class=\"data row8 col5\" >5.6538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_d63ab_row9_col0\" class=\"data row9 col0\" >65.0465</td>\n",
              "      <td id=\"T_d63ab_row9_col1\" class=\"data row9 col1\" >17872.0185</td>\n",
              "      <td id=\"T_d63ab_row9_col2\" class=\"data row9 col2\" >133.6863</td>\n",
              "      <td id=\"T_d63ab_row9_col3\" class=\"data row9 col3\" >0.5672</td>\n",
              "      <td id=\"T_d63ab_row9_col4\" class=\"data row9 col4\" >1.2802</td>\n",
              "      <td id=\"T_d63ab_row9_col5\" class=\"data row9 col5\" >6.0775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
              "      <td id=\"T_d63ab_row10_col0\" class=\"data row10 col0\" >65.8615</td>\n",
              "      <td id=\"T_d63ab_row10_col1\" class=\"data row10 col1\" >18749.6994</td>\n",
              "      <td id=\"T_d63ab_row10_col2\" class=\"data row10 col2\" >136.8830</td>\n",
              "      <td id=\"T_d63ab_row10_col3\" class=\"data row10 col3\" >0.5800</td>\n",
              "      <td id=\"T_d63ab_row10_col4\" class=\"data row10 col4\" >1.2669</td>\n",
              "      <td id=\"T_d63ab_row10_col5\" class=\"data row10 col5\" >5.8638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d63ab_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
              "      <td id=\"T_d63ab_row11_col0\" class=\"data row11 col0\" >1.2731</td>\n",
              "      <td id=\"T_d63ab_row11_col1\" class=\"data row11 col1\" >977.4209</td>\n",
              "      <td id=\"T_d63ab_row11_col2\" class=\"data row11 col2\" >3.5701</td>\n",
              "      <td id=\"T_d63ab_row11_col3\" class=\"data row11 col3\" >0.0127</td>\n",
              "      <td id=\"T_d63ab_row11_col4\" class=\"data row11 col4\" >0.0133</td>\n",
              "      <td id=\"T_d63ab_row11_col5\" class=\"data row11 col5\" >0.3787</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1f8c8099fa0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostRegressor at 0x1f8d521f5e0>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cat_t = tune_model(cat, optimize='MAE', n_iter=100) #lgbm\n",
        "cat_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Initiated</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>04:03:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Status</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Searching Hyperparameters</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estimator</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>CatBoost Regressor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         \n",
              "                                                                         \n",
              "Initiated  . . . . . . . . . . . . . . . . . .                   04:03:18\n",
              "Status     . . . . . . . . . . . . . . . . . .  Searching Hyperparameters\n",
              "Estimator  . . . . . . . . . . . . . . . . . .         CatBoost Regressor"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6e0a3ffb85c40cfb3e555da78efcc14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v2.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m best_tuned \u001b[39m=\u001b[39m tune_model(cat_t, optimize\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m) \u001b[39m#lgbm\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m best_tuned\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\functional.py:1197\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1007\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1026\u001b[0m ):\n\u001b[0;32m   1027\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \n\u001b[0;32m   1195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39mtune_model(\n\u001b[0;32m   1198\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1199\u001b[0m         fold\u001b[39m=\u001b[39mfold,\n\u001b[0;32m   1200\u001b[0m         \u001b[39mround\u001b[39m\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m,\n\u001b[0;32m   1201\u001b[0m         n_iter\u001b[39m=\u001b[39mn_iter,\n\u001b[0;32m   1202\u001b[0m         custom_grid\u001b[39m=\u001b[39mcustom_grid,\n\u001b[0;32m   1203\u001b[0m         optimize\u001b[39m=\u001b[39moptimize,\n\u001b[0;32m   1204\u001b[0m         custom_scorer\u001b[39m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1205\u001b[0m         search_library\u001b[39m=\u001b[39msearch_library,\n\u001b[0;32m   1206\u001b[0m         search_algorithm\u001b[39m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1207\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m   1208\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1209\u001b[0m         choose_better\u001b[39m=\u001b[39mchoose_better,\n\u001b[0;32m   1210\u001b[0m         fit_kwargs\u001b[39m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1211\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m   1212\u001b[0m         return_tuner\u001b[39m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1213\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   1214\u001b[0m         tuner_verbose\u001b[39m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1215\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1216\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1217\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\oop.py:1499\u001b[0m, in \u001b[0;36mRegressionExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1308\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1309\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1328\u001b[0m ):\n\u001b[0;32m   1329\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \n\u001b[0;32m   1497\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1499\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtune_model(\n\u001b[0;32m   1500\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1501\u001b[0m         fold\u001b[39m=\u001b[39mfold,\n\u001b[0;32m   1502\u001b[0m         \u001b[39mround\u001b[39m\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m,\n\u001b[0;32m   1503\u001b[0m         n_iter\u001b[39m=\u001b[39mn_iter,\n\u001b[0;32m   1504\u001b[0m         custom_grid\u001b[39m=\u001b[39mcustom_grid,\n\u001b[0;32m   1505\u001b[0m         optimize\u001b[39m=\u001b[39moptimize,\n\u001b[0;32m   1506\u001b[0m         custom_scorer\u001b[39m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1507\u001b[0m         search_library\u001b[39m=\u001b[39msearch_library,\n\u001b[0;32m   1508\u001b[0m         search_algorithm\u001b[39m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1509\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m   1510\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1511\u001b[0m         choose_better\u001b[39m=\u001b[39mchoose_better,\n\u001b[0;32m   1512\u001b[0m         fit_kwargs\u001b[39m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1513\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m   1514\u001b[0m         return_tuner\u001b[39m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1515\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   1516\u001b[0m         tuner_verbose\u001b[39m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1517\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1518\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1519\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:2665\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2658\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.sample_without_replacement\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2659\u001b[0m         pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_sample_without_replacement,\n\u001b[0;32m   2660\u001b[0m     ):\n\u001b[0;32m   2661\u001b[0m         \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2662\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.ParameterGrid.__getitem__\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2663\u001b[0m             pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_ParameterGrid_getitem,\n\u001b[0;32m   2664\u001b[0m         ):\n\u001b[1;32m-> 2665\u001b[0m             model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m   2666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2667\u001b[0m     model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1769\u001b[0m         ParameterSampler(\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1771\u001b[0m         )\n\u001b[0;32m   1772\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pipeline.py:262\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[0;32m    261\u001b[0m     fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 262\u001b[0m     X, y, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    264\u001b[0m     \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    265\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pipeline.py:245\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[39m# Fit or load the current transformer from cache\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     fitted_transformer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_memory_fit(\n\u001b[0;32m    239\u001b[0m         transformer\u001b[39m=\u001b[39mcloned,\n\u001b[0;32m    240\u001b[0m         X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps\u001b[39m.\u001b[39mget(name, {}),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 245\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_memory_transform(\n\u001b[0;32m    246\u001b[0m         transformer\u001b[39m=\u001b[39;49mfitted_transformer,\n\u001b[0;32m    247\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    248\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    249\u001b[0m     )\n\u001b[0;32m    251\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39m# transformer (necessary when loading from the cache)\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pipeline.py:77\u001b[0m, in \u001b[0;36m_transform_one\u001b[1;34m(transformer, X, y)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m signature(transformer\u001b[39m.\u001b[39mtransform)\u001b[39m.\u001b[39mparameters:\n\u001b[0;32m     76\u001b[0m     args\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m---> 77\u001b[0m output \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mtransform(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m     80\u001b[0m     X, y \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m], output[\u001b[39m1\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\preprocess\\transformers.py:255\u001b[0m, in \u001b[0;36mTransformerWrapper.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m transform_params:\n\u001b[0;32m    253\u001b[0m         \u001b[39mreturn\u001b[39;00m X, y\n\u001b[1;32m--> 255\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mtransform(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    257\u001b[0m \u001b[39m# Transform can return X, y or both\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, \u001b[39mtuple\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\category_encoders\\utils.py:484\u001b[0m, in \u001b[0;36mUnsupervisedTransformerMixin.transform\u001b[1;34m(self, X, override_return_df)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcols):\n\u001b[0;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n\u001b[1;32m--> 484\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X)\n\u001b[0;32m    485\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drop_invariants(X, override_return_df)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\category_encoders\\one_hot.py:195\u001b[0m, in \u001b[0;36mOneHotEncoder._transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[39mif\u001b[39;00m X[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcols]\u001b[39m.\u001b[39misin([\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39many():\n\u001b[0;32m    193\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mColumns to be encoded can not contain new values\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dummies(X)\n\u001b[0;32m    196\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\category_encoders\\one_hot.py:269\u001b[0m, in \u001b[0;36mOneHotEncoder.get_dummies\u001b[1;34m(self, X_in)\u001b[0m\n\u001b[0;32m    267\u001b[0m base_df \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mreindex(X[col]\u001b[39m.\u001b[39mfillna(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m    268\u001b[0m base_df \u001b[39m=\u001b[39m base_df\u001b[39m.\u001b[39mset_index(X\u001b[39m.\u001b[39mindex)\n\u001b[1;32m--> 269\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([base_df, X], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    271\u001b[0m old_column_index \u001b[39m=\u001b[39m cols\u001b[39m.\u001b[39mindex(col)\n\u001b[0;32m    272\u001b[0m cols[old_column_index: old_column_index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mcolumns\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m     objs,\n\u001b[0;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy:\n\u001b[0;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pandas\\core\\internals\\concat.py:212\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    210\u001b[0m values \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mvalues\n\u001b[0;32m    211\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m--> 212\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    213\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mview()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_tuned = tune_model(cat_t, optimize='MAE', n_iter=100) #lgbm\n",
        "best_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'CatBoostRegressor' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v2.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stack_lr \u001b[39m=\u001b[39m stack_models(cat_t, optimize\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m, choose_better\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v2.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stack_lr\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\functional.py:1530\u001b[0m, in \u001b[0;36mstack_models\u001b[1;34m(estimator_list, meta_model, meta_model_fold, fold, round, restack, choose_better, optimize, fit_kwargs, groups, verbose, return_train_score)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack_models\u001b[39m(\n\u001b[0;32m   1431\u001b[0m     estimator_list: \u001b[39mlist\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1442\u001b[0m     return_train_score: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1443\u001b[0m ):\n\u001b[0;32m   1444\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m \u001b[39m    This function trains a meta model over select estimators passed in\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39m    the ``estimator_list`` parameter. The output of this function is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1527\u001b[0m \n\u001b[0;32m   1528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1530\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39;49mstack_models(\n\u001b[0;32m   1531\u001b[0m         estimator_list\u001b[39m=\u001b[39;49mestimator_list,\n\u001b[0;32m   1532\u001b[0m         meta_model\u001b[39m=\u001b[39;49mmeta_model,\n\u001b[0;32m   1533\u001b[0m         meta_model_fold\u001b[39m=\u001b[39;49mmeta_model_fold,\n\u001b[0;32m   1534\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m   1535\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[0;32m   1536\u001b[0m         restack\u001b[39m=\u001b[39;49mrestack,\n\u001b[0;32m   1537\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[0;32m   1538\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[0;32m   1539\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1540\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1541\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1542\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1543\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\oop.py:1830\u001b[0m, in \u001b[0;36mRegressionExperiment.stack_models\u001b[1;34m(self, estimator_list, meta_model, meta_model_fold, fold, round, restack, choose_better, optimize, fit_kwargs, groups, verbose, return_train_score)\u001b[0m\n\u001b[0;32m   1729\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack_models\u001b[39m(\n\u001b[0;32m   1730\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1731\u001b[0m     estimator_list: \u001b[39mlist\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1742\u001b[0m     return_train_score: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1743\u001b[0m ):\n\u001b[0;32m   1744\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1745\u001b[0m \u001b[39m    This function trains a meta model over select estimators passed in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m    the ``estimator_list`` parameter. The output of this function is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \n\u001b[0;32m   1828\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1830\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstack_models(\n\u001b[0;32m   1831\u001b[0m         estimator_list\u001b[39m=\u001b[39;49mestimator_list,\n\u001b[0;32m   1832\u001b[0m         meta_model\u001b[39m=\u001b[39;49mmeta_model,\n\u001b[0;32m   1833\u001b[0m         meta_model_fold\u001b[39m=\u001b[39;49mmeta_model_fold,\n\u001b[0;32m   1834\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m   1835\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[0;32m   1836\u001b[0m         method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1837\u001b[0m         restack\u001b[39m=\u001b[39;49mrestack,\n\u001b[0;32m   1838\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[0;32m   1839\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[0;32m   1840\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1841\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1842\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1843\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1844\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:3688\u001b[0m, in \u001b[0;36m_SupervisedExperiment.stack_models\u001b[1;34m(self, estimator_list, meta_model, meta_model_fold, fold, round, method, restack, choose_better, optimize, fit_kwargs, groups, probability_threshold, verbose, return_train_score)\u001b[0m\n\u001b[0;32m   3685\u001b[0m     fit_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   3687\u001b[0m \u001b[39m# checking error for estimator_list\u001b[39;00m\n\u001b[1;32m-> 3688\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m estimator_list:\n\u001b[0;32m   3689\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(i, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   3690\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3691\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEstimator \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m does not have the required fit() method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3692\u001b[0m         )\n",
            "\u001b[1;31mTypeError\u001b[0m: 'CatBoostRegressor' object is not iterable"
          ]
        }
      ],
      "source": [
        "stack_lr = stack_models(cat_t, optimize='MAE', choose_better=True)\n",
        "stack_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=Memory(location=None),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;,\n",
              "                                             &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;, &#x27;DUBAI&#x27;,\n",
              "                                             &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                                             &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;,\n",
              "                                             &#x27;rounded_hour_sin&#x27;,\n",
              "                                             &#x27;rounded_hour_cos&#x27;,\n",
              "                                             &#x27;mean_enc_ARI...\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                                    &#x27;BREADTH&#x27;,\n",
              "                                                                    &#x27;DEPTH&#x27;,\n",
              "                                                                    &#x27;DRAUGHT&#x27;,\n",
              "                                                                    &#x27;country_cluster&#x27;,\n",
              "                                                                    &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                             &#x27;PO_Y_M_D&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;,\n",
              "                                                                    &#x27;PO_Y_M_D&#x27;]))),\n",
              "                (&#x27;actual_estimator&#x27;,\n",
              "                 &lt;catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0&gt;)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=Memory(location=None),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;,\n",
              "                                             &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;, &#x27;DUBAI&#x27;,\n",
              "                                             &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                                             &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;,\n",
              "                                             &#x27;rounded_hour_sin&#x27;,\n",
              "                                             &#x27;rounded_hour_cos&#x27;,\n",
              "                                             &#x27;mean_enc_ARI...\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                                    &#x27;BREADTH&#x27;,\n",
              "                                                                    &#x27;DEPTH&#x27;,\n",
              "                                                                    &#x27;DRAUGHT&#x27;,\n",
              "                                                                    &#x27;country_cluster&#x27;,\n",
              "                                                                    &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                             &#x27;PO_Y_M_D&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;,\n",
              "                                                                    &#x27;PO_Y_M_D&#x27;]))),\n",
              "                (&#x27;actual_estimator&#x27;,\n",
              "                 &lt;catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;,\n",
              "                            &#x27;DUBAI&#x27;, &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;, &#x27;PORT_SIZE&#x27;,\n",
              "                            &#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                            &#x27;month_sin&#x27;, &#x27;month_cos&#x27;, &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                            &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;, &#x27;rounded_hour_sin&#x27;,\n",
              "                            &#x27;rounded_hour_cos&#x27;, &#x27;mean_enc_ARI_CO&#x27;,\n",
              "                            &#x27;std_enc_ARI_CO&#x27;, &#x27;mean_enc_ARI_PO&#x27;,\n",
              "                            &#x27;std_enc_ARI_PO&#x27;, &#x27;mean_enc_year&#x27;, &#x27;std_enc_year&#x27;,\n",
              "                            &#x27;mean_enc_month&#x27;, ...],\n",
              "                   transformer=SimpleImputer())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;ARI_PO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;ID&#x27;,\n",
              "                            &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;, &#x27;DRAUGHT&#x27;, &#x27;FLAG&#x27;,\n",
              "                            &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;, &#x27;PO_Y_M_D&#x27;],\n",
              "                   transformer=SimpleImputer(strategy=&#x27;most_frequent&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                            &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                   transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                   &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                   &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                                                   &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;,\n",
              "                                                   &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                             handle_missing=&#x27;return_nan&#x27;,\n",
              "                                             use_cat_names=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                    &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                    &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rest_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;],\n",
              "                   transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                                   &#x27;PO_Y_M_D&#x27;]))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;])</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0&gt;</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(memory=Memory(location=None),\n",
              "         steps=[('numerical_imputer',\n",
              "                 TransformerWrapper(include=['DIST', 'DEADWEIGHT', 'GT',\n",
              "                                             'LENGTH', 'ATA_LT', 'DUBAI',\n",
              "                                             'BRENT', 'WTI', 'BDI_ADJ',\n",
              "                                             'PORT_SIZE', 'year', 'month',\n",
              "                                             'day', 'weekday', 'rounded_hour',\n",
              "                                             'month_sin', 'month_cos',\n",
              "                                             'day_sin', 'day_cos',\n",
              "                                             'weekday_sin', 'weekday_cos',\n",
              "                                             'rounded_hour_sin',\n",
              "                                             'rounded_hour_cos',\n",
              "                                             'mean_enc_ARI...\n",
              "                                                                    'SHIP_TYPE_CATEGORY',\n",
              "                                                                    'BREADTH',\n",
              "                                                                    'DEPTH',\n",
              "                                                                    'DRAUGHT',\n",
              "                                                                    'country_cluster',\n",
              "                                                                    'PORT_SIZE_Zone'],\n",
              "                                                              handle_missing='return_nan',\n",
              "                                                              use_cat_names=True))),\n",
              "                ('rest_encoding',\n",
              "                 TransformerWrapper(include=['ARI_PO', 'ID', 'FLAG',\n",
              "                                             'PO_Y_M_D'],\n",
              "                                    transformer=TargetEncoder(cols=['ARI_PO',\n",
              "                                                                    'ID',\n",
              "                                                                    'FLAG',\n",
              "                                                                    'PO_Y_M_D']))),\n",
              "                ('actual_estimator',\n",
              "                 <catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0>)])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stack_finalized = finalize_model(cat_t)\n",
        "stack_finalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>90.917114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>339.572906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>24.792130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>130.493073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>383.237549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1316.202637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "0       TEST_000000    90.917114\n",
              "1       TEST_000001   339.572906\n",
              "2       TEST_000002     0.000000\n",
              "3       TEST_000003     0.000000\n",
              "4       TEST_000004    24.792130\n",
              "...             ...          ...\n",
              "244984  TEST_244984   130.493073\n",
              "244985  TEST_244985   383.237549\n",
              "244986  TEST_244986     0.000000\n",
              "244987  TEST_244987     0.000000\n",
              "244988  TEST_244988  1316.202637\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission = pd.read_csv('(f)bestaccuracy_nofit_scaled_sample_submission.csv')\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('(f)bestaccuracy_nofit_scaled_sample_submission_2.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>98.878138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>206.534507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>67.811210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>29.505385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>205.425372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>954.309087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID     CI_HOUR\n",
              "0       TEST_000000   98.878138\n",
              "1       TEST_000001  206.534507\n",
              "2       TEST_000002    0.000000\n",
              "3       TEST_000003    0.000000\n",
              "4       TEST_000004   67.811210\n",
              "...             ...         ...\n",
              "244984  TEST_244984   29.505385\n",
              "244985  TEST_244985  205.425372\n",
              "244986  TEST_244986    0.000000\n",
              "244987  TEST_244987    0.000000\n",
              "244988  TEST_244988  954.309087\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission['CI_HOUR'] = stack_finalized.predict(test)\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('pycat.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "#import xgboost as xgb\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def objective(trial: Trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "            'iterations':trial.suggest_int(\"iterations\", 300, 1000),\n",
        "            'learning_rate' : trial.suggest_uniform('learning_rate',0.1, 1),\n",
        "            'depth': trial.suggest_int('depth',5, 16),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
        "            'reg_lambda': trial.suggest_uniform('reg_lambda',30,100),\n",
        "            'subsample': trial.suggest_uniform('subsample',0.3,1),\n",
        "            'random_strength': trial.suggest_uniform('random_strength',10,100),\n",
        "            'od_wait':trial.suggest_int('od_wait', 10, 150),\n",
        "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,20),\n",
        "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 1, 100),\n",
        "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0., 1.0),\n",
        "            'random_state' : 313,\n",
        "            'verbose' : 0,\n",
        "        }\n",
        "    #'task_type' : 'GPU',\n",
        "    #\"eval_metric\":'RMSE',\n",
        "    cat = CatBoostRegressor(**params)\n",
        "    cat.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val,y_val)],cat_features=cat_features,\n",
        "              verbose=False)\n",
        "    cat_pred = cat.predict(X_val)\n",
        "    score = mean_absolute_error(y_val, cat_pred)\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import bisect\n",
        "'''\n",
        "# Categorical 컬럼 인코딩\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "encoders = {}\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    le = LabelEncoder()\n",
        "    train[feature] = le.fit_transform(train[feature].astype(str))\n",
        "    le_classes_set = set(le.classes_)\n",
        "    test[feature] = test[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
        "    le_classes = le.classes_.tolist()\n",
        "    bisect.insort_left(le_classes, '-1')\n",
        "    le.classes_ = np.array(le_classes)\n",
        "    test[feature] = le.transform(test[feature].astype(str))\n",
        "    encoders[feature] = le\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "\n",
        "# 결측치 처리\n",
        "train_x.fillna(train_x.mean(), inplace=True)\n",
        "test.fillna(train_x.mean(), inplace=True)\n",
        "'''\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'country_cluster', 'PORT_SIZE_Zone', 'BREADTH', 'DEPTH', 'DRAUGHT', 'PO_Y_M_D', 'FLAG']\n",
        "# 수치형 변수만 대상으로 결측치 대체\n",
        "numeric_cols = train_x.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 훈련 데이터의 평균 계산\n",
        "mean_values = train_x[numeric_cols].mean()\n",
        "\n",
        "# 결측치 대체\n",
        "train_x[numeric_cols] = train_x[numeric_cols].fillna(mean_values)\n",
        "test[numeric_cols] = test[numeric_cols].fillna(mean_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모든 카테고리형 변수를 문자열로 변환\n",
        "categorical_cols = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "train_x[categorical_cols] = train_x[categorical_cols].astype(str)\n",
        "test[categorical_cols] = test[categorical_cols].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 04:38:20,348] A new study created in memory with name: no-name-d4b11179-9f09-4b01-91fe-bacc3ca82743\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 04:44:30,311] Trial 0 finished with value: 56.54932100528273 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 56.54932100528273.\n",
            "[I 2023-10-08 04:48:20,495] Trial 1 finished with value: 39.51877743888855 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 04:55:17,617] Trial 2 finished with value: 43.83782520780801 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 04:59:58,790] Trial 3 finished with value: 43.411084110826415 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:04:07,689] Trial 4 finished with value: 40.694680718281475 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:09:39,768] Trial 5 finished with value: 76.67180974474168 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:12:56,512] Trial 6 finished with value: 44.436445954065924 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:26:47,262] Trial 7 finished with value: 53.612205724301404 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:28:39,447] Trial 8 finished with value: 56.22923936696853 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:31:56,182] Trial 9 finished with value: 47.42645341792423 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:37:22,495] Trial 10 finished with value: 51.78062697267762 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:41:19,770] Trial 11 finished with value: 40.613149165555036 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:44:30,752] Trial 12 finished with value: 43.5865594636709 and parameters: {'iterations': 743, 'learning_rate': 0.7356346127088652, 'depth': 9, 'min_data_in_leaf': 17, 'reg_lambda': 49.16423147603391, 'subsample': 0.5854290622440648, 'random_strength': 31.588734753749215, 'od_wait': 122, 'leaf_estimation_iterations': 4, 'bagging_temperature': 23.993242751388063, 'colsample_bylevel': 0.3642319611435846}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:47:01,253] Trial 13 finished with value: 50.711451516394156 and parameters: {'iterations': 757, 'learning_rate': 0.7501800009545696, 'depth': 11, 'min_data_in_leaf': 13, 'reg_lambda': 30.221360663173336, 'subsample': 0.4774950955421056, 'random_strength': 30.328584321736876, 'od_wait': 18, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.6071397112449649}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:51:21,010] Trial 14 finished with value: 39.74867525041899 and parameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 05:54:53,207] Trial 15 finished with value: 41.106675540464664 and parameters: {'iterations': 951, 'learning_rate': 0.378680852488155, 'depth': 7, 'min_data_in_leaf': 23, 'reg_lambda': 39.94535569998841, 'subsample': 0.6610237845565569, 'random_strength': 20.618074524594604, 'od_wait': 150, 'leaf_estimation_iterations': 5, 'bagging_temperature': 19.000173884005733, 'colsample_bylevel': 0.5429293784058689}. Best is trial 1 with value: 39.51877743888855.\n",
            "[I 2023-10-08 06:04:18,209] Trial 16 finished with value: 38.956845627351825 and parameters: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}. Best is trial 16 with value: 38.956845627351825.\n",
            "[I 2023-10-08 06:08:04,111] Trial 17 finished with value: 64.00128660331325 and parameters: {'iterations': 305, 'learning_rate': 0.10410309839558177, 'depth': 12, 'min_data_in_leaf': 22, 'reg_lambda': 62.93129990724812, 'subsample': 0.7426081894256986, 'random_strength': 44.362030019540654, 'od_wait': 125, 'leaf_estimation_iterations': 9, 'bagging_temperature': 13.937831218004703, 'colsample_bylevel': 0.9486708135061371}. Best is trial 16 with value: 38.956845627351825.\n",
            "[I 2023-10-08 06:12:54,998] Trial 18 finished with value: 45.732066501214824 and parameters: {'iterations': 796, 'learning_rate': 0.13599792999265126, 'depth': 11, 'min_data_in_leaf': 24, 'reg_lambda': 71.62634725545307, 'subsample': 0.44325233558749344, 'random_strength': 20.973759752682703, 'od_wait': 100, 'leaf_estimation_iterations': 6, 'bagging_temperature': 5.065329413975176, 'colsample_bylevel': 0.32674260541968214}. Best is trial 16 with value: 38.956845627351825.\n",
            "[I 2023-10-08 06:33:26,360] Trial 19 finished with value: 47.4411507363553 and parameters: {'iterations': 919, 'learning_rate': 0.36343405833274767, 'depth': 13, 'min_data_in_leaf': 30, 'reg_lambda': 40.746702935917256, 'subsample': 0.5980837495644987, 'random_strength': 42.94431966534945, 'od_wait': 98, 'leaf_estimation_iterations': 16, 'bagging_temperature': 42.15873557931633, 'colsample_bylevel': 0.8019932574851661}. Best is trial 16 with value: 38.956845627351825.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 1: 38.956845627351825\n",
            "Best hyperparameters for fold 1: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}\n",
            "0:\tlearn: 209.6808434\ttotal: 146ms\tremaining: 2m 20s\n",
            "1:\tlearn: 207.5929570\ttotal: 703ms\tremaining: 5m 36s\n",
            "2:\tlearn: 206.4174889\ttotal: 1.16s\tremaining: 6m 9s\n",
            "3:\tlearn: 205.5849703\ttotal: 1.76s\tremaining: 7m\n",
            "4:\tlearn: 204.6263561\ttotal: 2.21s\tremaining: 7m 2s\n",
            "5:\tlearn: 204.1411002\ttotal: 2.75s\tremaining: 7m 17s\n",
            "6:\tlearn: 203.4873165\ttotal: 3.3s\tremaining: 7m 29s\n",
            "7:\tlearn: 202.8260611\ttotal: 3.79s\tremaining: 7m 30s\n",
            "8:\tlearn: 202.5731433\ttotal: 4.13s\tremaining: 7m 15s\n",
            "9:\tlearn: 201.3134053\ttotal: 4.7s\tremaining: 7m 25s\n",
            "10:\tlearn: 200.8060111\ttotal: 5.25s\tremaining: 7m 32s\n",
            "11:\tlearn: 200.7614536\ttotal: 5.34s\tremaining: 7m 1s\n",
            "12:\tlearn: 200.7272271\ttotal: 5.44s\tremaining: 6m 35s\n",
            "13:\tlearn: 199.7366982\ttotal: 6.02s\tremaining: 6m 46s\n",
            "14:\tlearn: 199.6970648\ttotal: 6.16s\tremaining: 6m 27s\n",
            "15:\tlearn: 199.3448511\ttotal: 6.74s\tremaining: 6m 37s\n",
            "16:\tlearn: 198.7836059\ttotal: 7.27s\tremaining: 6m 42s\n",
            "17:\tlearn: 198.2351019\ttotal: 7.8s\tremaining: 6m 47s\n",
            "18:\tlearn: 198.0608129\ttotal: 8.01s\tremaining: 6m 36s\n",
            "19:\tlearn: 198.0004280\ttotal: 8.59s\tremaining: 6m 43s\n",
            "20:\tlearn: 197.5434251\ttotal: 9.09s\tremaining: 6m 45s\n",
            "21:\tlearn: 197.3740672\ttotal: 9.63s\tremaining: 6m 50s\n",
            "22:\tlearn: 197.2375605\ttotal: 10s\tremaining: 6m 48s\n",
            "23:\tlearn: 197.1794585\ttotal: 10.4s\tremaining: 6m 46s\n",
            "24:\tlearn: 197.0445072\ttotal: 11s\tremaining: 6m 50s\n",
            "25:\tlearn: 196.9849378\ttotal: 11.6s\tremaining: 6m 57s\n",
            "26:\tlearn: 196.8605568\ttotal: 12s\tremaining: 6m 55s\n",
            "27:\tlearn: 196.8582724\ttotal: 12.2s\tremaining: 6m 45s\n",
            "28:\tlearn: 196.8465005\ttotal: 12.4s\tremaining: 6m 36s\n",
            "29:\tlearn: 196.7582327\ttotal: 12.7s\tremaining: 6m 33s\n",
            "30:\tlearn: 196.3701276\ttotal: 13.3s\tremaining: 6m 38s\n",
            "31:\tlearn: 196.3699528\ttotal: 13.4s\tremaining: 6m 28s\n",
            "32:\tlearn: 196.1056270\ttotal: 14.1s\tremaining: 6m 34s\n",
            "33:\tlearn: 196.0932095\ttotal: 14.4s\tremaining: 6m 31s\n",
            "34:\tlearn: 196.0346424\ttotal: 14.7s\tremaining: 6m 28s\n",
            "35:\tlearn: 196.0163564\ttotal: 14.9s\tremaining: 6m 22s\n",
            "36:\tlearn: 195.5075696\ttotal: 15.3s\tremaining: 6m 21s\n",
            "37:\tlearn: 195.4564730\ttotal: 15.6s\tremaining: 6m 18s\n",
            "38:\tlearn: 195.4065505\ttotal: 15.8s\tremaining: 6m 13s\n",
            "39:\tlearn: 195.3661384\ttotal: 16.1s\tremaining: 6m 10s\n",
            "40:\tlearn: 195.3412278\ttotal: 16.7s\tremaining: 6m 14s\n",
            "41:\tlearn: 195.3412277\ttotal: 16.8s\tremaining: 6m 6s\n",
            "42:\tlearn: 195.2876343\ttotal: 17s\tremaining: 6m 1s\n",
            "43:\tlearn: 195.1728095\ttotal: 17.6s\tremaining: 6m 5s\n",
            "44:\tlearn: 195.0978847\ttotal: 18.1s\tremaining: 6m 7s\n",
            "45:\tlearn: 195.0977959\ttotal: 18.2s\tremaining: 6m\n",
            "46:\tlearn: 195.0181613\ttotal: 18.7s\tremaining: 6m 2s\n",
            "47:\tlearn: 195.0181047\ttotal: 18.8s\tremaining: 5m 55s\n",
            "48:\tlearn: 194.9437488\ttotal: 19.3s\tremaining: 5m 58s\n",
            "49:\tlearn: 194.9172675\ttotal: 19.7s\tremaining: 5m 57s\n",
            "50:\tlearn: 194.9121489\ttotal: 19.9s\tremaining: 5m 53s\n",
            "51:\tlearn: 194.9015454\ttotal: 20s\tremaining: 5m 49s\n",
            "52:\tlearn: 194.9008139\ttotal: 20.2s\tremaining: 5m 45s\n",
            "53:\tlearn: 194.5822989\ttotal: 20.7s\tremaining: 5m 46s\n",
            "54:\tlearn: 194.5479121\ttotal: 21s\tremaining: 5m 45s\n",
            "55:\tlearn: 194.5424219\ttotal: 21.1s\tremaining: 5m 40s\n",
            "56:\tlearn: 194.4115820\ttotal: 21.7s\tremaining: 5m 43s\n",
            "57:\tlearn: 194.3152975\ttotal: 22.3s\tremaining: 5m 45s\n",
            "58:\tlearn: 194.2257389\ttotal: 22.9s\tremaining: 5m 48s\n",
            "59:\tlearn: 194.2093128\ttotal: 23.2s\tremaining: 5m 47s\n",
            "60:\tlearn: 194.0870188\ttotal: 23.8s\tremaining: 5m 49s\n",
            "61:\tlearn: 193.9867471\ttotal: 24.3s\tremaining: 5m 51s\n",
            "62:\tlearn: 193.9723538\ttotal: 24.6s\tremaining: 5m 49s\n",
            "63:\tlearn: 193.9480458\ttotal: 24.8s\tremaining: 5m 47s\n",
            "64:\tlearn: 193.9472376\ttotal: 24.9s\tremaining: 5m 43s\n",
            "65:\tlearn: 193.8572992\ttotal: 25.4s\tremaining: 5m 44s\n",
            "66:\tlearn: 193.8572476\ttotal: 25.5s\tremaining: 5m 39s\n",
            "67:\tlearn: 193.7582976\ttotal: 26.2s\tremaining: 5m 42s\n",
            "68:\tlearn: 193.7307437\ttotal: 26.4s\tremaining: 5m 40s\n",
            "69:\tlearn: 193.6996435\ttotal: 26.6s\tremaining: 5m 37s\n",
            "70:\tlearn: 193.6995266\ttotal: 26.7s\tremaining: 5m 33s\n",
            "71:\tlearn: 193.3658525\ttotal: 27.3s\tremaining: 5m 36s\n",
            "72:\tlearn: 193.3595650\ttotal: 27.4s\tremaining: 5m 32s\n",
            "73:\tlearn: 193.3054801\ttotal: 27.6s\tremaining: 5m 30s\n",
            "74:\tlearn: 193.1451511\ttotal: 28.3s\tremaining: 5m 33s\n",
            "75:\tlearn: 193.1439406\ttotal: 28.4s\tremaining: 5m 30s\n",
            "76:\tlearn: 193.1427049\ttotal: 28.6s\tremaining: 5m 27s\n",
            "77:\tlearn: 193.1086622\ttotal: 29.2s\tremaining: 5m 29s\n",
            "78:\tlearn: 192.9833340\ttotal: 29.8s\tremaining: 5m 31s\n",
            "79:\tlearn: 192.8881289\ttotal: 30.3s\tremaining: 5m 33s\n",
            "80:\tlearn: 192.7005688\ttotal: 31s\tremaining: 5m 35s\n",
            "81:\tlearn: 192.5920896\ttotal: 31.6s\tremaining: 5m 37s\n",
            "82:\tlearn: 192.2681098\ttotal: 32.1s\tremaining: 5m 38s\n",
            "83:\tlearn: 192.1644562\ttotal: 32.7s\tremaining: 5m 40s\n",
            "84:\tlearn: 192.0011461\ttotal: 33.3s\tremaining: 5m 42s\n",
            "85:\tlearn: 191.7650312\ttotal: 33.6s\tremaining: 5m 40s\n",
            "86:\tlearn: 191.6095899\ttotal: 33.9s\tremaining: 5m 40s\n",
            "87:\tlearn: 191.5219072\ttotal: 34.5s\tremaining: 5m 41s\n",
            "88:\tlearn: 191.3925099\ttotal: 34.9s\tremaining: 5m 40s\n",
            "89:\tlearn: 191.2044136\ttotal: 35.4s\tremaining: 5m 41s\n",
            "90:\tlearn: 189.3067306\ttotal: 35.9s\tremaining: 5m 42s\n",
            "91:\tlearn: 189.1135995\ttotal: 36.5s\tremaining: 5m 44s\n",
            "92:\tlearn: 188.8121729\ttotal: 37s\tremaining: 5m 44s\n",
            "93:\tlearn: 188.6403278\ttotal: 37.6s\tremaining: 5m 45s\n",
            "94:\tlearn: 188.0595100\ttotal: 38.1s\tremaining: 5m 46s\n",
            "95:\tlearn: 187.9307850\ttotal: 38.2s\tremaining: 5m 43s\n",
            "96:\tlearn: 187.6412586\ttotal: 38.8s\tremaining: 5m 44s\n",
            "97:\tlearn: 185.5702274\ttotal: 39.2s\tremaining: 5m 44s\n",
            "98:\tlearn: 185.1609892\ttotal: 39.9s\tremaining: 5m 46s\n",
            "99:\tlearn: 184.0415000\ttotal: 40.3s\tremaining: 5m 46s\n",
            "100:\tlearn: 183.4539058\ttotal: 40.8s\tremaining: 5m 46s\n",
            "101:\tlearn: 183.0313626\ttotal: 41.3s\tremaining: 5m 46s\n",
            "102:\tlearn: 182.5750397\ttotal: 41.8s\tremaining: 5m 47s\n",
            "103:\tlearn: 182.1529364\ttotal: 42.4s\tremaining: 5m 48s\n",
            "104:\tlearn: 181.7292823\ttotal: 43s\tremaining: 5m 49s\n",
            "105:\tlearn: 181.3320283\ttotal: 43.6s\tremaining: 5m 50s\n",
            "106:\tlearn: 180.9915041\ttotal: 44.1s\tremaining: 5m 51s\n",
            "107:\tlearn: 180.6397686\ttotal: 44.7s\tremaining: 5m 52s\n",
            "108:\tlearn: 180.1620027\ttotal: 45.3s\tremaining: 5m 53s\n",
            "109:\tlearn: 179.6669933\ttotal: 45.8s\tremaining: 5m 53s\n",
            "110:\tlearn: 179.2734206\ttotal: 46.4s\tremaining: 5m 54s\n",
            "111:\tlearn: 179.0676714\ttotal: 47s\tremaining: 5m 55s\n",
            "112:\tlearn: 177.7208132\ttotal: 47.5s\tremaining: 5m 55s\n",
            "113:\tlearn: 177.4078332\ttotal: 48s\tremaining: 5m 55s\n",
            "114:\tlearn: 176.2670652\ttotal: 48.4s\tremaining: 5m 55s\n",
            "115:\tlearn: 173.9895835\ttotal: 48.9s\tremaining: 5m 55s\n",
            "116:\tlearn: 172.2172808\ttotal: 49.3s\tremaining: 5m 54s\n",
            "117:\tlearn: 171.9989698\ttotal: 49.8s\tremaining: 5m 55s\n",
            "118:\tlearn: 170.6096937\ttotal: 50.3s\tremaining: 5m 54s\n",
            "119:\tlearn: 169.7012957\ttotal: 50.8s\tremaining: 5m 54s\n",
            "120:\tlearn: 167.6046333\ttotal: 51.2s\tremaining: 5m 54s\n",
            "121:\tlearn: 167.2398994\ttotal: 51.8s\tremaining: 5m 55s\n",
            "122:\tlearn: 166.5562278\ttotal: 52.3s\tremaining: 5m 55s\n",
            "123:\tlearn: 165.7033117\ttotal: 52.8s\tremaining: 5m 55s\n",
            "124:\tlearn: 165.4793929\ttotal: 53.3s\tremaining: 5m 55s\n",
            "125:\tlearn: 165.2488054\ttotal: 53.8s\tremaining: 5m 55s\n",
            "126:\tlearn: 164.2645051\ttotal: 54.3s\tremaining: 5m 55s\n",
            "127:\tlearn: 163.4980957\ttotal: 54.7s\tremaining: 5m 55s\n",
            "128:\tlearn: 163.2272180\ttotal: 55.3s\tremaining: 5m 55s\n",
            "129:\tlearn: 162.4165536\ttotal: 55.6s\tremaining: 5m 54s\n",
            "130:\tlearn: 161.7484547\ttotal: 56.1s\tremaining: 5m 54s\n",
            "131:\tlearn: 161.2142995\ttotal: 56.6s\tremaining: 5m 54s\n",
            "132:\tlearn: 159.2685565\ttotal: 57.1s\tremaining: 5m 54s\n",
            "133:\tlearn: 157.8096663\ttotal: 57.6s\tremaining: 5m 54s\n",
            "134:\tlearn: 157.5362991\ttotal: 58.1s\tremaining: 5m 54s\n",
            "135:\tlearn: 156.8171981\ttotal: 58.6s\tremaining: 5m 54s\n",
            "136:\tlearn: 156.2909737\ttotal: 59.1s\tremaining: 5m 54s\n",
            "137:\tlearn: 155.5050791\ttotal: 59.6s\tremaining: 5m 54s\n",
            "138:\tlearn: 154.5528624\ttotal: 1m\tremaining: 5m 54s\n",
            "139:\tlearn: 154.3427682\ttotal: 1m\tremaining: 5m 54s\n",
            "140:\tlearn: 154.0908579\ttotal: 1m 1s\tremaining: 5m 55s\n",
            "141:\tlearn: 152.9472736\ttotal: 1m 1s\tremaining: 5m 55s\n",
            "142:\tlearn: 151.8866176\ttotal: 1m 2s\tremaining: 5m 55s\n",
            "143:\tlearn: 151.0770722\ttotal: 1m 2s\tremaining: 5m 55s\n",
            "144:\tlearn: 150.0426493\ttotal: 1m 3s\tremaining: 5m 55s\n",
            "145:\tlearn: 149.5184947\ttotal: 1m 3s\tremaining: 5m 55s\n",
            "146:\tlearn: 148.9559270\ttotal: 1m 4s\tremaining: 5m 55s\n",
            "147:\tlearn: 148.0626081\ttotal: 1m 4s\tremaining: 5m 55s\n",
            "148:\tlearn: 147.8532781\ttotal: 1m 5s\tremaining: 5m 55s\n",
            "149:\tlearn: 146.7652258\ttotal: 1m 5s\tremaining: 5m 54s\n",
            "150:\tlearn: 146.5910730\ttotal: 1m 6s\tremaining: 5m 54s\n",
            "151:\tlearn: 145.7096995\ttotal: 1m 6s\tremaining: 5m 54s\n",
            "152:\tlearn: 145.1129996\ttotal: 1m 7s\tremaining: 5m 54s\n",
            "153:\tlearn: 144.3023645\ttotal: 1m 7s\tremaining: 5m 54s\n",
            "154:\tlearn: 143.6304202\ttotal: 1m 8s\tremaining: 5m 54s\n",
            "155:\tlearn: 143.2059637\ttotal: 1m 8s\tremaining: 5m 53s\n",
            "156:\tlearn: 142.4549183\ttotal: 1m 9s\tremaining: 5m 53s\n",
            "157:\tlearn: 142.1519689\ttotal: 1m 9s\tremaining: 5m 53s\n",
            "158:\tlearn: 141.9175289\ttotal: 1m 10s\tremaining: 5m 53s\n",
            "159:\tlearn: 141.6255487\ttotal: 1m 10s\tremaining: 5m 53s\n",
            "160:\tlearn: 141.3880211\ttotal: 1m 11s\tremaining: 5m 53s\n",
            "161:\tlearn: 141.2397044\ttotal: 1m 11s\tremaining: 5m 53s\n",
            "162:\tlearn: 140.7575276\ttotal: 1m 12s\tremaining: 5m 53s\n",
            "163:\tlearn: 140.4726957\ttotal: 1m 12s\tremaining: 5m 53s\n",
            "164:\tlearn: 139.3663118\ttotal: 1m 13s\tremaining: 5m 53s\n",
            "165:\tlearn: 138.7202396\ttotal: 1m 13s\tremaining: 5m 53s\n",
            "166:\tlearn: 138.4163084\ttotal: 1m 14s\tremaining: 5m 53s\n",
            "167:\tlearn: 137.9185043\ttotal: 1m 15s\tremaining: 5m 53s\n",
            "168:\tlearn: 137.5605135\ttotal: 1m 15s\tremaining: 5m 53s\n",
            "169:\tlearn: 137.3142304\ttotal: 1m 16s\tremaining: 5m 53s\n",
            "170:\tlearn: 137.1199791\ttotal: 1m 16s\tremaining: 5m 54s\n",
            "171:\tlearn: 136.8558514\ttotal: 1m 17s\tremaining: 5m 53s\n",
            "172:\tlearn: 136.6299583\ttotal: 1m 17s\tremaining: 5m 53s\n",
            "173:\tlearn: 136.3592140\ttotal: 1m 18s\tremaining: 5m 54s\n",
            "174:\tlearn: 135.6534232\ttotal: 1m 19s\tremaining: 5m 54s\n",
            "175:\tlearn: 135.1634251\ttotal: 1m 19s\tremaining: 5m 53s\n",
            "176:\tlearn: 134.8711497\ttotal: 1m 20s\tremaining: 5m 53s\n",
            "177:\tlearn: 134.5180779\ttotal: 1m 20s\tremaining: 5m 53s\n",
            "178:\tlearn: 134.0029774\ttotal: 1m 20s\tremaining: 5m 52s\n",
            "179:\tlearn: 133.8529226\ttotal: 1m 21s\tremaining: 5m 52s\n",
            "180:\tlearn: 133.8071099\ttotal: 1m 21s\tremaining: 5m 51s\n",
            "181:\tlearn: 133.5474578\ttotal: 1m 22s\tremaining: 5m 51s\n",
            "182:\tlearn: 132.3641442\ttotal: 1m 22s\tremaining: 5m 51s\n",
            "183:\tlearn: 131.4791692\ttotal: 1m 23s\tremaining: 5m 50s\n",
            "184:\tlearn: 130.6814769\ttotal: 1m 23s\tremaining: 5m 50s\n",
            "185:\tlearn: 130.5703854\ttotal: 1m 24s\tremaining: 5m 50s\n",
            "186:\tlearn: 130.3986262\ttotal: 1m 24s\tremaining: 5m 50s\n",
            "187:\tlearn: 130.1119238\ttotal: 1m 25s\tremaining: 5m 49s\n",
            "188:\tlearn: 129.8662748\ttotal: 1m 25s\tremaining: 5m 49s\n",
            "189:\tlearn: 129.7582118\ttotal: 1m 26s\tremaining: 5m 49s\n",
            "190:\tlearn: 129.0146578\ttotal: 1m 26s\tremaining: 5m 49s\n",
            "191:\tlearn: 128.8259058\ttotal: 1m 27s\tremaining: 5m 48s\n",
            "192:\tlearn: 128.6731287\ttotal: 1m 27s\tremaining: 5m 48s\n",
            "193:\tlearn: 127.9307302\ttotal: 1m 28s\tremaining: 5m 48s\n",
            "194:\tlearn: 127.4510932\ttotal: 1m 28s\tremaining: 5m 47s\n",
            "195:\tlearn: 127.2314351\ttotal: 1m 29s\tremaining: 5m 47s\n",
            "196:\tlearn: 127.0662170\ttotal: 1m 29s\tremaining: 5m 47s\n",
            "197:\tlearn: 126.7167778\ttotal: 1m 30s\tremaining: 5m 47s\n",
            "198:\tlearn: 126.0910515\ttotal: 1m 30s\tremaining: 5m 46s\n",
            "199:\tlearn: 125.9135002\ttotal: 1m 31s\tremaining: 5m 46s\n",
            "200:\tlearn: 125.6195426\ttotal: 1m 31s\tremaining: 5m 46s\n",
            "201:\tlearn: 125.4042601\ttotal: 1m 32s\tremaining: 5m 46s\n",
            "202:\tlearn: 124.7422716\ttotal: 1m 32s\tremaining: 5m 46s\n",
            "203:\tlearn: 124.4603983\ttotal: 1m 33s\tremaining: 5m 46s\n",
            "204:\tlearn: 124.2639238\ttotal: 1m 34s\tremaining: 5m 46s\n",
            "205:\tlearn: 124.1704127\ttotal: 1m 34s\tremaining: 5m 46s\n",
            "206:\tlearn: 124.0916877\ttotal: 1m 35s\tremaining: 5m 46s\n",
            "207:\tlearn: 123.9782870\ttotal: 1m 35s\tremaining: 5m 46s\n",
            "208:\tlearn: 123.8477428\ttotal: 1m 36s\tremaining: 5m 45s\n",
            "209:\tlearn: 122.5614331\ttotal: 1m 36s\tremaining: 5m 45s\n",
            "210:\tlearn: 122.3021426\ttotal: 1m 37s\tremaining: 5m 44s\n",
            "211:\tlearn: 121.6800822\ttotal: 1m 37s\tremaining: 5m 44s\n",
            "212:\tlearn: 121.4808255\ttotal: 1m 38s\tremaining: 5m 44s\n",
            "213:\tlearn: 121.1517657\ttotal: 1m 38s\tremaining: 5m 44s\n",
            "214:\tlearn: 120.9785046\ttotal: 1m 39s\tremaining: 5m 43s\n",
            "215:\tlearn: 120.8413145\ttotal: 1m 39s\tremaining: 5m 43s\n",
            "216:\tlearn: 120.5734003\ttotal: 1m 40s\tremaining: 5m 43s\n",
            "217:\tlearn: 120.4688967\ttotal: 1m 40s\tremaining: 5m 42s\n",
            "218:\tlearn: 120.2690610\ttotal: 1m 41s\tremaining: 5m 42s\n",
            "219:\tlearn: 119.9182444\ttotal: 1m 41s\tremaining: 5m 41s\n",
            "220:\tlearn: 119.5336463\ttotal: 1m 42s\tremaining: 5m 41s\n",
            "221:\tlearn: 118.9302555\ttotal: 1m 42s\tremaining: 5m 41s\n",
            "222:\tlearn: 118.7461236\ttotal: 1m 43s\tremaining: 5m 41s\n",
            "223:\tlearn: 118.5796700\ttotal: 1m 43s\tremaining: 5m 40s\n",
            "224:\tlearn: 117.9238176\ttotal: 1m 44s\tremaining: 5m 40s\n",
            "225:\tlearn: 117.7816286\ttotal: 1m 44s\tremaining: 5m 40s\n",
            "226:\tlearn: 117.1478122\ttotal: 1m 45s\tremaining: 5m 40s\n",
            "227:\tlearn: 116.8437277\ttotal: 1m 46s\tremaining: 5m 39s\n",
            "228:\tlearn: 116.6924678\ttotal: 1m 46s\tremaining: 5m 39s\n",
            "229:\tlearn: 116.6232147\ttotal: 1m 47s\tremaining: 5m 39s\n",
            "230:\tlearn: 116.3469006\ttotal: 1m 47s\tremaining: 5m 39s\n",
            "231:\tlearn: 116.2060507\ttotal: 1m 48s\tremaining: 5m 39s\n",
            "232:\tlearn: 115.8109644\ttotal: 1m 48s\tremaining: 5m 38s\n",
            "233:\tlearn: 115.3555692\ttotal: 1m 49s\tremaining: 5m 38s\n",
            "234:\tlearn: 115.2465317\ttotal: 1m 49s\tremaining: 5m 38s\n",
            "235:\tlearn: 114.7780406\ttotal: 1m 50s\tremaining: 5m 38s\n",
            "236:\tlearn: 114.4261576\ttotal: 1m 50s\tremaining: 5m 38s\n",
            "237:\tlearn: 114.2416275\ttotal: 1m 51s\tremaining: 5m 37s\n",
            "238:\tlearn: 113.6255935\ttotal: 1m 51s\tremaining: 5m 37s\n",
            "239:\tlearn: 113.4480386\ttotal: 1m 52s\tremaining: 5m 36s\n",
            "240:\tlearn: 113.2611414\ttotal: 1m 52s\tremaining: 5m 36s\n",
            "241:\tlearn: 113.1394495\ttotal: 1m 53s\tremaining: 5m 36s\n",
            "242:\tlearn: 112.9149669\ttotal: 1m 53s\tremaining: 5m 35s\n",
            "243:\tlearn: 112.7398835\ttotal: 1m 54s\tremaining: 5m 35s\n",
            "244:\tlearn: 112.6382598\ttotal: 1m 54s\tremaining: 5m 34s\n",
            "245:\tlearn: 112.6032718\ttotal: 1m 55s\tremaining: 5m 34s\n",
            "246:\tlearn: 112.4063815\ttotal: 1m 55s\tremaining: 5m 34s\n",
            "247:\tlearn: 111.9272259\ttotal: 1m 56s\tremaining: 5m 33s\n",
            "248:\tlearn: 111.8256120\ttotal: 1m 56s\tremaining: 5m 33s\n",
            "249:\tlearn: 111.4589665\ttotal: 1m 57s\tremaining: 5m 33s\n",
            "250:\tlearn: 111.2278054\ttotal: 1m 57s\tremaining: 5m 32s\n",
            "251:\tlearn: 110.7745158\ttotal: 1m 58s\tremaining: 5m 32s\n",
            "252:\tlearn: 110.5484440\ttotal: 1m 58s\tremaining: 5m 31s\n",
            "253:\tlearn: 110.1245048\ttotal: 1m 59s\tremaining: 5m 31s\n",
            "254:\tlearn: 109.9653192\ttotal: 1m 59s\tremaining: 5m 31s\n",
            "255:\tlearn: 109.8178598\ttotal: 2m\tremaining: 5m 30s\n",
            "256:\tlearn: 109.2953001\ttotal: 2m\tremaining: 5m 30s\n",
            "257:\tlearn: 108.8012189\ttotal: 2m 1s\tremaining: 5m 29s\n",
            "258:\tlearn: 108.6913528\ttotal: 2m 1s\tremaining: 5m 29s\n",
            "259:\tlearn: 108.5130177\ttotal: 2m 2s\tremaining: 5m 29s\n",
            "260:\tlearn: 108.2891395\ttotal: 2m 2s\tremaining: 5m 28s\n",
            "261:\tlearn: 108.1525818\ttotal: 2m 3s\tremaining: 5m 28s\n",
            "262:\tlearn: 108.0566188\ttotal: 2m 4s\tremaining: 5m 28s\n",
            "263:\tlearn: 107.7513020\ttotal: 2m 4s\tremaining: 5m 27s\n",
            "264:\tlearn: 107.5723173\ttotal: 2m 5s\tremaining: 5m 27s\n",
            "265:\tlearn: 107.2981468\ttotal: 2m 5s\tremaining: 5m 27s\n",
            "266:\tlearn: 107.1980962\ttotal: 2m 6s\tremaining: 5m 27s\n",
            "267:\tlearn: 106.8402782\ttotal: 2m 6s\tremaining: 5m 26s\n",
            "268:\tlearn: 106.7914124\ttotal: 2m 7s\tremaining: 5m 26s\n",
            "269:\tlearn: 106.6830501\ttotal: 2m 7s\tremaining: 5m 26s\n",
            "270:\tlearn: 106.2556113\ttotal: 2m 8s\tremaining: 5m 25s\n",
            "271:\tlearn: 106.1449413\ttotal: 2m 8s\tremaining: 5m 25s\n",
            "272:\tlearn: 105.9334859\ttotal: 2m 9s\tremaining: 5m 24s\n",
            "273:\tlearn: 105.8137893\ttotal: 2m 9s\tremaining: 5m 24s\n",
            "274:\tlearn: 105.7620590\ttotal: 2m 10s\tremaining: 5m 24s\n",
            "275:\tlearn: 105.4494129\ttotal: 2m 10s\tremaining: 5m 23s\n",
            "276:\tlearn: 105.4369907\ttotal: 2m 11s\tremaining: 5m 23s\n",
            "277:\tlearn: 105.1631173\ttotal: 2m 11s\tremaining: 5m 22s\n",
            "278:\tlearn: 105.0952482\ttotal: 2m 12s\tremaining: 5m 22s\n",
            "279:\tlearn: 105.0210799\ttotal: 2m 12s\tremaining: 5m 22s\n",
            "280:\tlearn: 104.9171791\ttotal: 2m 13s\tremaining: 5m 22s\n",
            "281:\tlearn: 104.5908443\ttotal: 2m 13s\tremaining: 5m 21s\n",
            "282:\tlearn: 104.3015170\ttotal: 2m 14s\tremaining: 5m 21s\n",
            "283:\tlearn: 104.3005590\ttotal: 2m 14s\tremaining: 5m 20s\n",
            "284:\tlearn: 104.2420217\ttotal: 2m 15s\tremaining: 5m 20s\n",
            "285:\tlearn: 104.2416335\ttotal: 2m 15s\tremaining: 5m 19s\n",
            "286:\tlearn: 104.1700156\ttotal: 2m 16s\tremaining: 5m 19s\n",
            "287:\tlearn: 103.9971167\ttotal: 2m 16s\tremaining: 5m 18s\n",
            "288:\tlearn: 103.7548077\ttotal: 2m 17s\tremaining: 5m 18s\n",
            "289:\tlearn: 103.3240287\ttotal: 2m 17s\tremaining: 5m 17s\n",
            "290:\tlearn: 103.1755963\ttotal: 2m 18s\tremaining: 5m 17s\n",
            "291:\tlearn: 102.8866737\ttotal: 2m 18s\tremaining: 5m 17s\n",
            "292:\tlearn: 102.8312116\ttotal: 2m 19s\tremaining: 5m 16s\n",
            "293:\tlearn: 102.3448594\ttotal: 2m 19s\tremaining: 5m 16s\n",
            "294:\tlearn: 102.2722151\ttotal: 2m 20s\tremaining: 5m 16s\n",
            "295:\tlearn: 102.1445668\ttotal: 2m 21s\tremaining: 5m 16s\n",
            "296:\tlearn: 101.9247106\ttotal: 2m 21s\tremaining: 5m 15s\n",
            "297:\tlearn: 101.8624650\ttotal: 2m 22s\tremaining: 5m 15s\n",
            "298:\tlearn: 101.5917351\ttotal: 2m 22s\tremaining: 5m 14s\n",
            "299:\tlearn: 101.2814813\ttotal: 2m 23s\tremaining: 5m 14s\n",
            "300:\tlearn: 101.1562801\ttotal: 2m 23s\tremaining: 5m 14s\n",
            "301:\tlearn: 100.9681276\ttotal: 2m 24s\tremaining: 5m 13s\n",
            "302:\tlearn: 100.5443576\ttotal: 2m 24s\tremaining: 5m 13s\n",
            "303:\tlearn: 100.4769605\ttotal: 2m 25s\tremaining: 5m 13s\n",
            "304:\tlearn: 100.4224768\ttotal: 2m 25s\tremaining: 5m 12s\n",
            "305:\tlearn: 100.2244318\ttotal: 2m 26s\tremaining: 5m 12s\n",
            "306:\tlearn: 100.0389459\ttotal: 2m 26s\tremaining: 5m 11s\n",
            "307:\tlearn: 99.9425600\ttotal: 2m 27s\tremaining: 5m 11s\n",
            "308:\tlearn: 99.8975717\ttotal: 2m 27s\tremaining: 5m 11s\n",
            "309:\tlearn: 99.3747808\ttotal: 2m 28s\tremaining: 5m 10s\n",
            "310:\tlearn: 98.9395703\ttotal: 2m 28s\tremaining: 5m 10s\n",
            "311:\tlearn: 98.7213890\ttotal: 2m 29s\tremaining: 5m 9s\n",
            "312:\tlearn: 98.6041945\ttotal: 2m 29s\tremaining: 5m 9s\n",
            "313:\tlearn: 98.5086166\ttotal: 2m 30s\tremaining: 5m 8s\n",
            "314:\tlearn: 98.3562821\ttotal: 2m 30s\tremaining: 5m 8s\n",
            "315:\tlearn: 98.1599913\ttotal: 2m 31s\tremaining: 5m 8s\n",
            "316:\tlearn: 97.9779104\ttotal: 2m 31s\tremaining: 5m 7s\n",
            "317:\tlearn: 97.8478943\ttotal: 2m 32s\tremaining: 5m 7s\n",
            "318:\tlearn: 97.3210039\ttotal: 2m 32s\tremaining: 5m 6s\n",
            "319:\tlearn: 97.2093520\ttotal: 2m 33s\tremaining: 5m 6s\n",
            "320:\tlearn: 97.0504944\ttotal: 2m 34s\tremaining: 5m 6s\n",
            "321:\tlearn: 96.9476762\ttotal: 2m 34s\tremaining: 5m 5s\n",
            "322:\tlearn: 96.7863585\ttotal: 2m 35s\tremaining: 5m 5s\n",
            "323:\tlearn: 96.7340064\ttotal: 2m 35s\tremaining: 5m 5s\n",
            "324:\tlearn: 96.4698870\ttotal: 2m 36s\tremaining: 5m 4s\n",
            "325:\tlearn: 96.0818442\ttotal: 2m 36s\tremaining: 5m 4s\n",
            "326:\tlearn: 95.9171486\ttotal: 2m 37s\tremaining: 5m 4s\n",
            "327:\tlearn: 95.5708117\ttotal: 2m 37s\tremaining: 5m 3s\n",
            "328:\tlearn: 95.2700735\ttotal: 2m 38s\tremaining: 5m 3s\n",
            "329:\tlearn: 95.0841758\ttotal: 2m 38s\tremaining: 5m 2s\n",
            "330:\tlearn: 94.9578848\ttotal: 2m 39s\tremaining: 5m 2s\n",
            "331:\tlearn: 94.8417387\ttotal: 2m 40s\tremaining: 5m 2s\n",
            "332:\tlearn: 94.5894168\ttotal: 2m 40s\tremaining: 5m 1s\n",
            "333:\tlearn: 94.3145957\ttotal: 2m 40s\tremaining: 5m 1s\n",
            "334:\tlearn: 94.2200126\ttotal: 2m 41s\tremaining: 5m\n",
            "335:\tlearn: 94.1497871\ttotal: 2m 41s\tremaining: 5m\n",
            "336:\tlearn: 93.8944742\ttotal: 2m 42s\tremaining: 4m 59s\n",
            "337:\tlearn: 93.7814408\ttotal: 2m 42s\tremaining: 4m 59s\n",
            "338:\tlearn: 93.6905957\ttotal: 2m 43s\tremaining: 4m 58s\n",
            "339:\tlearn: 93.5141394\ttotal: 2m 43s\tremaining: 4m 58s\n",
            "340:\tlearn: 93.4435283\ttotal: 2m 44s\tremaining: 4m 58s\n",
            "341:\tlearn: 93.3390971\ttotal: 2m 45s\tremaining: 4m 57s\n",
            "342:\tlearn: 93.0240778\ttotal: 2m 45s\tremaining: 4m 57s\n",
            "343:\tlearn: 92.7607689\ttotal: 2m 45s\tremaining: 4m 56s\n",
            "344:\tlearn: 92.6551821\ttotal: 2m 46s\tremaining: 4m 56s\n",
            "345:\tlearn: 92.3963966\ttotal: 2m 46s\tremaining: 4m 55s\n",
            "346:\tlearn: 92.3859065\ttotal: 2m 47s\tremaining: 4m 55s\n",
            "347:\tlearn: 92.3094713\ttotal: 2m 47s\tremaining: 4m 54s\n",
            "348:\tlearn: 92.0616832\ttotal: 2m 48s\tremaining: 4m 54s\n",
            "349:\tlearn: 92.0232986\ttotal: 2m 48s\tremaining: 4m 53s\n",
            "350:\tlearn: 92.0011319\ttotal: 2m 49s\tremaining: 4m 53s\n",
            "351:\tlearn: 91.9513752\ttotal: 2m 50s\tremaining: 4m 53s\n",
            "352:\tlearn: 91.7547174\ttotal: 2m 50s\tremaining: 4m 52s\n",
            "353:\tlearn: 91.6222220\ttotal: 2m 51s\tremaining: 4m 52s\n",
            "354:\tlearn: 91.5809909\ttotal: 2m 51s\tremaining: 4m 51s\n",
            "355:\tlearn: 91.5724136\ttotal: 2m 52s\tremaining: 4m 51s\n",
            "356:\tlearn: 91.5320882\ttotal: 2m 52s\tremaining: 4m 51s\n",
            "357:\tlearn: 91.4381705\ttotal: 2m 53s\tremaining: 4m 51s\n",
            "358:\tlearn: 91.3754560\ttotal: 2m 53s\tremaining: 4m 50s\n",
            "359:\tlearn: 91.3129460\ttotal: 2m 54s\tremaining: 4m 50s\n",
            "360:\tlearn: 91.2577447\ttotal: 2m 55s\tremaining: 4m 50s\n",
            "361:\tlearn: 91.1465407\ttotal: 2m 55s\tremaining: 4m 49s\n",
            "362:\tlearn: 91.1462866\ttotal: 2m 56s\tremaining: 4m 49s\n",
            "363:\tlearn: 90.6828145\ttotal: 2m 56s\tremaining: 4m 48s\n",
            "364:\tlearn: 90.6121222\ttotal: 2m 57s\tremaining: 4m 48s\n",
            "365:\tlearn: 90.5445996\ttotal: 2m 57s\tremaining: 4m 47s\n",
            "366:\tlearn: 90.2960172\ttotal: 2m 58s\tremaining: 4m 47s\n",
            "367:\tlearn: 90.1777673\ttotal: 2m 58s\tremaining: 4m 46s\n",
            "368:\tlearn: 90.0891025\ttotal: 2m 59s\tremaining: 4m 46s\n",
            "369:\tlearn: 89.8090104\ttotal: 2m 59s\tremaining: 4m 45s\n",
            "370:\tlearn: 89.7302244\ttotal: 3m\tremaining: 4m 45s\n",
            "371:\tlearn: 89.6258917\ttotal: 3m\tremaining: 4m 45s\n",
            "372:\tlearn: 89.5589572\ttotal: 3m 1s\tremaining: 4m 44s\n",
            "373:\tlearn: 89.4969275\ttotal: 3m 1s\tremaining: 4m 44s\n",
            "374:\tlearn: 89.4577358\ttotal: 3m 2s\tremaining: 4m 43s\n",
            "375:\tlearn: 89.3664057\ttotal: 3m 2s\tremaining: 4m 43s\n",
            "376:\tlearn: 89.2622573\ttotal: 3m 3s\tremaining: 4m 42s\n",
            "377:\tlearn: 89.1928803\ttotal: 3m 3s\tremaining: 4m 42s\n",
            "378:\tlearn: 89.0877554\ttotal: 3m 4s\tremaining: 4m 41s\n",
            "379:\tlearn: 88.7087197\ttotal: 3m 4s\tremaining: 4m 41s\n",
            "380:\tlearn: 88.6672609\ttotal: 3m 5s\tremaining: 4m 41s\n",
            "381:\tlearn: 88.4845634\ttotal: 3m 5s\tremaining: 4m 40s\n",
            "382:\tlearn: 88.1881669\ttotal: 3m 6s\tremaining: 4m 40s\n",
            "383:\tlearn: 88.1014989\ttotal: 3m 6s\tremaining: 4m 39s\n",
            "384:\tlearn: 88.0314306\ttotal: 3m 7s\tremaining: 4m 39s\n",
            "385:\tlearn: 87.9164943\ttotal: 3m 7s\tremaining: 4m 38s\n",
            "386:\tlearn: 87.8080153\ttotal: 3m 8s\tremaining: 4m 38s\n",
            "387:\tlearn: 87.4193722\ttotal: 3m 8s\tremaining: 4m 38s\n",
            "388:\tlearn: 87.1093191\ttotal: 3m 9s\tremaining: 4m 37s\n",
            "389:\tlearn: 86.8482294\ttotal: 3m 10s\tremaining: 4m 37s\n",
            "390:\tlearn: 86.7154494\ttotal: 3m 10s\tremaining: 4m 36s\n",
            "391:\tlearn: 86.5976693\ttotal: 3m 11s\tremaining: 4m 36s\n",
            "392:\tlearn: 86.5969788\ttotal: 3m 11s\tremaining: 4m 36s\n",
            "393:\tlearn: 86.2028572\ttotal: 3m 12s\tremaining: 4m 35s\n",
            "394:\tlearn: 86.1260322\ttotal: 3m 12s\tremaining: 4m 35s\n",
            "395:\tlearn: 86.0623581\ttotal: 3m 13s\tremaining: 4m 34s\n",
            "396:\tlearn: 86.0225124\ttotal: 3m 13s\tremaining: 4m 34s\n",
            "397:\tlearn: 85.8701791\ttotal: 3m 14s\tremaining: 4m 34s\n",
            "398:\tlearn: 85.7572156\ttotal: 3m 14s\tremaining: 4m 33s\n",
            "399:\tlearn: 85.4920724\ttotal: 3m 15s\tremaining: 4m 33s\n",
            "400:\tlearn: 85.4242531\ttotal: 3m 16s\tremaining: 4m 32s\n",
            "401:\tlearn: 85.3885288\ttotal: 3m 16s\tremaining: 4m 32s\n",
            "402:\tlearn: 85.3068571\ttotal: 3m 17s\tremaining: 4m 32s\n",
            "403:\tlearn: 85.2065285\ttotal: 3m 17s\tremaining: 4m 31s\n",
            "404:\tlearn: 84.8459470\ttotal: 3m 18s\tremaining: 4m 31s\n",
            "405:\tlearn: 84.6535583\ttotal: 3m 18s\tremaining: 4m 30s\n",
            "406:\tlearn: 84.5009817\ttotal: 3m 19s\tremaining: 4m 30s\n",
            "407:\tlearn: 84.4603520\ttotal: 3m 19s\tremaining: 4m 29s\n",
            "408:\tlearn: 84.3120759\ttotal: 3m 20s\tremaining: 4m 29s\n",
            "409:\tlearn: 84.2400139\ttotal: 3m 20s\tremaining: 4m 28s\n",
            "410:\tlearn: 83.8856508\ttotal: 3m 21s\tremaining: 4m 28s\n",
            "411:\tlearn: 83.7172615\ttotal: 3m 21s\tremaining: 4m 27s\n",
            "412:\tlearn: 83.5727600\ttotal: 3m 22s\tremaining: 4m 27s\n",
            "413:\tlearn: 83.5130975\ttotal: 3m 22s\tremaining: 4m 27s\n",
            "414:\tlearn: 83.4528487\ttotal: 3m 23s\tremaining: 4m 26s\n",
            "415:\tlearn: 83.4134441\ttotal: 3m 24s\tremaining: 4m 26s\n",
            "416:\tlearn: 83.3722749\ttotal: 3m 24s\tremaining: 4m 26s\n",
            "417:\tlearn: 83.2701801\ttotal: 3m 25s\tremaining: 4m 25s\n",
            "418:\tlearn: 82.9681454\ttotal: 3m 25s\tremaining: 4m 25s\n",
            "419:\tlearn: 82.8718954\ttotal: 3m 26s\tremaining: 4m 24s\n",
            "420:\tlearn: 82.8523653\ttotal: 3m 26s\tremaining: 4m 24s\n",
            "421:\tlearn: 82.7797961\ttotal: 3m 27s\tremaining: 4m 23s\n",
            "422:\tlearn: 82.5506582\ttotal: 3m 27s\tremaining: 4m 23s\n",
            "423:\tlearn: 82.4895854\ttotal: 3m 28s\tremaining: 4m 22s\n",
            "424:\tlearn: 82.2442125\ttotal: 3m 28s\tremaining: 4m 22s\n",
            "425:\tlearn: 82.1755045\ttotal: 3m 29s\tremaining: 4m 21s\n",
            "426:\tlearn: 82.0381275\ttotal: 3m 29s\tremaining: 4m 21s\n",
            "427:\tlearn: 81.9242773\ttotal: 3m 30s\tremaining: 4m 20s\n",
            "428:\tlearn: 81.7153359\ttotal: 3m 30s\tremaining: 4m 20s\n",
            "429:\tlearn: 81.6564876\ttotal: 3m 31s\tremaining: 4m 19s\n",
            "430:\tlearn: 81.5933740\ttotal: 3m 31s\tremaining: 4m 19s\n",
            "431:\tlearn: 81.4390458\ttotal: 3m 32s\tremaining: 4m 18s\n",
            "432:\tlearn: 81.1055814\ttotal: 3m 32s\tremaining: 4m 18s\n",
            "433:\tlearn: 81.0832960\ttotal: 3m 33s\tremaining: 4m 18s\n",
            "434:\tlearn: 80.9918671\ttotal: 3m 33s\tremaining: 4m 17s\n",
            "435:\tlearn: 80.8335435\ttotal: 3m 34s\tremaining: 4m 17s\n",
            "436:\tlearn: 80.7431897\ttotal: 3m 34s\tremaining: 4m 16s\n",
            "437:\tlearn: 80.6660917\ttotal: 3m 35s\tremaining: 4m 16s\n",
            "438:\tlearn: 80.5856941\ttotal: 3m 35s\tremaining: 4m 15s\n",
            "439:\tlearn: 80.4662920\ttotal: 3m 36s\tremaining: 4m 15s\n",
            "440:\tlearn: 80.2643934\ttotal: 3m 36s\tremaining: 4m 14s\n",
            "441:\tlearn: 80.1937462\ttotal: 3m 37s\tremaining: 4m 14s\n",
            "442:\tlearn: 80.0529009\ttotal: 3m 37s\tremaining: 4m 13s\n",
            "443:\tlearn: 80.0349852\ttotal: 3m 38s\tremaining: 4m 13s\n",
            "444:\tlearn: 79.8057482\ttotal: 3m 39s\tremaining: 4m 13s\n",
            "445:\tlearn: 79.6235282\ttotal: 3m 39s\tremaining: 4m 12s\n",
            "446:\tlearn: 79.4682369\ttotal: 3m 40s\tremaining: 4m 12s\n",
            "447:\tlearn: 79.3866518\ttotal: 3m 40s\tremaining: 4m 11s\n",
            "448:\tlearn: 79.2078132\ttotal: 3m 41s\tremaining: 4m 11s\n",
            "449:\tlearn: 79.1512048\ttotal: 3m 41s\tremaining: 4m 10s\n",
            "450:\tlearn: 79.0622735\ttotal: 3m 42s\tremaining: 4m 10s\n",
            "451:\tlearn: 78.9336653\ttotal: 3m 42s\tremaining: 4m 9s\n",
            "452:\tlearn: 78.8660822\ttotal: 3m 43s\tremaining: 4m 9s\n",
            "453:\tlearn: 78.8086060\ttotal: 3m 43s\tremaining: 4m 8s\n",
            "454:\tlearn: 78.7500075\ttotal: 3m 44s\tremaining: 4m 8s\n",
            "455:\tlearn: 78.6835161\ttotal: 3m 44s\tremaining: 4m 7s\n",
            "456:\tlearn: 78.5898217\ttotal: 3m 45s\tremaining: 4m 7s\n",
            "457:\tlearn: 78.4796846\ttotal: 3m 45s\tremaining: 4m 7s\n",
            "458:\tlearn: 78.3793993\ttotal: 3m 46s\tremaining: 4m 6s\n",
            "459:\tlearn: 78.3045901\ttotal: 3m 46s\tremaining: 4m 6s\n",
            "460:\tlearn: 78.2448327\ttotal: 3m 47s\tremaining: 4m 5s\n",
            "461:\tlearn: 78.0392291\ttotal: 3m 47s\tremaining: 4m 4s\n",
            "462:\tlearn: 78.0103541\ttotal: 3m 48s\tremaining: 4m 4s\n",
            "463:\tlearn: 77.8718039\ttotal: 3m 48s\tremaining: 4m 4s\n",
            "464:\tlearn: 77.8044620\ttotal: 3m 49s\tremaining: 4m 3s\n",
            "465:\tlearn: 77.7321909\ttotal: 3m 49s\tremaining: 4m 3s\n",
            "466:\tlearn: 77.5998209\ttotal: 3m 50s\tremaining: 4m 2s\n",
            "467:\tlearn: 77.5342794\ttotal: 3m 50s\tremaining: 4m 2s\n",
            "468:\tlearn: 77.4571946\ttotal: 3m 51s\tremaining: 4m 1s\n",
            "469:\tlearn: 77.2700181\ttotal: 3m 51s\tremaining: 4m 1s\n",
            "470:\tlearn: 77.2016606\ttotal: 3m 52s\tremaining: 4m\n",
            "471:\tlearn: 77.1717908\ttotal: 3m 52s\tremaining: 4m\n",
            "472:\tlearn: 77.1591004\ttotal: 3m 53s\tremaining: 3m 59s\n",
            "473:\tlearn: 76.9862640\ttotal: 3m 53s\tremaining: 3m 59s\n",
            "474:\tlearn: 76.9391635\ttotal: 3m 54s\tremaining: 3m 58s\n",
            "475:\tlearn: 76.8040034\ttotal: 3m 55s\tremaining: 3m 58s\n",
            "476:\tlearn: 76.7649725\ttotal: 3m 55s\tremaining: 3m 58s\n",
            "477:\tlearn: 76.5812953\ttotal: 3m 56s\tremaining: 3m 57s\n",
            "478:\tlearn: 76.5399148\ttotal: 3m 56s\tremaining: 3m 57s\n",
            "479:\tlearn: 76.3686795\ttotal: 3m 57s\tremaining: 3m 56s\n",
            "480:\tlearn: 76.1954448\ttotal: 3m 57s\tremaining: 3m 56s\n",
            "481:\tlearn: 76.0780388\ttotal: 3m 58s\tremaining: 3m 55s\n",
            "482:\tlearn: 75.9989202\ttotal: 3m 58s\tremaining: 3m 55s\n",
            "483:\tlearn: 75.9619335\ttotal: 3m 59s\tremaining: 3m 55s\n",
            "484:\tlearn: 75.9317526\ttotal: 4m\tremaining: 3m 54s\n",
            "485:\tlearn: 75.8063855\ttotal: 4m\tremaining: 3m 54s\n",
            "486:\tlearn: 75.7188992\ttotal: 4m\tremaining: 3m 53s\n",
            "487:\tlearn: 75.6845430\ttotal: 4m 1s\tremaining: 3m 53s\n",
            "488:\tlearn: 75.5941892\ttotal: 4m 2s\tremaining: 3m 52s\n",
            "489:\tlearn: 75.5291662\ttotal: 4m 2s\tremaining: 3m 52s\n",
            "490:\tlearn: 75.4729847\ttotal: 4m 3s\tremaining: 3m 51s\n",
            "491:\tlearn: 75.4195762\ttotal: 4m 3s\tremaining: 3m 51s\n",
            "492:\tlearn: 75.3179038\ttotal: 4m 4s\tremaining: 3m 50s\n",
            "493:\tlearn: 75.2308722\ttotal: 4m 4s\tremaining: 3m 50s\n",
            "494:\tlearn: 75.1801090\ttotal: 4m 5s\tremaining: 3m 49s\n",
            "495:\tlearn: 75.0731962\ttotal: 4m 5s\tremaining: 3m 49s\n",
            "496:\tlearn: 74.9969089\ttotal: 4m 6s\tremaining: 3m 48s\n",
            "497:\tlearn: 74.9965610\ttotal: 4m 6s\tremaining: 3m 48s\n",
            "498:\tlearn: 74.9344703\ttotal: 4m 7s\tremaining: 3m 47s\n",
            "499:\tlearn: 74.8160926\ttotal: 4m 7s\tremaining: 3m 47s\n",
            "500:\tlearn: 74.7836280\ttotal: 4m 8s\tremaining: 3m 46s\n",
            "501:\tlearn: 74.6627358\ttotal: 4m 8s\tremaining: 3m 46s\n",
            "502:\tlearn: 74.5676454\ttotal: 4m 9s\tremaining: 3m 45s\n",
            "503:\tlearn: 74.4583970\ttotal: 4m 9s\tremaining: 3m 45s\n",
            "504:\tlearn: 74.3985819\ttotal: 4m 10s\tremaining: 3m 44s\n",
            "505:\tlearn: 74.2150579\ttotal: 4m 10s\tremaining: 3m 44s\n",
            "506:\tlearn: 74.1329955\ttotal: 4m 11s\tremaining: 3m 43s\n",
            "507:\tlearn: 74.0983266\ttotal: 4m 11s\tremaining: 3m 43s\n",
            "508:\tlearn: 74.0094038\ttotal: 4m 12s\tremaining: 3m 42s\n",
            "509:\tlearn: 73.9122521\ttotal: 4m 12s\tremaining: 3m 42s\n",
            "510:\tlearn: 73.7386915\ttotal: 4m 13s\tremaining: 3m 42s\n",
            "511:\tlearn: 73.6872493\ttotal: 4m 13s\tremaining: 3m 41s\n",
            "512:\tlearn: 73.5872162\ttotal: 4m 14s\tremaining: 3m 41s\n",
            "513:\tlearn: 73.5571562\ttotal: 4m 14s\tremaining: 3m 40s\n",
            "514:\tlearn: 73.5231868\ttotal: 4m 15s\tremaining: 3m 40s\n",
            "515:\tlearn: 73.4669229\ttotal: 4m 15s\tremaining: 3m 39s\n",
            "516:\tlearn: 73.4019347\ttotal: 4m 16s\tremaining: 3m 39s\n",
            "517:\tlearn: 73.3954438\ttotal: 4m 16s\tremaining: 3m 38s\n",
            "518:\tlearn: 73.3049168\ttotal: 4m 17s\tremaining: 3m 38s\n",
            "519:\tlearn: 73.2526797\ttotal: 4m 18s\tremaining: 3m 37s\n",
            "520:\tlearn: 73.1389383\ttotal: 4m 18s\tremaining: 3m 37s\n",
            "521:\tlearn: 73.0215452\ttotal: 4m 18s\tremaining: 3m 36s\n",
            "522:\tlearn: 72.9680591\ttotal: 4m 19s\tremaining: 3m 36s\n",
            "523:\tlearn: 72.9386518\ttotal: 4m 19s\tremaining: 3m 35s\n",
            "524:\tlearn: 72.9078228\ttotal: 4m 20s\tremaining: 3m 35s\n",
            "525:\tlearn: 72.7898128\ttotal: 4m 21s\tremaining: 3m 34s\n",
            "526:\tlearn: 72.7391048\ttotal: 4m 21s\tremaining: 3m 34s\n",
            "527:\tlearn: 72.7354503\ttotal: 4m 22s\tremaining: 3m 33s\n",
            "528:\tlearn: 72.5550215\ttotal: 4m 22s\tremaining: 3m 33s\n",
            "529:\tlearn: 72.5079256\ttotal: 4m 22s\tremaining: 3m 32s\n",
            "530:\tlearn: 72.4415703\ttotal: 4m 23s\tremaining: 3m 32s\n",
            "531:\tlearn: 72.3380949\ttotal: 4m 24s\tremaining: 3m 31s\n",
            "532:\tlearn: 72.2519548\ttotal: 4m 24s\tremaining: 3m 31s\n",
            "533:\tlearn: 72.1660898\ttotal: 4m 25s\tremaining: 3m 30s\n",
            "534:\tlearn: 72.0176774\ttotal: 4m 25s\tremaining: 3m 30s\n",
            "535:\tlearn: 71.9134760\ttotal: 4m 26s\tremaining: 3m 29s\n",
            "536:\tlearn: 71.7961076\ttotal: 4m 26s\tremaining: 3m 29s\n",
            "537:\tlearn: 71.6948769\ttotal: 4m 27s\tremaining: 3m 29s\n",
            "538:\tlearn: 71.6495186\ttotal: 4m 27s\tremaining: 3m 28s\n",
            "539:\tlearn: 71.5382182\ttotal: 4m 28s\tremaining: 3m 28s\n",
            "540:\tlearn: 71.4183362\ttotal: 4m 28s\tremaining: 3m 27s\n",
            "541:\tlearn: 71.3389666\ttotal: 4m 29s\tremaining: 3m 27s\n",
            "542:\tlearn: 71.2991206\ttotal: 4m 29s\tremaining: 3m 26s\n",
            "543:\tlearn: 71.2621863\ttotal: 4m 30s\tremaining: 3m 26s\n",
            "544:\tlearn: 71.2249111\ttotal: 4m 30s\tremaining: 3m 25s\n",
            "545:\tlearn: 71.1525965\ttotal: 4m 31s\tremaining: 3m 25s\n",
            "546:\tlearn: 71.0260060\ttotal: 4m 31s\tremaining: 3m 24s\n",
            "547:\tlearn: 70.9042893\ttotal: 4m 32s\tremaining: 3m 24s\n",
            "548:\tlearn: 70.8741977\ttotal: 4m 32s\tremaining: 3m 23s\n",
            "549:\tlearn: 70.8049884\ttotal: 4m 33s\tremaining: 3m 23s\n",
            "550:\tlearn: 70.6707757\ttotal: 4m 33s\tremaining: 3m 22s\n",
            "551:\tlearn: 70.6248122\ttotal: 4m 34s\tremaining: 3m 22s\n",
            "552:\tlearn: 70.5774002\ttotal: 4m 35s\tremaining: 3m 21s\n",
            "553:\tlearn: 70.4561293\ttotal: 4m 35s\tremaining: 3m 21s\n",
            "554:\tlearn: 70.4228285\ttotal: 4m 36s\tremaining: 3m 21s\n",
            "555:\tlearn: 70.1670465\ttotal: 4m 36s\tremaining: 3m 20s\n",
            "556:\tlearn: 70.0857797\ttotal: 4m 37s\tremaining: 3m 20s\n",
            "557:\tlearn: 69.9643660\ttotal: 4m 37s\tremaining: 3m 19s\n",
            "558:\tlearn: 69.7792931\ttotal: 4m 38s\tremaining: 3m 19s\n",
            "559:\tlearn: 69.7112123\ttotal: 4m 38s\tremaining: 3m 18s\n",
            "560:\tlearn: 69.6413544\ttotal: 4m 39s\tremaining: 3m 18s\n",
            "561:\tlearn: 69.5829690\ttotal: 4m 39s\tremaining: 3m 17s\n",
            "562:\tlearn: 69.5201779\ttotal: 4m 40s\tremaining: 3m 17s\n",
            "563:\tlearn: 69.4597946\ttotal: 4m 40s\tremaining: 3m 16s\n",
            "564:\tlearn: 69.2129769\ttotal: 4m 41s\tremaining: 3m 16s\n",
            "565:\tlearn: 69.0172174\ttotal: 4m 41s\tremaining: 3m 15s\n",
            "566:\tlearn: 68.9693983\ttotal: 4m 42s\tremaining: 3m 15s\n",
            "567:\tlearn: 68.8629873\ttotal: 4m 42s\tremaining: 3m 14s\n",
            "568:\tlearn: 68.8404160\ttotal: 4m 43s\tremaining: 3m 14s\n",
            "569:\tlearn: 68.7933512\ttotal: 4m 43s\tremaining: 3m 13s\n",
            "570:\tlearn: 68.7598628\ttotal: 4m 44s\tremaining: 3m 13s\n",
            "571:\tlearn: 68.6947368\ttotal: 4m 45s\tremaining: 3m 12s\n",
            "572:\tlearn: 68.6514057\ttotal: 4m 45s\tremaining: 3m 12s\n",
            "573:\tlearn: 68.5067141\ttotal: 4m 46s\tremaining: 3m 11s\n",
            "574:\tlearn: 68.4726555\ttotal: 4m 46s\tremaining: 3m 11s\n",
            "575:\tlearn: 68.4140230\ttotal: 4m 47s\tremaining: 3m 11s\n",
            "576:\tlearn: 68.3774641\ttotal: 4m 47s\tremaining: 3m 10s\n",
            "577:\tlearn: 68.2108469\ttotal: 4m 48s\tremaining: 3m 10s\n",
            "578:\tlearn: 68.1129021\ttotal: 4m 48s\tremaining: 3m 9s\n",
            "579:\tlearn: 68.0713551\ttotal: 4m 49s\tremaining: 3m 9s\n",
            "580:\tlearn: 67.9831550\ttotal: 4m 50s\tremaining: 3m 8s\n",
            "581:\tlearn: 67.9050949\ttotal: 4m 50s\tremaining: 3m 8s\n",
            "582:\tlearn: 67.8501043\ttotal: 4m 51s\tremaining: 3m 7s\n",
            "583:\tlearn: 67.7058723\ttotal: 4m 51s\tremaining: 3m 7s\n",
            "584:\tlearn: 67.6016523\ttotal: 4m 51s\tremaining: 3m 6s\n",
            "585:\tlearn: 67.4540990\ttotal: 4m 52s\tremaining: 3m 6s\n",
            "586:\tlearn: 67.4158702\ttotal: 4m 52s\tremaining: 3m 5s\n",
            "587:\tlearn: 67.3605741\ttotal: 4m 53s\tremaining: 3m 5s\n",
            "588:\tlearn: 67.2748717\ttotal: 4m 53s\tremaining: 3m 4s\n",
            "589:\tlearn: 67.2298279\ttotal: 4m 54s\tremaining: 3m 4s\n",
            "590:\tlearn: 67.1844159\ttotal: 4m 54s\tremaining: 3m 3s\n",
            "591:\tlearn: 67.0957581\ttotal: 4m 55s\tremaining: 3m 3s\n",
            "592:\tlearn: 67.0523728\ttotal: 4m 55s\tremaining: 3m 2s\n",
            "593:\tlearn: 67.0201958\ttotal: 4m 56s\tremaining: 3m 2s\n",
            "594:\tlearn: 66.9620976\ttotal: 4m 57s\tremaining: 3m 1s\n",
            "595:\tlearn: 66.9201828\ttotal: 4m 57s\tremaining: 3m 1s\n",
            "596:\tlearn: 66.7402490\ttotal: 4m 58s\tremaining: 3m\n",
            "597:\tlearn: 66.7239841\ttotal: 4m 58s\tremaining: 3m\n",
            "598:\tlearn: 66.6740517\ttotal: 4m 59s\tremaining: 2m 59s\n",
            "599:\tlearn: 66.6201438\ttotal: 4m 59s\tremaining: 2m 59s\n",
            "600:\tlearn: 66.5334786\ttotal: 5m\tremaining: 2m 58s\n",
            "601:\tlearn: 66.3910120\ttotal: 5m\tremaining: 2m 58s\n",
            "602:\tlearn: 66.3901139\ttotal: 5m 1s\tremaining: 2m 57s\n",
            "603:\tlearn: 66.3493932\ttotal: 5m 2s\tremaining: 2m 57s\n",
            "604:\tlearn: 66.3205289\ttotal: 5m 2s\tremaining: 2m 57s\n",
            "605:\tlearn: 66.3204165\ttotal: 5m 3s\tremaining: 2m 56s\n",
            "606:\tlearn: 66.2293858\ttotal: 5m 3s\tremaining: 2m 56s\n",
            "607:\tlearn: 66.1705130\ttotal: 5m 4s\tremaining: 2m 55s\n",
            "608:\tlearn: 66.1244048\ttotal: 5m 4s\tremaining: 2m 55s\n",
            "609:\tlearn: 66.0594363\ttotal: 5m 5s\tremaining: 2m 54s\n",
            "610:\tlearn: 65.9229395\ttotal: 5m 5s\tremaining: 2m 54s\n",
            "611:\tlearn: 65.7730347\ttotal: 5m 6s\tremaining: 2m 53s\n",
            "612:\tlearn: 65.6254269\ttotal: 5m 6s\tremaining: 2m 53s\n",
            "613:\tlearn: 65.5388248\ttotal: 5m 7s\tremaining: 2m 52s\n",
            "614:\tlearn: 65.4934101\ttotal: 5m 7s\tremaining: 2m 52s\n",
            "615:\tlearn: 65.4545682\ttotal: 5m 8s\tremaining: 2m 51s\n",
            "616:\tlearn: 65.3328235\ttotal: 5m 8s\tremaining: 2m 51s\n",
            "617:\tlearn: 65.2982952\ttotal: 5m 9s\tremaining: 2m 50s\n",
            "618:\tlearn: 65.2645243\ttotal: 5m 9s\tremaining: 2m 50s\n",
            "619:\tlearn: 65.2343340\ttotal: 5m 10s\tremaining: 2m 49s\n",
            "620:\tlearn: 64.9607653\ttotal: 5m 10s\tremaining: 2m 49s\n",
            "621:\tlearn: 64.8863324\ttotal: 5m 11s\tremaining: 2m 48s\n",
            "622:\tlearn: 64.8355136\ttotal: 5m 11s\tremaining: 2m 48s\n",
            "623:\tlearn: 64.7815079\ttotal: 5m 12s\tremaining: 2m 47s\n",
            "624:\tlearn: 64.6661200\ttotal: 5m 12s\tremaining: 2m 47s\n",
            "625:\tlearn: 64.6476685\ttotal: 5m 13s\tremaining: 2m 46s\n",
            "626:\tlearn: 64.5798704\ttotal: 5m 13s\tremaining: 2m 46s\n",
            "627:\tlearn: 64.5237530\ttotal: 5m 14s\tremaining: 2m 45s\n",
            "628:\tlearn: 64.4682932\ttotal: 5m 15s\tremaining: 2m 45s\n",
            "629:\tlearn: 64.3336893\ttotal: 5m 15s\tremaining: 2m 44s\n",
            "630:\tlearn: 64.1893887\ttotal: 5m 16s\tremaining: 2m 44s\n",
            "631:\tlearn: 64.0877552\ttotal: 5m 16s\tremaining: 2m 43s\n",
            "632:\tlearn: 64.0260876\ttotal: 5m 17s\tremaining: 2m 43s\n",
            "633:\tlearn: 64.0045309\ttotal: 5m 17s\tremaining: 2m 42s\n",
            "634:\tlearn: 63.9205945\ttotal: 5m 18s\tremaining: 2m 42s\n",
            "635:\tlearn: 63.7619625\ttotal: 5m 18s\tremaining: 2m 41s\n",
            "636:\tlearn: 63.6681411\ttotal: 5m 19s\tremaining: 2m 41s\n",
            "637:\tlearn: 63.5792006\ttotal: 5m 19s\tremaining: 2m 40s\n",
            "638:\tlearn: 63.5146311\ttotal: 5m 20s\tremaining: 2m 40s\n",
            "639:\tlearn: 63.4524436\ttotal: 5m 20s\tremaining: 2m 39s\n",
            "640:\tlearn: 63.4086362\ttotal: 5m 21s\tremaining: 2m 39s\n",
            "641:\tlearn: 63.3606618\ttotal: 5m 21s\tremaining: 2m 38s\n",
            "642:\tlearn: 63.2865567\ttotal: 5m 22s\tremaining: 2m 38s\n",
            "643:\tlearn: 63.2462974\ttotal: 5m 22s\tremaining: 2m 37s\n",
            "644:\tlearn: 63.1832016\ttotal: 5m 23s\tremaining: 2m 37s\n",
            "645:\tlearn: 63.1372747\ttotal: 5m 23s\tremaining: 2m 36s\n",
            "646:\tlearn: 63.0758279\ttotal: 5m 24s\tremaining: 2m 36s\n",
            "647:\tlearn: 62.9986323\ttotal: 5m 24s\tremaining: 2m 35s\n",
            "648:\tlearn: 62.9338051\ttotal: 5m 25s\tremaining: 2m 35s\n",
            "649:\tlearn: 62.8552788\ttotal: 5m 25s\tremaining: 2m 34s\n",
            "650:\tlearn: 62.8371729\ttotal: 5m 26s\tremaining: 2m 34s\n",
            "651:\tlearn: 62.8049068\ttotal: 5m 26s\tremaining: 2m 33s\n",
            "652:\tlearn: 62.7373956\ttotal: 5m 27s\tremaining: 2m 33s\n",
            "653:\tlearn: 62.6640885\ttotal: 5m 27s\tremaining: 2m 32s\n",
            "654:\tlearn: 62.5941379\ttotal: 5m 28s\tremaining: 2m 32s\n",
            "655:\tlearn: 62.4920903\ttotal: 5m 28s\tremaining: 2m 31s\n",
            "656:\tlearn: 62.3228558\ttotal: 5m 29s\tremaining: 2m 31s\n",
            "657:\tlearn: 62.2527073\ttotal: 5m 29s\tremaining: 2m 30s\n",
            "658:\tlearn: 62.1901646\ttotal: 5m 30s\tremaining: 2m 30s\n",
            "659:\tlearn: 62.1334384\ttotal: 5m 30s\tremaining: 2m 29s\n",
            "660:\tlearn: 62.1065947\ttotal: 5m 31s\tremaining: 2m 29s\n",
            "661:\tlearn: 62.0105671\ttotal: 5m 32s\tremaining: 2m 28s\n",
            "662:\tlearn: 61.9902443\ttotal: 5m 32s\tremaining: 2m 28s\n",
            "663:\tlearn: 61.9890661\ttotal: 5m 33s\tremaining: 2m 28s\n",
            "664:\tlearn: 61.9265380\ttotal: 5m 33s\tremaining: 2m 27s\n",
            "665:\tlearn: 61.9040435\ttotal: 5m 34s\tremaining: 2m 27s\n",
            "666:\tlearn: 61.8651034\ttotal: 5m 34s\tremaining: 2m 26s\n",
            "667:\tlearn: 61.8337598\ttotal: 5m 35s\tremaining: 2m 26s\n",
            "668:\tlearn: 61.8113178\ttotal: 5m 35s\tremaining: 2m 25s\n",
            "669:\tlearn: 61.7924093\ttotal: 5m 36s\tremaining: 2m 25s\n",
            "670:\tlearn: 61.7512318\ttotal: 5m 36s\tremaining: 2m 24s\n",
            "671:\tlearn: 61.7506281\ttotal: 5m 37s\tremaining: 2m 24s\n",
            "672:\tlearn: 61.6968084\ttotal: 5m 37s\tremaining: 2m 23s\n",
            "673:\tlearn: 61.6008494\ttotal: 5m 38s\tremaining: 2m 23s\n",
            "674:\tlearn: 61.5841014\ttotal: 5m 38s\tremaining: 2m 22s\n",
            "675:\tlearn: 61.5251818\ttotal: 5m 39s\tremaining: 2m 22s\n",
            "676:\tlearn: 61.4432799\ttotal: 5m 39s\tremaining: 2m 21s\n",
            "677:\tlearn: 61.3718072\ttotal: 5m 40s\tremaining: 2m 21s\n",
            "678:\tlearn: 61.2388695\ttotal: 5m 40s\tremaining: 2m 20s\n",
            "679:\tlearn: 61.1780992\ttotal: 5m 41s\tremaining: 2m 20s\n",
            "680:\tlearn: 61.0778409\ttotal: 5m 41s\tremaining: 2m 19s\n",
            "681:\tlearn: 61.0055116\ttotal: 5m 42s\tremaining: 2m 19s\n",
            "682:\tlearn: 60.9801689\ttotal: 5m 43s\tremaining: 2m 18s\n",
            "683:\tlearn: 60.9309196\ttotal: 5m 43s\tremaining: 2m 18s\n",
            "684:\tlearn: 60.8690670\ttotal: 5m 43s\tremaining: 2m 17s\n",
            "685:\tlearn: 60.7700090\ttotal: 5m 44s\tremaining: 2m 17s\n",
            "686:\tlearn: 60.6691979\ttotal: 5m 44s\tremaining: 2m 16s\n",
            "687:\tlearn: 60.5630168\ttotal: 5m 45s\tremaining: 2m 16s\n",
            "688:\tlearn: 60.4666166\ttotal: 5m 45s\tremaining: 2m 15s\n",
            "689:\tlearn: 60.3909397\ttotal: 5m 46s\tremaining: 2m 15s\n",
            "690:\tlearn: 60.3325673\ttotal: 5m 47s\tremaining: 2m 14s\n",
            "691:\tlearn: 60.2509249\ttotal: 5m 47s\tremaining: 2m 14s\n",
            "692:\tlearn: 60.2414329\ttotal: 5m 48s\tremaining: 2m 13s\n",
            "693:\tlearn: 60.1614833\ttotal: 5m 48s\tremaining: 2m 13s\n",
            "694:\tlearn: 60.1086921\ttotal: 5m 49s\tremaining: 2m 12s\n",
            "695:\tlearn: 60.0405206\ttotal: 5m 49s\tremaining: 2m 12s\n",
            "696:\tlearn: 60.0093132\ttotal: 5m 50s\tremaining: 2m 11s\n",
            "697:\tlearn: 59.9105471\ttotal: 5m 50s\tremaining: 2m 11s\n",
            "698:\tlearn: 59.9010275\ttotal: 5m 50s\tremaining: 2m 10s\n",
            "699:\tlearn: 59.8620301\ttotal: 5m 51s\tremaining: 2m 10s\n",
            "700:\tlearn: 59.8185418\ttotal: 5m 51s\tremaining: 2m 9s\n",
            "701:\tlearn: 59.7605336\ttotal: 5m 52s\tremaining: 2m 9s\n",
            "702:\tlearn: 59.7127219\ttotal: 5m 52s\tremaining: 2m 8s\n",
            "703:\tlearn: 59.6564634\ttotal: 5m 53s\tremaining: 2m 8s\n",
            "704:\tlearn: 59.6352607\ttotal: 5m 53s\tremaining: 2m 7s\n",
            "705:\tlearn: 59.5674969\ttotal: 5m 54s\tremaining: 2m 6s\n",
            "706:\tlearn: 59.5400976\ttotal: 5m 54s\tremaining: 2m 6s\n",
            "707:\tlearn: 59.4685542\ttotal: 5m 55s\tremaining: 2m 5s\n",
            "708:\tlearn: 59.3734426\ttotal: 5m 55s\tremaining: 2m 5s\n",
            "709:\tlearn: 59.2834902\ttotal: 5m 56s\tremaining: 2m 4s\n",
            "710:\tlearn: 59.1616727\ttotal: 5m 56s\tremaining: 2m 4s\n",
            "711:\tlearn: 59.1354211\ttotal: 5m 57s\tremaining: 2m 3s\n",
            "712:\tlearn: 59.1172258\ttotal: 5m 57s\tremaining: 2m 3s\n",
            "713:\tlearn: 58.9024520\ttotal: 5m 58s\tremaining: 2m 3s\n",
            "714:\tlearn: 58.8717799\ttotal: 5m 58s\tremaining: 2m 2s\n",
            "715:\tlearn: 58.8551587\ttotal: 5m 59s\tremaining: 2m 2s\n",
            "716:\tlearn: 58.8311552\ttotal: 6m\tremaining: 2m 1s\n",
            "717:\tlearn: 58.7635659\ttotal: 6m\tremaining: 2m 1s\n",
            "718:\tlearn: 58.7015249\ttotal: 6m 1s\tremaining: 2m\n",
            "719:\tlearn: 58.6360474\ttotal: 6m 1s\tremaining: 2m\n",
            "720:\tlearn: 58.4400196\ttotal: 6m 2s\tremaining: 1m 59s\n",
            "721:\tlearn: 58.3809756\ttotal: 6m 2s\tremaining: 1m 59s\n",
            "722:\tlearn: 58.3557502\ttotal: 6m 3s\tremaining: 1m 58s\n",
            "723:\tlearn: 58.2917688\ttotal: 6m 3s\tremaining: 1m 58s\n",
            "724:\tlearn: 58.2602187\ttotal: 6m 4s\tremaining: 1m 57s\n",
            "725:\tlearn: 58.2263772\ttotal: 6m 4s\tremaining: 1m 57s\n",
            "726:\tlearn: 58.2112122\ttotal: 6m 5s\tremaining: 1m 56s\n",
            "727:\tlearn: 58.1253665\ttotal: 6m 5s\tremaining: 1m 56s\n",
            "728:\tlearn: 58.1097330\ttotal: 6m 6s\tremaining: 1m 55s\n",
            "729:\tlearn: 58.0763764\ttotal: 6m 7s\tremaining: 1m 55s\n",
            "730:\tlearn: 58.0221648\ttotal: 6m 7s\tremaining: 1m 54s\n",
            "731:\tlearn: 57.9268624\ttotal: 6m 8s\tremaining: 1m 54s\n",
            "732:\tlearn: 57.9245313\ttotal: 6m 8s\tremaining: 1m 53s\n",
            "733:\tlearn: 57.8696546\ttotal: 6m 9s\tremaining: 1m 53s\n",
            "734:\tlearn: 57.8445498\ttotal: 6m 9s\tremaining: 1m 52s\n",
            "735:\tlearn: 57.8053706\ttotal: 6m 10s\tremaining: 1m 52s\n",
            "736:\tlearn: 57.7519290\ttotal: 6m 10s\tremaining: 1m 51s\n",
            "737:\tlearn: 57.6104138\ttotal: 6m 11s\tremaining: 1m 51s\n",
            "738:\tlearn: 57.4938411\ttotal: 6m 11s\tremaining: 1m 50s\n",
            "739:\tlearn: 57.3867184\ttotal: 6m 12s\tremaining: 1m 50s\n",
            "740:\tlearn: 57.3137691\ttotal: 6m 12s\tremaining: 1m 49s\n",
            "741:\tlearn: 57.2842550\ttotal: 6m 13s\tremaining: 1m 49s\n",
            "742:\tlearn: 57.2157534\ttotal: 6m 13s\tremaining: 1m 48s\n",
            "743:\tlearn: 57.1853572\ttotal: 6m 14s\tremaining: 1m 48s\n",
            "744:\tlearn: 57.1486736\ttotal: 6m 14s\tremaining: 1m 47s\n",
            "745:\tlearn: 57.1024653\ttotal: 6m 15s\tremaining: 1m 47s\n",
            "746:\tlearn: 57.0745254\ttotal: 6m 15s\tremaining: 1m 46s\n",
            "747:\tlearn: 57.0188542\ttotal: 6m 16s\tremaining: 1m 46s\n",
            "748:\tlearn: 56.9954878\ttotal: 6m 16s\tremaining: 1m 45s\n",
            "749:\tlearn: 56.9559146\ttotal: 6m 17s\tremaining: 1m 45s\n",
            "750:\tlearn: 56.9135046\ttotal: 6m 17s\tremaining: 1m 44s\n",
            "751:\tlearn: 56.8818476\ttotal: 6m 18s\tremaining: 1m 44s\n",
            "752:\tlearn: 56.8502885\ttotal: 6m 18s\tremaining: 1m 43s\n",
            "753:\tlearn: 56.8371973\ttotal: 6m 19s\tremaining: 1m 43s\n",
            "754:\tlearn: 56.7999971\ttotal: 6m 19s\tremaining: 1m 42s\n",
            "755:\tlearn: 56.7682912\ttotal: 6m 20s\tremaining: 1m 42s\n",
            "756:\tlearn: 56.7297146\ttotal: 6m 21s\tremaining: 1m 41s\n",
            "757:\tlearn: 56.6866375\ttotal: 6m 21s\tremaining: 1m 41s\n",
            "758:\tlearn: 56.6106696\ttotal: 6m 22s\tremaining: 1m 40s\n",
            "759:\tlearn: 56.5761120\ttotal: 6m 22s\tremaining: 1m 40s\n",
            "760:\tlearn: 56.5322753\ttotal: 6m 23s\tremaining: 1m 39s\n",
            "761:\tlearn: 56.5001640\ttotal: 6m 23s\tremaining: 1m 39s\n",
            "762:\tlearn: 56.4861733\ttotal: 6m 24s\tremaining: 1m 38s\n",
            "763:\tlearn: 56.4604876\ttotal: 6m 24s\tremaining: 1m 38s\n",
            "764:\tlearn: 56.4459029\ttotal: 6m 25s\tremaining: 1m 37s\n",
            "765:\tlearn: 56.4049163\ttotal: 6m 26s\tremaining: 1m 37s\n",
            "766:\tlearn: 56.3451133\ttotal: 6m 26s\tremaining: 1m 36s\n",
            "767:\tlearn: 56.2343818\ttotal: 6m 27s\tremaining: 1m 36s\n",
            "768:\tlearn: 56.1733481\ttotal: 6m 27s\tremaining: 1m 35s\n",
            "769:\tlearn: 56.1160682\ttotal: 6m 28s\tremaining: 1m 35s\n",
            "770:\tlearn: 56.0772527\ttotal: 6m 28s\tremaining: 1m 34s\n",
            "771:\tlearn: 56.0648341\ttotal: 6m 29s\tremaining: 1m 34s\n",
            "772:\tlearn: 55.9914728\ttotal: 6m 29s\tremaining: 1m 33s\n",
            "773:\tlearn: 55.9235387\ttotal: 6m 30s\tremaining: 1m 33s\n",
            "774:\tlearn: 55.8988048\ttotal: 6m 30s\tremaining: 1m 32s\n",
            "775:\tlearn: 55.8431450\ttotal: 6m 31s\tremaining: 1m 32s\n",
            "776:\tlearn: 55.7925820\ttotal: 6m 31s\tremaining: 1m 31s\n",
            "777:\tlearn: 55.7205713\ttotal: 6m 32s\tremaining: 1m 31s\n",
            "778:\tlearn: 55.6259105\ttotal: 6m 32s\tremaining: 1m 30s\n",
            "779:\tlearn: 55.5960682\ttotal: 6m 33s\tremaining: 1m 30s\n",
            "780:\tlearn: 55.5266926\ttotal: 6m 33s\tremaining: 1m 29s\n",
            "781:\tlearn: 55.4757263\ttotal: 6m 34s\tremaining: 1m 29s\n",
            "782:\tlearn: 55.4139688\ttotal: 6m 34s\tremaining: 1m 28s\n",
            "783:\tlearn: 55.2774034\ttotal: 6m 35s\tremaining: 1m 28s\n",
            "784:\tlearn: 55.2426926\ttotal: 6m 35s\tremaining: 1m 27s\n",
            "785:\tlearn: 55.2011473\ttotal: 6m 36s\tremaining: 1m 27s\n",
            "786:\tlearn: 55.1591479\ttotal: 6m 36s\tremaining: 1m 26s\n",
            "787:\tlearn: 55.1310394\ttotal: 6m 37s\tremaining: 1m 26s\n",
            "788:\tlearn: 55.1066658\ttotal: 6m 38s\tremaining: 1m 25s\n",
            "789:\tlearn: 54.9929464\ttotal: 6m 38s\tremaining: 1m 25s\n",
            "790:\tlearn: 54.9573219\ttotal: 6m 39s\tremaining: 1m 24s\n",
            "791:\tlearn: 54.8822021\ttotal: 6m 39s\tremaining: 1m 24s\n",
            "792:\tlearn: 54.8472013\ttotal: 6m 40s\tremaining: 1m 23s\n",
            "793:\tlearn: 54.8081413\ttotal: 6m 40s\tremaining: 1m 23s\n",
            "794:\tlearn: 54.7621480\ttotal: 6m 41s\tremaining: 1m 22s\n",
            "795:\tlearn: 54.6870987\ttotal: 6m 41s\tremaining: 1m 22s\n",
            "796:\tlearn: 54.6290000\ttotal: 6m 42s\tremaining: 1m 21s\n",
            "797:\tlearn: 54.5470315\ttotal: 6m 42s\tremaining: 1m 21s\n",
            "798:\tlearn: 54.5103944\ttotal: 6m 43s\tremaining: 1m 20s\n",
            "799:\tlearn: 54.4803767\ttotal: 6m 43s\tremaining: 1m 20s\n",
            "800:\tlearn: 54.4526506\ttotal: 6m 44s\tremaining: 1m 19s\n",
            "801:\tlearn: 54.4250653\ttotal: 6m 44s\tremaining: 1m 19s\n",
            "802:\tlearn: 54.3732771\ttotal: 6m 45s\tremaining: 1m 18s\n",
            "803:\tlearn: 54.3446735\ttotal: 6m 45s\tremaining: 1m 18s\n",
            "804:\tlearn: 54.2630262\ttotal: 6m 46s\tremaining: 1m 17s\n",
            "805:\tlearn: 54.2050189\ttotal: 6m 46s\tremaining: 1m 17s\n",
            "806:\tlearn: 54.1702768\ttotal: 6m 47s\tremaining: 1m 16s\n",
            "807:\tlearn: 54.1701133\ttotal: 6m 47s\tremaining: 1m 16s\n",
            "808:\tlearn: 54.1543524\ttotal: 6m 48s\tremaining: 1m 15s\n",
            "809:\tlearn: 54.1261867\ttotal: 6m 48s\tremaining: 1m 15s\n",
            "810:\tlearn: 54.0887932\ttotal: 6m 49s\tremaining: 1m 14s\n",
            "811:\tlearn: 53.9482052\ttotal: 6m 49s\tremaining: 1m 14s\n",
            "812:\tlearn: 53.9387377\ttotal: 6m 50s\tremaining: 1m 13s\n",
            "813:\tlearn: 53.9268745\ttotal: 6m 51s\tremaining: 1m 13s\n",
            "814:\tlearn: 53.8852561\ttotal: 6m 51s\tremaining: 1m 12s\n",
            "815:\tlearn: 53.8684527\ttotal: 6m 52s\tremaining: 1m 12s\n",
            "816:\tlearn: 53.8254809\ttotal: 6m 52s\tremaining: 1m 11s\n",
            "817:\tlearn: 53.6335387\ttotal: 6m 53s\tremaining: 1m 11s\n",
            "818:\tlearn: 53.6001126\ttotal: 6m 53s\tremaining: 1m 10s\n",
            "819:\tlearn: 53.5393464\ttotal: 6m 54s\tremaining: 1m 10s\n",
            "820:\tlearn: 53.5120409\ttotal: 6m 54s\tremaining: 1m 9s\n",
            "821:\tlearn: 53.4682852\ttotal: 6m 55s\tremaining: 1m 9s\n",
            "822:\tlearn: 53.4574917\ttotal: 6m 55s\tremaining: 1m 8s\n",
            "823:\tlearn: 53.4127337\ttotal: 6m 56s\tremaining: 1m 8s\n",
            "824:\tlearn: 53.3754910\ttotal: 6m 56s\tremaining: 1m 7s\n",
            "825:\tlearn: 53.3459281\ttotal: 6m 57s\tremaining: 1m 7s\n",
            "826:\tlearn: 53.3133780\ttotal: 6m 57s\tremaining: 1m 6s\n",
            "827:\tlearn: 53.2965689\ttotal: 6m 58s\tremaining: 1m 6s\n",
            "828:\tlearn: 53.2956996\ttotal: 6m 58s\tremaining: 1m 5s\n",
            "829:\tlearn: 53.2519586\ttotal: 6m 59s\tremaining: 1m 5s\n",
            "830:\tlearn: 53.2040045\ttotal: 6m 59s\tremaining: 1m 4s\n",
            "831:\tlearn: 53.1757460\ttotal: 7m\tremaining: 1m 4s\n",
            "832:\tlearn: 53.1263184\ttotal: 7m\tremaining: 1m 3s\n",
            "833:\tlearn: 53.0633024\ttotal: 7m 1s\tremaining: 1m 3s\n",
            "834:\tlearn: 53.0341184\ttotal: 7m 1s\tremaining: 1m 2s\n",
            "835:\tlearn: 52.9867586\ttotal: 7m 2s\tremaining: 1m 2s\n",
            "836:\tlearn: 52.9499489\ttotal: 7m 2s\tremaining: 1m 1s\n",
            "837:\tlearn: 52.8413602\ttotal: 7m 3s\tremaining: 1m 1s\n",
            "838:\tlearn: 52.8143660\ttotal: 7m 3s\tremaining: 1m\n",
            "839:\tlearn: 52.7507788\ttotal: 7m 4s\tremaining: 1m\n",
            "840:\tlearn: 52.7002580\ttotal: 7m 4s\tremaining: 59.6s\n",
            "841:\tlearn: 52.5599176\ttotal: 7m 5s\tremaining: 59.1s\n",
            "842:\tlearn: 52.4994704\ttotal: 7m 5s\tremaining: 58.6s\n",
            "843:\tlearn: 52.4012896\ttotal: 7m 6s\tremaining: 58.1s\n",
            "844:\tlearn: 52.3529499\ttotal: 7m 6s\tremaining: 57.6s\n",
            "845:\tlearn: 52.3163102\ttotal: 7m 7s\tremaining: 57.1s\n",
            "846:\tlearn: 52.2845140\ttotal: 7m 8s\tremaining: 56.6s\n",
            "847:\tlearn: 52.1883600\ttotal: 7m 8s\tremaining: 56.1s\n",
            "848:\tlearn: 52.1482417\ttotal: 7m 9s\tremaining: 55.6s\n",
            "849:\tlearn: 52.1038208\ttotal: 7m 9s\tremaining: 55.1s\n",
            "850:\tlearn: 52.0514402\ttotal: 7m 10s\tremaining: 54.6s\n",
            "851:\tlearn: 51.9893661\ttotal: 7m 10s\tremaining: 54.1s\n",
            "852:\tlearn: 51.9618380\ttotal: 7m 11s\tremaining: 53.6s\n",
            "853:\tlearn: 51.9313621\ttotal: 7m 11s\tremaining: 53.1s\n",
            "854:\tlearn: 51.8813142\ttotal: 7m 12s\tremaining: 52.6s\n",
            "855:\tlearn: 51.8662534\ttotal: 7m 12s\tremaining: 52.1s\n",
            "856:\tlearn: 51.7832204\ttotal: 7m 13s\tremaining: 51.6s\n",
            "857:\tlearn: 51.7415563\ttotal: 7m 14s\tremaining: 51.1s\n",
            "858:\tlearn: 51.7209473\ttotal: 7m 14s\tremaining: 50.6s\n",
            "859:\tlearn: 51.6300683\ttotal: 7m 15s\tremaining: 50.1s\n",
            "860:\tlearn: 51.5734705\ttotal: 7m 15s\tremaining: 49.6s\n",
            "861:\tlearn: 51.5385861\ttotal: 7m 16s\tremaining: 49.1s\n",
            "862:\tlearn: 51.5156816\ttotal: 7m 16s\tremaining: 48.6s\n",
            "863:\tlearn: 51.4334917\ttotal: 7m 17s\tremaining: 48.1s\n",
            "864:\tlearn: 51.3901614\ttotal: 7m 17s\tremaining: 47.6s\n",
            "865:\tlearn: 51.3311575\ttotal: 7m 18s\tremaining: 47.1s\n",
            "866:\tlearn: 51.2639377\ttotal: 7m 18s\tremaining: 46.6s\n",
            "867:\tlearn: 51.2353534\ttotal: 7m 19s\tremaining: 46.1s\n",
            "868:\tlearn: 51.1748491\ttotal: 7m 19s\tremaining: 45.6s\n",
            "869:\tlearn: 51.1429592\ttotal: 7m 20s\tremaining: 45.1s\n",
            "870:\tlearn: 51.0917534\ttotal: 7m 20s\tremaining: 44.5s\n",
            "871:\tlearn: 51.0275876\ttotal: 7m 21s\tremaining: 44s\n",
            "872:\tlearn: 50.9826785\ttotal: 7m 21s\tremaining: 43.5s\n",
            "873:\tlearn: 50.9128859\ttotal: 7m 22s\tremaining: 43s\n",
            "874:\tlearn: 50.8578411\ttotal: 7m 23s\tremaining: 42.5s\n",
            "875:\tlearn: 50.8168704\ttotal: 7m 23s\tremaining: 42s\n",
            "876:\tlearn: 50.7864659\ttotal: 7m 24s\tremaining: 41.5s\n",
            "877:\tlearn: 50.7345775\ttotal: 7m 24s\tremaining: 41s\n",
            "878:\tlearn: 50.7095230\ttotal: 7m 25s\tremaining: 40.5s\n",
            "879:\tlearn: 50.6740749\ttotal: 7m 25s\tremaining: 40s\n",
            "880:\tlearn: 50.6327881\ttotal: 7m 26s\tremaining: 39.5s\n",
            "881:\tlearn: 50.5889049\ttotal: 7m 26s\tremaining: 39s\n",
            "882:\tlearn: 50.4538484\ttotal: 7m 27s\tremaining: 38.5s\n",
            "883:\tlearn: 50.4308449\ttotal: 7m 27s\tremaining: 38s\n",
            "884:\tlearn: 50.3989159\ttotal: 7m 28s\tremaining: 37.5s\n",
            "885:\tlearn: 50.3704818\ttotal: 7m 28s\tremaining: 37s\n",
            "886:\tlearn: 50.3490339\ttotal: 7m 29s\tremaining: 36.5s\n",
            "887:\tlearn: 50.2965515\ttotal: 7m 29s\tremaining: 36s\n",
            "888:\tlearn: 50.2611602\ttotal: 7m 30s\tremaining: 35.5s\n",
            "889:\tlearn: 50.2160938\ttotal: 7m 30s\tremaining: 35s\n",
            "890:\tlearn: 50.1844283\ttotal: 7m 31s\tremaining: 34.4s\n",
            "891:\tlearn: 50.1483528\ttotal: 7m 31s\tremaining: 33.9s\n",
            "892:\tlearn: 50.0818777\ttotal: 7m 32s\tremaining: 33.4s\n",
            "893:\tlearn: 50.0529313\ttotal: 7m 32s\tremaining: 32.9s\n",
            "894:\tlearn: 50.0029599\ttotal: 7m 33s\tremaining: 32.4s\n",
            "895:\tlearn: 49.9766559\ttotal: 7m 33s\tremaining: 31.9s\n",
            "896:\tlearn: 49.9289360\ttotal: 7m 34s\tremaining: 31.4s\n",
            "897:\tlearn: 49.8822632\ttotal: 7m 34s\tremaining: 30.9s\n",
            "898:\tlearn: 49.8572823\ttotal: 7m 35s\tremaining: 30.4s\n",
            "899:\tlearn: 49.7872376\ttotal: 7m 35s\tremaining: 29.9s\n",
            "900:\tlearn: 49.7532669\ttotal: 7m 36s\tremaining: 29.4s\n",
            "901:\tlearn: 49.7057380\ttotal: 7m 36s\tremaining: 28.9s\n",
            "902:\tlearn: 49.6732864\ttotal: 7m 37s\tremaining: 28.4s\n",
            "903:\tlearn: 49.6293349\ttotal: 7m 37s\tremaining: 27.9s\n",
            "904:\tlearn: 49.5604555\ttotal: 7m 38s\tremaining: 27.4s\n",
            "905:\tlearn: 49.5108017\ttotal: 7m 38s\tremaining: 26.8s\n",
            "906:\tlearn: 49.4724738\ttotal: 7m 39s\tremaining: 26.3s\n",
            "907:\tlearn: 49.4327988\ttotal: 7m 39s\tremaining: 25.8s\n",
            "908:\tlearn: 49.4191319\ttotal: 7m 40s\tremaining: 25.3s\n",
            "909:\tlearn: 49.3895881\ttotal: 7m 41s\tremaining: 24.8s\n",
            "910:\tlearn: 49.3421136\ttotal: 7m 41s\tremaining: 24.3s\n",
            "911:\tlearn: 49.2979583\ttotal: 7m 42s\tremaining: 23.8s\n",
            "912:\tlearn: 49.2388979\ttotal: 7m 42s\tremaining: 23.3s\n",
            "913:\tlearn: 49.2160112\ttotal: 7m 43s\tremaining: 22.8s\n",
            "914:\tlearn: 49.1538407\ttotal: 7m 43s\tremaining: 22.3s\n",
            "915:\tlearn: 49.1011232\ttotal: 7m 44s\tremaining: 21.8s\n",
            "916:\tlearn: 49.0617565\ttotal: 7m 44s\tremaining: 21.3s\n",
            "917:\tlearn: 49.0222710\ttotal: 7m 45s\tremaining: 20.8s\n",
            "918:\tlearn: 48.9445268\ttotal: 7m 45s\tremaining: 20.3s\n",
            "919:\tlearn: 48.9158675\ttotal: 7m 46s\tremaining: 19.8s\n",
            "920:\tlearn: 48.8811667\ttotal: 7m 46s\tremaining: 19.3s\n",
            "921:\tlearn: 48.8546136\ttotal: 7m 47s\tremaining: 18.8s\n",
            "922:\tlearn: 48.8344727\ttotal: 7m 47s\tremaining: 18.2s\n",
            "923:\tlearn: 48.7763037\ttotal: 7m 48s\tremaining: 17.7s\n",
            "924:\tlearn: 48.6833978\ttotal: 7m 48s\tremaining: 17.2s\n",
            "925:\tlearn: 48.6507036\ttotal: 7m 49s\tremaining: 16.7s\n",
            "926:\tlearn: 48.6209581\ttotal: 7m 49s\tremaining: 16.2s\n",
            "927:\tlearn: 48.5912905\ttotal: 7m 50s\tremaining: 15.7s\n",
            "928:\tlearn: 48.4906515\ttotal: 7m 50s\tremaining: 15.2s\n",
            "929:\tlearn: 48.4452201\ttotal: 7m 51s\tremaining: 14.7s\n",
            "930:\tlearn: 48.3926633\ttotal: 7m 51s\tremaining: 14.2s\n",
            "931:\tlearn: 48.3581761\ttotal: 7m 52s\tremaining: 13.7s\n",
            "932:\tlearn: 48.3237366\ttotal: 7m 52s\tremaining: 13.2s\n",
            "933:\tlearn: 48.2971818\ttotal: 7m 53s\tremaining: 12.7s\n",
            "934:\tlearn: 48.2927699\ttotal: 7m 54s\tremaining: 12.2s\n",
            "935:\tlearn: 48.2730208\ttotal: 7m 54s\tremaining: 11.7s\n",
            "936:\tlearn: 48.2176233\ttotal: 7m 55s\tremaining: 11.2s\n",
            "937:\tlearn: 48.1971821\ttotal: 7m 55s\tremaining: 10.6s\n",
            "938:\tlearn: 48.1683050\ttotal: 7m 56s\tremaining: 10.1s\n",
            "939:\tlearn: 48.1075785\ttotal: 7m 56s\tremaining: 9.63s\n",
            "940:\tlearn: 48.0941774\ttotal: 7m 57s\tremaining: 9.13s\n",
            "941:\tlearn: 48.0444341\ttotal: 7m 57s\tremaining: 8.62s\n",
            "942:\tlearn: 47.9973723\ttotal: 7m 58s\tremaining: 8.11s\n",
            "943:\tlearn: 47.9386717\ttotal: 7m 58s\tremaining: 7.6s\n",
            "944:\tlearn: 47.9331040\ttotal: 7m 59s\tremaining: 7.1s\n",
            "945:\tlearn: 47.9328695\ttotal: 7m 59s\tremaining: 6.59s\n",
            "946:\tlearn: 47.9153462\ttotal: 8m\tremaining: 6.08s\n",
            "947:\tlearn: 47.8814635\ttotal: 8m\tremaining: 5.58s\n",
            "948:\tlearn: 47.8491100\ttotal: 8m 1s\tremaining: 5.07s\n",
            "949:\tlearn: 47.7857723\ttotal: 8m 1s\tremaining: 4.56s\n",
            "950:\tlearn: 47.7347727\ttotal: 8m 2s\tremaining: 4.06s\n",
            "951:\tlearn: 47.7034115\ttotal: 8m 2s\tremaining: 3.55s\n",
            "952:\tlearn: 47.6932920\ttotal: 8m 3s\tremaining: 3.04s\n",
            "953:\tlearn: 47.6455145\ttotal: 8m 3s\tremaining: 2.54s\n",
            "954:\tlearn: 47.5984624\ttotal: 8m 4s\tremaining: 2.03s\n",
            "955:\tlearn: 47.5678916\ttotal: 8m 4s\tremaining: 1.52s\n",
            "956:\tlearn: 47.5230279\ttotal: 8m 5s\tremaining: 1.01s\n",
            "957:\tlearn: 47.5030514\ttotal: 8m 5s\tremaining: 507ms\n",
            "958:\tlearn: 47.4620713\ttotal: 8m 6s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 06:41:36,126] A new study created in memory with name: no-name-a5bad0cc-5028-48de-8b94-64e52911ba47\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 06:51:57,644] Trial 0 finished with value: 59.55890177422416 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 59.55890177422416.\n",
            "[I 2023-10-08 06:55:46,631] Trial 1 finished with value: 40.2642328865347 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 40.2642328865347.\n",
            "[I 2023-10-08 07:02:44,365] Trial 2 finished with value: 43.47338223950768 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 40.2642328865347.\n",
            "[I 2023-10-08 07:07:11,336] Trial 3 finished with value: 43.482320256548604 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 40.2642328865347.\n",
            "[I 2023-10-08 07:11:09,059] Trial 4 finished with value: 40.1973161069217 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:16:52,155] Trial 5 finished with value: 74.13135057913716 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:20:04,675] Trial 6 finished with value: 44.18571744592658 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:34:05,886] Trial 7 finished with value: 52.898528873868585 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:36:00,533] Trial 8 finished with value: 55.26517445406939 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:39:22,031] Trial 9 finished with value: 46.777955695029526 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:43:10,516] Trial 10 finished with value: 45.51545605151314 and parameters: {'iterations': 994, 'learning_rate': 0.8097283490059026, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 99.85162236802904, 'subsample': 0.977445433799178, 'random_strength': 38.86027028730574, 'od_wait': 10, 'leaf_estimation_iterations': 1, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.9937641531018804}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:46:30,413] Trial 11 finished with value: 42.42224102603963 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 21, 'reg_lambda': 52.01743365000489, 'subsample': 0.5721183515791496, 'random_strength': 10.15982146169808, 'od_wait': 145, 'leaf_estimation_iterations': 4, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.35067266469560193}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:50:03,169] Trial 12 finished with value: 51.11421800126864 and parameters: {'iterations': 758, 'learning_rate': 0.983343987565645, 'depth': 8, 'min_data_in_leaf': 3, 'reg_lambda': 46.91007356584483, 'subsample': 0.5140127429398726, 'random_strength': 29.182549248552736, 'od_wait': 108, 'leaf_estimation_iterations': 20, 'bagging_temperature': 24.356631192739087, 'colsample_bylevel': 0.3074183969440657}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:56:05,407] Trial 13 finished with value: 50.37661590223908 and parameters: {'iterations': 971, 'learning_rate': 0.7345739305255561, 'depth': 11, 'min_data_in_leaf': 10, 'reg_lambda': 77.21591430401493, 'subsample': 0.4710018087923153, 'random_strength': 67.17789626310656, 'od_wait': 145, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.7353464012961046}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 07:59:03,051] Trial 14 finished with value: 41.772788507492045 and parameters: {'iterations': 725, 'learning_rate': 0.8586986413649368, 'depth': 8, 'min_data_in_leaf': 21, 'reg_lambda': 58.113406993512264, 'subsample': 0.6338482090954157, 'random_strength': 45.00496191936034, 'od_wait': 126, 'leaf_estimation_iterations': 1, 'bagging_temperature': 14.031025575549046, 'colsample_bylevel': 0.5544022353458152}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 08:02:14,023] Trial 15 finished with value: 50.67348370051868 and parameters: {'iterations': 811, 'learning_rate': 0.9945210671429636, 'depth': 10, 'min_data_in_leaf': 6, 'reg_lambda': 63.595495838471635, 'subsample': 0.46082857874135125, 'random_strength': 74.06415650341042, 'od_wait': 98, 'leaf_estimation_iterations': 5, 'bagging_temperature': 43.040289080815256, 'colsample_bylevel': 0.7670727501880628}. Best is trial 4 with value: 40.1973161069217.\n",
            "[I 2023-10-08 08:06:14,025] Trial 16 finished with value: 39.25723835375426 and parameters: {'iterations': 932, 'learning_rate': 0.6707699152504077, 'depth': 7, 'min_data_in_leaf': 15, 'reg_lambda': 30.26423248378008, 'subsample': 0.630224100896121, 'random_strength': 36.46720207402407, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6193404395539011}. Best is trial 16 with value: 39.25723835375426.\n",
            "[I 2023-10-08 08:07:23,742] Trial 17 finished with value: 52.746872982335105 and parameters: {'iterations': 305, 'learning_rate': 0.8455325558544353, 'depth': 7, 'min_data_in_leaf': 1, 'reg_lambda': 40.07580701957684, 'subsample': 0.43840876886682856, 'random_strength': 41.804455906060724, 'od_wait': 81, 'leaf_estimation_iterations': 4, 'bagging_temperature': 95.38524978295371, 'colsample_bylevel': 0.6233897960353694}. Best is trial 16 with value: 39.25723835375426.\n",
            "[I 2023-10-08 08:11:46,500] Trial 18 finished with value: 52.63178805348222 and parameters: {'iterations': 946, 'learning_rate': 0.751729220847763, 'depth': 12, 'min_data_in_leaf': 14, 'reg_lambda': 75.86868408846398, 'subsample': 0.5708673273336423, 'random_strength': 55.82048901072418, 'od_wait': 69, 'leaf_estimation_iterations': 9, 'bagging_temperature': 17.224182126605285, 'colsample_bylevel': 0.800712550834435}. Best is trial 16 with value: 39.25723835375426.\n",
            "[I 2023-10-08 08:14:59,101] Trial 19 finished with value: 41.373242162839745 and parameters: {'iterations': 920, 'learning_rate': 0.6893487254496348, 'depth': 6, 'min_data_in_leaf': 18, 'reg_lambda': 46.99749551383755, 'subsample': 0.6374687462211199, 'random_strength': 34.217175208731206, 'od_wait': 13, 'leaf_estimation_iterations': 6, 'bagging_temperature': 48.60689561818979, 'colsample_bylevel': 0.6818744770955735}. Best is trial 16 with value: 39.25723835375426.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 2: 39.25723835375426\n",
            "Best hyperparameters for fold 2: {'iterations': 932, 'learning_rate': 0.6707699152504077, 'depth': 7, 'min_data_in_leaf': 15, 'reg_lambda': 30.26423248378008, 'subsample': 0.630224100896121, 'random_strength': 36.46720207402407, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6193404395539011}\n",
            "0:\tlearn: 204.8496333\ttotal: 261ms\tremaining: 4m 2s\n",
            "1:\tlearn: 202.4083800\ttotal: 535ms\tremaining: 4m 8s\n",
            "2:\tlearn: 202.4069292\ttotal: 634ms\tremaining: 3m 16s\n",
            "3:\tlearn: 201.9972923\ttotal: 913ms\tremaining: 3m 31s\n",
            "4:\tlearn: 201.9972923\ttotal: 982ms\tremaining: 3m 2s\n",
            "5:\tlearn: 201.6448743\ttotal: 1.18s\tremaining: 3m 2s\n",
            "6:\tlearn: 201.1392491\ttotal: 1.44s\tremaining: 3m 9s\n",
            "7:\tlearn: 200.9538547\ttotal: 1.68s\tremaining: 3m 14s\n",
            "8:\tlearn: 200.7515112\ttotal: 1.9s\tremaining: 3m 15s\n",
            "9:\tlearn: 200.5559184\ttotal: 2.15s\tremaining: 3m 18s\n",
            "10:\tlearn: 200.5285504\ttotal: 2.33s\tremaining: 3m 15s\n",
            "11:\tlearn: 200.4747886\ttotal: 2.44s\tremaining: 3m 6s\n",
            "12:\tlearn: 200.3515645\ttotal: 2.62s\tremaining: 3m 5s\n",
            "13:\tlearn: 200.0672491\ttotal: 2.85s\tremaining: 3m 7s\n",
            "14:\tlearn: 200.0672490\ttotal: 2.91s\tremaining: 2m 57s\n",
            "15:\tlearn: 199.3523704\ttotal: 3.14s\tremaining: 2m 59s\n",
            "16:\tlearn: 199.3523704\ttotal: 3.19s\tremaining: 2m 51s\n",
            "17:\tlearn: 199.3505509\ttotal: 3.27s\tremaining: 2m 46s\n",
            "18:\tlearn: 197.8343393\ttotal: 3.48s\tremaining: 2m 47s\n",
            "19:\tlearn: 196.6147642\ttotal: 3.71s\tremaining: 2m 49s\n",
            "20:\tlearn: 194.8016217\ttotal: 3.91s\tremaining: 2m 49s\n",
            "21:\tlearn: 193.8847987\ttotal: 4.15s\tremaining: 2m 51s\n",
            "22:\tlearn: 193.1983741\ttotal: 4.37s\tremaining: 2m 52s\n",
            "23:\tlearn: 192.2509064\ttotal: 4.59s\tremaining: 2m 53s\n",
            "24:\tlearn: 190.6125614\ttotal: 4.81s\tremaining: 2m 54s\n",
            "25:\tlearn: 189.8389705\ttotal: 5.04s\tremaining: 2m 55s\n",
            "26:\tlearn: 188.9043494\ttotal: 5.29s\tremaining: 2m 57s\n",
            "27:\tlearn: 188.2197215\ttotal: 5.51s\tremaining: 2m 57s\n",
            "28:\tlearn: 187.9197489\ttotal: 5.7s\tremaining: 2m 57s\n",
            "29:\tlearn: 186.4990272\ttotal: 5.92s\tremaining: 2m 57s\n",
            "30:\tlearn: 186.0929876\ttotal: 6.14s\tremaining: 2m 58s\n",
            "31:\tlearn: 185.5677066\ttotal: 6.36s\tremaining: 2m 58s\n",
            "32:\tlearn: 182.2552852\ttotal: 6.56s\tremaining: 2m 58s\n",
            "33:\tlearn: 179.2575938\ttotal: 6.76s\tremaining: 2m 58s\n",
            "34:\tlearn: 176.6947768\ttotal: 6.99s\tremaining: 2m 59s\n",
            "35:\tlearn: 175.0205385\ttotal: 7.18s\tremaining: 2m 58s\n",
            "36:\tlearn: 171.9023671\ttotal: 7.39s\tremaining: 2m 58s\n",
            "37:\tlearn: 170.3358585\ttotal: 7.59s\tremaining: 2m 58s\n",
            "38:\tlearn: 168.1638516\ttotal: 7.83s\tremaining: 2m 59s\n",
            "39:\tlearn: 167.8622556\ttotal: 8.05s\tremaining: 2m 59s\n",
            "40:\tlearn: 166.7420549\ttotal: 8.25s\tremaining: 2m 59s\n",
            "41:\tlearn: 166.4536169\ttotal: 8.5s\tremaining: 3m\n",
            "42:\tlearn: 164.9801706\ttotal: 8.72s\tremaining: 3m\n",
            "43:\tlearn: 164.5149936\ttotal: 8.92s\tremaining: 3m\n",
            "44:\tlearn: 162.2500320\ttotal: 9.14s\tremaining: 3m\n",
            "45:\tlearn: 161.0978273\ttotal: 9.35s\tremaining: 3m\n",
            "46:\tlearn: 160.6624939\ttotal: 9.57s\tremaining: 3m\n",
            "47:\tlearn: 159.3514768\ttotal: 9.77s\tremaining: 2m 59s\n",
            "48:\tlearn: 159.0911555\ttotal: 9.98s\tremaining: 2m 59s\n",
            "49:\tlearn: 158.6259707\ttotal: 10.2s\tremaining: 2m 59s\n",
            "50:\tlearn: 156.8209331\ttotal: 10.4s\tremaining: 2m 59s\n",
            "51:\tlearn: 156.2460867\ttotal: 10.6s\tremaining: 2m 59s\n",
            "52:\tlearn: 154.9230143\ttotal: 10.8s\tremaining: 2m 59s\n",
            "53:\tlearn: 153.2038698\ttotal: 11.1s\tremaining: 2m 59s\n",
            "54:\tlearn: 152.8974381\ttotal: 11.3s\tremaining: 3m\n",
            "55:\tlearn: 151.8610857\ttotal: 11.5s\tremaining: 2m 59s\n",
            "56:\tlearn: 151.6820116\ttotal: 11.7s\tremaining: 3m\n",
            "57:\tlearn: 149.9033235\ttotal: 11.9s\tremaining: 2m 59s\n",
            "58:\tlearn: 149.0205974\ttotal: 12.1s\tremaining: 2m 59s\n",
            "59:\tlearn: 147.9669460\ttotal: 12.3s\tremaining: 2m 59s\n",
            "60:\tlearn: 147.1670569\ttotal: 12.5s\tremaining: 2m 58s\n",
            "61:\tlearn: 146.5202302\ttotal: 12.7s\tremaining: 2m 58s\n",
            "62:\tlearn: 146.2721850\ttotal: 12.9s\tremaining: 2m 58s\n",
            "63:\tlearn: 145.3886377\ttotal: 13.1s\tremaining: 2m 58s\n",
            "64:\tlearn: 144.6174303\ttotal: 13.3s\tremaining: 2m 58s\n",
            "65:\tlearn: 144.4490774\ttotal: 13.6s\tremaining: 2m 58s\n",
            "66:\tlearn: 144.2420163\ttotal: 13.9s\tremaining: 2m 58s\n",
            "67:\tlearn: 143.9998214\ttotal: 14.1s\tremaining: 2m 58s\n",
            "68:\tlearn: 143.7895630\ttotal: 14.3s\tremaining: 2m 59s\n",
            "69:\tlearn: 143.6118635\ttotal: 14.5s\tremaining: 2m 58s\n",
            "70:\tlearn: 142.9602477\ttotal: 14.7s\tremaining: 2m 58s\n",
            "71:\tlearn: 142.7886266\ttotal: 15s\tremaining: 2m 58s\n",
            "72:\tlearn: 141.8668923\ttotal: 15.2s\tremaining: 2m 58s\n",
            "73:\tlearn: 140.6043466\ttotal: 15.4s\tremaining: 2m 58s\n",
            "74:\tlearn: 139.9277088\ttotal: 15.6s\tremaining: 2m 58s\n",
            "75:\tlearn: 139.7234121\ttotal: 15.8s\tremaining: 2m 57s\n",
            "76:\tlearn: 139.6409887\ttotal: 16s\tremaining: 2m 57s\n",
            "77:\tlearn: 137.9999204\ttotal: 16.2s\tremaining: 2m 57s\n",
            "78:\tlearn: 135.5192645\ttotal: 16.4s\tremaining: 2m 57s\n",
            "79:\tlearn: 135.2065054\ttotal: 16.6s\tremaining: 2m 57s\n",
            "80:\tlearn: 134.8805611\ttotal: 16.8s\tremaining: 2m 56s\n",
            "81:\tlearn: 134.4239166\ttotal: 17s\tremaining: 2m 56s\n",
            "82:\tlearn: 133.4301496\ttotal: 17.3s\tremaining: 2m 56s\n",
            "83:\tlearn: 132.4738178\ttotal: 17.5s\tremaining: 2m 56s\n",
            "84:\tlearn: 132.2842062\ttotal: 17.7s\tremaining: 2m 56s\n",
            "85:\tlearn: 131.8217356\ttotal: 18s\tremaining: 2m 56s\n",
            "86:\tlearn: 131.6098791\ttotal: 18.2s\tremaining: 2m 56s\n",
            "87:\tlearn: 131.4174347\ttotal: 18.4s\tremaining: 2m 56s\n",
            "88:\tlearn: 131.1610446\ttotal: 18.6s\tremaining: 2m 56s\n",
            "89:\tlearn: 131.0004370\ttotal: 18.8s\tremaining: 2m 56s\n",
            "90:\tlearn: 129.9693330\ttotal: 19s\tremaining: 2m 55s\n",
            "91:\tlearn: 129.7509462\ttotal: 19.2s\tremaining: 2m 55s\n",
            "92:\tlearn: 129.5257811\ttotal: 19.4s\tremaining: 2m 55s\n",
            "93:\tlearn: 129.3067094\ttotal: 19.7s\tremaining: 2m 55s\n",
            "94:\tlearn: 129.1319915\ttotal: 19.9s\tremaining: 2m 55s\n",
            "95:\tlearn: 128.8141492\ttotal: 20.1s\tremaining: 2m 54s\n",
            "96:\tlearn: 128.5346034\ttotal: 20.3s\tremaining: 2m 54s\n",
            "97:\tlearn: 128.0512642\ttotal: 20.5s\tremaining: 2m 54s\n",
            "98:\tlearn: 127.3905804\ttotal: 20.7s\tremaining: 2m 53s\n",
            "99:\tlearn: 127.2313559\ttotal: 20.9s\tremaining: 2m 53s\n",
            "100:\tlearn: 126.6074991\ttotal: 21.1s\tremaining: 2m 53s\n",
            "101:\tlearn: 126.4626944\ttotal: 21.3s\tremaining: 2m 53s\n",
            "102:\tlearn: 126.2893036\ttotal: 21.5s\tremaining: 2m 53s\n",
            "103:\tlearn: 125.9918833\ttotal: 21.7s\tremaining: 2m 52s\n",
            "104:\tlearn: 125.1168090\ttotal: 21.9s\tremaining: 2m 52s\n",
            "105:\tlearn: 124.2419131\ttotal: 22.1s\tremaining: 2m 51s\n",
            "106:\tlearn: 123.0329993\ttotal: 22.3s\tremaining: 2m 51s\n",
            "107:\tlearn: 122.8041735\ttotal: 22.5s\tremaining: 2m 51s\n",
            "108:\tlearn: 122.1264813\ttotal: 22.7s\tremaining: 2m 51s\n",
            "109:\tlearn: 121.5169735\ttotal: 22.9s\tremaining: 2m 51s\n",
            "110:\tlearn: 121.3214772\ttotal: 23.1s\tremaining: 2m 51s\n",
            "111:\tlearn: 121.2842550\ttotal: 23.4s\tremaining: 2m 51s\n",
            "112:\tlearn: 120.9519326\ttotal: 23.6s\tremaining: 2m 50s\n",
            "113:\tlearn: 120.8094023\ttotal: 23.8s\tremaining: 2m 50s\n",
            "114:\tlearn: 120.7405180\ttotal: 24s\tremaining: 2m 50s\n",
            "115:\tlearn: 119.4248292\ttotal: 24.3s\tremaining: 2m 50s\n",
            "116:\tlearn: 119.1353459\ttotal: 24.5s\tremaining: 2m 50s\n",
            "117:\tlearn: 118.9342833\ttotal: 24.7s\tremaining: 2m 50s\n",
            "118:\tlearn: 118.7875281\ttotal: 24.9s\tremaining: 2m 50s\n",
            "119:\tlearn: 118.2000334\ttotal: 25.1s\tremaining: 2m 50s\n",
            "120:\tlearn: 118.0189547\ttotal: 25.4s\tremaining: 2m 49s\n",
            "121:\tlearn: 117.2678858\ttotal: 25.6s\tremaining: 2m 49s\n",
            "122:\tlearn: 116.9683575\ttotal: 25.8s\tremaining: 2m 49s\n",
            "123:\tlearn: 116.0632705\ttotal: 26s\tremaining: 2m 49s\n",
            "124:\tlearn: 115.8582073\ttotal: 26.2s\tremaining: 2m 49s\n",
            "125:\tlearn: 115.6779912\ttotal: 26.4s\tremaining: 2m 49s\n",
            "126:\tlearn: 115.2969082\ttotal: 26.6s\tremaining: 2m 48s\n",
            "127:\tlearn: 114.9455218\ttotal: 26.8s\tremaining: 2m 48s\n",
            "128:\tlearn: 114.0454128\ttotal: 27s\tremaining: 2m 48s\n",
            "129:\tlearn: 113.7910527\ttotal: 27.2s\tremaining: 2m 48s\n",
            "130:\tlearn: 113.2614898\ttotal: 27.5s\tremaining: 2m 47s\n",
            "131:\tlearn: 112.8075988\ttotal: 27.7s\tremaining: 2m 47s\n",
            "132:\tlearn: 112.5978018\ttotal: 27.9s\tremaining: 2m 47s\n",
            "133:\tlearn: 112.3677062\ttotal: 28.1s\tremaining: 2m 47s\n",
            "134:\tlearn: 112.2541719\ttotal: 28.3s\tremaining: 2m 47s\n",
            "135:\tlearn: 111.8819053\ttotal: 28.5s\tremaining: 2m 47s\n",
            "136:\tlearn: 111.7007608\ttotal: 28.8s\tremaining: 2m 46s\n",
            "137:\tlearn: 111.3192963\ttotal: 29s\tremaining: 2m 46s\n",
            "138:\tlearn: 111.0318589\ttotal: 29.2s\tremaining: 2m 46s\n",
            "139:\tlearn: 110.8703208\ttotal: 29.4s\tremaining: 2m 46s\n",
            "140:\tlearn: 110.7171574\ttotal: 29.6s\tremaining: 2m 46s\n",
            "141:\tlearn: 110.1861136\ttotal: 29.8s\tremaining: 2m 45s\n",
            "142:\tlearn: 110.0939710\ttotal: 30s\tremaining: 2m 45s\n",
            "143:\tlearn: 109.6259470\ttotal: 30.3s\tremaining: 2m 45s\n",
            "144:\tlearn: 109.3230260\ttotal: 30.5s\tremaining: 2m 45s\n",
            "145:\tlearn: 109.3010149\ttotal: 30.7s\tremaining: 2m 45s\n",
            "146:\tlearn: 109.0346599\ttotal: 30.9s\tremaining: 2m 45s\n",
            "147:\tlearn: 108.8594779\ttotal: 31.2s\tremaining: 2m 45s\n",
            "148:\tlearn: 108.7251477\ttotal: 31.4s\tremaining: 2m 44s\n",
            "149:\tlearn: 108.2273314\ttotal: 31.6s\tremaining: 2m 44s\n",
            "150:\tlearn: 107.8854910\ttotal: 31.8s\tremaining: 2m 44s\n",
            "151:\tlearn: 107.7988929\ttotal: 32.1s\tremaining: 2m 44s\n",
            "152:\tlearn: 107.6934337\ttotal: 32.3s\tremaining: 2m 44s\n",
            "153:\tlearn: 107.3916065\ttotal: 32.6s\tremaining: 2m 44s\n",
            "154:\tlearn: 107.0386939\ttotal: 32.8s\tremaining: 2m 44s\n",
            "155:\tlearn: 106.9246856\ttotal: 33s\tremaining: 2m 44s\n",
            "156:\tlearn: 106.1278355\ttotal: 33.2s\tremaining: 2m 44s\n",
            "157:\tlearn: 105.9319984\ttotal: 33.5s\tremaining: 2m 43s\n",
            "158:\tlearn: 105.7333276\ttotal: 33.7s\tremaining: 2m 43s\n",
            "159:\tlearn: 105.5178102\ttotal: 33.9s\tremaining: 2m 43s\n",
            "160:\tlearn: 105.4428843\ttotal: 34.1s\tremaining: 2m 43s\n",
            "161:\tlearn: 105.3760251\ttotal: 34.3s\tremaining: 2m 43s\n",
            "162:\tlearn: 105.0705388\ttotal: 34.5s\tremaining: 2m 42s\n",
            "163:\tlearn: 104.7362426\ttotal: 34.7s\tremaining: 2m 42s\n",
            "164:\tlearn: 104.6097904\ttotal: 34.9s\tremaining: 2m 42s\n",
            "165:\tlearn: 104.4866243\ttotal: 35.1s\tremaining: 2m 42s\n",
            "166:\tlearn: 104.2655635\ttotal: 35.3s\tremaining: 2m 41s\n",
            "167:\tlearn: 104.0157889\ttotal: 35.5s\tremaining: 2m 41s\n",
            "168:\tlearn: 103.7494841\ttotal: 35.7s\tremaining: 2m 41s\n",
            "169:\tlearn: 103.2844664\ttotal: 35.9s\tremaining: 2m 41s\n",
            "170:\tlearn: 103.1346800\ttotal: 36.1s\tremaining: 2m 40s\n",
            "171:\tlearn: 102.8890772\ttotal: 36.4s\tremaining: 2m 40s\n",
            "172:\tlearn: 102.8086169\ttotal: 36.6s\tremaining: 2m 40s\n",
            "173:\tlearn: 102.4766940\ttotal: 36.8s\tremaining: 2m 40s\n",
            "174:\tlearn: 102.4198305\ttotal: 37s\tremaining: 2m 40s\n",
            "175:\tlearn: 102.3224023\ttotal: 37.2s\tremaining: 2m 39s\n",
            "176:\tlearn: 102.1536802\ttotal: 37.4s\tremaining: 2m 39s\n",
            "177:\tlearn: 101.7614691\ttotal: 37.6s\tremaining: 2m 39s\n",
            "178:\tlearn: 101.5503011\ttotal: 37.8s\tremaining: 2m 39s\n",
            "179:\tlearn: 101.1938606\ttotal: 38s\tremaining: 2m 38s\n",
            "180:\tlearn: 101.1302161\ttotal: 38.3s\tremaining: 2m 38s\n",
            "181:\tlearn: 101.0234678\ttotal: 38.6s\tremaining: 2m 38s\n",
            "182:\tlearn: 100.8210212\ttotal: 38.7s\tremaining: 2m 38s\n",
            "183:\tlearn: 100.7230233\ttotal: 38.9s\tremaining: 2m 38s\n",
            "184:\tlearn: 100.0928347\ttotal: 39.2s\tremaining: 2m 38s\n",
            "185:\tlearn: 100.0309376\ttotal: 39.5s\tremaining: 2m 38s\n",
            "186:\tlearn: 99.7660118\ttotal: 39.8s\tremaining: 2m 38s\n",
            "187:\tlearn: 99.6331587\ttotal: 39.9s\tremaining: 2m 38s\n",
            "188:\tlearn: 99.5194811\ttotal: 40.2s\tremaining: 2m 37s\n",
            "189:\tlearn: 99.3997352\ttotal: 40.4s\tremaining: 2m 37s\n",
            "190:\tlearn: 99.2028198\ttotal: 40.6s\tremaining: 2m 37s\n",
            "191:\tlearn: 99.1535024\ttotal: 40.8s\tremaining: 2m 37s\n",
            "192:\tlearn: 98.7214575\ttotal: 41s\tremaining: 2m 37s\n",
            "193:\tlearn: 98.6451654\ttotal: 41.2s\tremaining: 2m 36s\n",
            "194:\tlearn: 98.2941228\ttotal: 41.4s\tremaining: 2m 36s\n",
            "195:\tlearn: 98.1831952\ttotal: 41.6s\tremaining: 2m 36s\n",
            "196:\tlearn: 98.0775955\ttotal: 41.8s\tremaining: 2m 36s\n",
            "197:\tlearn: 97.9686268\ttotal: 42s\tremaining: 2m 35s\n",
            "198:\tlearn: 97.9084236\ttotal: 42.3s\tremaining: 2m 35s\n",
            "199:\tlearn: 97.6859788\ttotal: 42.4s\tremaining: 2m 35s\n",
            "200:\tlearn: 97.4750722\ttotal: 42.7s\tremaining: 2m 35s\n",
            "201:\tlearn: 97.4735431\ttotal: 42.9s\tremaining: 2m 34s\n",
            "202:\tlearn: 97.2982258\ttotal: 43.1s\tremaining: 2m 34s\n",
            "203:\tlearn: 97.2571652\ttotal: 43.3s\tremaining: 2m 34s\n",
            "204:\tlearn: 97.2018636\ttotal: 43.5s\tremaining: 2m 34s\n",
            "205:\tlearn: 97.1312405\ttotal: 43.8s\tremaining: 2m 34s\n",
            "206:\tlearn: 97.0275005\ttotal: 44s\tremaining: 2m 34s\n",
            "207:\tlearn: 97.0234753\ttotal: 44.3s\tremaining: 2m 34s\n",
            "208:\tlearn: 96.9851767\ttotal: 44.5s\tremaining: 2m 33s\n",
            "209:\tlearn: 96.1603640\ttotal: 44.7s\tremaining: 2m 33s\n",
            "210:\tlearn: 96.1561946\ttotal: 44.9s\tremaining: 2m 33s\n",
            "211:\tlearn: 95.7137364\ttotal: 45.1s\tremaining: 2m 33s\n",
            "212:\tlearn: 95.5424847\ttotal: 45.3s\tremaining: 2m 33s\n",
            "213:\tlearn: 95.4882613\ttotal: 45.6s\tremaining: 2m 32s\n",
            "214:\tlearn: 95.4055413\ttotal: 45.8s\tremaining: 2m 32s\n",
            "215:\tlearn: 95.3632540\ttotal: 46.1s\tremaining: 2m 32s\n",
            "216:\tlearn: 95.3295756\ttotal: 46.3s\tremaining: 2m 32s\n",
            "217:\tlearn: 95.2495532\ttotal: 46.5s\tremaining: 2m 32s\n",
            "218:\tlearn: 94.9671943\ttotal: 46.7s\tremaining: 2m 32s\n",
            "219:\tlearn: 94.7519335\ttotal: 47s\tremaining: 2m 31s\n",
            "220:\tlearn: 94.6732567\ttotal: 47.2s\tremaining: 2m 31s\n",
            "221:\tlearn: 94.4055132\ttotal: 47.4s\tremaining: 2m 31s\n",
            "222:\tlearn: 94.3571820\ttotal: 47.7s\tremaining: 2m 31s\n",
            "223:\tlearn: 94.3138485\ttotal: 47.9s\tremaining: 2m 31s\n",
            "224:\tlearn: 93.9850901\ttotal: 48.1s\tremaining: 2m 31s\n",
            "225:\tlearn: 93.8676277\ttotal: 48.3s\tremaining: 2m 30s\n",
            "226:\tlearn: 93.7545713\ttotal: 48.6s\tremaining: 2m 30s\n",
            "227:\tlearn: 93.5894533\ttotal: 48.8s\tremaining: 2m 30s\n",
            "228:\tlearn: 93.3682757\ttotal: 49.1s\tremaining: 2m 30s\n",
            "229:\tlearn: 93.1479557\ttotal: 49.3s\tremaining: 2m 30s\n",
            "230:\tlearn: 92.9204110\ttotal: 49.6s\tremaining: 2m 30s\n",
            "231:\tlearn: 92.7465577\ttotal: 49.8s\tremaining: 2m 30s\n",
            "232:\tlearn: 92.7455846\ttotal: 50s\tremaining: 2m 29s\n",
            "233:\tlearn: 92.6999499\ttotal: 50.2s\tremaining: 2m 29s\n",
            "234:\tlearn: 92.5571643\ttotal: 50.4s\tremaining: 2m 29s\n",
            "235:\tlearn: 92.5100789\ttotal: 50.6s\tremaining: 2m 29s\n",
            "236:\tlearn: 92.4321719\ttotal: 50.8s\tremaining: 2m 29s\n",
            "237:\tlearn: 92.3396263\ttotal: 51s\tremaining: 2m 28s\n",
            "238:\tlearn: 92.2367611\ttotal: 51.2s\tremaining: 2m 28s\n",
            "239:\tlearn: 91.6976486\ttotal: 51.4s\tremaining: 2m 28s\n",
            "240:\tlearn: 91.4994788\ttotal: 51.7s\tremaining: 2m 28s\n",
            "241:\tlearn: 91.3383938\ttotal: 51.9s\tremaining: 2m 27s\n",
            "242:\tlearn: 91.1071923\ttotal: 52.1s\tremaining: 2m 27s\n",
            "243:\tlearn: 91.0276895\ttotal: 52.3s\tremaining: 2m 27s\n",
            "244:\tlearn: 90.9648724\ttotal: 52.5s\tremaining: 2m 27s\n",
            "245:\tlearn: 90.9024726\ttotal: 52.8s\tremaining: 2m 27s\n",
            "246:\tlearn: 90.6173530\ttotal: 53s\tremaining: 2m 26s\n",
            "247:\tlearn: 90.4797501\ttotal: 53.2s\tremaining: 2m 26s\n",
            "248:\tlearn: 90.3731800\ttotal: 53.4s\tremaining: 2m 26s\n",
            "249:\tlearn: 90.3177267\ttotal: 53.6s\tremaining: 2m 26s\n",
            "250:\tlearn: 90.2120540\ttotal: 53.8s\tremaining: 2m 25s\n",
            "251:\tlearn: 90.0760109\ttotal: 54s\tremaining: 2m 25s\n",
            "252:\tlearn: 89.9014145\ttotal: 54.2s\tremaining: 2m 25s\n",
            "253:\tlearn: 89.7092517\ttotal: 54.4s\tremaining: 2m 25s\n",
            "254:\tlearn: 89.6422076\ttotal: 54.6s\tremaining: 2m 25s\n",
            "255:\tlearn: 89.3696747\ttotal: 54.8s\tremaining: 2m 24s\n",
            "256:\tlearn: 89.3060379\ttotal: 55s\tremaining: 2m 24s\n",
            "257:\tlearn: 89.1010050\ttotal: 55.2s\tremaining: 2m 24s\n",
            "258:\tlearn: 88.9893900\ttotal: 55.4s\tremaining: 2m 24s\n",
            "259:\tlearn: 88.6060135\ttotal: 55.6s\tremaining: 2m 23s\n",
            "260:\tlearn: 88.4972087\ttotal: 55.8s\tremaining: 2m 23s\n",
            "261:\tlearn: 88.4284563\ttotal: 56.1s\tremaining: 2m 23s\n",
            "262:\tlearn: 88.2227326\ttotal: 56.3s\tremaining: 2m 23s\n",
            "263:\tlearn: 87.9497729\ttotal: 56.5s\tremaining: 2m 22s\n",
            "264:\tlearn: 87.8351688\ttotal: 56.7s\tremaining: 2m 22s\n",
            "265:\tlearn: 87.6073123\ttotal: 56.9s\tremaining: 2m 22s\n",
            "266:\tlearn: 87.5329725\ttotal: 57.1s\tremaining: 2m 22s\n",
            "267:\tlearn: 87.2732069\ttotal: 57.3s\tremaining: 2m 22s\n",
            "268:\tlearn: 87.0337927\ttotal: 57.5s\tremaining: 2m 21s\n",
            "269:\tlearn: 86.9838728\ttotal: 57.8s\tremaining: 2m 21s\n",
            "270:\tlearn: 86.9197858\ttotal: 58s\tremaining: 2m 21s\n",
            "271:\tlearn: 86.7590512\ttotal: 58.2s\tremaining: 2m 21s\n",
            "272:\tlearn: 86.6649909\ttotal: 58.4s\tremaining: 2m 21s\n",
            "273:\tlearn: 86.4295590\ttotal: 58.6s\tremaining: 2m 20s\n",
            "274:\tlearn: 85.8585075\ttotal: 58.8s\tremaining: 2m 20s\n",
            "275:\tlearn: 85.6591351\ttotal: 59s\tremaining: 2m 20s\n",
            "276:\tlearn: 85.6042061\ttotal: 59.2s\tremaining: 2m 20s\n",
            "277:\tlearn: 85.4033839\ttotal: 59.5s\tremaining: 2m 19s\n",
            "278:\tlearn: 85.2107072\ttotal: 59.7s\tremaining: 2m 19s\n",
            "279:\tlearn: 85.0773229\ttotal: 59.9s\tremaining: 2m 19s\n",
            "280:\tlearn: 84.9998794\ttotal: 1m\tremaining: 2m 19s\n",
            "281:\tlearn: 84.8537625\ttotal: 1m\tremaining: 2m 19s\n",
            "282:\tlearn: 84.7193496\ttotal: 1m\tremaining: 2m 18s\n",
            "283:\tlearn: 84.5342552\ttotal: 1m\tremaining: 2m 18s\n",
            "284:\tlearn: 84.4875303\ttotal: 1m\tremaining: 2m 18s\n",
            "285:\tlearn: 84.4032597\ttotal: 1m 1s\tremaining: 2m 18s\n",
            "286:\tlearn: 84.0426499\ttotal: 1m 1s\tremaining: 2m 18s\n",
            "287:\tlearn: 83.9755377\ttotal: 1m 1s\tremaining: 2m 17s\n",
            "288:\tlearn: 83.9435879\ttotal: 1m 1s\tremaining: 2m 17s\n",
            "289:\tlearn: 83.7147489\ttotal: 1m 2s\tremaining: 2m 17s\n",
            "290:\tlearn: 83.6441612\ttotal: 1m 2s\tremaining: 2m 17s\n",
            "291:\tlearn: 83.5480302\ttotal: 1m 2s\tremaining: 2m 17s\n",
            "292:\tlearn: 83.4924680\ttotal: 1m 2s\tremaining: 2m 17s\n",
            "293:\tlearn: 83.4172464\ttotal: 1m 3s\tremaining: 2m 16s\n",
            "294:\tlearn: 83.3251665\ttotal: 1m 3s\tremaining: 2m 16s\n",
            "295:\tlearn: 83.1844618\ttotal: 1m 3s\tremaining: 2m 16s\n",
            "296:\tlearn: 83.0616879\ttotal: 1m 3s\tremaining: 2m 16s\n",
            "297:\tlearn: 82.9154619\ttotal: 1m 3s\tremaining: 2m 16s\n",
            "298:\tlearn: 82.9016102\ttotal: 1m 4s\tremaining: 2m 15s\n",
            "299:\tlearn: 82.8162541\ttotal: 1m 4s\tremaining: 2m 15s\n",
            "300:\tlearn: 82.7596489\ttotal: 1m 4s\tremaining: 2m 15s\n",
            "301:\tlearn: 82.5015818\ttotal: 1m 4s\tremaining: 2m 15s\n",
            "302:\tlearn: 82.4273783\ttotal: 1m 5s\tremaining: 2m 15s\n",
            "303:\tlearn: 82.3460713\ttotal: 1m 5s\tremaining: 2m 14s\n",
            "304:\tlearn: 82.2867149\ttotal: 1m 5s\tremaining: 2m 14s\n",
            "305:\tlearn: 82.2259103\ttotal: 1m 5s\tremaining: 2m 14s\n",
            "306:\tlearn: 82.0930114\ttotal: 1m 6s\tremaining: 2m 14s\n",
            "307:\tlearn: 82.0645058\ttotal: 1m 6s\tremaining: 2m 14s\n",
            "308:\tlearn: 82.0603293\ttotal: 1m 6s\tremaining: 2m 14s\n",
            "309:\tlearn: 81.8551941\ttotal: 1m 6s\tremaining: 2m 13s\n",
            "310:\tlearn: 81.7098104\ttotal: 1m 6s\tremaining: 2m 13s\n",
            "311:\tlearn: 81.5912782\ttotal: 1m 7s\tremaining: 2m 13s\n",
            "312:\tlearn: 81.5787473\ttotal: 1m 7s\tremaining: 2m 13s\n",
            "313:\tlearn: 81.3510164\ttotal: 1m 7s\tremaining: 2m 12s\n",
            "314:\tlearn: 81.3124891\ttotal: 1m 7s\tremaining: 2m 12s\n",
            "315:\tlearn: 81.1830958\ttotal: 1m 7s\tremaining: 2m 12s\n",
            "316:\tlearn: 81.0555162\ttotal: 1m 8s\tremaining: 2m 12s\n",
            "317:\tlearn: 80.8782033\ttotal: 1m 8s\tremaining: 2m 11s\n",
            "318:\tlearn: 80.8287869\ttotal: 1m 8s\tremaining: 2m 11s\n",
            "319:\tlearn: 80.7419162\ttotal: 1m 8s\tremaining: 2m 11s\n",
            "320:\tlearn: 80.6459556\ttotal: 1m 8s\tremaining: 2m 11s\n",
            "321:\tlearn: 80.5973941\ttotal: 1m 9s\tremaining: 2m 11s\n",
            "322:\tlearn: 80.4144891\ttotal: 1m 9s\tremaining: 2m 10s\n",
            "323:\tlearn: 80.3607977\ttotal: 1m 9s\tremaining: 2m 10s\n",
            "324:\tlearn: 80.2806332\ttotal: 1m 9s\tremaining: 2m 10s\n",
            "325:\tlearn: 80.1730662\ttotal: 1m 10s\tremaining: 2m 10s\n",
            "326:\tlearn: 80.1237318\ttotal: 1m 10s\tremaining: 2m 9s\n",
            "327:\tlearn: 80.0136846\ttotal: 1m 10s\tremaining: 2m 9s\n",
            "328:\tlearn: 79.9677590\ttotal: 1m 10s\tremaining: 2m 9s\n",
            "329:\tlearn: 79.6244914\ttotal: 1m 10s\tremaining: 2m 9s\n",
            "330:\tlearn: 79.5412329\ttotal: 1m 11s\tremaining: 2m 9s\n",
            "331:\tlearn: 79.2492577\ttotal: 1m 11s\tremaining: 2m 8s\n",
            "332:\tlearn: 79.0584962\ttotal: 1m 11s\tremaining: 2m 8s\n",
            "333:\tlearn: 78.9064703\ttotal: 1m 11s\tremaining: 2m 8s\n",
            "334:\tlearn: 78.8581583\ttotal: 1m 11s\tremaining: 2m 8s\n",
            "335:\tlearn: 78.7971279\ttotal: 1m 12s\tremaining: 2m 7s\n",
            "336:\tlearn: 78.5411781\ttotal: 1m 12s\tremaining: 2m 7s\n",
            "337:\tlearn: 78.4796543\ttotal: 1m 12s\tremaining: 2m 7s\n",
            "338:\tlearn: 78.4282152\ttotal: 1m 12s\tremaining: 2m 7s\n",
            "339:\tlearn: 78.1833335\ttotal: 1m 12s\tremaining: 2m 7s\n",
            "340:\tlearn: 78.1817352\ttotal: 1m 13s\tremaining: 2m 6s\n",
            "341:\tlearn: 78.1487228\ttotal: 1m 13s\tremaining: 2m 6s\n",
            "342:\tlearn: 78.1150521\ttotal: 1m 13s\tremaining: 2m 6s\n",
            "343:\tlearn: 78.0592233\ttotal: 1m 13s\tremaining: 2m 6s\n",
            "344:\tlearn: 78.0335726\ttotal: 1m 14s\tremaining: 2m 6s\n",
            "345:\tlearn: 77.9671311\ttotal: 1m 14s\tremaining: 2m 6s\n",
            "346:\tlearn: 77.8997951\ttotal: 1m 14s\tremaining: 2m 5s\n",
            "347:\tlearn: 77.7867538\ttotal: 1m 14s\tremaining: 2m 5s\n",
            "348:\tlearn: 77.6868381\ttotal: 1m 15s\tremaining: 2m 5s\n",
            "349:\tlearn: 77.6497585\ttotal: 1m 15s\tremaining: 2m 5s\n",
            "350:\tlearn: 77.6329038\ttotal: 1m 15s\tremaining: 2m 4s\n",
            "351:\tlearn: 77.5100168\ttotal: 1m 15s\tremaining: 2m 4s\n",
            "352:\tlearn: 77.5025363\ttotal: 1m 15s\tremaining: 2m 4s\n",
            "353:\tlearn: 77.4093884\ttotal: 1m 16s\tremaining: 2m 4s\n",
            "354:\tlearn: 77.2403095\ttotal: 1m 16s\tremaining: 2m 4s\n",
            "355:\tlearn: 77.1026290\ttotal: 1m 16s\tremaining: 2m 3s\n",
            "356:\tlearn: 76.9394075\ttotal: 1m 16s\tremaining: 2m 3s\n",
            "357:\tlearn: 76.8313823\ttotal: 1m 16s\tremaining: 2m 3s\n",
            "358:\tlearn: 76.8252904\ttotal: 1m 17s\tremaining: 2m 3s\n",
            "359:\tlearn: 76.6476965\ttotal: 1m 17s\tremaining: 2m 3s\n",
            "360:\tlearn: 76.6063815\ttotal: 1m 17s\tremaining: 2m 2s\n",
            "361:\tlearn: 76.4931427\ttotal: 1m 17s\tremaining: 2m 2s\n",
            "362:\tlearn: 76.4175109\ttotal: 1m 18s\tremaining: 2m 2s\n",
            "363:\tlearn: 76.3910524\ttotal: 1m 18s\tremaining: 2m 2s\n",
            "364:\tlearn: 76.2200945\ttotal: 1m 18s\tremaining: 2m 2s\n",
            "365:\tlearn: 76.2182562\ttotal: 1m 18s\tremaining: 2m 1s\n",
            "366:\tlearn: 76.0672420\ttotal: 1m 19s\tremaining: 2m 1s\n",
            "367:\tlearn: 75.9811448\ttotal: 1m 19s\tremaining: 2m 1s\n",
            "368:\tlearn: 75.9441291\ttotal: 1m 19s\tremaining: 2m 1s\n",
            "369:\tlearn: 75.8072094\ttotal: 1m 19s\tremaining: 2m 1s\n",
            "370:\tlearn: 75.7548327\ttotal: 1m 19s\tremaining: 2m\n",
            "371:\tlearn: 75.6860001\ttotal: 1m 20s\tremaining: 2m\n",
            "372:\tlearn: 75.6197747\ttotal: 1m 20s\tremaining: 2m\n",
            "373:\tlearn: 75.4696351\ttotal: 1m 20s\tremaining: 2m\n",
            "374:\tlearn: 75.4066334\ttotal: 1m 20s\tremaining: 1m 59s\n",
            "375:\tlearn: 75.2007265\ttotal: 1m 20s\tremaining: 1m 59s\n",
            "376:\tlearn: 75.1149135\ttotal: 1m 21s\tremaining: 1m 59s\n",
            "377:\tlearn: 75.0150478\ttotal: 1m 21s\tremaining: 1m 59s\n",
            "378:\tlearn: 74.8502307\ttotal: 1m 21s\tremaining: 1m 59s\n",
            "379:\tlearn: 74.8176819\ttotal: 1m 21s\tremaining: 1m 58s\n",
            "380:\tlearn: 74.7994640\ttotal: 1m 22s\tremaining: 1m 58s\n",
            "381:\tlearn: 74.7789964\ttotal: 1m 22s\tremaining: 1m 58s\n",
            "382:\tlearn: 74.7129801\ttotal: 1m 22s\tremaining: 1m 58s\n",
            "383:\tlearn: 74.5469345\ttotal: 1m 22s\tremaining: 1m 58s\n",
            "384:\tlearn: 74.3909986\ttotal: 1m 23s\tremaining: 1m 58s\n",
            "385:\tlearn: 74.3123293\ttotal: 1m 23s\tremaining: 1m 57s\n",
            "386:\tlearn: 74.2691772\ttotal: 1m 23s\tremaining: 1m 57s\n",
            "387:\tlearn: 74.1114756\ttotal: 1m 23s\tremaining: 1m 57s\n",
            "388:\tlearn: 74.0693976\ttotal: 1m 23s\tremaining: 1m 56s\n",
            "389:\tlearn: 73.8783132\ttotal: 1m 24s\tremaining: 1m 56s\n",
            "390:\tlearn: 73.8121842\ttotal: 1m 24s\tremaining: 1m 56s\n",
            "391:\tlearn: 73.7340990\ttotal: 1m 24s\tremaining: 1m 56s\n",
            "392:\tlearn: 73.5740104\ttotal: 1m 24s\tremaining: 1m 56s\n",
            "393:\tlearn: 73.5426428\ttotal: 1m 24s\tremaining: 1m 55s\n",
            "394:\tlearn: 73.5047519\ttotal: 1m 25s\tremaining: 1m 55s\n",
            "395:\tlearn: 73.4769518\ttotal: 1m 25s\tremaining: 1m 55s\n",
            "396:\tlearn: 73.4110619\ttotal: 1m 25s\tremaining: 1m 55s\n",
            "397:\tlearn: 73.3207156\ttotal: 1m 25s\tremaining: 1m 55s\n",
            "398:\tlearn: 73.2925941\ttotal: 1m 25s\tremaining: 1m 54s\n",
            "399:\tlearn: 73.2503963\ttotal: 1m 26s\tremaining: 1m 54s\n",
            "400:\tlearn: 73.2093719\ttotal: 1m 26s\tremaining: 1m 54s\n",
            "401:\tlearn: 72.7560081\ttotal: 1m 26s\tremaining: 1m 54s\n",
            "402:\tlearn: 72.5780170\ttotal: 1m 26s\tremaining: 1m 53s\n",
            "403:\tlearn: 72.4626175\ttotal: 1m 26s\tremaining: 1m 53s\n",
            "404:\tlearn: 72.3662387\ttotal: 1m 27s\tremaining: 1m 53s\n",
            "405:\tlearn: 72.2713128\ttotal: 1m 27s\tremaining: 1m 53s\n",
            "406:\tlearn: 72.2465472\ttotal: 1m 27s\tremaining: 1m 52s\n",
            "407:\tlearn: 71.9853033\ttotal: 1m 27s\tremaining: 1m 52s\n",
            "408:\tlearn: 71.9048277\ttotal: 1m 27s\tremaining: 1m 52s\n",
            "409:\tlearn: 71.8307075\ttotal: 1m 28s\tremaining: 1m 52s\n",
            "410:\tlearn: 71.7116655\ttotal: 1m 28s\tremaining: 1m 52s\n",
            "411:\tlearn: 71.5525972\ttotal: 1m 28s\tremaining: 1m 51s\n",
            "412:\tlearn: 71.4266281\ttotal: 1m 28s\tremaining: 1m 51s\n",
            "413:\tlearn: 71.3339665\ttotal: 1m 29s\tremaining: 1m 51s\n",
            "414:\tlearn: 71.2924497\ttotal: 1m 29s\tremaining: 1m 51s\n",
            "415:\tlearn: 71.1593312\ttotal: 1m 29s\tremaining: 1m 50s\n",
            "416:\tlearn: 71.1227261\ttotal: 1m 29s\tremaining: 1m 50s\n",
            "417:\tlearn: 71.0848688\ttotal: 1m 30s\tremaining: 1m 50s\n",
            "418:\tlearn: 70.9839495\ttotal: 1m 30s\tremaining: 1m 50s\n",
            "419:\tlearn: 70.9092060\ttotal: 1m 30s\tremaining: 1m 50s\n",
            "420:\tlearn: 70.8558634\ttotal: 1m 30s\tremaining: 1m 50s\n",
            "421:\tlearn: 70.7858364\ttotal: 1m 30s\tremaining: 1m 49s\n",
            "422:\tlearn: 70.7746947\ttotal: 1m 31s\tremaining: 1m 49s\n",
            "423:\tlearn: 70.7364557\ttotal: 1m 31s\tremaining: 1m 49s\n",
            "424:\tlearn: 70.6359248\ttotal: 1m 31s\tremaining: 1m 49s\n",
            "425:\tlearn: 70.5731271\ttotal: 1m 31s\tremaining: 1m 48s\n",
            "426:\tlearn: 70.5416009\ttotal: 1m 31s\tremaining: 1m 48s\n",
            "427:\tlearn: 70.4787795\ttotal: 1m 32s\tremaining: 1m 48s\n",
            "428:\tlearn: 70.4597324\ttotal: 1m 32s\tremaining: 1m 48s\n",
            "429:\tlearn: 70.3933856\ttotal: 1m 32s\tremaining: 1m 48s\n",
            "430:\tlearn: 70.3392270\ttotal: 1m 32s\tremaining: 1m 47s\n",
            "431:\tlearn: 70.3200717\ttotal: 1m 33s\tremaining: 1m 47s\n",
            "432:\tlearn: 70.2084439\ttotal: 1m 33s\tremaining: 1m 47s\n",
            "433:\tlearn: 70.1682232\ttotal: 1m 33s\tremaining: 1m 47s\n",
            "434:\tlearn: 70.1338455\ttotal: 1m 33s\tremaining: 1m 47s\n",
            "435:\tlearn: 69.9242289\ttotal: 1m 34s\tremaining: 1m 46s\n",
            "436:\tlearn: 69.7897567\ttotal: 1m 34s\tremaining: 1m 46s\n",
            "437:\tlearn: 69.7619519\ttotal: 1m 34s\tremaining: 1m 46s\n",
            "438:\tlearn: 69.6768085\ttotal: 1m 34s\tremaining: 1m 46s\n",
            "439:\tlearn: 69.5897259\ttotal: 1m 34s\tremaining: 1m 46s\n",
            "440:\tlearn: 69.4625210\ttotal: 1m 35s\tremaining: 1m 45s\n",
            "441:\tlearn: 69.4018066\ttotal: 1m 35s\tremaining: 1m 45s\n",
            "442:\tlearn: 69.3769471\ttotal: 1m 35s\tremaining: 1m 45s\n",
            "443:\tlearn: 69.2971779\ttotal: 1m 35s\tremaining: 1m 45s\n",
            "444:\tlearn: 69.2506722\ttotal: 1m 36s\tremaining: 1m 45s\n",
            "445:\tlearn: 69.2351698\ttotal: 1m 36s\tremaining: 1m 44s\n",
            "446:\tlearn: 69.1969464\ttotal: 1m 36s\tremaining: 1m 44s\n",
            "447:\tlearn: 69.1600996\ttotal: 1m 36s\tremaining: 1m 44s\n",
            "448:\tlearn: 69.1484931\ttotal: 1m 37s\tremaining: 1m 44s\n",
            "449:\tlearn: 69.1434438\ttotal: 1m 37s\tremaining: 1m 44s\n",
            "450:\tlearn: 69.0025761\ttotal: 1m 37s\tremaining: 1m 44s\n",
            "451:\tlearn: 68.9301737\ttotal: 1m 37s\tremaining: 1m 43s\n",
            "452:\tlearn: 68.9089553\ttotal: 1m 37s\tremaining: 1m 43s\n",
            "453:\tlearn: 68.8604017\ttotal: 1m 38s\tremaining: 1m 43s\n",
            "454:\tlearn: 68.7820090\ttotal: 1m 38s\tremaining: 1m 43s\n",
            "455:\tlearn: 68.6788105\ttotal: 1m 38s\tremaining: 1m 43s\n",
            "456:\tlearn: 68.6360146\ttotal: 1m 38s\tremaining: 1m 42s\n",
            "457:\tlearn: 68.5170991\ttotal: 1m 39s\tremaining: 1m 42s\n",
            "458:\tlearn: 68.4477145\ttotal: 1m 39s\tremaining: 1m 42s\n",
            "459:\tlearn: 68.3696440\ttotal: 1m 39s\tremaining: 1m 42s\n",
            "460:\tlearn: 68.2402157\ttotal: 1m 39s\tremaining: 1m 41s\n",
            "461:\tlearn: 68.1684602\ttotal: 1m 39s\tremaining: 1m 41s\n",
            "462:\tlearn: 68.1459092\ttotal: 1m 40s\tremaining: 1m 41s\n",
            "463:\tlearn: 68.1100399\ttotal: 1m 40s\tremaining: 1m 41s\n",
            "464:\tlearn: 68.0515013\ttotal: 1m 40s\tremaining: 1m 41s\n",
            "465:\tlearn: 67.9869881\ttotal: 1m 40s\tremaining: 1m 40s\n",
            "466:\tlearn: 67.9473104\ttotal: 1m 40s\tremaining: 1m 40s\n",
            "467:\tlearn: 67.9354125\ttotal: 1m 41s\tremaining: 1m 40s\n",
            "468:\tlearn: 67.8328195\ttotal: 1m 41s\tremaining: 1m 40s\n",
            "469:\tlearn: 67.7731661\ttotal: 1m 41s\tremaining: 1m 39s\n",
            "470:\tlearn: 67.7264694\ttotal: 1m 41s\tremaining: 1m 39s\n",
            "471:\tlearn: 67.6955882\ttotal: 1m 42s\tremaining: 1m 39s\n",
            "472:\tlearn: 67.5882368\ttotal: 1m 42s\tremaining: 1m 39s\n",
            "473:\tlearn: 67.5497154\ttotal: 1m 42s\tremaining: 1m 39s\n",
            "474:\tlearn: 67.5054376\ttotal: 1m 42s\tremaining: 1m 38s\n",
            "475:\tlearn: 67.4346959\ttotal: 1m 42s\tremaining: 1m 38s\n",
            "476:\tlearn: 67.3394550\ttotal: 1m 43s\tremaining: 1m 38s\n",
            "477:\tlearn: 67.3007785\ttotal: 1m 43s\tremaining: 1m 38s\n",
            "478:\tlearn: 67.2508363\ttotal: 1m 43s\tremaining: 1m 37s\n",
            "479:\tlearn: 67.1741688\ttotal: 1m 43s\tremaining: 1m 37s\n",
            "480:\tlearn: 67.1119536\ttotal: 1m 43s\tremaining: 1m 37s\n",
            "481:\tlearn: 67.0848835\ttotal: 1m 44s\tremaining: 1m 37s\n",
            "482:\tlearn: 67.0404382\ttotal: 1m 44s\tremaining: 1m 37s\n",
            "483:\tlearn: 66.9599571\ttotal: 1m 44s\tremaining: 1m 36s\n",
            "484:\tlearn: 66.9125470\ttotal: 1m 44s\tremaining: 1m 36s\n",
            "485:\tlearn: 66.7414518\ttotal: 1m 44s\tremaining: 1m 36s\n",
            "486:\tlearn: 66.6986034\ttotal: 1m 45s\tremaining: 1m 36s\n",
            "487:\tlearn: 66.6605896\ttotal: 1m 45s\tremaining: 1m 35s\n",
            "488:\tlearn: 66.5261467\ttotal: 1m 45s\tremaining: 1m 35s\n",
            "489:\tlearn: 66.4938801\ttotal: 1m 45s\tremaining: 1m 35s\n",
            "490:\tlearn: 66.4675317\ttotal: 1m 46s\tremaining: 1m 35s\n",
            "491:\tlearn: 66.3494542\ttotal: 1m 46s\tremaining: 1m 35s\n",
            "492:\tlearn: 66.2959023\ttotal: 1m 46s\tremaining: 1m 34s\n",
            "493:\tlearn: 66.2265508\ttotal: 1m 46s\tremaining: 1m 34s\n",
            "494:\tlearn: 66.1640795\ttotal: 1m 46s\tremaining: 1m 34s\n",
            "495:\tlearn: 66.0819585\ttotal: 1m 47s\tremaining: 1m 34s\n",
            "496:\tlearn: 65.9499398\ttotal: 1m 47s\tremaining: 1m 33s\n",
            "497:\tlearn: 65.9016905\ttotal: 1m 47s\tremaining: 1m 33s\n",
            "498:\tlearn: 65.8843502\ttotal: 1m 47s\tremaining: 1m 33s\n",
            "499:\tlearn: 65.8473497\ttotal: 1m 48s\tremaining: 1m 33s\n",
            "500:\tlearn: 65.8057511\ttotal: 1m 48s\tremaining: 1m 33s\n",
            "501:\tlearn: 65.7787155\ttotal: 1m 48s\tremaining: 1m 32s\n",
            "502:\tlearn: 65.7309118\ttotal: 1m 48s\tremaining: 1m 32s\n",
            "503:\tlearn: 65.7045618\ttotal: 1m 48s\tremaining: 1m 32s\n",
            "504:\tlearn: 65.6318568\ttotal: 1m 49s\tremaining: 1m 32s\n",
            "505:\tlearn: 65.5735164\ttotal: 1m 49s\tremaining: 1m 32s\n",
            "506:\tlearn: 65.5289368\ttotal: 1m 49s\tremaining: 1m 31s\n",
            "507:\tlearn: 65.5215593\ttotal: 1m 49s\tremaining: 1m 31s\n",
            "508:\tlearn: 65.5175280\ttotal: 1m 50s\tremaining: 1m 31s\n",
            "509:\tlearn: 65.4668931\ttotal: 1m 50s\tremaining: 1m 31s\n",
            "510:\tlearn: 65.4428705\ttotal: 1m 50s\tremaining: 1m 31s\n",
            "511:\tlearn: 65.3056663\ttotal: 1m 50s\tremaining: 1m 30s\n",
            "512:\tlearn: 65.2608814\ttotal: 1m 50s\tremaining: 1m 30s\n",
            "513:\tlearn: 65.2362264\ttotal: 1m 51s\tremaining: 1m 30s\n",
            "514:\tlearn: 65.2059532\ttotal: 1m 51s\tremaining: 1m 30s\n",
            "515:\tlearn: 65.1533142\ttotal: 1m 51s\tremaining: 1m 30s\n",
            "516:\tlearn: 65.0188852\ttotal: 1m 51s\tremaining: 1m 29s\n",
            "517:\tlearn: 64.9916717\ttotal: 1m 52s\tremaining: 1m 29s\n",
            "518:\tlearn: 64.8244893\ttotal: 1m 52s\tremaining: 1m 29s\n",
            "519:\tlearn: 64.7472183\ttotal: 1m 52s\tremaining: 1m 29s\n",
            "520:\tlearn: 64.6446663\ttotal: 1m 52s\tremaining: 1m 28s\n",
            "521:\tlearn: 64.5683041\ttotal: 1m 52s\tremaining: 1m 28s\n",
            "522:\tlearn: 64.5476334\ttotal: 1m 53s\tremaining: 1m 28s\n",
            "523:\tlearn: 64.5205977\ttotal: 1m 53s\tremaining: 1m 28s\n",
            "524:\tlearn: 64.4757390\ttotal: 1m 53s\tremaining: 1m 28s\n",
            "525:\tlearn: 64.4656615\ttotal: 1m 53s\tremaining: 1m 27s\n",
            "526:\tlearn: 64.4302470\ttotal: 1m 54s\tremaining: 1m 27s\n",
            "527:\tlearn: 64.3954724\ttotal: 1m 54s\tremaining: 1m 27s\n",
            "528:\tlearn: 64.3659123\ttotal: 1m 54s\tremaining: 1m 27s\n",
            "529:\tlearn: 64.3258349\ttotal: 1m 54s\tremaining: 1m 27s\n",
            "530:\tlearn: 64.2713013\ttotal: 1m 55s\tremaining: 1m 26s\n",
            "531:\tlearn: 64.2692867\ttotal: 1m 55s\tremaining: 1m 26s\n",
            "532:\tlearn: 64.2522220\ttotal: 1m 55s\tremaining: 1m 26s\n",
            "533:\tlearn: 64.1917253\ttotal: 1m 55s\tremaining: 1m 26s\n",
            "534:\tlearn: 64.1904259\ttotal: 1m 56s\tremaining: 1m 26s\n",
            "535:\tlearn: 64.1581031\ttotal: 1m 56s\tremaining: 1m 25s\n",
            "536:\tlearn: 64.1245264\ttotal: 1m 56s\tremaining: 1m 25s\n",
            "537:\tlearn: 64.0957005\ttotal: 1m 56s\tremaining: 1m 25s\n",
            "538:\tlearn: 63.9888998\ttotal: 1m 56s\tremaining: 1m 25s\n",
            "539:\tlearn: 63.9323698\ttotal: 1m 57s\tremaining: 1m 24s\n",
            "540:\tlearn: 63.9047087\ttotal: 1m 57s\tremaining: 1m 24s\n",
            "541:\tlearn: 63.7602660\ttotal: 1m 57s\tremaining: 1m 24s\n",
            "542:\tlearn: 63.7023592\ttotal: 1m 57s\tremaining: 1m 24s\n",
            "543:\tlearn: 63.4963182\ttotal: 1m 57s\tremaining: 1m 24s\n",
            "544:\tlearn: 63.4095973\ttotal: 1m 58s\tremaining: 1m 23s\n",
            "545:\tlearn: 63.2904607\ttotal: 1m 58s\tremaining: 1m 23s\n",
            "546:\tlearn: 63.2489369\ttotal: 1m 58s\tremaining: 1m 23s\n",
            "547:\tlearn: 63.1327996\ttotal: 1m 58s\tremaining: 1m 23s\n",
            "548:\tlearn: 63.0660595\ttotal: 1m 59s\tremaining: 1m 23s\n",
            "549:\tlearn: 62.9782567\ttotal: 1m 59s\tremaining: 1m 22s\n",
            "550:\tlearn: 62.7955989\ttotal: 1m 59s\tremaining: 1m 22s\n",
            "551:\tlearn: 62.6992191\ttotal: 1m 59s\tremaining: 1m 22s\n",
            "552:\tlearn: 62.6625125\ttotal: 1m 59s\tremaining: 1m 22s\n",
            "553:\tlearn: 62.6199739\ttotal: 2m\tremaining: 1m 21s\n",
            "554:\tlearn: 62.6048762\ttotal: 2m\tremaining: 1m 21s\n",
            "555:\tlearn: 62.5301947\ttotal: 2m\tremaining: 1m 21s\n",
            "556:\tlearn: 62.5108123\ttotal: 2m\tremaining: 1m 21s\n",
            "557:\tlearn: 62.4359142\ttotal: 2m 1s\tremaining: 1m 21s\n",
            "558:\tlearn: 62.2816911\ttotal: 2m 1s\tremaining: 1m 20s\n",
            "559:\tlearn: 62.2488630\ttotal: 2m 1s\tremaining: 1m 20s\n",
            "560:\tlearn: 62.2057028\ttotal: 2m 1s\tremaining: 1m 20s\n",
            "561:\tlearn: 62.1819522\ttotal: 2m 1s\tremaining: 1m 20s\n",
            "562:\tlearn: 62.1426547\ttotal: 2m 2s\tremaining: 1m 20s\n",
            "563:\tlearn: 62.0871383\ttotal: 2m 2s\tremaining: 1m 19s\n",
            "564:\tlearn: 61.9749785\ttotal: 2m 2s\tremaining: 1m 19s\n",
            "565:\tlearn: 61.9251439\ttotal: 2m 2s\tremaining: 1m 19s\n",
            "566:\tlearn: 61.8813199\ttotal: 2m 3s\tremaining: 1m 19s\n",
            "567:\tlearn: 61.7491815\ttotal: 2m 3s\tremaining: 1m 18s\n",
            "568:\tlearn: 61.7014470\ttotal: 2m 3s\tremaining: 1m 18s\n",
            "569:\tlearn: 61.6445029\ttotal: 2m 3s\tremaining: 1m 18s\n",
            "570:\tlearn: 61.6375200\ttotal: 2m 3s\tremaining: 1m 18s\n",
            "571:\tlearn: 61.6126720\ttotal: 2m 4s\tremaining: 1m 18s\n",
            "572:\tlearn: 61.5802848\ttotal: 2m 4s\tremaining: 1m 17s\n",
            "573:\tlearn: 61.5783565\ttotal: 2m 4s\tremaining: 1m 17s\n",
            "574:\tlearn: 61.5477648\ttotal: 2m 4s\tremaining: 1m 17s\n",
            "575:\tlearn: 61.5218127\ttotal: 2m 4s\tremaining: 1m 17s\n",
            "576:\tlearn: 61.4887367\ttotal: 2m 5s\tremaining: 1m 17s\n",
            "577:\tlearn: 61.4547687\ttotal: 2m 5s\tremaining: 1m 16s\n",
            "578:\tlearn: 61.3907793\ttotal: 2m 5s\tremaining: 1m 16s\n",
            "579:\tlearn: 61.3552094\ttotal: 2m 5s\tremaining: 1m 16s\n",
            "580:\tlearn: 61.3211593\ttotal: 2m 6s\tremaining: 1m 16s\n",
            "581:\tlearn: 61.3097682\ttotal: 2m 6s\tremaining: 1m 15s\n",
            "582:\tlearn: 61.2557052\ttotal: 2m 6s\tremaining: 1m 15s\n",
            "583:\tlearn: 61.2212888\ttotal: 2m 6s\tremaining: 1m 15s\n",
            "584:\tlearn: 61.1487959\ttotal: 2m 6s\tremaining: 1m 15s\n",
            "585:\tlearn: 61.0239934\ttotal: 2m 7s\tremaining: 1m 15s\n",
            "586:\tlearn: 60.9504854\ttotal: 2m 7s\tremaining: 1m 14s\n",
            "587:\tlearn: 60.8323195\ttotal: 2m 7s\tremaining: 1m 14s\n",
            "588:\tlearn: 60.8161866\ttotal: 2m 7s\tremaining: 1m 14s\n",
            "589:\tlearn: 60.7862672\ttotal: 2m 8s\tremaining: 1m 14s\n",
            "590:\tlearn: 60.7174206\ttotal: 2m 8s\tremaining: 1m 14s\n",
            "591:\tlearn: 60.6904291\ttotal: 2m 8s\tremaining: 1m 13s\n",
            "592:\tlearn: 60.6888464\ttotal: 2m 8s\tremaining: 1m 13s\n",
            "593:\tlearn: 60.6868625\ttotal: 2m 9s\tremaining: 1m 13s\n",
            "594:\tlearn: 60.6286019\ttotal: 2m 9s\tremaining: 1m 13s\n",
            "595:\tlearn: 60.5235043\ttotal: 2m 9s\tremaining: 1m 12s\n",
            "596:\tlearn: 60.4912733\ttotal: 2m 9s\tremaining: 1m 12s\n",
            "597:\tlearn: 60.4101387\ttotal: 2m 9s\tremaining: 1m 12s\n",
            "598:\tlearn: 60.3795384\ttotal: 2m 10s\tremaining: 1m 12s\n",
            "599:\tlearn: 60.3501348\ttotal: 2m 10s\tremaining: 1m 12s\n",
            "600:\tlearn: 60.3201567\ttotal: 2m 10s\tremaining: 1m 11s\n",
            "601:\tlearn: 60.2563182\ttotal: 2m 10s\tremaining: 1m 11s\n",
            "602:\tlearn: 60.1667055\ttotal: 2m 10s\tremaining: 1m 11s\n",
            "603:\tlearn: 60.1164113\ttotal: 2m 11s\tremaining: 1m 11s\n",
            "604:\tlearn: 60.0706677\ttotal: 2m 11s\tremaining: 1m 11s\n",
            "605:\tlearn: 60.0515059\ttotal: 2m 11s\tremaining: 1m 10s\n",
            "606:\tlearn: 60.0300243\ttotal: 2m 11s\tremaining: 1m 10s\n",
            "607:\tlearn: 59.9934179\ttotal: 2m 12s\tremaining: 1m 10s\n",
            "608:\tlearn: 59.9741940\ttotal: 2m 12s\tremaining: 1m 10s\n",
            "609:\tlearn: 59.9246155\ttotal: 2m 12s\tremaining: 1m 9s\n",
            "610:\tlearn: 59.9240564\ttotal: 2m 12s\tremaining: 1m 9s\n",
            "611:\tlearn: 59.8351694\ttotal: 2m 12s\tremaining: 1m 9s\n",
            "612:\tlearn: 59.7366860\ttotal: 2m 13s\tremaining: 1m 9s\n",
            "613:\tlearn: 59.6745283\ttotal: 2m 13s\tremaining: 1m 9s\n",
            "614:\tlearn: 59.6508026\ttotal: 2m 13s\tremaining: 1m 8s\n",
            "615:\tlearn: 59.6177435\ttotal: 2m 13s\tremaining: 1m 8s\n",
            "616:\tlearn: 59.5745258\ttotal: 2m 13s\tremaining: 1m 8s\n",
            "617:\tlearn: 59.4759793\ttotal: 2m 14s\tremaining: 1m 8s\n",
            "618:\tlearn: 59.4447602\ttotal: 2m 14s\tremaining: 1m 8s\n",
            "619:\tlearn: 59.3844055\ttotal: 2m 14s\tremaining: 1m 7s\n",
            "620:\tlearn: 59.3154270\ttotal: 2m 14s\tremaining: 1m 7s\n",
            "621:\tlearn: 59.2892838\ttotal: 2m 15s\tremaining: 1m 7s\n",
            "622:\tlearn: 59.2020058\ttotal: 2m 15s\tremaining: 1m 7s\n",
            "623:\tlearn: 59.1954126\ttotal: 2m 15s\tremaining: 1m 6s\n",
            "624:\tlearn: 59.0848570\ttotal: 2m 15s\tremaining: 1m 6s\n",
            "625:\tlearn: 59.0609802\ttotal: 2m 15s\tremaining: 1m 6s\n",
            "626:\tlearn: 58.9934379\ttotal: 2m 16s\tremaining: 1m 6s\n",
            "627:\tlearn: 58.9739885\ttotal: 2m 16s\tremaining: 1m 5s\n",
            "628:\tlearn: 58.9443711\ttotal: 2m 16s\tremaining: 1m 5s\n",
            "629:\tlearn: 58.9214126\ttotal: 2m 16s\tremaining: 1m 5s\n",
            "630:\tlearn: 58.8183299\ttotal: 2m 16s\tremaining: 1m 5s\n",
            "631:\tlearn: 58.7987281\ttotal: 2m 17s\tremaining: 1m 5s\n",
            "632:\tlearn: 58.7927846\ttotal: 2m 17s\tremaining: 1m 4s\n",
            "633:\tlearn: 58.7242635\ttotal: 2m 17s\tremaining: 1m 4s\n",
            "634:\tlearn: 58.6429665\ttotal: 2m 17s\tremaining: 1m 4s\n",
            "635:\tlearn: 58.6145853\ttotal: 2m 17s\tremaining: 1m 4s\n",
            "636:\tlearn: 58.5884943\ttotal: 2m 18s\tremaining: 1m 3s\n",
            "637:\tlearn: 58.5248312\ttotal: 2m 18s\tremaining: 1m 3s\n",
            "638:\tlearn: 58.5070521\ttotal: 2m 18s\tremaining: 1m 3s\n",
            "639:\tlearn: 58.4821223\ttotal: 2m 18s\tremaining: 1m 3s\n",
            "640:\tlearn: 58.4626208\ttotal: 2m 19s\tremaining: 1m 3s\n",
            "641:\tlearn: 58.4393817\ttotal: 2m 19s\tremaining: 1m 2s\n",
            "642:\tlearn: 58.4202245\ttotal: 2m 19s\tremaining: 1m 2s\n",
            "643:\tlearn: 58.3965833\ttotal: 2m 19s\tremaining: 1m 2s\n",
            "644:\tlearn: 58.3657147\ttotal: 2m 19s\tremaining: 1m 2s\n",
            "645:\tlearn: 58.3386846\ttotal: 2m 20s\tremaining: 1m 2s\n",
            "646:\tlearn: 58.3149057\ttotal: 2m 20s\tremaining: 1m 1s\n",
            "647:\tlearn: 58.2885123\ttotal: 2m 20s\tremaining: 1m 1s\n",
            "648:\tlearn: 58.2467968\ttotal: 2m 20s\tremaining: 1m 1s\n",
            "649:\tlearn: 58.1967258\ttotal: 2m 21s\tremaining: 1m 1s\n",
            "650:\tlearn: 58.0406525\ttotal: 2m 21s\tremaining: 1m 1s\n",
            "651:\tlearn: 58.0152642\ttotal: 2m 21s\tremaining: 1m\n",
            "652:\tlearn: 57.9788197\ttotal: 2m 21s\tremaining: 1m\n",
            "653:\tlearn: 57.9609550\ttotal: 2m 22s\tremaining: 1m\n",
            "654:\tlearn: 57.8875261\ttotal: 2m 22s\tremaining: 1m\n",
            "655:\tlearn: 57.8855325\ttotal: 2m 22s\tremaining: 60s\n",
            "656:\tlearn: 57.8707322\ttotal: 2m 22s\tremaining: 59.8s\n",
            "657:\tlearn: 57.7824641\ttotal: 2m 22s\tremaining: 59.5s\n",
            "658:\tlearn: 57.7748879\ttotal: 2m 23s\tremaining: 59.3s\n",
            "659:\tlearn: 57.7536308\ttotal: 2m 23s\tremaining: 59.1s\n",
            "660:\tlearn: 57.7260839\ttotal: 2m 23s\tremaining: 58.9s\n",
            "661:\tlearn: 57.7153439\ttotal: 2m 23s\tremaining: 58.7s\n",
            "662:\tlearn: 57.6724637\ttotal: 2m 24s\tremaining: 58.5s\n",
            "663:\tlearn: 57.6469894\ttotal: 2m 24s\tremaining: 58.3s\n",
            "664:\tlearn: 57.5377234\ttotal: 2m 24s\tremaining: 58.1s\n",
            "665:\tlearn: 57.5168741\ttotal: 2m 24s\tremaining: 57.9s\n",
            "666:\tlearn: 57.4606124\ttotal: 2m 25s\tremaining: 57.7s\n",
            "667:\tlearn: 57.4600755\ttotal: 2m 25s\tremaining: 57.5s\n",
            "668:\tlearn: 57.3725742\ttotal: 2m 25s\tremaining: 57.3s\n",
            "669:\tlearn: 57.3394254\ttotal: 2m 25s\tremaining: 57s\n",
            "670:\tlearn: 57.3049819\ttotal: 2m 26s\tremaining: 56.8s\n",
            "671:\tlearn: 57.2225301\ttotal: 2m 26s\tremaining: 56.6s\n",
            "672:\tlearn: 57.1786503\ttotal: 2m 26s\tremaining: 56.4s\n",
            "673:\tlearn: 57.0668659\ttotal: 2m 26s\tremaining: 56.2s\n",
            "674:\tlearn: 57.0395774\ttotal: 2m 26s\tremaining: 55.9s\n",
            "675:\tlearn: 56.9700997\ttotal: 2m 27s\tremaining: 55.7s\n",
            "676:\tlearn: 56.9233120\ttotal: 2m 27s\tremaining: 55.5s\n",
            "677:\tlearn: 56.8731293\ttotal: 2m 27s\tremaining: 55.3s\n",
            "678:\tlearn: 56.8314324\ttotal: 2m 27s\tremaining: 55s\n",
            "679:\tlearn: 56.8181517\ttotal: 2m 27s\tremaining: 54.8s\n",
            "680:\tlearn: 56.7825055\ttotal: 2m 28s\tremaining: 54.6s\n",
            "681:\tlearn: 56.6248162\ttotal: 2m 28s\tremaining: 54.4s\n",
            "682:\tlearn: 56.6082936\ttotal: 2m 28s\tremaining: 54.2s\n",
            "683:\tlearn: 56.5473371\ttotal: 2m 28s\tremaining: 54s\n",
            "684:\tlearn: 56.5327149\ttotal: 2m 29s\tremaining: 53.8s\n",
            "685:\tlearn: 56.4983845\ttotal: 2m 29s\tremaining: 53.6s\n",
            "686:\tlearn: 56.4638428\ttotal: 2m 29s\tremaining: 53.4s\n",
            "687:\tlearn: 56.3870775\ttotal: 2m 29s\tremaining: 53.1s\n",
            "688:\tlearn: 56.3491211\ttotal: 2m 30s\tremaining: 52.9s\n",
            "689:\tlearn: 56.3216271\ttotal: 2m 30s\tremaining: 52.7s\n",
            "690:\tlearn: 56.3013236\ttotal: 2m 30s\tremaining: 52.5s\n",
            "691:\tlearn: 56.2463658\ttotal: 2m 30s\tremaining: 52.3s\n",
            "692:\tlearn: 56.1498953\ttotal: 2m 30s\tremaining: 52s\n",
            "693:\tlearn: 56.0672355\ttotal: 2m 31s\tremaining: 51.8s\n",
            "694:\tlearn: 56.0097160\ttotal: 2m 31s\tremaining: 51.6s\n",
            "695:\tlearn: 55.9827152\ttotal: 2m 31s\tremaining: 51.4s\n",
            "696:\tlearn: 55.9388721\ttotal: 2m 31s\tremaining: 51.1s\n",
            "697:\tlearn: 55.8795492\ttotal: 2m 31s\tremaining: 50.9s\n",
            "698:\tlearn: 55.8551342\ttotal: 2m 32s\tremaining: 50.7s\n",
            "699:\tlearn: 55.8501591\ttotal: 2m 32s\tremaining: 50.5s\n",
            "700:\tlearn: 55.7634513\ttotal: 2m 32s\tremaining: 50.2s\n",
            "701:\tlearn: 55.7289984\ttotal: 2m 32s\tremaining: 50s\n",
            "702:\tlearn: 55.6887188\ttotal: 2m 32s\tremaining: 49.8s\n",
            "703:\tlearn: 55.6762167\ttotal: 2m 33s\tremaining: 49.6s\n",
            "704:\tlearn: 55.6177817\ttotal: 2m 33s\tremaining: 49.4s\n",
            "705:\tlearn: 55.5721318\ttotal: 2m 33s\tremaining: 49.1s\n",
            "706:\tlearn: 55.5183706\ttotal: 2m 33s\tremaining: 48.9s\n",
            "707:\tlearn: 55.4884741\ttotal: 2m 33s\tremaining: 48.7s\n",
            "708:\tlearn: 55.4479520\ttotal: 2m 34s\tremaining: 48.5s\n",
            "709:\tlearn: 55.4225521\ttotal: 2m 34s\tremaining: 48.3s\n",
            "710:\tlearn: 55.3595104\ttotal: 2m 34s\tremaining: 48.1s\n",
            "711:\tlearn: 55.3286338\ttotal: 2m 34s\tremaining: 47.8s\n",
            "712:\tlearn: 55.3068445\ttotal: 2m 35s\tremaining: 47.6s\n",
            "713:\tlearn: 55.2619675\ttotal: 2m 35s\tremaining: 47.4s\n",
            "714:\tlearn: 55.1379736\ttotal: 2m 35s\tremaining: 47.2s\n",
            "715:\tlearn: 55.0840509\ttotal: 2m 35s\tremaining: 47s\n",
            "716:\tlearn: 55.0583136\ttotal: 2m 35s\tremaining: 46.7s\n",
            "717:\tlearn: 55.0540227\ttotal: 2m 36s\tremaining: 46.5s\n",
            "718:\tlearn: 55.0432205\ttotal: 2m 36s\tremaining: 46.3s\n",
            "719:\tlearn: 54.9992908\ttotal: 2m 36s\tremaining: 46.1s\n",
            "720:\tlearn: 54.9824233\ttotal: 2m 36s\tremaining: 45.9s\n",
            "721:\tlearn: 54.9382855\ttotal: 2m 37s\tremaining: 45.7s\n",
            "722:\tlearn: 54.9220337\ttotal: 2m 37s\tremaining: 45.4s\n",
            "723:\tlearn: 54.9042505\ttotal: 2m 37s\tremaining: 45.2s\n",
            "724:\tlearn: 54.8754170\ttotal: 2m 37s\tremaining: 45s\n",
            "725:\tlearn: 54.8206975\ttotal: 2m 37s\tremaining: 44.8s\n",
            "726:\tlearn: 54.7587541\ttotal: 2m 38s\tremaining: 44.6s\n",
            "727:\tlearn: 54.7283736\ttotal: 2m 38s\tremaining: 44.4s\n",
            "728:\tlearn: 54.6217913\ttotal: 2m 38s\tremaining: 44.2s\n",
            "729:\tlearn: 54.5749995\ttotal: 2m 38s\tremaining: 44s\n",
            "730:\tlearn: 54.5262870\ttotal: 2m 39s\tremaining: 43.8s\n",
            "731:\tlearn: 54.5084079\ttotal: 2m 39s\tremaining: 43.5s\n",
            "732:\tlearn: 54.4791742\ttotal: 2m 39s\tremaining: 43.3s\n",
            "733:\tlearn: 54.4454627\ttotal: 2m 39s\tremaining: 43.1s\n",
            "734:\tlearn: 54.4169448\ttotal: 2m 40s\tremaining: 42.9s\n",
            "735:\tlearn: 54.3724213\ttotal: 2m 40s\tremaining: 42.7s\n",
            "736:\tlearn: 54.3522613\ttotal: 2m 40s\tremaining: 42.5s\n",
            "737:\tlearn: 54.3369609\ttotal: 2m 40s\tremaining: 42.2s\n",
            "738:\tlearn: 54.3196072\ttotal: 2m 40s\tremaining: 42s\n",
            "739:\tlearn: 54.3060160\ttotal: 2m 41s\tremaining: 41.8s\n",
            "740:\tlearn: 54.2841108\ttotal: 2m 41s\tremaining: 41.6s\n",
            "741:\tlearn: 54.1431808\ttotal: 2m 41s\tremaining: 41.4s\n",
            "742:\tlearn: 54.1327123\ttotal: 2m 41s\tremaining: 41.2s\n",
            "743:\tlearn: 54.0994384\ttotal: 2m 42s\tremaining: 41s\n",
            "744:\tlearn: 54.0845556\ttotal: 2m 42s\tremaining: 40.7s\n",
            "745:\tlearn: 54.0483590\ttotal: 2m 42s\tremaining: 40.5s\n",
            "746:\tlearn: 53.9258968\ttotal: 2m 42s\tremaining: 40.3s\n",
            "747:\tlearn: 53.9142235\ttotal: 2m 43s\tremaining: 40.1s\n",
            "748:\tlearn: 53.7921376\ttotal: 2m 43s\tremaining: 39.9s\n",
            "749:\tlearn: 53.7829269\ttotal: 2m 43s\tremaining: 39.7s\n",
            "750:\tlearn: 53.6826565\ttotal: 2m 43s\tremaining: 39.5s\n",
            "751:\tlearn: 53.6215837\ttotal: 2m 43s\tremaining: 39.2s\n",
            "752:\tlearn: 53.6028090\ttotal: 2m 44s\tremaining: 39s\n",
            "753:\tlearn: 53.5826840\ttotal: 2m 44s\tremaining: 38.8s\n",
            "754:\tlearn: 53.5604353\ttotal: 2m 44s\tremaining: 38.6s\n",
            "755:\tlearn: 53.5450587\ttotal: 2m 44s\tremaining: 38.4s\n",
            "756:\tlearn: 53.5295923\ttotal: 2m 44s\tremaining: 38.1s\n",
            "757:\tlearn: 53.4769533\ttotal: 2m 45s\tremaining: 37.9s\n",
            "758:\tlearn: 53.4248643\ttotal: 2m 45s\tremaining: 37.7s\n",
            "759:\tlearn: 53.4174152\ttotal: 2m 45s\tremaining: 37.5s\n",
            "760:\tlearn: 53.3864816\ttotal: 2m 45s\tremaining: 37.3s\n",
            "761:\tlearn: 53.3326395\ttotal: 2m 46s\tremaining: 37s\n",
            "762:\tlearn: 53.3225198\ttotal: 2m 46s\tremaining: 36.8s\n",
            "763:\tlearn: 53.3080015\ttotal: 2m 46s\tremaining: 36.6s\n",
            "764:\tlearn: 53.2756457\ttotal: 2m 46s\tremaining: 36.4s\n",
            "765:\tlearn: 53.2683729\ttotal: 2m 46s\tremaining: 36.2s\n",
            "766:\tlearn: 53.2179119\ttotal: 2m 47s\tremaining: 36s\n",
            "767:\tlearn: 53.2009146\ttotal: 2m 47s\tremaining: 35.7s\n",
            "768:\tlearn: 53.1758811\ttotal: 2m 47s\tremaining: 35.5s\n",
            "769:\tlearn: 53.1404496\ttotal: 2m 47s\tremaining: 35.3s\n",
            "770:\tlearn: 53.1009517\ttotal: 2m 48s\tremaining: 35.1s\n",
            "771:\tlearn: 53.0942135\ttotal: 2m 48s\tremaining: 34.9s\n",
            "772:\tlearn: 53.0724938\ttotal: 2m 48s\tremaining: 34.7s\n",
            "773:\tlearn: 53.0615896\ttotal: 2m 48s\tremaining: 34.4s\n",
            "774:\tlearn: 53.0558743\ttotal: 2m 48s\tremaining: 34.2s\n",
            "775:\tlearn: 53.0079992\ttotal: 2m 49s\tremaining: 34s\n",
            "776:\tlearn: 52.9543221\ttotal: 2m 49s\tremaining: 33.8s\n",
            "777:\tlearn: 52.9102053\ttotal: 2m 49s\tremaining: 33.6s\n",
            "778:\tlearn: 52.8245411\ttotal: 2m 49s\tremaining: 33.3s\n",
            "779:\tlearn: 52.6842215\ttotal: 2m 49s\tremaining: 33.1s\n",
            "780:\tlearn: 52.5799132\ttotal: 2m 50s\tremaining: 32.9s\n",
            "781:\tlearn: 52.5705254\ttotal: 2m 50s\tremaining: 32.7s\n",
            "782:\tlearn: 52.5695576\ttotal: 2m 50s\tremaining: 32.5s\n",
            "783:\tlearn: 52.5418131\ttotal: 2m 50s\tremaining: 32.3s\n",
            "784:\tlearn: 52.5082830\ttotal: 2m 51s\tremaining: 32s\n",
            "785:\tlearn: 52.4848434\ttotal: 2m 51s\tremaining: 31.8s\n",
            "786:\tlearn: 52.4432346\ttotal: 2m 51s\tremaining: 31.6s\n",
            "787:\tlearn: 52.4223724\ttotal: 2m 51s\tremaining: 31.4s\n",
            "788:\tlearn: 52.4043256\ttotal: 2m 51s\tremaining: 31.2s\n",
            "789:\tlearn: 52.3885746\ttotal: 2m 52s\tremaining: 31s\n",
            "790:\tlearn: 52.3661834\ttotal: 2m 52s\tremaining: 30.7s\n",
            "791:\tlearn: 52.3379560\ttotal: 2m 52s\tremaining: 30.5s\n",
            "792:\tlearn: 52.3245896\ttotal: 2m 52s\tremaining: 30.3s\n",
            "793:\tlearn: 52.3224350\ttotal: 2m 53s\tremaining: 30.1s\n",
            "794:\tlearn: 52.3210847\ttotal: 2m 53s\tremaining: 29.9s\n",
            "795:\tlearn: 52.3100762\ttotal: 2m 53s\tremaining: 29.7s\n",
            "796:\tlearn: 52.3034582\ttotal: 2m 53s\tremaining: 29.4s\n",
            "797:\tlearn: 52.2799898\ttotal: 2m 54s\tremaining: 29.2s\n",
            "798:\tlearn: 52.2538154\ttotal: 2m 54s\tremaining: 29s\n",
            "799:\tlearn: 52.2304424\ttotal: 2m 54s\tremaining: 28.8s\n",
            "800:\tlearn: 52.1740206\ttotal: 2m 54s\tremaining: 28.6s\n",
            "801:\tlearn: 52.1538388\ttotal: 2m 55s\tremaining: 28.4s\n",
            "802:\tlearn: 52.1485124\ttotal: 2m 55s\tremaining: 28.2s\n",
            "803:\tlearn: 52.1347811\ttotal: 2m 55s\tremaining: 27.9s\n",
            "804:\tlearn: 52.1336244\ttotal: 2m 55s\tremaining: 27.7s\n",
            "805:\tlearn: 52.1134705\ttotal: 2m 55s\tremaining: 27.5s\n",
            "806:\tlearn: 51.9966015\ttotal: 2m 56s\tremaining: 27.3s\n",
            "807:\tlearn: 51.9557456\ttotal: 2m 56s\tremaining: 27.1s\n",
            "808:\tlearn: 51.9089187\ttotal: 2m 56s\tremaining: 26.9s\n",
            "809:\tlearn: 51.8840473\ttotal: 2m 56s\tremaining: 26.6s\n",
            "810:\tlearn: 51.8580034\ttotal: 2m 57s\tremaining: 26.4s\n",
            "811:\tlearn: 51.8190767\ttotal: 2m 57s\tremaining: 26.2s\n",
            "812:\tlearn: 51.7736609\ttotal: 2m 57s\tremaining: 26s\n",
            "813:\tlearn: 51.7184128\ttotal: 2m 57s\tremaining: 25.8s\n",
            "814:\tlearn: 51.6876969\ttotal: 2m 58s\tremaining: 25.6s\n",
            "815:\tlearn: 51.6512873\ttotal: 2m 58s\tremaining: 25.3s\n",
            "816:\tlearn: 51.6292139\ttotal: 2m 58s\tremaining: 25.1s\n",
            "817:\tlearn: 51.6099801\ttotal: 2m 58s\tremaining: 24.9s\n",
            "818:\tlearn: 51.5666951\ttotal: 2m 58s\tremaining: 24.7s\n",
            "819:\tlearn: 51.5390370\ttotal: 2m 59s\tremaining: 24.5s\n",
            "820:\tlearn: 51.4600126\ttotal: 2m 59s\tremaining: 24.2s\n",
            "821:\tlearn: 51.4177782\ttotal: 2m 59s\tremaining: 24s\n",
            "822:\tlearn: 51.4122052\ttotal: 2m 59s\tremaining: 23.8s\n",
            "823:\tlearn: 51.4037393\ttotal: 3m\tremaining: 23.6s\n",
            "824:\tlearn: 51.4017823\ttotal: 3m\tremaining: 23.4s\n",
            "825:\tlearn: 51.3346472\ttotal: 3m\tremaining: 23.2s\n",
            "826:\tlearn: 51.3285956\ttotal: 3m\tremaining: 23s\n",
            "827:\tlearn: 51.3059340\ttotal: 3m 1s\tremaining: 22.7s\n",
            "828:\tlearn: 51.2710063\ttotal: 3m 1s\tremaining: 22.5s\n",
            "829:\tlearn: 51.1985541\ttotal: 3m 1s\tremaining: 22.3s\n",
            "830:\tlearn: 51.1448561\ttotal: 3m 1s\tremaining: 22.1s\n",
            "831:\tlearn: 51.1218374\ttotal: 3m 1s\tremaining: 21.9s\n",
            "832:\tlearn: 51.0907638\ttotal: 3m 2s\tremaining: 21.6s\n",
            "833:\tlearn: 51.0696444\ttotal: 3m 2s\tremaining: 21.4s\n",
            "834:\tlearn: 51.0499697\ttotal: 3m 2s\tremaining: 21.2s\n",
            "835:\tlearn: 50.9858347\ttotal: 3m 2s\tremaining: 21s\n",
            "836:\tlearn: 50.9724967\ttotal: 3m 3s\tremaining: 20.8s\n",
            "837:\tlearn: 50.9548890\ttotal: 3m 3s\tremaining: 20.6s\n",
            "838:\tlearn: 50.9482190\ttotal: 3m 3s\tremaining: 20.3s\n",
            "839:\tlearn: 50.9071999\ttotal: 3m 3s\tremaining: 20.1s\n",
            "840:\tlearn: 50.8652288\ttotal: 3m 3s\tremaining: 19.9s\n",
            "841:\tlearn: 50.7927226\ttotal: 3m 4s\tremaining: 19.7s\n",
            "842:\tlearn: 50.7495528\ttotal: 3m 4s\tremaining: 19.5s\n",
            "843:\tlearn: 50.7075009\ttotal: 3m 4s\tremaining: 19.2s\n",
            "844:\tlearn: 50.6850775\ttotal: 3m 4s\tremaining: 19s\n",
            "845:\tlearn: 50.6615955\ttotal: 3m 5s\tremaining: 18.8s\n",
            "846:\tlearn: 50.6470180\ttotal: 3m 5s\tremaining: 18.6s\n",
            "847:\tlearn: 50.6333499\ttotal: 3m 5s\tremaining: 18.4s\n",
            "848:\tlearn: 50.5953569\ttotal: 3m 5s\tremaining: 18.1s\n",
            "849:\tlearn: 50.5684944\ttotal: 3m 5s\tremaining: 17.9s\n",
            "850:\tlearn: 50.5277361\ttotal: 3m 6s\tremaining: 17.7s\n",
            "851:\tlearn: 50.5254640\ttotal: 3m 6s\tremaining: 17.5s\n",
            "852:\tlearn: 50.4908964\ttotal: 3m 6s\tremaining: 17.3s\n",
            "853:\tlearn: 50.4756673\ttotal: 3m 6s\tremaining: 17.1s\n",
            "854:\tlearn: 50.4520673\ttotal: 3m 6s\tremaining: 16.8s\n",
            "855:\tlearn: 50.4388237\ttotal: 3m 7s\tremaining: 16.6s\n",
            "856:\tlearn: 50.4073329\ttotal: 3m 7s\tremaining: 16.4s\n",
            "857:\tlearn: 50.3899771\ttotal: 3m 7s\tremaining: 16.2s\n",
            "858:\tlearn: 50.3736887\ttotal: 3m 7s\tremaining: 16s\n",
            "859:\tlearn: 50.3417812\ttotal: 3m 7s\tremaining: 15.7s\n",
            "860:\tlearn: 50.3038627\ttotal: 3m 8s\tremaining: 15.5s\n",
            "861:\tlearn: 50.2744922\ttotal: 3m 8s\tremaining: 15.3s\n",
            "862:\tlearn: 50.2508855\ttotal: 3m 8s\tremaining: 15.1s\n",
            "863:\tlearn: 50.2373622\ttotal: 3m 8s\tremaining: 14.9s\n",
            "864:\tlearn: 50.1887450\ttotal: 3m 9s\tremaining: 14.6s\n",
            "865:\tlearn: 50.1520373\ttotal: 3m 9s\tremaining: 14.4s\n",
            "866:\tlearn: 50.1281811\ttotal: 3m 9s\tremaining: 14.2s\n",
            "867:\tlearn: 50.1098972\ttotal: 3m 9s\tremaining: 14s\n",
            "868:\tlearn: 50.0824246\ttotal: 3m 9s\tremaining: 13.8s\n",
            "869:\tlearn: 50.0401428\ttotal: 3m 10s\tremaining: 13.6s\n",
            "870:\tlearn: 50.0092363\ttotal: 3m 10s\tremaining: 13.3s\n",
            "871:\tlearn: 49.9907545\ttotal: 3m 10s\tremaining: 13.1s\n",
            "872:\tlearn: 49.9722736\ttotal: 3m 10s\tremaining: 12.9s\n",
            "873:\tlearn: 49.9499856\ttotal: 3m 11s\tremaining: 12.7s\n",
            "874:\tlearn: 49.9318008\ttotal: 3m 11s\tremaining: 12.5s\n",
            "875:\tlearn: 49.9182872\ttotal: 3m 11s\tremaining: 12.3s\n",
            "876:\tlearn: 49.8639171\ttotal: 3m 11s\tremaining: 12s\n",
            "877:\tlearn: 49.8383332\ttotal: 3m 12s\tremaining: 11.8s\n",
            "878:\tlearn: 49.7968776\ttotal: 3m 12s\tremaining: 11.6s\n",
            "879:\tlearn: 49.7881831\ttotal: 3m 12s\tremaining: 11.4s\n",
            "880:\tlearn: 49.7754126\ttotal: 3m 12s\tremaining: 11.2s\n",
            "881:\tlearn: 49.7469674\ttotal: 3m 13s\tremaining: 10.9s\n",
            "882:\tlearn: 49.7249570\ttotal: 3m 13s\tremaining: 10.7s\n",
            "883:\tlearn: 49.6699821\ttotal: 3m 13s\tremaining: 10.5s\n",
            "884:\tlearn: 49.6662080\ttotal: 3m 13s\tremaining: 10.3s\n",
            "885:\tlearn: 49.6586839\ttotal: 3m 13s\tremaining: 10.1s\n",
            "886:\tlearn: 49.6103797\ttotal: 3m 14s\tremaining: 9.85s\n",
            "887:\tlearn: 49.5586869\ttotal: 3m 14s\tremaining: 9.63s\n",
            "888:\tlearn: 49.5475709\ttotal: 3m 14s\tremaining: 9.41s\n",
            "889:\tlearn: 49.5128277\ttotal: 3m 14s\tremaining: 9.2s\n",
            "890:\tlearn: 49.4766292\ttotal: 3m 15s\tremaining: 8.97s\n",
            "891:\tlearn: 49.4423452\ttotal: 3m 15s\tremaining: 8.76s\n",
            "892:\tlearn: 49.4037337\ttotal: 3m 15s\tremaining: 8.54s\n",
            "893:\tlearn: 49.3582957\ttotal: 3m 15s\tremaining: 8.32s\n",
            "894:\tlearn: 49.3435304\ttotal: 3m 15s\tremaining: 8.1s\n",
            "895:\tlearn: 49.2953072\ttotal: 3m 16s\tremaining: 7.88s\n",
            "896:\tlearn: 49.2761633\ttotal: 3m 16s\tremaining: 7.66s\n",
            "897:\tlearn: 49.2580392\ttotal: 3m 16s\tremaining: 7.44s\n",
            "898:\tlearn: 49.2569018\ttotal: 3m 16s\tremaining: 7.22s\n",
            "899:\tlearn: 49.2336061\ttotal: 3m 16s\tremaining: 7s\n",
            "900:\tlearn: 49.2239901\ttotal: 3m 17s\tremaining: 6.79s\n",
            "901:\tlearn: 49.2078836\ttotal: 3m 17s\tremaining: 6.57s\n",
            "902:\tlearn: 49.1885911\ttotal: 3m 17s\tremaining: 6.35s\n",
            "903:\tlearn: 49.1872995\ttotal: 3m 17s\tremaining: 6.13s\n",
            "904:\tlearn: 49.1734414\ttotal: 3m 18s\tremaining: 5.91s\n",
            "905:\tlearn: 49.1315979\ttotal: 3m 18s\tremaining: 5.69s\n",
            "906:\tlearn: 49.1069085\ttotal: 3m 18s\tremaining: 5.47s\n",
            "907:\tlearn: 49.0867514\ttotal: 3m 18s\tremaining: 5.25s\n",
            "908:\tlearn: 49.0708619\ttotal: 3m 19s\tremaining: 5.04s\n",
            "909:\tlearn: 49.0650517\ttotal: 3m 19s\tremaining: 4.82s\n",
            "910:\tlearn: 49.0485113\ttotal: 3m 19s\tremaining: 4.6s\n",
            "911:\tlearn: 49.0019510\ttotal: 3m 19s\tremaining: 4.38s\n",
            "912:\tlearn: 48.9941730\ttotal: 3m 19s\tremaining: 4.16s\n",
            "913:\tlearn: 48.9822128\ttotal: 3m 20s\tremaining: 3.94s\n",
            "914:\tlearn: 48.9595473\ttotal: 3m 20s\tremaining: 3.72s\n",
            "915:\tlearn: 48.9442200\ttotal: 3m 20s\tremaining: 3.5s\n",
            "916:\tlearn: 48.9228853\ttotal: 3m 20s\tremaining: 3.29s\n",
            "917:\tlearn: 48.9033323\ttotal: 3m 21s\tremaining: 3.07s\n",
            "918:\tlearn: 48.8927069\ttotal: 3m 21s\tremaining: 2.85s\n",
            "919:\tlearn: 48.8849174\ttotal: 3m 21s\tremaining: 2.63s\n",
            "920:\tlearn: 48.8594377\ttotal: 3m 21s\tremaining: 2.41s\n",
            "921:\tlearn: 48.8498763\ttotal: 3m 21s\tremaining: 2.19s\n",
            "922:\tlearn: 48.8400840\ttotal: 3m 22s\tremaining: 1.97s\n",
            "923:\tlearn: 48.8271164\ttotal: 3m 22s\tremaining: 1.75s\n",
            "924:\tlearn: 48.8143398\ttotal: 3m 22s\tremaining: 1.53s\n",
            "925:\tlearn: 48.7967897\ttotal: 3m 22s\tremaining: 1.31s\n",
            "926:\tlearn: 48.7766245\ttotal: 3m 23s\tremaining: 1.09s\n",
            "927:\tlearn: 48.7693570\ttotal: 3m 23s\tremaining: 877ms\n",
            "928:\tlearn: 48.7632743\ttotal: 3m 23s\tremaining: 657ms\n",
            "929:\tlearn: 48.7630320\ttotal: 3m 23s\tremaining: 438ms\n",
            "930:\tlearn: 48.7603310\ttotal: 3m 23s\tremaining: 219ms\n",
            "931:\tlearn: 48.7442026\ttotal: 3m 24s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 08:18:26,136] A new study created in memory with name: no-name-b8f3d910-b69d-4f59-9feb-6a22ff404995\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 08:29:57,152] Trial 0 finished with value: 59.38910744661363 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 59.38910744661363.\n",
            "[I 2023-10-08 08:33:52,556] Trial 1 finished with value: 39.92216668967806 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 08:40:29,677] Trial 2 finished with value: 43.50426376336511 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 08:45:07,491] Trial 3 finished with value: 42.84497278837277 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 08:49:07,502] Trial 4 finished with value: 42.01023463560461 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 08:55:47,878] Trial 5 finished with value: 75.31109728754012 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 08:59:14,814] Trial 6 finished with value: 44.90765387325468 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:21:04,793] Trial 7 finished with value: 57.22833127432207 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:23:05,809] Trial 8 finished with value: 57.09785239389797 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:26:37,580] Trial 9 finished with value: 47.514347441187645 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:32:36,127] Trial 10 finished with value: 49.56535400932865 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:36:38,321] Trial 11 finished with value: 41.16944586612322 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:39:57,648] Trial 12 finished with value: 44.95049444357092 and parameters: {'iterations': 743, 'learning_rate': 0.7356346127088652, 'depth': 9, 'min_data_in_leaf': 17, 'reg_lambda': 49.16423147603391, 'subsample': 0.5854290622440648, 'random_strength': 31.588734753749215, 'od_wait': 122, 'leaf_estimation_iterations': 4, 'bagging_temperature': 23.993242751388063, 'colsample_bylevel': 0.3642319611435846}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:42:57,347] Trial 13 finished with value: 50.48753384609501 and parameters: {'iterations': 757, 'learning_rate': 0.7501800009545696, 'depth': 11, 'min_data_in_leaf': 13, 'reg_lambda': 30.221360663173336, 'subsample': 0.4774950955421056, 'random_strength': 30.328584321736876, 'od_wait': 18, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.6071397112449649}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:47:18,400] Trial 14 finished with value: 40.77873955180915 and parameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 09:51:01,216] Trial 15 finished with value: 41.17311249454637 and parameters: {'iterations': 951, 'learning_rate': 0.378680852488155, 'depth': 7, 'min_data_in_leaf': 23, 'reg_lambda': 39.94535569998841, 'subsample': 0.6610237845565569, 'random_strength': 20.618074524594604, 'od_wait': 150, 'leaf_estimation_iterations': 5, 'bagging_temperature': 19.000173884005733, 'colsample_bylevel': 0.5429293784058689}. Best is trial 1 with value: 39.92216668967806.\n",
            "[I 2023-10-08 10:00:54,724] Trial 16 finished with value: 39.88297935806743 and parameters: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}. Best is trial 16 with value: 39.88297935806743.\n",
            "[I 2023-10-08 10:04:46,579] Trial 17 finished with value: 65.36293558014182 and parameters: {'iterations': 305, 'learning_rate': 0.10410309839558177, 'depth': 12, 'min_data_in_leaf': 22, 'reg_lambda': 62.93129990724812, 'subsample': 0.7426081894256986, 'random_strength': 44.362030019540654, 'od_wait': 125, 'leaf_estimation_iterations': 9, 'bagging_temperature': 13.937831218004703, 'colsample_bylevel': 0.9486708135061371}. Best is trial 16 with value: 39.88297935806743.\n",
            "[I 2023-10-08 10:09:50,006] Trial 18 finished with value: 46.29858795045331 and parameters: {'iterations': 796, 'learning_rate': 0.13599792999265126, 'depth': 11, 'min_data_in_leaf': 24, 'reg_lambda': 71.62634725545307, 'subsample': 0.44325233558749344, 'random_strength': 20.973759752682703, 'od_wait': 100, 'leaf_estimation_iterations': 6, 'bagging_temperature': 5.065329413975176, 'colsample_bylevel': 0.32674260541968214}. Best is trial 16 with value: 39.88297935806743.\n",
            "[I 2023-10-08 10:33:54,135] Trial 19 finished with value: 48.89308068253256 and parameters: {'iterations': 919, 'learning_rate': 0.36343405833274767, 'depth': 13, 'min_data_in_leaf': 30, 'reg_lambda': 40.746702935917256, 'subsample': 0.5980837495644987, 'random_strength': 42.94431966534945, 'od_wait': 98, 'leaf_estimation_iterations': 16, 'bagging_temperature': 42.15873557931633, 'colsample_bylevel': 0.8019932574851661}. Best is trial 16 with value: 39.88297935806743.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 3: 39.88297935806743\n",
            "Best hyperparameters for fold 3: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}\n",
            "0:\tlearn: 208.4511454\ttotal: 152ms\tremaining: 2m 26s\n",
            "1:\tlearn: 206.5374784\ttotal: 706ms\tremaining: 5m 38s\n",
            "2:\tlearn: 205.2874674\ttotal: 1.31s\tremaining: 6m 56s\n",
            "3:\tlearn: 203.9878829\ttotal: 1.81s\tremaining: 7m 12s\n",
            "4:\tlearn: 203.7116450\ttotal: 1.99s\tremaining: 6m 20s\n",
            "5:\tlearn: 203.6136096\ttotal: 2.12s\tremaining: 5m 37s\n",
            "6:\tlearn: 202.1049483\ttotal: 2.65s\tremaining: 6m 1s\n",
            "7:\tlearn: 201.3970486\ttotal: 3.21s\tremaining: 6m 22s\n",
            "8:\tlearn: 200.8669624\ttotal: 3.83s\tremaining: 6m 44s\n",
            "9:\tlearn: 200.4949168\ttotal: 4.42s\tremaining: 6m 59s\n",
            "10:\tlearn: 200.2756577\ttotal: 4.74s\tremaining: 6m 48s\n",
            "11:\tlearn: 199.6685068\ttotal: 5.42s\tremaining: 7m 7s\n",
            "12:\tlearn: 199.5534157\ttotal: 5.72s\tremaining: 6m 56s\n",
            "13:\tlearn: 199.4857409\ttotal: 5.96s\tremaining: 6m 42s\n",
            "14:\tlearn: 199.1007566\ttotal: 6.61s\tremaining: 6m 56s\n",
            "15:\tlearn: 199.1007565\ttotal: 6.68s\tremaining: 6m 33s\n",
            "16:\tlearn: 198.6664019\ttotal: 7.3s\tremaining: 6m 44s\n",
            "17:\tlearn: 198.3437666\ttotal: 7.84s\tremaining: 6m 49s\n",
            "18:\tlearn: 198.2337619\ttotal: 8.03s\tremaining: 6m 37s\n",
            "19:\tlearn: 198.1729600\ttotal: 8.56s\tremaining: 6m 41s\n",
            "20:\tlearn: 197.7188287\ttotal: 9.13s\tremaining: 6m 47s\n",
            "21:\tlearn: 197.5265075\ttotal: 9.67s\tremaining: 6m 51s\n",
            "22:\tlearn: 197.5265073\ttotal: 9.73s\tremaining: 6m 35s\n",
            "23:\tlearn: 197.3879398\ttotal: 10.3s\tremaining: 6m 42s\n",
            "24:\tlearn: 197.3367910\ttotal: 10.4s\tremaining: 6m 30s\n",
            "25:\tlearn: 197.2286106\ttotal: 11.1s\tremaining: 6m 37s\n",
            "26:\tlearn: 197.1355191\ttotal: 11.7s\tremaining: 6m 44s\n",
            "27:\tlearn: 196.9057977\ttotal: 12.3s\tremaining: 6m 49s\n",
            "28:\tlearn: 196.7779853\ttotal: 13s\tremaining: 6m 55s\n",
            "29:\tlearn: 196.6950017\ttotal: 13.4s\tremaining: 6m 54s\n",
            "30:\tlearn: 196.3479105\ttotal: 13.9s\tremaining: 6m 56s\n",
            "31:\tlearn: 196.1877667\ttotal: 14.3s\tremaining: 6m 55s\n",
            "32:\tlearn: 195.9231510\ttotal: 14.7s\tremaining: 6m 52s\n",
            "33:\tlearn: 195.9159860\ttotal: 14.8s\tremaining: 6m 42s\n",
            "34:\tlearn: 195.9155189\ttotal: 14.9s\tremaining: 6m 32s\n",
            "35:\tlearn: 195.3292947\ttotal: 15.5s\tremaining: 6m 36s\n",
            "36:\tlearn: 195.3292917\ttotal: 15.5s\tremaining: 6m 27s\n",
            "37:\tlearn: 195.2224430\ttotal: 16.1s\tremaining: 6m 30s\n",
            "38:\tlearn: 195.1414206\ttotal: 16.8s\tremaining: 6m 36s\n",
            "39:\tlearn: 194.7810186\ttotal: 17.4s\tremaining: 6m 39s\n",
            "40:\tlearn: 194.7810159\ttotal: 17.4s\tremaining: 6m 30s\n",
            "41:\tlearn: 194.4206425\ttotal: 18.1s\tremaining: 6m 35s\n",
            "42:\tlearn: 194.2342634\ttotal: 18.7s\tremaining: 6m 38s\n",
            "43:\tlearn: 194.2342621\ttotal: 18.8s\tremaining: 6m 30s\n",
            "44:\tlearn: 193.9057731\ttotal: 19.4s\tremaining: 6m 34s\n",
            "45:\tlearn: 193.7127136\ttotal: 20s\tremaining: 6m 37s\n",
            "46:\tlearn: 193.3096180\ttotal: 20.6s\tremaining: 6m 39s\n",
            "47:\tlearn: 193.3045756\ttotal: 20.8s\tremaining: 6m 34s\n",
            "48:\tlearn: 193.0805976\ttotal: 21.4s\tremaining: 6m 36s\n",
            "49:\tlearn: 193.0691632\ttotal: 21.7s\tremaining: 6m 34s\n",
            "50:\tlearn: 192.9849054\ttotal: 22s\tremaining: 6m 30s\n",
            "51:\tlearn: 192.9837726\ttotal: 22.1s\tremaining: 6m 25s\n",
            "52:\tlearn: 192.7081606\ttotal: 22.7s\tremaining: 6m 27s\n",
            "53:\tlearn: 192.7075694\ttotal: 22.8s\tremaining: 6m 22s\n",
            "54:\tlearn: 192.7044021\ttotal: 23s\tremaining: 6m 17s\n",
            "55:\tlearn: 192.6834065\ttotal: 23s\tremaining: 6m 11s\n",
            "56:\tlearn: 191.9212379\ttotal: 23.6s\tremaining: 6m 12s\n",
            "57:\tlearn: 191.8705439\ttotal: 24.2s\tremaining: 6m 15s\n",
            "58:\tlearn: 191.8209629\ttotal: 24.8s\tremaining: 6m 17s\n",
            "59:\tlearn: 191.8202141\ttotal: 24.9s\tremaining: 6m 13s\n",
            "60:\tlearn: 191.7782209\ttotal: 25.4s\tremaining: 6m 13s\n",
            "61:\tlearn: 191.7564697\ttotal: 25.6s\tremaining: 6m 10s\n",
            "62:\tlearn: 191.6087613\ttotal: 25.7s\tremaining: 6m 6s\n",
            "63:\tlearn: 191.6077261\ttotal: 25.9s\tremaining: 6m 1s\n",
            "64:\tlearn: 191.6077261\ttotal: 25.9s\tremaining: 5m 56s\n",
            "65:\tlearn: 191.4107251\ttotal: 26.5s\tremaining: 5m 58s\n",
            "66:\tlearn: 191.2123041\ttotal: 27.1s\tremaining: 6m\n",
            "67:\tlearn: 191.1766243\ttotal: 27.4s\tremaining: 5m 59s\n",
            "68:\tlearn: 191.1710929\ttotal: 27.7s\tremaining: 5m 57s\n",
            "69:\tlearn: 191.1147916\ttotal: 28.2s\tremaining: 5m 58s\n",
            "70:\tlearn: 190.7463658\ttotal: 28.9s\tremaining: 6m\n",
            "71:\tlearn: 190.6508084\ttotal: 29.4s\tremaining: 6m 2s\n",
            "72:\tlearn: 190.6456346\ttotal: 29.6s\tremaining: 5m 58s\n",
            "73:\tlearn: 190.6386568\ttotal: 29.8s\tremaining: 5m 56s\n",
            "74:\tlearn: 190.5307439\ttotal: 30.4s\tremaining: 5m 58s\n",
            "75:\tlearn: 190.5021438\ttotal: 30.9s\tremaining: 5m 59s\n",
            "76:\tlearn: 190.4920652\ttotal: 31.3s\tremaining: 5m 58s\n",
            "77:\tlearn: 190.4399461\ttotal: 32s\tremaining: 6m 1s\n",
            "78:\tlearn: 190.3735464\ttotal: 32.6s\tremaining: 6m 3s\n",
            "79:\tlearn: 190.0614365\ttotal: 33.2s\tremaining: 6m 4s\n",
            "80:\tlearn: 189.9486915\ttotal: 33.8s\tremaining: 6m 6s\n",
            "81:\tlearn: 189.7668888\ttotal: 34.4s\tremaining: 6m 7s\n",
            "82:\tlearn: 189.6430848\ttotal: 35s\tremaining: 6m 9s\n",
            "83:\tlearn: 189.4076726\ttotal: 35.6s\tremaining: 6m 10s\n",
            "84:\tlearn: 189.3449257\ttotal: 35.9s\tremaining: 6m 8s\n",
            "85:\tlearn: 189.1938741\ttotal: 36.1s\tremaining: 6m 6s\n",
            "86:\tlearn: 189.0641346\ttotal: 36.4s\tremaining: 6m 5s\n",
            "87:\tlearn: 189.0562869\ttotal: 36.7s\tremaining: 6m 3s\n",
            "88:\tlearn: 188.9085727\ttotal: 37.3s\tremaining: 6m 4s\n",
            "89:\tlearn: 188.6738971\ttotal: 37.8s\tremaining: 6m 4s\n",
            "90:\tlearn: 188.5038413\ttotal: 38s\tremaining: 6m 2s\n",
            "91:\tlearn: 187.9203320\ttotal: 38.5s\tremaining: 6m 2s\n",
            "92:\tlearn: 187.0452832\ttotal: 39s\tremaining: 6m 3s\n",
            "93:\tlearn: 186.7047326\ttotal: 39.2s\tremaining: 6m\n",
            "94:\tlearn: 186.4123803\ttotal: 39.7s\tremaining: 6m 1s\n",
            "95:\tlearn: 185.1923645\ttotal: 40.1s\tremaining: 6m\n",
            "96:\tlearn: 184.8397555\ttotal: 40.6s\tremaining: 6m 1s\n",
            "97:\tlearn: 184.3084741\ttotal: 41.2s\tremaining: 6m 1s\n",
            "98:\tlearn: 183.9353564\ttotal: 41.8s\tremaining: 6m 2s\n",
            "99:\tlearn: 182.9792050\ttotal: 42.3s\tremaining: 6m 3s\n",
            "100:\tlearn: 181.2341383\ttotal: 42.8s\tremaining: 6m 3s\n",
            "101:\tlearn: 180.8865965\ttotal: 43.3s\tremaining: 6m 4s\n",
            "102:\tlearn: 180.1432206\ttotal: 43.8s\tremaining: 6m 4s\n",
            "103:\tlearn: 177.5683054\ttotal: 44.3s\tremaining: 6m 4s\n",
            "104:\tlearn: 177.0626327\ttotal: 44.8s\tremaining: 6m 4s\n",
            "105:\tlearn: 175.7546340\ttotal: 45.4s\tremaining: 6m 4s\n",
            "106:\tlearn: 175.4173001\ttotal: 46s\tremaining: 6m 5s\n",
            "107:\tlearn: 174.2250424\ttotal: 46.4s\tremaining: 6m 5s\n",
            "108:\tlearn: 173.9354335\ttotal: 47s\tremaining: 6m 6s\n",
            "109:\tlearn: 173.5927421\ttotal: 47.5s\tremaining: 6m 6s\n",
            "110:\tlearn: 172.6186160\ttotal: 48.1s\tremaining: 6m 7s\n",
            "111:\tlearn: 172.1304748\ttotal: 48.6s\tremaining: 6m 7s\n",
            "112:\tlearn: 170.6697277\ttotal: 49.1s\tremaining: 6m 7s\n",
            "113:\tlearn: 168.9580542\ttotal: 49.7s\tremaining: 6m 8s\n",
            "114:\tlearn: 168.7516705\ttotal: 50.2s\tremaining: 6m 8s\n",
            "115:\tlearn: 168.2180163\ttotal: 50.8s\tremaining: 6m 9s\n",
            "116:\tlearn: 167.0451113\ttotal: 51.3s\tremaining: 6m 9s\n",
            "117:\tlearn: 166.0184915\ttotal: 51.8s\tremaining: 6m 9s\n",
            "118:\tlearn: 165.0257851\ttotal: 52.3s\tremaining: 6m 9s\n",
            "119:\tlearn: 163.8511198\ttotal: 52.8s\tremaining: 6m 9s\n",
            "120:\tlearn: 161.3942495\ttotal: 53.3s\tremaining: 6m 9s\n",
            "121:\tlearn: 161.0072544\ttotal: 53.8s\tremaining: 6m 9s\n",
            "122:\tlearn: 159.7232932\ttotal: 54.3s\tremaining: 6m 8s\n",
            "123:\tlearn: 159.1187191\ttotal: 54.7s\tremaining: 6m 8s\n",
            "124:\tlearn: 158.7982590\ttotal: 55.3s\tremaining: 6m 8s\n",
            "125:\tlearn: 157.7734831\ttotal: 55.7s\tremaining: 6m 8s\n",
            "126:\tlearn: 156.5041830\ttotal: 56.2s\tremaining: 6m 8s\n",
            "127:\tlearn: 156.0999110\ttotal: 56.8s\tremaining: 6m 8s\n",
            "128:\tlearn: 155.3232883\ttotal: 57.3s\tremaining: 6m 8s\n",
            "129:\tlearn: 155.0946424\ttotal: 57.9s\tremaining: 6m 8s\n",
            "130:\tlearn: 154.7616796\ttotal: 58.4s\tremaining: 6m 9s\n",
            "131:\tlearn: 153.9488779\ttotal: 58.9s\tremaining: 6m 8s\n",
            "132:\tlearn: 153.4131042\ttotal: 59.4s\tremaining: 6m 8s\n",
            "133:\tlearn: 152.1684214\ttotal: 59.9s\tremaining: 6m 8s\n",
            "134:\tlearn: 151.2465368\ttotal: 1m\tremaining: 6m 8s\n",
            "135:\tlearn: 150.9776326\ttotal: 1m\tremaining: 6m 8s\n",
            "136:\tlearn: 150.6666369\ttotal: 1m 1s\tremaining: 6m 8s\n",
            "137:\tlearn: 149.2935129\ttotal: 1m 1s\tremaining: 6m 8s\n",
            "138:\tlearn: 149.0613504\ttotal: 1m 2s\tremaining: 6m 8s\n",
            "139:\tlearn: 148.6065407\ttotal: 1m 3s\tremaining: 6m 8s\n",
            "140:\tlearn: 147.9727693\ttotal: 1m 3s\tremaining: 6m 8s\n",
            "141:\tlearn: 147.7347105\ttotal: 1m 4s\tremaining: 6m 9s\n",
            "142:\tlearn: 147.4439626\ttotal: 1m 4s\tremaining: 6m 9s\n",
            "143:\tlearn: 147.3165724\ttotal: 1m 5s\tremaining: 6m 10s\n",
            "144:\tlearn: 146.3705262\ttotal: 1m 6s\tremaining: 6m 11s\n",
            "145:\tlearn: 146.0226869\ttotal: 1m 6s\tremaining: 6m 11s\n",
            "146:\tlearn: 145.2447708\ttotal: 1m 7s\tremaining: 6m 11s\n",
            "147:\tlearn: 144.9755271\ttotal: 1m 7s\tremaining: 6m 11s\n",
            "148:\tlearn: 144.2830902\ttotal: 1m 8s\tremaining: 6m 11s\n",
            "149:\tlearn: 144.0884484\ttotal: 1m 8s\tremaining: 6m 11s\n",
            "150:\tlearn: 143.4087637\ttotal: 1m 9s\tremaining: 6m 10s\n",
            "151:\tlearn: 142.9211706\ttotal: 1m 9s\tremaining: 6m 10s\n",
            "152:\tlearn: 142.6017920\ttotal: 1m 10s\tremaining: 6m 11s\n",
            "153:\tlearn: 142.3093091\ttotal: 1m 11s\tremaining: 6m 11s\n",
            "154:\tlearn: 142.1359755\ttotal: 1m 11s\tremaining: 6m 11s\n",
            "155:\tlearn: 140.9188635\ttotal: 1m 12s\tremaining: 6m 11s\n",
            "156:\tlearn: 140.4611577\ttotal: 1m 12s\tremaining: 6m 11s\n",
            "157:\tlearn: 139.6761634\ttotal: 1m 13s\tremaining: 6m 10s\n",
            "158:\tlearn: 139.3343960\ttotal: 1m 13s\tremaining: 6m 10s\n",
            "159:\tlearn: 138.7682739\ttotal: 1m 14s\tremaining: 6m 10s\n",
            "160:\tlearn: 137.9504660\ttotal: 1m 14s\tremaining: 6m 10s\n",
            "161:\tlearn: 137.6828077\ttotal: 1m 15s\tremaining: 6m 10s\n",
            "162:\tlearn: 137.3131896\ttotal: 1m 15s\tremaining: 6m 10s\n",
            "163:\tlearn: 137.0231033\ttotal: 1m 16s\tremaining: 6m 9s\n",
            "164:\tlearn: 136.7369627\ttotal: 1m 16s\tremaining: 6m 9s\n",
            "165:\tlearn: 136.1309326\ttotal: 1m 17s\tremaining: 6m 9s\n",
            "166:\tlearn: 135.4317319\ttotal: 1m 17s\tremaining: 6m 8s\n",
            "167:\tlearn: 135.2968495\ttotal: 1m 18s\tremaining: 6m 8s\n",
            "168:\tlearn: 135.1211785\ttotal: 1m 18s\tremaining: 6m 8s\n",
            "169:\tlearn: 135.0528071\ttotal: 1m 19s\tremaining: 6m 8s\n",
            "170:\tlearn: 134.1980366\ttotal: 1m 19s\tremaining: 6m 8s\n",
            "171:\tlearn: 134.1585130\ttotal: 1m 20s\tremaining: 6m 8s\n",
            "172:\tlearn: 133.7217288\ttotal: 1m 21s\tremaining: 6m 8s\n",
            "173:\tlearn: 133.5087623\ttotal: 1m 21s\tremaining: 6m 8s\n",
            "174:\tlearn: 132.8120846\ttotal: 1m 22s\tremaining: 6m 8s\n",
            "175:\tlearn: 132.5945780\ttotal: 1m 22s\tremaining: 6m 8s\n",
            "176:\tlearn: 132.3746808\ttotal: 1m 23s\tremaining: 6m 8s\n",
            "177:\tlearn: 131.1914365\ttotal: 1m 24s\tremaining: 6m 8s\n",
            "178:\tlearn: 130.8015332\ttotal: 1m 24s\tremaining: 6m 8s\n",
            "179:\tlearn: 130.2568977\ttotal: 1m 25s\tremaining: 6m 7s\n",
            "180:\tlearn: 130.0308869\ttotal: 1m 25s\tremaining: 6m 7s\n",
            "181:\tlearn: 129.8838009\ttotal: 1m 26s\tremaining: 6m 7s\n",
            "182:\tlearn: 129.8834247\ttotal: 1m 26s\tremaining: 6m 7s\n",
            "183:\tlearn: 129.6008773\ttotal: 1m 27s\tremaining: 6m 7s\n",
            "184:\tlearn: 129.4843797\ttotal: 1m 27s\tremaining: 6m 6s\n",
            "185:\tlearn: 129.2331961\ttotal: 1m 28s\tremaining: 6m 6s\n",
            "186:\tlearn: 129.0875120\ttotal: 1m 28s\tremaining: 6m 6s\n",
            "187:\tlearn: 128.3979145\ttotal: 1m 29s\tremaining: 6m 6s\n",
            "188:\tlearn: 127.9713948\ttotal: 1m 29s\tremaining: 6m 5s\n",
            "189:\tlearn: 127.9706399\ttotal: 1m 30s\tremaining: 6m 5s\n",
            "190:\tlearn: 127.6920806\ttotal: 1m 30s\tremaining: 6m 5s\n",
            "191:\tlearn: 126.9355703\ttotal: 1m 31s\tremaining: 6m 4s\n",
            "192:\tlearn: 126.5615743\ttotal: 1m 31s\tremaining: 6m 4s\n",
            "193:\tlearn: 125.9005496\ttotal: 1m 32s\tremaining: 6m 4s\n",
            "194:\tlearn: 125.7681156\ttotal: 1m 32s\tremaining: 6m 4s\n",
            "195:\tlearn: 125.4750702\ttotal: 1m 33s\tremaining: 6m 3s\n",
            "196:\tlearn: 124.6216433\ttotal: 1m 33s\tremaining: 6m 3s\n",
            "197:\tlearn: 123.9830654\ttotal: 1m 34s\tremaining: 6m 3s\n",
            "198:\tlearn: 123.7316175\ttotal: 1m 35s\tremaining: 6m 3s\n",
            "199:\tlearn: 123.5780787\ttotal: 1m 35s\tremaining: 6m 3s\n",
            "200:\tlearn: 123.4390429\ttotal: 1m 36s\tremaining: 6m 3s\n",
            "201:\tlearn: 123.2936473\ttotal: 1m 36s\tremaining: 6m 3s\n",
            "202:\tlearn: 123.1626399\ttotal: 1m 37s\tremaining: 6m 3s\n",
            "203:\tlearn: 122.7155113\ttotal: 1m 38s\tremaining: 6m 3s\n",
            "204:\tlearn: 122.4401730\ttotal: 1m 38s\tremaining: 6m 3s\n",
            "205:\tlearn: 122.1825129\ttotal: 1m 39s\tremaining: 6m 2s\n",
            "206:\tlearn: 121.7546652\ttotal: 1m 39s\tremaining: 6m 2s\n",
            "207:\tlearn: 121.3906054\ttotal: 1m 40s\tremaining: 6m 2s\n",
            "208:\tlearn: 121.1430301\ttotal: 1m 40s\tremaining: 6m 1s\n",
            "209:\tlearn: 121.0388872\ttotal: 1m 41s\tremaining: 6m 1s\n",
            "210:\tlearn: 120.8306243\ttotal: 1m 41s\tremaining: 6m\n",
            "211:\tlearn: 120.5419804\ttotal: 1m 42s\tremaining: 6m\n",
            "212:\tlearn: 120.0213482\ttotal: 1m 42s\tremaining: 5m 59s\n",
            "213:\tlearn: 119.7140207\ttotal: 1m 43s\tremaining: 5m 59s\n",
            "214:\tlearn: 119.5751184\ttotal: 1m 43s\tremaining: 5m 59s\n",
            "215:\tlearn: 119.0416098\ttotal: 1m 44s\tremaining: 5m 58s\n",
            "216:\tlearn: 118.7529966\ttotal: 1m 44s\tremaining: 5m 58s\n",
            "217:\tlearn: 118.6882019\ttotal: 1m 45s\tremaining: 5m 58s\n",
            "218:\tlearn: 118.4709474\ttotal: 1m 45s\tremaining: 5m 57s\n",
            "219:\tlearn: 118.3801680\ttotal: 1m 46s\tremaining: 5m 57s\n",
            "220:\tlearn: 118.1526896\ttotal: 1m 46s\tremaining: 5m 57s\n",
            "221:\tlearn: 117.9439447\ttotal: 1m 47s\tremaining: 5m 56s\n",
            "222:\tlearn: 117.8664542\ttotal: 1m 48s\tremaining: 5m 56s\n",
            "223:\tlearn: 117.3222091\ttotal: 1m 48s\tremaining: 5m 56s\n",
            "224:\tlearn: 116.8578475\ttotal: 1m 49s\tremaining: 5m 55s\n",
            "225:\tlearn: 116.1922852\ttotal: 1m 49s\tremaining: 5m 55s\n",
            "226:\tlearn: 115.7916133\ttotal: 1m 50s\tremaining: 5m 54s\n",
            "227:\tlearn: 115.5632889\ttotal: 1m 50s\tremaining: 5m 54s\n",
            "228:\tlearn: 115.3973034\ttotal: 1m 51s\tremaining: 5m 54s\n",
            "229:\tlearn: 115.2152956\ttotal: 1m 51s\tremaining: 5m 53s\n",
            "230:\tlearn: 115.1057113\ttotal: 1m 52s\tremaining: 5m 54s\n",
            "231:\tlearn: 114.7558985\ttotal: 1m 52s\tremaining: 5m 53s\n",
            "232:\tlearn: 114.4559840\ttotal: 1m 53s\tremaining: 5m 53s\n",
            "233:\tlearn: 114.2895019\ttotal: 1m 53s\tremaining: 5m 53s\n",
            "234:\tlearn: 114.1504847\ttotal: 1m 54s\tremaining: 5m 53s\n",
            "235:\tlearn: 113.8187502\ttotal: 1m 55s\tremaining: 5m 52s\n",
            "236:\tlearn: 113.6775102\ttotal: 1m 55s\tremaining: 5m 52s\n",
            "237:\tlearn: 113.3782367\ttotal: 1m 56s\tremaining: 5m 52s\n",
            "238:\tlearn: 113.0206702\ttotal: 1m 56s\tremaining: 5m 51s\n",
            "239:\tlearn: 112.8937874\ttotal: 1m 57s\tremaining: 5m 51s\n",
            "240:\tlearn: 112.1251001\ttotal: 1m 57s\tremaining: 5m 51s\n",
            "241:\tlearn: 111.9288981\ttotal: 1m 58s\tremaining: 5m 51s\n",
            "242:\tlearn: 111.0603027\ttotal: 1m 59s\tremaining: 5m 50s\n",
            "243:\tlearn: 110.0982545\ttotal: 1m 59s\tremaining: 5m 50s\n",
            "244:\tlearn: 109.9346529\ttotal: 2m\tremaining: 5m 49s\n",
            "245:\tlearn: 109.3792292\ttotal: 2m\tremaining: 5m 49s\n",
            "246:\tlearn: 109.0123135\ttotal: 2m 1s\tremaining: 5m 49s\n",
            "247:\tlearn: 108.2412094\ttotal: 2m 1s\tremaining: 5m 48s\n",
            "248:\tlearn: 108.1653482\ttotal: 2m 2s\tremaining: 5m 48s\n",
            "249:\tlearn: 108.0943499\ttotal: 2m 2s\tremaining: 5m 48s\n",
            "250:\tlearn: 107.8522880\ttotal: 2m 3s\tremaining: 5m 47s\n",
            "251:\tlearn: 107.4726316\ttotal: 2m 3s\tremaining: 5m 47s\n",
            "252:\tlearn: 107.2934299\ttotal: 2m 4s\tremaining: 5m 47s\n",
            "253:\tlearn: 107.1083369\ttotal: 2m 4s\tremaining: 5m 46s\n",
            "254:\tlearn: 107.0023049\ttotal: 2m 5s\tremaining: 5m 46s\n",
            "255:\tlearn: 106.6815455\ttotal: 2m 6s\tremaining: 5m 46s\n",
            "256:\tlearn: 106.1827092\ttotal: 2m 6s\tremaining: 5m 45s\n",
            "257:\tlearn: 106.0577893\ttotal: 2m 7s\tremaining: 5m 45s\n",
            "258:\tlearn: 105.9349917\ttotal: 2m 7s\tremaining: 5m 45s\n",
            "259:\tlearn: 105.7258866\ttotal: 2m 8s\tremaining: 5m 45s\n",
            "260:\tlearn: 105.2999042\ttotal: 2m 8s\tremaining: 5m 44s\n",
            "261:\tlearn: 105.0926384\ttotal: 2m 9s\tremaining: 5m 44s\n",
            "262:\tlearn: 104.8277160\ttotal: 2m 9s\tremaining: 5m 43s\n",
            "263:\tlearn: 104.6235442\ttotal: 2m 10s\tremaining: 5m 43s\n",
            "264:\tlearn: 104.5251820\ttotal: 2m 11s\tremaining: 5m 43s\n",
            "265:\tlearn: 104.4432963\ttotal: 2m 11s\tremaining: 5m 43s\n",
            "266:\tlearn: 103.7743995\ttotal: 2m 12s\tremaining: 5m 42s\n",
            "267:\tlearn: 103.6855385\ttotal: 2m 12s\tremaining: 5m 42s\n",
            "268:\tlearn: 103.5963413\ttotal: 2m 13s\tremaining: 5m 42s\n",
            "269:\tlearn: 103.4935366\ttotal: 2m 13s\tremaining: 5m 41s\n",
            "270:\tlearn: 103.3074186\ttotal: 2m 14s\tremaining: 5m 41s\n",
            "271:\tlearn: 103.2211925\ttotal: 2m 14s\tremaining: 5m 40s\n",
            "272:\tlearn: 103.1227325\ttotal: 2m 15s\tremaining: 5m 40s\n",
            "273:\tlearn: 102.6823820\ttotal: 2m 15s\tremaining: 5m 39s\n",
            "274:\tlearn: 102.3663445\ttotal: 2m 16s\tremaining: 5m 39s\n",
            "275:\tlearn: 102.2470829\ttotal: 2m 16s\tremaining: 5m 38s\n",
            "276:\tlearn: 101.6725688\ttotal: 2m 17s\tremaining: 5m 38s\n",
            "277:\tlearn: 101.5419549\ttotal: 2m 18s\tremaining: 5m 38s\n",
            "278:\tlearn: 101.4011440\ttotal: 2m 18s\tremaining: 5m 37s\n",
            "279:\tlearn: 101.2810965\ttotal: 2m 19s\tremaining: 5m 37s\n",
            "280:\tlearn: 100.8550702\ttotal: 2m 19s\tremaining: 5m 36s\n",
            "281:\tlearn: 100.6058763\ttotal: 2m 20s\tremaining: 5m 36s\n",
            "282:\tlearn: 100.4553868\ttotal: 2m 20s\tremaining: 5m 35s\n",
            "283:\tlearn: 100.3717671\ttotal: 2m 21s\tremaining: 5m 35s\n",
            "284:\tlearn: 100.2188070\ttotal: 2m 21s\tremaining: 5m 35s\n",
            "285:\tlearn: 99.7261636\ttotal: 2m 22s\tremaining: 5m 34s\n",
            "286:\tlearn: 99.6769867\ttotal: 2m 22s\tremaining: 5m 34s\n",
            "287:\tlearn: 99.5945005\ttotal: 2m 23s\tremaining: 5m 33s\n",
            "288:\tlearn: 99.4834671\ttotal: 2m 23s\tremaining: 5m 33s\n",
            "289:\tlearn: 99.4010815\ttotal: 2m 24s\tremaining: 5m 33s\n",
            "290:\tlearn: 99.2068193\ttotal: 2m 24s\tremaining: 5m 32s\n",
            "291:\tlearn: 98.9342048\ttotal: 2m 25s\tremaining: 5m 32s\n",
            "292:\tlearn: 98.7633366\ttotal: 2m 26s\tremaining: 5m 32s\n",
            "293:\tlearn: 98.6409489\ttotal: 2m 26s\tremaining: 5m 31s\n",
            "294:\tlearn: 98.3237067\ttotal: 2m 27s\tremaining: 5m 31s\n",
            "295:\tlearn: 98.2554596\ttotal: 2m 27s\tremaining: 5m 31s\n",
            "296:\tlearn: 98.1719542\ttotal: 2m 28s\tremaining: 5m 30s\n",
            "297:\tlearn: 98.0690983\ttotal: 2m 29s\tremaining: 5m 30s\n",
            "298:\tlearn: 97.7752563\ttotal: 2m 29s\tremaining: 5m 30s\n",
            "299:\tlearn: 97.7169057\ttotal: 2m 30s\tremaining: 5m 29s\n",
            "300:\tlearn: 97.6591378\ttotal: 2m 30s\tremaining: 5m 29s\n",
            "301:\tlearn: 97.0693309\ttotal: 2m 31s\tremaining: 5m 29s\n",
            "302:\tlearn: 96.9923949\ttotal: 2m 31s\tremaining: 5m 28s\n",
            "303:\tlearn: 96.9892680\ttotal: 2m 32s\tremaining: 5m 28s\n",
            "304:\tlearn: 96.5670406\ttotal: 2m 32s\tremaining: 5m 27s\n",
            "305:\tlearn: 96.2721626\ttotal: 2m 33s\tremaining: 5m 27s\n",
            "306:\tlearn: 96.0950747\ttotal: 2m 33s\tremaining: 5m 26s\n",
            "307:\tlearn: 96.0414620\ttotal: 2m 34s\tremaining: 5m 26s\n",
            "308:\tlearn: 95.9960730\ttotal: 2m 34s\tremaining: 5m 25s\n",
            "309:\tlearn: 95.9484594\ttotal: 2m 35s\tremaining: 5m 25s\n",
            "310:\tlearn: 95.4767251\ttotal: 2m 35s\tremaining: 5m 24s\n",
            "311:\tlearn: 95.3787051\ttotal: 2m 36s\tremaining: 5m 24s\n",
            "312:\tlearn: 95.3216001\ttotal: 2m 36s\tremaining: 5m 23s\n",
            "313:\tlearn: 95.2184987\ttotal: 2m 37s\tremaining: 5m 23s\n",
            "314:\tlearn: 95.0787238\ttotal: 2m 37s\tremaining: 5m 22s\n",
            "315:\tlearn: 94.9802394\ttotal: 2m 38s\tremaining: 5m 22s\n",
            "316:\tlearn: 94.7575535\ttotal: 2m 39s\tremaining: 5m 22s\n",
            "317:\tlearn: 94.6810777\ttotal: 2m 39s\tremaining: 5m 22s\n",
            "318:\tlearn: 94.5613265\ttotal: 2m 40s\tremaining: 5m 21s\n",
            "319:\tlearn: 94.5577153\ttotal: 2m 40s\tremaining: 5m 21s\n",
            "320:\tlearn: 94.3263383\ttotal: 2m 41s\tremaining: 5m 21s\n",
            "321:\tlearn: 94.2854851\ttotal: 2m 42s\tremaining: 5m 20s\n",
            "322:\tlearn: 94.2696577\ttotal: 2m 42s\tremaining: 5m 20s\n",
            "323:\tlearn: 93.9411313\ttotal: 2m 43s\tremaining: 5m 20s\n",
            "324:\tlearn: 93.8803892\ttotal: 2m 43s\tremaining: 5m 19s\n",
            "325:\tlearn: 93.7943751\ttotal: 2m 44s\tremaining: 5m 19s\n",
            "326:\tlearn: 93.6565331\ttotal: 2m 44s\tremaining: 5m 18s\n",
            "327:\tlearn: 93.4939556\ttotal: 2m 45s\tremaining: 5m 18s\n",
            "328:\tlearn: 93.4909454\ttotal: 2m 46s\tremaining: 5m 17s\n",
            "329:\tlearn: 93.1802904\ttotal: 2m 46s\tremaining: 5m 17s\n",
            "330:\tlearn: 92.9654935\ttotal: 2m 46s\tremaining: 5m 16s\n",
            "331:\tlearn: 92.8134214\ttotal: 2m 47s\tremaining: 5m 16s\n",
            "332:\tlearn: 92.5515880\ttotal: 2m 48s\tremaining: 5m 15s\n",
            "333:\tlearn: 92.5066490\ttotal: 2m 48s\tremaining: 5m 15s\n",
            "334:\tlearn: 92.2960544\ttotal: 2m 49s\tremaining: 5m 15s\n",
            "335:\tlearn: 91.9373206\ttotal: 2m 49s\tremaining: 5m 14s\n",
            "336:\tlearn: 91.8457414\ttotal: 2m 50s\tremaining: 5m 13s\n",
            "337:\tlearn: 91.7389534\ttotal: 2m 50s\tremaining: 5m 13s\n",
            "338:\tlearn: 91.6262010\ttotal: 2m 51s\tremaining: 5m 13s\n",
            "339:\tlearn: 91.5234291\ttotal: 2m 51s\tremaining: 5m 12s\n",
            "340:\tlearn: 91.3117293\ttotal: 2m 52s\tremaining: 5m 12s\n",
            "341:\tlearn: 91.0304220\ttotal: 2m 52s\tremaining: 5m 11s\n",
            "342:\tlearn: 90.9405986\ttotal: 2m 53s\tremaining: 5m 11s\n",
            "343:\tlearn: 90.9386275\ttotal: 2m 53s\tremaining: 5m 10s\n",
            "344:\tlearn: 90.6195906\ttotal: 2m 54s\tremaining: 5m 10s\n",
            "345:\tlearn: 90.3719238\ttotal: 2m 54s\tremaining: 5m 9s\n",
            "346:\tlearn: 90.2272884\ttotal: 2m 55s\tremaining: 5m 9s\n",
            "347:\tlearn: 90.1105324\ttotal: 2m 55s\tremaining: 5m 8s\n",
            "348:\tlearn: 90.0422773\ttotal: 2m 56s\tremaining: 5m 8s\n",
            "349:\tlearn: 89.7128121\ttotal: 2m 57s\tremaining: 5m 7s\n",
            "350:\tlearn: 89.5022499\ttotal: 2m 57s\tremaining: 5m 7s\n",
            "351:\tlearn: 89.4355493\ttotal: 2m 58s\tremaining: 5m 7s\n",
            "352:\tlearn: 89.1918379\ttotal: 2m 58s\tremaining: 5m 6s\n",
            "353:\tlearn: 89.0069300\ttotal: 2m 59s\tremaining: 5m 6s\n",
            "354:\tlearn: 88.9293212\ttotal: 2m 59s\tremaining: 5m 5s\n",
            "355:\tlearn: 88.8391254\ttotal: 3m\tremaining: 5m 5s\n",
            "356:\tlearn: 88.5292755\ttotal: 3m\tremaining: 5m 4s\n",
            "357:\tlearn: 88.4220660\ttotal: 3m 1s\tremaining: 5m 4s\n",
            "358:\tlearn: 88.3682538\ttotal: 3m 1s\tremaining: 5m 4s\n",
            "359:\tlearn: 88.2552415\ttotal: 3m 2s\tremaining: 5m 3s\n",
            "360:\tlearn: 87.9897219\ttotal: 3m 2s\tremaining: 5m 2s\n",
            "361:\tlearn: 87.8625739\ttotal: 3m 3s\tremaining: 5m 2s\n",
            "362:\tlearn: 87.6140100\ttotal: 3m 3s\tremaining: 5m 1s\n",
            "363:\tlearn: 87.5611885\ttotal: 3m 4s\tremaining: 5m 1s\n",
            "364:\tlearn: 87.3499533\ttotal: 3m 4s\tremaining: 5m\n",
            "365:\tlearn: 87.2372693\ttotal: 3m 5s\tremaining: 5m\n",
            "366:\tlearn: 87.1509461\ttotal: 3m 6s\tremaining: 5m\n",
            "367:\tlearn: 87.0435306\ttotal: 3m 6s\tremaining: 4m 59s\n",
            "368:\tlearn: 86.7938984\ttotal: 3m 7s\tremaining: 4m 59s\n",
            "369:\tlearn: 86.6164141\ttotal: 3m 7s\tremaining: 4m 58s\n",
            "370:\tlearn: 86.5103247\ttotal: 3m 8s\tremaining: 4m 58s\n",
            "371:\tlearn: 86.3371744\ttotal: 3m 8s\tremaining: 4m 57s\n",
            "372:\tlearn: 86.2678244\ttotal: 3m 9s\tremaining: 4m 57s\n",
            "373:\tlearn: 86.1940899\ttotal: 3m 9s\tremaining: 4m 56s\n",
            "374:\tlearn: 85.8565108\ttotal: 3m 10s\tremaining: 4m 56s\n",
            "375:\tlearn: 85.7512389\ttotal: 3m 10s\tremaining: 4m 55s\n",
            "376:\tlearn: 85.5391539\ttotal: 3m 11s\tremaining: 4m 55s\n",
            "377:\tlearn: 85.4869415\ttotal: 3m 11s\tremaining: 4m 54s\n",
            "378:\tlearn: 85.3821641\ttotal: 3m 12s\tremaining: 4m 54s\n",
            "379:\tlearn: 85.2452190\ttotal: 3m 13s\tremaining: 4m 54s\n",
            "380:\tlearn: 85.0804721\ttotal: 3m 13s\tremaining: 4m 53s\n",
            "381:\tlearn: 84.7588726\ttotal: 3m 14s\tremaining: 4m 53s\n",
            "382:\tlearn: 84.5097843\ttotal: 3m 14s\tremaining: 4m 52s\n",
            "383:\tlearn: 84.3830938\ttotal: 3m 15s\tremaining: 4m 52s\n",
            "384:\tlearn: 84.3003492\ttotal: 3m 15s\tremaining: 4m 52s\n",
            "385:\tlearn: 84.2100754\ttotal: 3m 16s\tremaining: 4m 51s\n",
            "386:\tlearn: 83.8864107\ttotal: 3m 17s\tremaining: 4m 51s\n",
            "387:\tlearn: 83.8142995\ttotal: 3m 17s\tremaining: 4m 50s\n",
            "388:\tlearn: 83.7697269\ttotal: 3m 18s\tremaining: 4m 50s\n",
            "389:\tlearn: 83.5772784\ttotal: 3m 18s\tremaining: 4m 49s\n",
            "390:\tlearn: 83.4521357\ttotal: 3m 19s\tremaining: 4m 49s\n",
            "391:\tlearn: 83.3959694\ttotal: 3m 19s\tremaining: 4m 48s\n",
            "392:\tlearn: 83.3580102\ttotal: 3m 20s\tremaining: 4m 48s\n",
            "393:\tlearn: 83.2888035\ttotal: 3m 20s\tremaining: 4m 47s\n",
            "394:\tlearn: 83.1199025\ttotal: 3m 21s\tremaining: 4m 47s\n",
            "395:\tlearn: 82.9389507\ttotal: 3m 21s\tremaining: 4m 46s\n",
            "396:\tlearn: 82.9356933\ttotal: 3m 22s\tremaining: 4m 46s\n",
            "397:\tlearn: 82.8546986\ttotal: 3m 22s\tremaining: 4m 45s\n",
            "398:\tlearn: 82.8079060\ttotal: 3m 23s\tremaining: 4m 45s\n",
            "399:\tlearn: 82.6460366\ttotal: 3m 23s\tremaining: 4m 44s\n",
            "400:\tlearn: 82.6174267\ttotal: 3m 24s\tremaining: 4m 44s\n",
            "401:\tlearn: 82.5567072\ttotal: 3m 24s\tremaining: 4m 43s\n",
            "402:\tlearn: 82.3841356\ttotal: 3m 25s\tremaining: 4m 43s\n",
            "403:\tlearn: 82.2159997\ttotal: 3m 25s\tremaining: 4m 42s\n",
            "404:\tlearn: 82.1969071\ttotal: 3m 26s\tremaining: 4m 42s\n",
            "405:\tlearn: 82.1463733\ttotal: 3m 27s\tremaining: 4m 41s\n",
            "406:\tlearn: 82.1069117\ttotal: 3m 27s\tremaining: 4m 41s\n",
            "407:\tlearn: 82.0431276\ttotal: 3m 28s\tremaining: 4m 41s\n",
            "408:\tlearn: 82.0424747\ttotal: 3m 28s\tremaining: 4m 40s\n",
            "409:\tlearn: 81.9428837\ttotal: 3m 29s\tremaining: 4m 40s\n",
            "410:\tlearn: 81.8721742\ttotal: 3m 29s\tremaining: 4m 39s\n",
            "411:\tlearn: 81.8250280\ttotal: 3m 30s\tremaining: 4m 39s\n",
            "412:\tlearn: 81.8226741\ttotal: 3m 31s\tremaining: 4m 39s\n",
            "413:\tlearn: 81.8033383\ttotal: 3m 31s\tremaining: 4m 38s\n",
            "414:\tlearn: 81.5834982\ttotal: 3m 32s\tremaining: 4m 38s\n",
            "415:\tlearn: 81.4668472\ttotal: 3m 32s\tremaining: 4m 37s\n",
            "416:\tlearn: 81.3617198\ttotal: 3m 33s\tremaining: 4m 37s\n",
            "417:\tlearn: 81.2602467\ttotal: 3m 33s\tremaining: 4m 36s\n",
            "418:\tlearn: 81.2593384\ttotal: 3m 34s\tremaining: 4m 36s\n",
            "419:\tlearn: 81.0824600\ttotal: 3m 34s\tremaining: 4m 35s\n",
            "420:\tlearn: 81.0349772\ttotal: 3m 35s\tremaining: 4m 35s\n",
            "421:\tlearn: 80.9709187\ttotal: 3m 35s\tremaining: 4m 34s\n",
            "422:\tlearn: 80.8470966\ttotal: 3m 36s\tremaining: 4m 34s\n",
            "423:\tlearn: 80.6754483\ttotal: 3m 36s\tremaining: 4m 33s\n",
            "424:\tlearn: 80.6077691\ttotal: 3m 37s\tremaining: 4m 33s\n",
            "425:\tlearn: 80.3703822\ttotal: 3m 37s\tremaining: 4m 32s\n",
            "426:\tlearn: 80.3054363\ttotal: 3m 38s\tremaining: 4m 32s\n",
            "427:\tlearn: 80.1170640\ttotal: 3m 38s\tremaining: 4m 31s\n",
            "428:\tlearn: 80.0043157\ttotal: 3m 39s\tremaining: 4m 31s\n",
            "429:\tlearn: 80.0035185\ttotal: 3m 39s\tremaining: 4m 30s\n",
            "430:\tlearn: 79.8585580\ttotal: 3m 40s\tremaining: 4m 30s\n",
            "431:\tlearn: 79.6878924\ttotal: 3m 40s\tremaining: 4m 29s\n",
            "432:\tlearn: 79.6050183\ttotal: 3m 41s\tremaining: 4m 28s\n",
            "433:\tlearn: 79.5422096\ttotal: 3m 42s\tremaining: 4m 28s\n",
            "434:\tlearn: 79.4827794\ttotal: 3m 42s\tremaining: 4m 28s\n",
            "435:\tlearn: 79.4138473\ttotal: 3m 43s\tremaining: 4m 27s\n",
            "436:\tlearn: 79.0897591\ttotal: 3m 43s\tremaining: 4m 27s\n",
            "437:\tlearn: 79.0313346\ttotal: 3m 44s\tremaining: 4m 26s\n",
            "438:\tlearn: 78.9682353\ttotal: 3m 44s\tremaining: 4m 26s\n",
            "439:\tlearn: 78.8437792\ttotal: 3m 45s\tremaining: 4m 25s\n",
            "440:\tlearn: 78.6995933\ttotal: 3m 45s\tremaining: 4m 25s\n",
            "441:\tlearn: 78.6328227\ttotal: 3m 46s\tremaining: 4m 24s\n",
            "442:\tlearn: 78.5934687\ttotal: 3m 47s\tremaining: 4m 24s\n",
            "443:\tlearn: 78.5650906\ttotal: 3m 47s\tremaining: 4m 24s\n",
            "444:\tlearn: 78.3855229\ttotal: 3m 48s\tremaining: 4m 23s\n",
            "445:\tlearn: 78.1697024\ttotal: 3m 48s\tremaining: 4m 23s\n",
            "446:\tlearn: 78.1512529\ttotal: 3m 49s\tremaining: 4m 22s\n",
            "447:\tlearn: 78.1161457\ttotal: 3m 49s\tremaining: 4m 22s\n",
            "448:\tlearn: 78.0659583\ttotal: 3m 50s\tremaining: 4m 21s\n",
            "449:\tlearn: 77.9224318\ttotal: 3m 50s\tremaining: 4m 21s\n",
            "450:\tlearn: 77.8645850\ttotal: 3m 51s\tremaining: 4m 20s\n",
            "451:\tlearn: 77.8370433\ttotal: 3m 52s\tremaining: 4m 20s\n",
            "452:\tlearn: 77.7967877\ttotal: 3m 52s\tremaining: 4m 19s\n",
            "453:\tlearn: 77.6319072\ttotal: 3m 53s\tremaining: 4m 19s\n",
            "454:\tlearn: 77.5398106\ttotal: 3m 53s\tremaining: 4m 18s\n",
            "455:\tlearn: 77.4663554\ttotal: 3m 54s\tremaining: 4m 18s\n",
            "456:\tlearn: 77.3953499\ttotal: 3m 54s\tremaining: 4m 17s\n",
            "457:\tlearn: 77.3274002\ttotal: 3m 55s\tremaining: 4m 17s\n",
            "458:\tlearn: 77.2916430\ttotal: 3m 55s\tremaining: 4m 16s\n",
            "459:\tlearn: 77.2411965\ttotal: 3m 56s\tremaining: 4m 16s\n",
            "460:\tlearn: 77.1104594\ttotal: 3m 56s\tremaining: 4m 15s\n",
            "461:\tlearn: 76.9887853\ttotal: 3m 57s\tremaining: 4m 15s\n",
            "462:\tlearn: 76.8542141\ttotal: 3m 57s\tremaining: 4m 14s\n",
            "463:\tlearn: 76.5432684\ttotal: 3m 58s\tremaining: 4m 13s\n",
            "464:\tlearn: 76.3638133\ttotal: 3m 58s\tremaining: 4m 13s\n",
            "465:\tlearn: 76.2901197\ttotal: 3m 59s\tremaining: 4m 13s\n",
            "466:\tlearn: 76.2093695\ttotal: 3m 59s\tremaining: 4m 12s\n",
            "467:\tlearn: 76.1672380\ttotal: 4m\tremaining: 4m 12s\n",
            "468:\tlearn: 76.0175114\ttotal: 4m 1s\tremaining: 4m 11s\n",
            "469:\tlearn: 75.9662019\ttotal: 4m 1s\tremaining: 4m 11s\n",
            "470:\tlearn: 75.7906606\ttotal: 4m 2s\tremaining: 4m 10s\n",
            "471:\tlearn: 75.6885134\ttotal: 4m 2s\tremaining: 4m 10s\n",
            "472:\tlearn: 75.5944131\ttotal: 4m 3s\tremaining: 4m 9s\n",
            "473:\tlearn: 75.5613622\ttotal: 4m 3s\tremaining: 4m 9s\n",
            "474:\tlearn: 75.4268151\ttotal: 4m 4s\tremaining: 4m 8s\n",
            "475:\tlearn: 75.3812048\ttotal: 4m 4s\tremaining: 4m 8s\n",
            "476:\tlearn: 75.3103827\ttotal: 4m 5s\tremaining: 4m 8s\n",
            "477:\tlearn: 75.2548501\ttotal: 4m 6s\tremaining: 4m 7s\n",
            "478:\tlearn: 75.1776606\ttotal: 4m 6s\tremaining: 4m 7s\n",
            "479:\tlearn: 75.1194051\ttotal: 4m 7s\tremaining: 4m 6s\n",
            "480:\tlearn: 74.9031402\ttotal: 4m 7s\tremaining: 4m 6s\n",
            "481:\tlearn: 74.8134927\ttotal: 4m 8s\tremaining: 4m 5s\n",
            "482:\tlearn: 74.7409688\ttotal: 4m 8s\tremaining: 4m 5s\n",
            "483:\tlearn: 74.6536774\ttotal: 4m 9s\tremaining: 4m 4s\n",
            "484:\tlearn: 74.6019533\ttotal: 4m 9s\tremaining: 4m 4s\n",
            "485:\tlearn: 74.5455437\ttotal: 4m 10s\tremaining: 4m 3s\n",
            "486:\tlearn: 74.4312107\ttotal: 4m 10s\tremaining: 4m 3s\n",
            "487:\tlearn: 74.3733214\ttotal: 4m 11s\tremaining: 4m 2s\n",
            "488:\tlearn: 74.3066453\ttotal: 4m 11s\tremaining: 4m 2s\n",
            "489:\tlearn: 74.0928872\ttotal: 4m 12s\tremaining: 4m 1s\n",
            "490:\tlearn: 74.0251429\ttotal: 4m 12s\tremaining: 4m\n",
            "491:\tlearn: 73.9564146\ttotal: 4m 13s\tremaining: 4m\n",
            "492:\tlearn: 73.8686662\ttotal: 4m 13s\tremaining: 3m 59s\n",
            "493:\tlearn: 73.7949454\ttotal: 4m 14s\tremaining: 3m 59s\n",
            "494:\tlearn: 73.6642989\ttotal: 4m 14s\tremaining: 3m 58s\n",
            "495:\tlearn: 73.6055218\ttotal: 4m 15s\tremaining: 3m 58s\n",
            "496:\tlearn: 73.5404197\ttotal: 4m 15s\tremaining: 3m 57s\n",
            "497:\tlearn: 73.4891569\ttotal: 4m 16s\tremaining: 3m 57s\n",
            "498:\tlearn: 73.3873203\ttotal: 4m 17s\tremaining: 3m 56s\n",
            "499:\tlearn: 73.2447574\ttotal: 4m 17s\tremaining: 3m 56s\n",
            "500:\tlearn: 73.0634892\ttotal: 4m 18s\tremaining: 3m 56s\n",
            "501:\tlearn: 72.9978246\ttotal: 4m 18s\tremaining: 3m 55s\n",
            "502:\tlearn: 72.8787907\ttotal: 4m 19s\tremaining: 3m 55s\n",
            "503:\tlearn: 72.8204832\ttotal: 4m 19s\tremaining: 3m 54s\n",
            "504:\tlearn: 72.7087821\ttotal: 4m 20s\tremaining: 3m 54s\n",
            "505:\tlearn: 72.6821862\ttotal: 4m 20s\tremaining: 3m 53s\n",
            "506:\tlearn: 72.6526938\ttotal: 4m 21s\tremaining: 3m 53s\n",
            "507:\tlearn: 72.5991928\ttotal: 4m 21s\tremaining: 3m 52s\n",
            "508:\tlearn: 72.5520619\ttotal: 4m 22s\tremaining: 3m 52s\n",
            "509:\tlearn: 72.4368944\ttotal: 4m 23s\tremaining: 3m 51s\n",
            "510:\tlearn: 72.3856388\ttotal: 4m 23s\tremaining: 3m 51s\n",
            "511:\tlearn: 72.3576254\ttotal: 4m 24s\tremaining: 3m 50s\n",
            "512:\tlearn: 72.2589953\ttotal: 4m 24s\tremaining: 3m 50s\n",
            "513:\tlearn: 72.1717984\ttotal: 4m 25s\tremaining: 3m 49s\n",
            "514:\tlearn: 71.9332913\ttotal: 4m 25s\tremaining: 3m 49s\n",
            "515:\tlearn: 71.8005682\ttotal: 4m 26s\tremaining: 3m 48s\n",
            "516:\tlearn: 71.6395184\ttotal: 4m 26s\tremaining: 3m 48s\n",
            "517:\tlearn: 71.6048992\ttotal: 4m 27s\tremaining: 3m 47s\n",
            "518:\tlearn: 71.4771140\ttotal: 4m 27s\tremaining: 3m 47s\n",
            "519:\tlearn: 71.4187419\ttotal: 4m 28s\tremaining: 3m 46s\n",
            "520:\tlearn: 71.3599416\ttotal: 4m 29s\tremaining: 3m 46s\n",
            "521:\tlearn: 71.3123685\ttotal: 4m 29s\tremaining: 3m 45s\n",
            "522:\tlearn: 71.2341022\ttotal: 4m 30s\tremaining: 3m 45s\n",
            "523:\tlearn: 71.1853579\ttotal: 4m 30s\tremaining: 3m 44s\n",
            "524:\tlearn: 71.1079591\ttotal: 4m 31s\tremaining: 3m 44s\n",
            "525:\tlearn: 71.0624600\ttotal: 4m 31s\tremaining: 3m 43s\n",
            "526:\tlearn: 70.8558591\ttotal: 4m 32s\tremaining: 3m 43s\n",
            "527:\tlearn: 70.6974745\ttotal: 4m 32s\tremaining: 3m 42s\n",
            "528:\tlearn: 70.6266997\ttotal: 4m 33s\tremaining: 3m 42s\n",
            "529:\tlearn: 70.5491123\ttotal: 4m 34s\tremaining: 3m 41s\n",
            "530:\tlearn: 70.4805168\ttotal: 4m 34s\tremaining: 3m 41s\n",
            "531:\tlearn: 70.3760836\ttotal: 4m 35s\tremaining: 3m 40s\n",
            "532:\tlearn: 70.3170230\ttotal: 4m 35s\tremaining: 3m 40s\n",
            "533:\tlearn: 70.2886279\ttotal: 4m 36s\tremaining: 3m 39s\n",
            "534:\tlearn: 70.2365360\ttotal: 4m 36s\tremaining: 3m 39s\n",
            "535:\tlearn: 70.2035499\ttotal: 4m 37s\tremaining: 3m 38s\n",
            "536:\tlearn: 70.1244078\ttotal: 4m 38s\tremaining: 3m 38s\n",
            "537:\tlearn: 70.0769667\ttotal: 4m 38s\tremaining: 3m 37s\n",
            "538:\tlearn: 69.9594101\ttotal: 4m 39s\tremaining: 3m 37s\n",
            "539:\tlearn: 69.9439567\ttotal: 4m 39s\tremaining: 3m 36s\n",
            "540:\tlearn: 69.9051003\ttotal: 4m 40s\tremaining: 3m 36s\n",
            "541:\tlearn: 69.7880190\ttotal: 4m 40s\tremaining: 3m 35s\n",
            "542:\tlearn: 69.5762110\ttotal: 4m 40s\tremaining: 3m 35s\n",
            "543:\tlearn: 69.5021555\ttotal: 4m 41s\tremaining: 3m 34s\n",
            "544:\tlearn: 69.4361183\ttotal: 4m 42s\tremaining: 3m 34s\n",
            "545:\tlearn: 69.4335739\ttotal: 4m 42s\tremaining: 3m 33s\n",
            "546:\tlearn: 69.4121109\ttotal: 4m 43s\tremaining: 3m 33s\n",
            "547:\tlearn: 69.1309162\ttotal: 4m 43s\tremaining: 3m 32s\n",
            "548:\tlearn: 69.0841007\ttotal: 4m 44s\tremaining: 3m 32s\n",
            "549:\tlearn: 68.9356440\ttotal: 4m 44s\tremaining: 3m 31s\n",
            "550:\tlearn: 68.8410783\ttotal: 4m 45s\tremaining: 3m 31s\n",
            "551:\tlearn: 68.6588260\ttotal: 4m 45s\tremaining: 3m 30s\n",
            "552:\tlearn: 68.5953963\ttotal: 4m 46s\tremaining: 3m 30s\n",
            "553:\tlearn: 68.5211085\ttotal: 4m 46s\tremaining: 3m 29s\n",
            "554:\tlearn: 68.4044146\ttotal: 4m 47s\tremaining: 3m 29s\n",
            "555:\tlearn: 68.3651204\ttotal: 4m 47s\tremaining: 3m 28s\n",
            "556:\tlearn: 68.3147863\ttotal: 4m 48s\tremaining: 3m 28s\n",
            "557:\tlearn: 68.1808935\ttotal: 4m 49s\tremaining: 3m 27s\n",
            "558:\tlearn: 68.0993485\ttotal: 4m 49s\tremaining: 3m 27s\n",
            "559:\tlearn: 67.9931720\ttotal: 4m 50s\tremaining: 3m 26s\n",
            "560:\tlearn: 67.9372278\ttotal: 4m 50s\tremaining: 3m 26s\n",
            "561:\tlearn: 67.8917158\ttotal: 4m 51s\tremaining: 3m 25s\n",
            "562:\tlearn: 67.8221423\ttotal: 4m 51s\tremaining: 3m 25s\n",
            "563:\tlearn: 67.7690686\ttotal: 4m 52s\tremaining: 3m 24s\n",
            "564:\tlearn: 67.7360162\ttotal: 4m 53s\tremaining: 3m 24s\n",
            "565:\tlearn: 67.6785032\ttotal: 4m 53s\tremaining: 3m 23s\n",
            "566:\tlearn: 67.5531559\ttotal: 4m 54s\tremaining: 3m 23s\n",
            "567:\tlearn: 67.4973391\ttotal: 4m 54s\tremaining: 3m 22s\n",
            "568:\tlearn: 67.4217983\ttotal: 4m 55s\tremaining: 3m 22s\n",
            "569:\tlearn: 67.3884324\ttotal: 4m 55s\tremaining: 3m 21s\n",
            "570:\tlearn: 67.3187328\ttotal: 4m 56s\tremaining: 3m 21s\n",
            "571:\tlearn: 67.2181263\ttotal: 4m 56s\tremaining: 3m 20s\n",
            "572:\tlearn: 67.1360992\ttotal: 4m 56s\tremaining: 3m 20s\n",
            "573:\tlearn: 67.0690091\ttotal: 4m 57s\tremaining: 3m 19s\n",
            "574:\tlearn: 67.0356656\ttotal: 4m 57s\tremaining: 3m 19s\n",
            "575:\tlearn: 67.0113757\ttotal: 4m 58s\tremaining: 3m 18s\n",
            "576:\tlearn: 66.9733393\ttotal: 4m 58s\tremaining: 3m 17s\n",
            "577:\tlearn: 66.8849670\ttotal: 4m 59s\tremaining: 3m 17s\n",
            "578:\tlearn: 66.8595613\ttotal: 5m\tremaining: 3m 16s\n",
            "579:\tlearn: 66.8261046\ttotal: 5m\tremaining: 3m 16s\n",
            "580:\tlearn: 66.7951186\ttotal: 5m 1s\tremaining: 3m 15s\n",
            "581:\tlearn: 66.6642420\ttotal: 5m 1s\tremaining: 3m 15s\n",
            "582:\tlearn: 66.6035056\ttotal: 5m 2s\tremaining: 3m 14s\n",
            "583:\tlearn: 66.5731583\ttotal: 5m 2s\tremaining: 3m 14s\n",
            "584:\tlearn: 66.4867686\ttotal: 5m 3s\tremaining: 3m 13s\n",
            "585:\tlearn: 66.3179290\ttotal: 5m 3s\tremaining: 3m 13s\n",
            "586:\tlearn: 66.2837789\ttotal: 5m 4s\tremaining: 3m 12s\n",
            "587:\tlearn: 66.2083607\ttotal: 5m 5s\tremaining: 3m 12s\n",
            "588:\tlearn: 66.0949035\ttotal: 5m 5s\tremaining: 3m 11s\n",
            "589:\tlearn: 66.0273292\ttotal: 5m 6s\tremaining: 3m 11s\n",
            "590:\tlearn: 65.9396984\ttotal: 5m 6s\tremaining: 3m 11s\n",
            "591:\tlearn: 65.8919701\ttotal: 5m 7s\tremaining: 3m 10s\n",
            "592:\tlearn: 65.8501018\ttotal: 5m 7s\tremaining: 3m 10s\n",
            "593:\tlearn: 65.8492352\ttotal: 5m 8s\tremaining: 3m 9s\n",
            "594:\tlearn: 65.8032367\ttotal: 5m 8s\tremaining: 3m 9s\n",
            "595:\tlearn: 65.6942488\ttotal: 5m 9s\tremaining: 3m 8s\n",
            "596:\tlearn: 65.6224297\ttotal: 5m 9s\tremaining: 3m 7s\n",
            "597:\tlearn: 65.5770073\ttotal: 5m 10s\tremaining: 3m 7s\n",
            "598:\tlearn: 65.5508305\ttotal: 5m 10s\tremaining: 3m 6s\n",
            "599:\tlearn: 65.5074411\ttotal: 5m 11s\tremaining: 3m 6s\n",
            "600:\tlearn: 65.4749853\ttotal: 5m 12s\tremaining: 3m 5s\n",
            "601:\tlearn: 65.4008920\ttotal: 5m 12s\tremaining: 3m 5s\n",
            "602:\tlearn: 65.3172946\ttotal: 5m 13s\tremaining: 3m 4s\n",
            "603:\tlearn: 65.2853695\ttotal: 5m 13s\tremaining: 3m 4s\n",
            "604:\tlearn: 65.2233166\ttotal: 5m 14s\tremaining: 3m 3s\n",
            "605:\tlearn: 65.1895497\ttotal: 5m 14s\tremaining: 3m 3s\n",
            "606:\tlearn: 65.0393540\ttotal: 5m 15s\tremaining: 3m 2s\n",
            "607:\tlearn: 64.9460211\ttotal: 5m 15s\tremaining: 3m 2s\n",
            "608:\tlearn: 64.8198962\ttotal: 5m 16s\tremaining: 3m 1s\n",
            "609:\tlearn: 64.7892808\ttotal: 5m 16s\tremaining: 3m 1s\n",
            "610:\tlearn: 64.6244667\ttotal: 5m 17s\tremaining: 3m\n",
            "611:\tlearn: 64.5019710\ttotal: 5m 17s\tremaining: 3m\n",
            "612:\tlearn: 64.4357783\ttotal: 5m 18s\tremaining: 2m 59s\n",
            "613:\tlearn: 64.3691093\ttotal: 5m 18s\tremaining: 2m 59s\n",
            "614:\tlearn: 64.2876319\ttotal: 5m 19s\tremaining: 2m 58s\n",
            "615:\tlearn: 64.2304869\ttotal: 5m 19s\tremaining: 2m 58s\n",
            "616:\tlearn: 64.1595422\ttotal: 5m 20s\tremaining: 2m 57s\n",
            "617:\tlearn: 64.1307675\ttotal: 5m 20s\tremaining: 2m 57s\n",
            "618:\tlearn: 64.0792982\ttotal: 5m 21s\tremaining: 2m 56s\n",
            "619:\tlearn: 64.0003151\ttotal: 5m 22s\tremaining: 2m 56s\n",
            "620:\tlearn: 63.9488629\ttotal: 5m 22s\tremaining: 2m 55s\n",
            "621:\tlearn: 63.8805125\ttotal: 5m 23s\tremaining: 2m 55s\n",
            "622:\tlearn: 63.7427227\ttotal: 5m 23s\tremaining: 2m 54s\n",
            "623:\tlearn: 63.6979110\ttotal: 5m 24s\tremaining: 2m 54s\n",
            "624:\tlearn: 63.6338951\ttotal: 5m 24s\tremaining: 2m 53s\n",
            "625:\tlearn: 63.4756929\ttotal: 5m 25s\tremaining: 2m 53s\n",
            "626:\tlearn: 63.4441023\ttotal: 5m 25s\tremaining: 2m 52s\n",
            "627:\tlearn: 63.3293010\ttotal: 5m 26s\tremaining: 2m 52s\n",
            "628:\tlearn: 63.2180531\ttotal: 5m 26s\tremaining: 2m 51s\n",
            "629:\tlearn: 63.1583172\ttotal: 5m 27s\tremaining: 2m 50s\n",
            "630:\tlearn: 63.1327649\ttotal: 5m 27s\tremaining: 2m 50s\n",
            "631:\tlearn: 63.0942335\ttotal: 5m 28s\tremaining: 2m 49s\n",
            "632:\tlearn: 63.0278794\ttotal: 5m 28s\tremaining: 2m 49s\n",
            "633:\tlearn: 62.9833086\ttotal: 5m 29s\tremaining: 2m 48s\n",
            "634:\tlearn: 62.9273894\ttotal: 5m 29s\tremaining: 2m 48s\n",
            "635:\tlearn: 62.7878057\ttotal: 5m 30s\tremaining: 2m 47s\n",
            "636:\tlearn: 62.7432856\ttotal: 5m 30s\tremaining: 2m 47s\n",
            "637:\tlearn: 62.6515658\ttotal: 5m 31s\tremaining: 2m 46s\n",
            "638:\tlearn: 62.6112831\ttotal: 5m 31s\tremaining: 2m 46s\n",
            "639:\tlearn: 62.5629785\ttotal: 5m 32s\tremaining: 2m 45s\n",
            "640:\tlearn: 62.4986043\ttotal: 5m 32s\tremaining: 2m 45s\n",
            "641:\tlearn: 62.3993389\ttotal: 5m 33s\tremaining: 2m 44s\n",
            "642:\tlearn: 62.3916632\ttotal: 5m 33s\tremaining: 2m 44s\n",
            "643:\tlearn: 62.2999029\ttotal: 5m 34s\tremaining: 2m 43s\n",
            "644:\tlearn: 62.2662667\ttotal: 5m 35s\tremaining: 2m 43s\n",
            "645:\tlearn: 62.1930557\ttotal: 5m 35s\tremaining: 2m 42s\n",
            "646:\tlearn: 62.0990102\ttotal: 5m 36s\tremaining: 2m 42s\n",
            "647:\tlearn: 62.0496484\ttotal: 5m 36s\tremaining: 2m 41s\n",
            "648:\tlearn: 62.0084348\ttotal: 5m 37s\tremaining: 2m 41s\n",
            "649:\tlearn: 61.9728337\ttotal: 5m 38s\tremaining: 2m 40s\n",
            "650:\tlearn: 61.9591264\ttotal: 5m 38s\tremaining: 2m 40s\n",
            "651:\tlearn: 61.9079745\ttotal: 5m 39s\tremaining: 2m 39s\n",
            "652:\tlearn: 61.8277111\ttotal: 5m 39s\tremaining: 2m 39s\n",
            "653:\tlearn: 61.7530360\ttotal: 5m 40s\tremaining: 2m 38s\n",
            "654:\tlearn: 61.6768550\ttotal: 5m 40s\tremaining: 2m 38s\n",
            "655:\tlearn: 61.6500560\ttotal: 5m 41s\tremaining: 2m 37s\n",
            "656:\tlearn: 61.5930662\ttotal: 5m 42s\tremaining: 2m 37s\n",
            "657:\tlearn: 61.5562531\ttotal: 5m 42s\tremaining: 2m 36s\n",
            "658:\tlearn: 61.5546564\ttotal: 5m 43s\tremaining: 2m 36s\n",
            "659:\tlearn: 61.4665535\ttotal: 5m 43s\tremaining: 2m 35s\n",
            "660:\tlearn: 61.4285806\ttotal: 5m 44s\tremaining: 2m 35s\n",
            "661:\tlearn: 61.3712871\ttotal: 5m 44s\tremaining: 2m 34s\n",
            "662:\tlearn: 61.2725270\ttotal: 5m 45s\tremaining: 2m 34s\n",
            "663:\tlearn: 61.2029903\ttotal: 5m 45s\tremaining: 2m 33s\n",
            "664:\tlearn: 61.0497582\ttotal: 5m 46s\tremaining: 2m 33s\n",
            "665:\tlearn: 61.0192714\ttotal: 5m 46s\tremaining: 2m 32s\n",
            "666:\tlearn: 61.0030626\ttotal: 5m 47s\tremaining: 2m 32s\n",
            "667:\tlearn: 60.9634670\ttotal: 5m 47s\tremaining: 2m 31s\n",
            "668:\tlearn: 60.9394568\ttotal: 5m 48s\tremaining: 2m 30s\n",
            "669:\tlearn: 60.9266707\ttotal: 5m 48s\tremaining: 2m 30s\n",
            "670:\tlearn: 60.8477209\ttotal: 5m 49s\tremaining: 2m 29s\n",
            "671:\tlearn: 60.7765336\ttotal: 5m 49s\tremaining: 2m 29s\n",
            "672:\tlearn: 60.7400352\ttotal: 5m 50s\tremaining: 2m 28s\n",
            "673:\tlearn: 60.6816224\ttotal: 5m 50s\tremaining: 2m 28s\n",
            "674:\tlearn: 60.6425310\ttotal: 5m 51s\tremaining: 2m 27s\n",
            "675:\tlearn: 60.5869011\ttotal: 5m 52s\tremaining: 2m 27s\n",
            "676:\tlearn: 60.5445801\ttotal: 5m 52s\tremaining: 2m 26s\n",
            "677:\tlearn: 60.4810380\ttotal: 5m 53s\tremaining: 2m 26s\n",
            "678:\tlearn: 60.4228047\ttotal: 5m 53s\tremaining: 2m 25s\n",
            "679:\tlearn: 60.3637239\ttotal: 5m 54s\tremaining: 2m 25s\n",
            "680:\tlearn: 60.3440690\ttotal: 5m 54s\tremaining: 2m 24s\n",
            "681:\tlearn: 60.3309204\ttotal: 5m 55s\tremaining: 2m 24s\n",
            "682:\tlearn: 60.2815739\ttotal: 5m 56s\tremaining: 2m 23s\n",
            "683:\tlearn: 60.2105967\ttotal: 5m 56s\tremaining: 2m 23s\n",
            "684:\tlearn: 60.1607602\ttotal: 5m 57s\tremaining: 2m 22s\n",
            "685:\tlearn: 60.0895251\ttotal: 5m 57s\tremaining: 2m 22s\n",
            "686:\tlearn: 60.0880678\ttotal: 5m 58s\tremaining: 2m 21s\n",
            "687:\tlearn: 59.9540616\ttotal: 5m 58s\tremaining: 2m 21s\n",
            "688:\tlearn: 59.8496098\ttotal: 5m 59s\tremaining: 2m 20s\n",
            "689:\tlearn: 59.8055373\ttotal: 5m 59s\tremaining: 2m 20s\n",
            "690:\tlearn: 59.7723389\ttotal: 6m\tremaining: 2m 19s\n",
            "691:\tlearn: 59.7441949\ttotal: 6m\tremaining: 2m 19s\n",
            "692:\tlearn: 59.7233823\ttotal: 6m 1s\tremaining: 2m 18s\n",
            "693:\tlearn: 59.6890266\ttotal: 6m 1s\tremaining: 2m 18s\n",
            "694:\tlearn: 59.6599586\ttotal: 6m 2s\tremaining: 2m 17s\n",
            "695:\tlearn: 59.5413451\ttotal: 6m 2s\tremaining: 2m 17s\n",
            "696:\tlearn: 59.4958969\ttotal: 6m 3s\tremaining: 2m 16s\n",
            "697:\tlearn: 59.4360851\ttotal: 6m 3s\tremaining: 2m 16s\n",
            "698:\tlearn: 59.3651097\ttotal: 6m 4s\tremaining: 2m 15s\n",
            "699:\tlearn: 59.3384312\ttotal: 6m 4s\tremaining: 2m 15s\n",
            "700:\tlearn: 59.2715050\ttotal: 6m 5s\tremaining: 2m 14s\n",
            "701:\tlearn: 59.2542437\ttotal: 6m 6s\tremaining: 2m 14s\n",
            "702:\tlearn: 59.2261160\ttotal: 6m 6s\tremaining: 2m 13s\n",
            "703:\tlearn: 59.1920693\ttotal: 6m 7s\tremaining: 2m 13s\n",
            "704:\tlearn: 59.1448507\ttotal: 6m 7s\tremaining: 2m 12s\n",
            "705:\tlearn: 59.1002424\ttotal: 6m 8s\tremaining: 2m 12s\n",
            "706:\tlearn: 59.0636817\ttotal: 6m 8s\tremaining: 2m 11s\n",
            "707:\tlearn: 58.9919202\ttotal: 6m 9s\tremaining: 2m 10s\n",
            "708:\tlearn: 58.9727265\ttotal: 6m 10s\tremaining: 2m 10s\n",
            "709:\tlearn: 58.9330241\ttotal: 6m 10s\tremaining: 2m 9s\n",
            "710:\tlearn: 58.9313495\ttotal: 6m 11s\tremaining: 2m 9s\n",
            "711:\tlearn: 58.7814231\ttotal: 6m 11s\tremaining: 2m 8s\n",
            "712:\tlearn: 58.7430051\ttotal: 6m 12s\tremaining: 2m 8s\n",
            "713:\tlearn: 58.6412040\ttotal: 6m 12s\tremaining: 2m 7s\n",
            "714:\tlearn: 58.5764041\ttotal: 6m 13s\tremaining: 2m 7s\n",
            "715:\tlearn: 58.5496237\ttotal: 6m 14s\tremaining: 2m 6s\n",
            "716:\tlearn: 58.4962368\ttotal: 6m 14s\tremaining: 2m 6s\n",
            "717:\tlearn: 58.4939038\ttotal: 6m 15s\tremaining: 2m 5s\n",
            "718:\tlearn: 58.4738678\ttotal: 6m 15s\tremaining: 2m 5s\n",
            "719:\tlearn: 58.4257888\ttotal: 6m 16s\tremaining: 2m 4s\n",
            "720:\tlearn: 58.3289213\ttotal: 6m 16s\tremaining: 2m 4s\n",
            "721:\tlearn: 58.2913588\ttotal: 6m 17s\tremaining: 2m 3s\n",
            "722:\tlearn: 58.2501920\ttotal: 6m 17s\tremaining: 2m 3s\n",
            "723:\tlearn: 58.2010462\ttotal: 6m 18s\tremaining: 2m 2s\n",
            "724:\tlearn: 58.1777744\ttotal: 6m 18s\tremaining: 2m 2s\n",
            "725:\tlearn: 58.1181859\ttotal: 6m 19s\tremaining: 2m 1s\n",
            "726:\tlearn: 58.0465370\ttotal: 6m 19s\tremaining: 2m 1s\n",
            "727:\tlearn: 57.9044125\ttotal: 6m 20s\tremaining: 2m\n",
            "728:\tlearn: 57.8757435\ttotal: 6m 20s\tremaining: 2m\n",
            "729:\tlearn: 57.8039050\ttotal: 6m 21s\tremaining: 1m 59s\n",
            "730:\tlearn: 57.7827292\ttotal: 6m 21s\tremaining: 1m 59s\n",
            "731:\tlearn: 57.7204591\ttotal: 6m 22s\tremaining: 1m 58s\n",
            "732:\tlearn: 57.6639612\ttotal: 6m 22s\tremaining: 1m 58s\n",
            "733:\tlearn: 57.6620886\ttotal: 6m 23s\tremaining: 1m 57s\n",
            "734:\tlearn: 57.6154496\ttotal: 6m 24s\tremaining: 1m 57s\n",
            "735:\tlearn: 57.5898583\ttotal: 6m 24s\tremaining: 1m 56s\n",
            "736:\tlearn: 57.5682229\ttotal: 6m 25s\tremaining: 1m 56s\n",
            "737:\tlearn: 57.5174795\ttotal: 6m 25s\tremaining: 1m 55s\n",
            "738:\tlearn: 57.4956761\ttotal: 6m 26s\tremaining: 1m 55s\n",
            "739:\tlearn: 57.4601103\ttotal: 6m 27s\tremaining: 1m 54s\n",
            "740:\tlearn: 57.3673709\ttotal: 6m 27s\tremaining: 1m 54s\n",
            "741:\tlearn: 57.3369851\ttotal: 6m 28s\tremaining: 1m 53s\n",
            "742:\tlearn: 57.2945817\ttotal: 6m 28s\tremaining: 1m 52s\n",
            "743:\tlearn: 57.2423755\ttotal: 6m 29s\tremaining: 1m 52s\n",
            "744:\tlearn: 57.1950630\ttotal: 6m 29s\tremaining: 1m 51s\n",
            "745:\tlearn: 57.0855597\ttotal: 6m 30s\tremaining: 1m 51s\n",
            "746:\tlearn: 57.0320324\ttotal: 6m 30s\tremaining: 1m 50s\n",
            "747:\tlearn: 56.9896743\ttotal: 6m 31s\tremaining: 1m 50s\n",
            "748:\tlearn: 56.9655360\ttotal: 6m 31s\tremaining: 1m 49s\n",
            "749:\tlearn: 56.8854817\ttotal: 6m 32s\tremaining: 1m 49s\n",
            "750:\tlearn: 56.8504496\ttotal: 6m 32s\tremaining: 1m 48s\n",
            "751:\tlearn: 56.8025569\ttotal: 6m 33s\tremaining: 1m 48s\n",
            "752:\tlearn: 56.7482807\ttotal: 6m 33s\tremaining: 1m 47s\n",
            "753:\tlearn: 56.6875690\ttotal: 6m 34s\tremaining: 1m 47s\n",
            "754:\tlearn: 56.6459799\ttotal: 6m 34s\tremaining: 1m 46s\n",
            "755:\tlearn: 56.6179294\ttotal: 6m 35s\tremaining: 1m 46s\n",
            "756:\tlearn: 56.5689120\ttotal: 6m 35s\tremaining: 1m 45s\n",
            "757:\tlearn: 56.5297356\ttotal: 6m 36s\tremaining: 1m 45s\n",
            "758:\tlearn: 56.4460453\ttotal: 6m 36s\tremaining: 1m 44s\n",
            "759:\tlearn: 56.3838072\ttotal: 6m 37s\tremaining: 1m 44s\n",
            "760:\tlearn: 56.3469969\ttotal: 6m 37s\tremaining: 1m 43s\n",
            "761:\tlearn: 56.3175909\ttotal: 6m 38s\tremaining: 1m 43s\n",
            "762:\tlearn: 56.2023993\ttotal: 6m 39s\tremaining: 1m 42s\n",
            "763:\tlearn: 56.1741461\ttotal: 6m 39s\tremaining: 1m 41s\n",
            "764:\tlearn: 56.1044703\ttotal: 6m 40s\tremaining: 1m 41s\n",
            "765:\tlearn: 56.0623717\ttotal: 6m 40s\tremaining: 1m 40s\n",
            "766:\tlearn: 56.0005669\ttotal: 6m 41s\tremaining: 1m 40s\n",
            "767:\tlearn: 55.9755221\ttotal: 6m 41s\tremaining: 1m 39s\n",
            "768:\tlearn: 55.8737356\ttotal: 6m 42s\tremaining: 1m 39s\n",
            "769:\tlearn: 55.8393816\ttotal: 6m 42s\tremaining: 1m 38s\n",
            "770:\tlearn: 55.7880400\ttotal: 6m 43s\tremaining: 1m 38s\n",
            "771:\tlearn: 55.7684698\ttotal: 6m 43s\tremaining: 1m 37s\n",
            "772:\tlearn: 55.7381148\ttotal: 6m 44s\tremaining: 1m 37s\n",
            "773:\tlearn: 55.7222120\ttotal: 6m 45s\tremaining: 1m 36s\n",
            "774:\tlearn: 55.6596393\ttotal: 6m 45s\tremaining: 1m 36s\n",
            "775:\tlearn: 55.6312784\ttotal: 6m 46s\tremaining: 1m 35s\n",
            "776:\tlearn: 55.5808841\ttotal: 6m 46s\tremaining: 1m 35s\n",
            "777:\tlearn: 55.5432741\ttotal: 6m 47s\tremaining: 1m 34s\n",
            "778:\tlearn: 55.4301132\ttotal: 6m 47s\tremaining: 1m 34s\n",
            "779:\tlearn: 55.4079581\ttotal: 6m 48s\tremaining: 1m 33s\n",
            "780:\tlearn: 55.3415592\ttotal: 6m 48s\tremaining: 1m 33s\n",
            "781:\tlearn: 55.3015603\ttotal: 6m 49s\tremaining: 1m 32s\n",
            "782:\tlearn: 55.2753452\ttotal: 6m 49s\tremaining: 1m 32s\n",
            "783:\tlearn: 55.2244711\ttotal: 6m 50s\tremaining: 1m 31s\n",
            "784:\tlearn: 55.1788227\ttotal: 6m 50s\tremaining: 1m 31s\n",
            "785:\tlearn: 55.1517381\ttotal: 6m 51s\tremaining: 1m 30s\n",
            "786:\tlearn: 55.1153018\ttotal: 6m 51s\tremaining: 1m 30s\n",
            "787:\tlearn: 55.0388279\ttotal: 6m 52s\tremaining: 1m 29s\n",
            "788:\tlearn: 54.9420842\ttotal: 6m 52s\tremaining: 1m 28s\n",
            "789:\tlearn: 54.8917268\ttotal: 6m 53s\tremaining: 1m 28s\n",
            "790:\tlearn: 54.7498738\ttotal: 6m 53s\tremaining: 1m 27s\n",
            "791:\tlearn: 54.6696729\ttotal: 6m 54s\tremaining: 1m 27s\n",
            "792:\tlearn: 54.6390595\ttotal: 6m 54s\tremaining: 1m 26s\n",
            "793:\tlearn: 54.6101910\ttotal: 6m 55s\tremaining: 1m 26s\n",
            "794:\tlearn: 54.5791456\ttotal: 6m 56s\tremaining: 1m 25s\n",
            "795:\tlearn: 54.5628344\ttotal: 6m 56s\tremaining: 1m 25s\n",
            "796:\tlearn: 54.5460485\ttotal: 6m 57s\tremaining: 1m 24s\n",
            "797:\tlearn: 54.5153628\ttotal: 6m 58s\tremaining: 1m 24s\n",
            "798:\tlearn: 54.4910832\ttotal: 6m 58s\tremaining: 1m 23s\n",
            "799:\tlearn: 54.4471393\ttotal: 6m 59s\tremaining: 1m 23s\n",
            "800:\tlearn: 54.4198024\ttotal: 6m 59s\tremaining: 1m 22s\n",
            "801:\tlearn: 54.3964087\ttotal: 7m\tremaining: 1m 22s\n",
            "802:\tlearn: 54.3907086\ttotal: 7m 1s\tremaining: 1m 21s\n",
            "803:\tlearn: 54.3408005\ttotal: 7m 1s\tremaining: 1m 21s\n",
            "804:\tlearn: 54.2662549\ttotal: 7m 2s\tremaining: 1m 20s\n",
            "805:\tlearn: 54.2389254\ttotal: 7m 2s\tremaining: 1m 20s\n",
            "806:\tlearn: 54.1136170\ttotal: 7m 3s\tremaining: 1m 19s\n",
            "807:\tlearn: 54.0556602\ttotal: 7m 3s\tremaining: 1m 19s\n",
            "808:\tlearn: 54.0083734\ttotal: 7m 4s\tremaining: 1m 18s\n",
            "809:\tlearn: 53.9859374\ttotal: 7m 4s\tremaining: 1m 18s\n",
            "810:\tlearn: 53.9360233\ttotal: 7m 5s\tremaining: 1m 17s\n",
            "811:\tlearn: 53.8823547\ttotal: 7m 5s\tremaining: 1m 17s\n",
            "812:\tlearn: 53.8595950\ttotal: 7m 6s\tremaining: 1m 16s\n",
            "813:\tlearn: 53.8068268\ttotal: 7m 6s\tremaining: 1m 16s\n",
            "814:\tlearn: 53.7662342\ttotal: 7m 7s\tremaining: 1m 15s\n",
            "815:\tlearn: 53.7411863\ttotal: 7m 7s\tremaining: 1m 14s\n",
            "816:\tlearn: 53.6954940\ttotal: 7m 8s\tremaining: 1m 14s\n",
            "817:\tlearn: 53.6575423\ttotal: 7m 8s\tremaining: 1m 13s\n",
            "818:\tlearn: 53.6149853\ttotal: 7m 9s\tremaining: 1m 13s\n",
            "819:\tlearn: 53.5894156\ttotal: 7m 9s\tremaining: 1m 12s\n",
            "820:\tlearn: 53.5707173\ttotal: 7m 10s\tremaining: 1m 12s\n",
            "821:\tlearn: 53.5694147\ttotal: 7m 11s\tremaining: 1m 11s\n",
            "822:\tlearn: 53.5432397\ttotal: 7m 11s\tremaining: 1m 11s\n",
            "823:\tlearn: 53.4943566\ttotal: 7m 12s\tremaining: 1m 10s\n",
            "824:\tlearn: 53.4198450\ttotal: 7m 12s\tremaining: 1m 10s\n",
            "825:\tlearn: 53.3209663\ttotal: 7m 13s\tremaining: 1m 9s\n",
            "826:\tlearn: 53.2403310\ttotal: 7m 13s\tremaining: 1m 9s\n",
            "827:\tlearn: 53.2400900\ttotal: 7m 14s\tremaining: 1m 8s\n",
            "828:\tlearn: 53.2015619\ttotal: 7m 14s\tremaining: 1m 8s\n",
            "829:\tlearn: 53.1302796\ttotal: 7m 15s\tremaining: 1m 7s\n",
            "830:\tlearn: 53.1060917\ttotal: 7m 15s\tremaining: 1m 7s\n",
            "831:\tlearn: 53.0654937\ttotal: 7m 16s\tremaining: 1m 6s\n",
            "832:\tlearn: 53.0207148\ttotal: 7m 17s\tremaining: 1m 6s\n",
            "833:\tlearn: 52.9910552\ttotal: 7m 17s\tremaining: 1m 5s\n",
            "834:\tlearn: 52.9477972\ttotal: 7m 18s\tremaining: 1m 5s\n",
            "835:\tlearn: 52.8894748\ttotal: 7m 18s\tremaining: 1m 4s\n",
            "836:\tlearn: 52.8540812\ttotal: 7m 19s\tremaining: 1m 4s\n",
            "837:\tlearn: 52.8054647\ttotal: 7m 19s\tremaining: 1m 3s\n",
            "838:\tlearn: 52.7870843\ttotal: 7m 20s\tremaining: 1m 2s\n",
            "839:\tlearn: 52.7625734\ttotal: 7m 20s\tremaining: 1m 2s\n",
            "840:\tlearn: 52.6581592\ttotal: 7m 21s\tremaining: 1m 1s\n",
            "841:\tlearn: 52.6424519\ttotal: 7m 21s\tremaining: 1m 1s\n",
            "842:\tlearn: 52.6145977\ttotal: 7m 22s\tremaining: 1m\n",
            "843:\tlearn: 52.5778500\ttotal: 7m 22s\tremaining: 1m\n",
            "844:\tlearn: 52.5657365\ttotal: 7m 23s\tremaining: 59.8s\n",
            "845:\tlearn: 52.4953369\ttotal: 7m 23s\tremaining: 59.3s\n",
            "846:\tlearn: 52.4950835\ttotal: 7m 24s\tremaining: 58.8s\n",
            "847:\tlearn: 52.4222171\ttotal: 7m 24s\tremaining: 58.2s\n",
            "848:\tlearn: 52.3971562\ttotal: 7m 25s\tremaining: 57.7s\n",
            "849:\tlearn: 52.2820742\ttotal: 7m 26s\tremaining: 57.2s\n",
            "850:\tlearn: 52.2461612\ttotal: 7m 26s\tremaining: 56.7s\n",
            "851:\tlearn: 52.1829499\ttotal: 7m 27s\tremaining: 56.2s\n",
            "852:\tlearn: 52.0884229\ttotal: 7m 27s\tremaining: 55.6s\n",
            "853:\tlearn: 52.0407897\ttotal: 7m 28s\tremaining: 55.1s\n",
            "854:\tlearn: 51.9958502\ttotal: 7m 28s\tremaining: 54.6s\n",
            "855:\tlearn: 51.9653554\ttotal: 7m 29s\tremaining: 54.1s\n",
            "856:\tlearn: 51.9405373\ttotal: 7m 29s\tremaining: 53.5s\n",
            "857:\tlearn: 51.8916396\ttotal: 7m 30s\tremaining: 53s\n",
            "858:\tlearn: 51.8465575\ttotal: 7m 31s\tremaining: 52.5s\n",
            "859:\tlearn: 51.8118842\ttotal: 7m 31s\tremaining: 52s\n",
            "860:\tlearn: 51.7862983\ttotal: 7m 32s\tremaining: 51.5s\n",
            "861:\tlearn: 51.7330848\ttotal: 7m 32s\tremaining: 51s\n",
            "862:\tlearn: 51.6267845\ttotal: 7m 33s\tremaining: 50.4s\n",
            "863:\tlearn: 51.5829538\ttotal: 7m 33s\tremaining: 49.9s\n",
            "864:\tlearn: 51.5188829\ttotal: 7m 34s\tremaining: 49.4s\n",
            "865:\tlearn: 51.4230249\ttotal: 7m 34s\tremaining: 48.9s\n",
            "866:\tlearn: 51.3883445\ttotal: 7m 35s\tremaining: 48.3s\n",
            "867:\tlearn: 51.3504564\ttotal: 7m 35s\tremaining: 47.8s\n",
            "868:\tlearn: 51.3065022\ttotal: 7m 36s\tremaining: 47.3s\n",
            "869:\tlearn: 51.2806415\ttotal: 7m 37s\tremaining: 46.8s\n",
            "870:\tlearn: 51.2487720\ttotal: 7m 37s\tremaining: 46.2s\n",
            "871:\tlearn: 51.2228722\ttotal: 7m 38s\tremaining: 45.7s\n",
            "872:\tlearn: 51.1271347\ttotal: 7m 38s\tremaining: 45.2s\n",
            "873:\tlearn: 51.0979462\ttotal: 7m 39s\tremaining: 44.7s\n",
            "874:\tlearn: 51.0505953\ttotal: 7m 39s\tremaining: 44.1s\n",
            "875:\tlearn: 51.0155936\ttotal: 7m 40s\tremaining: 43.6s\n",
            "876:\tlearn: 50.9668566\ttotal: 7m 40s\tremaining: 43.1s\n",
            "877:\tlearn: 50.9348882\ttotal: 7m 41s\tremaining: 42.6s\n",
            "878:\tlearn: 50.9085271\ttotal: 7m 42s\tremaining: 42.1s\n",
            "879:\tlearn: 50.8628573\ttotal: 7m 42s\tremaining: 41.5s\n",
            "880:\tlearn: 50.8203286\ttotal: 7m 43s\tremaining: 41s\n",
            "881:\tlearn: 50.7805437\ttotal: 7m 43s\tremaining: 40.5s\n",
            "882:\tlearn: 50.7394318\ttotal: 7m 44s\tremaining: 40s\n",
            "883:\tlearn: 50.7035802\ttotal: 7m 44s\tremaining: 39.4s\n",
            "884:\tlearn: 50.6512101\ttotal: 7m 45s\tremaining: 38.9s\n",
            "885:\tlearn: 50.6303746\ttotal: 7m 46s\tremaining: 38.4s\n",
            "886:\tlearn: 50.5912753\ttotal: 7m 46s\tremaining: 37.9s\n",
            "887:\tlearn: 50.5677162\ttotal: 7m 47s\tremaining: 37.4s\n",
            "888:\tlearn: 50.5330474\ttotal: 7m 47s\tremaining: 36.8s\n",
            "889:\tlearn: 50.5119017\ttotal: 7m 48s\tremaining: 36.3s\n",
            "890:\tlearn: 50.4825662\ttotal: 7m 49s\tremaining: 35.8s\n",
            "891:\tlearn: 50.4449795\ttotal: 7m 49s\tremaining: 35.3s\n",
            "892:\tlearn: 50.4046190\ttotal: 7m 50s\tremaining: 34.8s\n",
            "893:\tlearn: 50.3574193\ttotal: 7m 50s\tremaining: 34.2s\n",
            "894:\tlearn: 50.2946519\ttotal: 7m 51s\tremaining: 33.7s\n",
            "895:\tlearn: 50.2728526\ttotal: 7m 51s\tremaining: 33.2s\n",
            "896:\tlearn: 50.2036339\ttotal: 7m 52s\tremaining: 32.6s\n",
            "897:\tlearn: 50.1869147\ttotal: 7m 52s\tremaining: 32.1s\n",
            "898:\tlearn: 50.1248735\ttotal: 7m 53s\tremaining: 31.6s\n",
            "899:\tlearn: 50.0849684\ttotal: 7m 53s\tremaining: 31.1s\n",
            "900:\tlearn: 50.0562652\ttotal: 7m 54s\tremaining: 30.5s\n",
            "901:\tlearn: 50.0102171\ttotal: 7m 54s\tremaining: 30s\n",
            "902:\tlearn: 49.9710993\ttotal: 7m 55s\tremaining: 29.5s\n",
            "903:\tlearn: 49.9244054\ttotal: 7m 56s\tremaining: 29s\n",
            "904:\tlearn: 49.9063712\ttotal: 7m 56s\tremaining: 28.4s\n",
            "905:\tlearn: 49.8896977\ttotal: 7m 57s\tremaining: 27.9s\n",
            "906:\tlearn: 49.8307762\ttotal: 7m 57s\tremaining: 27.4s\n",
            "907:\tlearn: 49.7985705\ttotal: 7m 58s\tremaining: 26.9s\n",
            "908:\tlearn: 49.7522593\ttotal: 7m 58s\tremaining: 26.3s\n",
            "909:\tlearn: 49.7112850\ttotal: 7m 59s\tremaining: 25.8s\n",
            "910:\tlearn: 49.6800306\ttotal: 7m 59s\tremaining: 25.3s\n",
            "911:\tlearn: 49.6461348\ttotal: 8m\tremaining: 24.8s\n",
            "912:\tlearn: 49.6199369\ttotal: 8m 1s\tremaining: 24.2s\n",
            "913:\tlearn: 49.5907221\ttotal: 8m 1s\tremaining: 23.7s\n",
            "914:\tlearn: 49.5674952\ttotal: 8m 2s\tremaining: 23.2s\n",
            "915:\tlearn: 49.5431331\ttotal: 8m 2s\tremaining: 22.7s\n",
            "916:\tlearn: 49.5389997\ttotal: 8m 3s\tremaining: 22.1s\n",
            "917:\tlearn: 49.5038230\ttotal: 8m 3s\tremaining: 21.6s\n",
            "918:\tlearn: 49.4500347\ttotal: 8m 4s\tremaining: 21.1s\n",
            "919:\tlearn: 49.4359473\ttotal: 8m 4s\tremaining: 20.6s\n",
            "920:\tlearn: 49.4175229\ttotal: 8m 5s\tremaining: 20s\n",
            "921:\tlearn: 49.3000711\ttotal: 8m 6s\tremaining: 19.5s\n",
            "922:\tlearn: 49.2645718\ttotal: 8m 6s\tremaining: 19s\n",
            "923:\tlearn: 49.2108437\ttotal: 8m 7s\tremaining: 18.5s\n",
            "924:\tlearn: 49.1952237\ttotal: 8m 7s\tremaining: 17.9s\n",
            "925:\tlearn: 49.1604197\ttotal: 8m 8s\tremaining: 17.4s\n",
            "926:\tlearn: 49.1313844\ttotal: 8m 8s\tremaining: 16.9s\n",
            "927:\tlearn: 49.1023266\ttotal: 8m 9s\tremaining: 16.4s\n",
            "928:\tlearn: 49.0794636\ttotal: 8m 10s\tremaining: 15.8s\n",
            "929:\tlearn: 49.0507544\ttotal: 8m 10s\tremaining: 15.3s\n",
            "930:\tlearn: 49.0120404\ttotal: 8m 11s\tremaining: 14.8s\n",
            "931:\tlearn: 49.0041759\ttotal: 8m 11s\tremaining: 14.2s\n",
            "932:\tlearn: 48.9344469\ttotal: 8m 11s\tremaining: 13.7s\n",
            "933:\tlearn: 48.8942792\ttotal: 8m 12s\tremaining: 13.2s\n",
            "934:\tlearn: 48.8775569\ttotal: 8m 13s\tremaining: 12.7s\n",
            "935:\tlearn: 48.8463340\ttotal: 8m 13s\tremaining: 12.1s\n",
            "936:\tlearn: 48.8460376\ttotal: 8m 14s\tremaining: 11.6s\n",
            "937:\tlearn: 48.7902778\ttotal: 8m 14s\tremaining: 11.1s\n",
            "938:\tlearn: 48.7771872\ttotal: 8m 15s\tremaining: 10.6s\n",
            "939:\tlearn: 48.7553590\ttotal: 8m 15s\tremaining: 10s\n",
            "940:\tlearn: 48.7409339\ttotal: 8m 16s\tremaining: 9.5s\n",
            "941:\tlearn: 48.7222094\ttotal: 8m 17s\tremaining: 8.97s\n",
            "942:\tlearn: 48.6675462\ttotal: 8m 17s\tremaining: 8.45s\n",
            "943:\tlearn: 48.6485809\ttotal: 8m 18s\tremaining: 7.92s\n",
            "944:\tlearn: 48.6226294\ttotal: 8m 18s\tremaining: 7.39s\n",
            "945:\tlearn: 48.5668362\ttotal: 8m 19s\tremaining: 6.86s\n",
            "946:\tlearn: 48.5422509\ttotal: 8m 20s\tremaining: 6.34s\n",
            "947:\tlearn: 48.5419356\ttotal: 8m 20s\tremaining: 5.81s\n",
            "948:\tlearn: 48.4876283\ttotal: 8m 21s\tremaining: 5.28s\n",
            "949:\tlearn: 48.4700410\ttotal: 8m 21s\tremaining: 4.75s\n",
            "950:\tlearn: 48.4190383\ttotal: 8m 22s\tremaining: 4.22s\n",
            "951:\tlearn: 48.3781367\ttotal: 8m 22s\tremaining: 3.69s\n",
            "952:\tlearn: 48.3369429\ttotal: 8m 23s\tremaining: 3.17s\n",
            "953:\tlearn: 48.3050889\ttotal: 8m 23s\tremaining: 2.64s\n",
            "954:\tlearn: 48.2246441\ttotal: 8m 24s\tremaining: 2.11s\n",
            "955:\tlearn: 48.1509889\ttotal: 8m 24s\tremaining: 1.58s\n",
            "956:\tlearn: 48.1381476\ttotal: 8m 25s\tremaining: 1.05s\n",
            "957:\tlearn: 48.1230893\ttotal: 8m 25s\tremaining: 528ms\n",
            "958:\tlearn: 48.0882395\ttotal: 8m 26s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 10:42:24,042] A new study created in memory with name: no-name-e58c1cb4-4f7d-4488-b2f2-bc48c470d255\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 10:52:39,712] Trial 0 finished with value: 56.410630358629604 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 56.410630358629604.\n",
            "[I 2023-10-08 10:56:33,695] Trial 1 finished with value: 40.39368532162432 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:03:13,770] Trial 2 finished with value: 42.455848595673565 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:07:47,469] Trial 3 finished with value: 43.3723982152417 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:11:45,836] Trial 4 finished with value: 41.164449035031666 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:18:13,047] Trial 5 finished with value: 74.30049181416636 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:21:35,458] Trial 6 finished with value: 46.085594311203586 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:37:25,111] Trial 7 finished with value: 55.90120225591975 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:39:23,884] Trial 8 finished with value: 56.10245916547504 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:42:52,955] Trial 9 finished with value: 48.22237784780308 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:48:00,982] Trial 10 finished with value: 51.749226831282996 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:52:00,245] Trial 11 finished with value: 40.75597799953384 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:55:18,452] Trial 12 finished with value: 45.15559818255308 and parameters: {'iterations': 743, 'learning_rate': 0.7356346127088652, 'depth': 9, 'min_data_in_leaf': 17, 'reg_lambda': 49.16423147603391, 'subsample': 0.5854290622440648, 'random_strength': 31.588734753749215, 'od_wait': 122, 'leaf_estimation_iterations': 4, 'bagging_temperature': 23.993242751388063, 'colsample_bylevel': 0.3642319611435846}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 11:57:54,260] Trial 13 finished with value: 50.38895955358534 and parameters: {'iterations': 757, 'learning_rate': 0.7501800009545696, 'depth': 11, 'min_data_in_leaf': 13, 'reg_lambda': 30.221360663173336, 'subsample': 0.4774950955421056, 'random_strength': 30.328584321736876, 'od_wait': 18, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.6071397112449649}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 12:02:13,623] Trial 14 finished with value: 40.72535956873387 and parameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 12:05:55,407] Trial 15 finished with value: 41.217889929388065 and parameters: {'iterations': 951, 'learning_rate': 0.378680852488155, 'depth': 7, 'min_data_in_leaf': 23, 'reg_lambda': 39.94535569998841, 'subsample': 0.6610237845565569, 'random_strength': 20.618074524594604, 'od_wait': 150, 'leaf_estimation_iterations': 5, 'bagging_temperature': 19.000173884005733, 'colsample_bylevel': 0.5429293784058689}. Best is trial 1 with value: 40.39368532162432.\n",
            "[I 2023-10-08 12:15:58,077] Trial 16 finished with value: 39.96326864844413 and parameters: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}. Best is trial 16 with value: 39.96326864844413.\n",
            "[I 2023-10-08 12:20:05,129] Trial 17 finished with value: 64.64760523081057 and parameters: {'iterations': 305, 'learning_rate': 0.10410309839558177, 'depth': 12, 'min_data_in_leaf': 22, 'reg_lambda': 62.93129990724812, 'subsample': 0.7426081894256986, 'random_strength': 44.362030019540654, 'od_wait': 125, 'leaf_estimation_iterations': 9, 'bagging_temperature': 13.937831218004703, 'colsample_bylevel': 0.9486708135061371}. Best is trial 16 with value: 39.96326864844413.\n",
            "[I 2023-10-08 12:25:02,100] Trial 18 finished with value: 45.55036255802975 and parameters: {'iterations': 796, 'learning_rate': 0.13599792999265126, 'depth': 11, 'min_data_in_leaf': 24, 'reg_lambda': 71.62634725545307, 'subsample': 0.44325233558749344, 'random_strength': 20.973759752682703, 'od_wait': 100, 'leaf_estimation_iterations': 6, 'bagging_temperature': 5.065329413975176, 'colsample_bylevel': 0.32674260541968214}. Best is trial 16 with value: 39.96326864844413.\n",
            "[I 2023-10-08 12:47:04,577] Trial 19 finished with value: 45.883554881402674 and parameters: {'iterations': 919, 'learning_rate': 0.36343405833274767, 'depth': 13, 'min_data_in_leaf': 30, 'reg_lambda': 40.746702935917256, 'subsample': 0.5980837495644987, 'random_strength': 42.94431966534945, 'od_wait': 98, 'leaf_estimation_iterations': 16, 'bagging_temperature': 42.15873557931633, 'colsample_bylevel': 0.8019932574851661}. Best is trial 16 with value: 39.96326864844413.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 4: 39.96326864844413\n",
            "Best hyperparameters for fold 4: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}\n",
            "0:\tlearn: 209.7107437\ttotal: 153ms\tremaining: 2m 26s\n",
            "1:\tlearn: 207.5643984\ttotal: 754ms\tremaining: 6m\n",
            "2:\tlearn: 207.1322776\ttotal: 873ms\tremaining: 4m 38s\n",
            "3:\tlearn: 205.8244018\ttotal: 1.4s\tremaining: 5m 34s\n",
            "4:\tlearn: 205.5432983\ttotal: 1.52s\tremaining: 4m 49s\n",
            "5:\tlearn: 204.8892367\ttotal: 1.93s\tremaining: 5m 6s\n",
            "6:\tlearn: 204.5831083\ttotal: 2.24s\tremaining: 5m 4s\n",
            "7:\tlearn: 203.7611796\ttotal: 2.8s\tremaining: 5m 32s\n",
            "8:\tlearn: 203.4446658\ttotal: 2.95s\tremaining: 5m 11s\n",
            "9:\tlearn: 203.1534438\ttotal: 3.29s\tremaining: 5m 12s\n",
            "10:\tlearn: 203.0581216\ttotal: 3.42s\tremaining: 4m 54s\n",
            "11:\tlearn: 202.7011137\ttotal: 3.86s\tremaining: 5m 4s\n",
            "12:\tlearn: 202.2129483\ttotal: 4.43s\tremaining: 5m 22s\n",
            "13:\tlearn: 201.7411494\ttotal: 4.96s\tremaining: 5m 34s\n",
            "14:\tlearn: 201.7019143\ttotal: 5.09s\tremaining: 5m 20s\n",
            "15:\tlearn: 201.6658154\ttotal: 5.33s\tremaining: 5m 14s\n",
            "16:\tlearn: 200.9573842\ttotal: 5.88s\tremaining: 5m 25s\n",
            "17:\tlearn: 200.1701184\ttotal: 6.44s\tremaining: 5m 36s\n",
            "18:\tlearn: 200.1590454\ttotal: 6.59s\tremaining: 5m 26s\n",
            "19:\tlearn: 199.7618277\ttotal: 6.98s\tremaining: 5m 27s\n",
            "20:\tlearn: 199.3197001\ttotal: 7.54s\tremaining: 5m 36s\n",
            "21:\tlearn: 199.2194579\ttotal: 7.85s\tremaining: 5m 34s\n",
            "22:\tlearn: 199.0287207\ttotal: 8.39s\tremaining: 5m 41s\n",
            "23:\tlearn: 199.0070430\ttotal: 8.51s\tremaining: 5m 31s\n",
            "24:\tlearn: 198.8996592\ttotal: 8.65s\tremaining: 5m 23s\n",
            "25:\tlearn: 198.8841557\ttotal: 8.78s\tremaining: 5m 15s\n",
            "26:\tlearn: 198.7493691\ttotal: 9.41s\tremaining: 5m 24s\n",
            "27:\tlearn: 198.7493689\ttotal: 9.49s\tremaining: 5m 15s\n",
            "28:\tlearn: 198.7489431\ttotal: 9.59s\tremaining: 5m 7s\n",
            "29:\tlearn: 198.4680103\ttotal: 10.3s\tremaining: 5m 18s\n",
            "30:\tlearn: 198.3333196\ttotal: 10.9s\tremaining: 5m 25s\n",
            "31:\tlearn: 198.3325234\ttotal: 11s\tremaining: 5m 17s\n",
            "32:\tlearn: 197.9247666\ttotal: 11.6s\tremaining: 5m 24s\n",
            "33:\tlearn: 197.8556289\ttotal: 12.1s\tremaining: 5m 27s\n",
            "34:\tlearn: 197.7699846\ttotal: 12.5s\tremaining: 5m 30s\n",
            "35:\tlearn: 197.7697892\ttotal: 12.6s\tremaining: 5m 24s\n",
            "36:\tlearn: 197.7220087\ttotal: 13.3s\tremaining: 5m 31s\n",
            "37:\tlearn: 197.7132204\ttotal: 13.4s\tremaining: 5m 25s\n",
            "38:\tlearn: 197.7132116\ttotal: 13.6s\tremaining: 5m 19s\n",
            "39:\tlearn: 197.4593427\ttotal: 14.1s\tremaining: 5m 24s\n",
            "40:\tlearn: 197.2016523\ttotal: 14.7s\tremaining: 5m 29s\n",
            "41:\tlearn: 197.0882853\ttotal: 15.3s\tremaining: 5m 34s\n",
            "42:\tlearn: 197.0741515\ttotal: 15.5s\tremaining: 5m 30s\n",
            "43:\tlearn: 197.0006112\ttotal: 16.1s\tremaining: 5m 34s\n",
            "44:\tlearn: 196.8637755\ttotal: 16.6s\tremaining: 5m 37s\n",
            "45:\tlearn: 196.8628691\ttotal: 16.7s\tremaining: 5m 32s\n",
            "46:\tlearn: 196.4964093\ttotal: 17.3s\tremaining: 5m 35s\n",
            "47:\tlearn: 196.4536638\ttotal: 17.5s\tremaining: 5m 32s\n",
            "48:\tlearn: 196.4205461\ttotal: 17.8s\tremaining: 5m 30s\n",
            "49:\tlearn: 196.3990727\ttotal: 18s\tremaining: 5m 27s\n",
            "50:\tlearn: 196.2694191\ttotal: 18.6s\tremaining: 5m 30s\n",
            "51:\tlearn: 196.2372008\ttotal: 19.1s\tremaining: 5m 32s\n",
            "52:\tlearn: 196.0912349\ttotal: 19.7s\tremaining: 5m 36s\n",
            "53:\tlearn: 195.9684340\ttotal: 20.2s\tremaining: 5m 39s\n",
            "54:\tlearn: 195.7859507\ttotal: 20.8s\tremaining: 5m 42s\n",
            "55:\tlearn: 195.7858889\ttotal: 20.9s\tremaining: 5m 37s\n",
            "56:\tlearn: 195.7600044\ttotal: 21.1s\tremaining: 5m 33s\n",
            "57:\tlearn: 195.6728160\ttotal: 21.7s\tremaining: 5m 36s\n",
            "58:\tlearn: 195.2857887\ttotal: 22.2s\tremaining: 5m 38s\n",
            "59:\tlearn: 195.2119149\ttotal: 22.8s\tremaining: 5m 41s\n",
            "60:\tlearn: 195.2119149\ttotal: 22.8s\tremaining: 5m 36s\n",
            "61:\tlearn: 195.1512071\ttotal: 23.4s\tremaining: 5m 38s\n",
            "62:\tlearn: 194.9071136\ttotal: 23.7s\tremaining: 5m 37s\n",
            "63:\tlearn: 194.7074476\ttotal: 24s\tremaining: 5m 36s\n",
            "64:\tlearn: 194.6591572\ttotal: 24.6s\tremaining: 5m 38s\n",
            "65:\tlearn: 194.6591572\ttotal: 24.7s\tremaining: 5m 34s\n",
            "66:\tlearn: 194.2814558\ttotal: 25.2s\tremaining: 5m 36s\n",
            "67:\tlearn: 193.9429971\ttotal: 25.8s\tremaining: 5m 38s\n",
            "68:\tlearn: 193.8200418\ttotal: 26.5s\tremaining: 5m 42s\n",
            "69:\tlearn: 193.7053664\ttotal: 27.2s\tremaining: 5m 45s\n",
            "70:\tlearn: 193.4697856\ttotal: 27.9s\tremaining: 5m 48s\n",
            "71:\tlearn: 193.4258685\ttotal: 28.5s\tremaining: 5m 51s\n",
            "72:\tlearn: 193.2310255\ttotal: 29.1s\tremaining: 5m 53s\n",
            "73:\tlearn: 193.1797574\ttotal: 29.8s\tremaining: 5m 55s\n",
            "74:\tlearn: 192.8048418\ttotal: 30.3s\tremaining: 5m 57s\n",
            "75:\tlearn: 192.6058652\ttotal: 30.9s\tremaining: 5m 59s\n",
            "76:\tlearn: 192.6058481\ttotal: 31s\tremaining: 5m 55s\n",
            "77:\tlearn: 192.5043327\ttotal: 31.2s\tremaining: 5m 52s\n",
            "78:\tlearn: 192.5038645\ttotal: 31.4s\tremaining: 5m 49s\n",
            "79:\tlearn: 192.4280761\ttotal: 31.6s\tremaining: 5m 47s\n",
            "80:\tlearn: 191.9889876\ttotal: 32.2s\tremaining: 5m 49s\n",
            "81:\tlearn: 191.8731454\ttotal: 32.8s\tremaining: 5m 51s\n",
            "82:\tlearn: 191.8731447\ttotal: 32.9s\tremaining: 5m 47s\n",
            "83:\tlearn: 191.8731442\ttotal: 33s\tremaining: 5m 43s\n",
            "84:\tlearn: 191.5644012\ttotal: 33.6s\tremaining: 5m 45s\n",
            "85:\tlearn: 191.5511963\ttotal: 33.8s\tremaining: 5m 42s\n",
            "86:\tlearn: 191.4041594\ttotal: 33.9s\tremaining: 5m 40s\n",
            "87:\tlearn: 191.0505982\ttotal: 34.5s\tremaining: 5m 41s\n",
            "88:\tlearn: 190.6486536\ttotal: 35.1s\tremaining: 5m 42s\n",
            "89:\tlearn: 190.4113408\ttotal: 35.7s\tremaining: 5m 44s\n",
            "90:\tlearn: 190.2041940\ttotal: 36.3s\tremaining: 5m 46s\n",
            "91:\tlearn: 189.6688996\ttotal: 36.9s\tremaining: 5m 47s\n",
            "92:\tlearn: 188.6764496\ttotal: 37.5s\tremaining: 5m 49s\n",
            "93:\tlearn: 188.4024700\ttotal: 38.1s\tremaining: 5m 50s\n",
            "94:\tlearn: 188.0877565\ttotal: 38.8s\tremaining: 5m 52s\n",
            "95:\tlearn: 187.7950639\ttotal: 39.4s\tremaining: 5m 53s\n",
            "96:\tlearn: 187.6427307\ttotal: 40s\tremaining: 5m 55s\n",
            "97:\tlearn: 186.9392991\ttotal: 40.5s\tremaining: 5m 56s\n",
            "98:\tlearn: 185.5318951\ttotal: 41.1s\tremaining: 5m 57s\n",
            "99:\tlearn: 183.8734908\ttotal: 41.6s\tremaining: 5m 57s\n",
            "100:\tlearn: 182.6174750\ttotal: 42.2s\tremaining: 5m 58s\n",
            "101:\tlearn: 182.2232051\ttotal: 42.8s\tremaining: 5m 59s\n",
            "102:\tlearn: 181.3341594\ttotal: 43.4s\tremaining: 6m\n",
            "103:\tlearn: 180.5500184\ttotal: 43.9s\tremaining: 6m 1s\n",
            "104:\tlearn: 179.9021593\ttotal: 44.5s\tremaining: 6m 2s\n",
            "105:\tlearn: 177.9197857\ttotal: 45.1s\tremaining: 6m 2s\n",
            "106:\tlearn: 177.4591663\ttotal: 45.7s\tremaining: 6m 4s\n",
            "107:\tlearn: 176.6223681\ttotal: 46.2s\tremaining: 6m 4s\n",
            "108:\tlearn: 176.2728948\ttotal: 46.7s\tremaining: 6m 4s\n",
            "109:\tlearn: 175.8617361\ttotal: 47.2s\tremaining: 6m 4s\n",
            "110:\tlearn: 173.9100281\ttotal: 47.7s\tremaining: 6m 4s\n",
            "111:\tlearn: 173.6331043\ttotal: 48.2s\tremaining: 6m 4s\n",
            "112:\tlearn: 173.1820450\ttotal: 48.7s\tremaining: 6m 4s\n",
            "113:\tlearn: 172.0987985\ttotal: 49.2s\tremaining: 6m 5s\n",
            "114:\tlearn: 171.7969607\ttotal: 49.8s\tremaining: 6m 5s\n",
            "115:\tlearn: 170.4442085\ttotal: 50.3s\tremaining: 6m 5s\n",
            "116:\tlearn: 170.1358729\ttotal: 50.8s\tremaining: 6m 5s\n",
            "117:\tlearn: 169.3302360\ttotal: 51.3s\tremaining: 6m 5s\n",
            "118:\tlearn: 168.1316359\ttotal: 51.8s\tremaining: 6m 5s\n",
            "119:\tlearn: 167.2391822\ttotal: 52.2s\tremaining: 6m 5s\n",
            "120:\tlearn: 165.9995926\ttotal: 52.7s\tremaining: 6m 5s\n",
            "121:\tlearn: 164.5924765\ttotal: 53.2s\tremaining: 6m 5s\n",
            "122:\tlearn: 163.8431895\ttotal: 53.7s\tremaining: 6m 4s\n",
            "123:\tlearn: 163.5278939\ttotal: 54.2s\tremaining: 6m 4s\n",
            "124:\tlearn: 161.4265296\ttotal: 54.7s\tremaining: 6m 5s\n",
            "125:\tlearn: 161.0301249\ttotal: 55.3s\tremaining: 6m 5s\n",
            "126:\tlearn: 159.5226239\ttotal: 55.8s\tremaining: 6m 5s\n",
            "127:\tlearn: 158.9924381\ttotal: 56.3s\tremaining: 6m 5s\n",
            "128:\tlearn: 158.5180071\ttotal: 56.8s\tremaining: 6m 5s\n",
            "129:\tlearn: 156.6279999\ttotal: 57.4s\tremaining: 6m 6s\n",
            "130:\tlearn: 156.3816481\ttotal: 58s\tremaining: 6m 6s\n",
            "131:\tlearn: 156.0112229\ttotal: 58.5s\tremaining: 6m 6s\n",
            "132:\tlearn: 154.4340798\ttotal: 59.1s\tremaining: 6m 6s\n",
            "133:\tlearn: 153.5419493\ttotal: 59.6s\tremaining: 6m 6s\n",
            "134:\tlearn: 153.0131538\ttotal: 1m\tremaining: 6m 7s\n",
            "135:\tlearn: 152.7413009\ttotal: 1m\tremaining: 6m 7s\n",
            "136:\tlearn: 150.8732041\ttotal: 1m 1s\tremaining: 6m 7s\n",
            "137:\tlearn: 150.3456592\ttotal: 1m 1s\tremaining: 6m 7s\n",
            "138:\tlearn: 149.3279673\ttotal: 1m 2s\tremaining: 6m 7s\n",
            "139:\tlearn: 148.6001322\ttotal: 1m 2s\tremaining: 6m 7s\n",
            "140:\tlearn: 148.2518043\ttotal: 1m 3s\tremaining: 6m 7s\n",
            "141:\tlearn: 147.8344974\ttotal: 1m 3s\tremaining: 6m 7s\n",
            "142:\tlearn: 147.5948255\ttotal: 1m 4s\tremaining: 6m 7s\n",
            "143:\tlearn: 146.8443796\ttotal: 1m 4s\tremaining: 6m 7s\n",
            "144:\tlearn: 146.5887874\ttotal: 1m 5s\tremaining: 6m 7s\n",
            "145:\tlearn: 146.0968862\ttotal: 1m 5s\tremaining: 6m 7s\n",
            "146:\tlearn: 145.1274746\ttotal: 1m 6s\tremaining: 6m 7s\n",
            "147:\tlearn: 144.9284171\ttotal: 1m 7s\tremaining: 6m 7s\n",
            "148:\tlearn: 144.5405684\ttotal: 1m 7s\tremaining: 6m 7s\n",
            "149:\tlearn: 144.0709777\ttotal: 1m 8s\tremaining: 6m 7s\n",
            "150:\tlearn: 143.8855629\ttotal: 1m 8s\tremaining: 6m 7s\n",
            "151:\tlearn: 143.5023380\ttotal: 1m 9s\tremaining: 6m 7s\n",
            "152:\tlearn: 142.0470187\ttotal: 1m 9s\tremaining: 6m 7s\n",
            "153:\tlearn: 141.8636370\ttotal: 1m 10s\tremaining: 6m 7s\n",
            "154:\tlearn: 141.1285844\ttotal: 1m 10s\tremaining: 6m 7s\n",
            "155:\tlearn: 140.0379660\ttotal: 1m 11s\tremaining: 6m 7s\n",
            "156:\tlearn: 139.8829427\ttotal: 1m 11s\tremaining: 6m 7s\n",
            "157:\tlearn: 139.8813331\ttotal: 1m 12s\tremaining: 6m 7s\n",
            "158:\tlearn: 139.6464124\ttotal: 1m 13s\tremaining: 6m 7s\n",
            "159:\tlearn: 139.0060558\ttotal: 1m 13s\tremaining: 6m 7s\n",
            "160:\tlearn: 138.8094679\ttotal: 1m 14s\tremaining: 6m 7s\n",
            "161:\tlearn: 138.8084254\ttotal: 1m 14s\tremaining: 6m 7s\n",
            "162:\tlearn: 138.0803119\ttotal: 1m 15s\tremaining: 6m 7s\n",
            "163:\tlearn: 137.6683425\ttotal: 1m 15s\tremaining: 6m 7s\n",
            "164:\tlearn: 137.2017638\ttotal: 1m 16s\tremaining: 6m 7s\n",
            "165:\tlearn: 137.0544057\ttotal: 1m 16s\tremaining: 6m 7s\n",
            "166:\tlearn: 136.0711794\ttotal: 1m 17s\tremaining: 6m 7s\n",
            "167:\tlearn: 134.6255159\ttotal: 1m 18s\tremaining: 6m 7s\n",
            "168:\tlearn: 134.3356606\ttotal: 1m 18s\tremaining: 6m 7s\n",
            "169:\tlearn: 133.9017605\ttotal: 1m 19s\tremaining: 6m 7s\n",
            "170:\tlearn: 133.4992443\ttotal: 1m 19s\tremaining: 6m 6s\n",
            "171:\tlearn: 133.1915389\ttotal: 1m 20s\tremaining: 6m 6s\n",
            "172:\tlearn: 132.9293848\ttotal: 1m 20s\tremaining: 6m 6s\n",
            "173:\tlearn: 132.6583092\ttotal: 1m 21s\tremaining: 6m 7s\n",
            "174:\tlearn: 131.9661377\ttotal: 1m 21s\tremaining: 6m 6s\n",
            "175:\tlearn: 131.5942067\ttotal: 1m 22s\tremaining: 6m 6s\n",
            "176:\tlearn: 130.8548690\ttotal: 1m 22s\tremaining: 6m 6s\n",
            "177:\tlearn: 130.6884521\ttotal: 1m 23s\tremaining: 6m 6s\n",
            "178:\tlearn: 130.4928886\ttotal: 1m 24s\tremaining: 6m 6s\n",
            "179:\tlearn: 130.3223247\ttotal: 1m 24s\tremaining: 6m 6s\n",
            "180:\tlearn: 130.1791246\ttotal: 1m 25s\tremaining: 6m 6s\n",
            "181:\tlearn: 130.0494725\ttotal: 1m 25s\tremaining: 6m 6s\n",
            "182:\tlearn: 129.5541495\ttotal: 1m 26s\tremaining: 6m 5s\n",
            "183:\tlearn: 129.3848241\ttotal: 1m 26s\tremaining: 6m 5s\n",
            "184:\tlearn: 129.1196949\ttotal: 1m 27s\tremaining: 6m 4s\n",
            "185:\tlearn: 128.3845649\ttotal: 1m 27s\tremaining: 6m 4s\n",
            "186:\tlearn: 127.9783404\ttotal: 1m 28s\tremaining: 6m 4s\n",
            "187:\tlearn: 127.1035390\ttotal: 1m 28s\tremaining: 6m 3s\n",
            "188:\tlearn: 126.7965475\ttotal: 1m 29s\tremaining: 6m 3s\n",
            "189:\tlearn: 126.6193012\ttotal: 1m 29s\tremaining: 6m 3s\n",
            "190:\tlearn: 126.4369374\ttotal: 1m 30s\tremaining: 6m 3s\n",
            "191:\tlearn: 126.1471989\ttotal: 1m 31s\tremaining: 6m 3s\n",
            "192:\tlearn: 125.9365618\ttotal: 1m 31s\tremaining: 6m 3s\n",
            "193:\tlearn: 125.4519523\ttotal: 1m 32s\tremaining: 6m 3s\n",
            "194:\tlearn: 125.4512818\ttotal: 1m 32s\tremaining: 6m 3s\n",
            "195:\tlearn: 124.8793918\ttotal: 1m 33s\tremaining: 6m 3s\n",
            "196:\tlearn: 124.8792499\ttotal: 1m 33s\tremaining: 6m 3s\n",
            "197:\tlearn: 124.3844314\ttotal: 1m 34s\tremaining: 6m 2s\n",
            "198:\tlearn: 124.2133776\ttotal: 1m 34s\tremaining: 6m 2s\n",
            "199:\tlearn: 123.9625778\ttotal: 1m 35s\tremaining: 6m 2s\n",
            "200:\tlearn: 123.3594227\ttotal: 1m 35s\tremaining: 6m 1s\n",
            "201:\tlearn: 123.3593255\ttotal: 1m 36s\tremaining: 6m 1s\n",
            "202:\tlearn: 123.0317597\ttotal: 1m 37s\tremaining: 6m 1s\n",
            "203:\tlearn: 122.5839804\ttotal: 1m 37s\tremaining: 6m 1s\n",
            "204:\tlearn: 122.5833621\ttotal: 1m 38s\tremaining: 6m 1s\n",
            "205:\tlearn: 122.3614092\ttotal: 1m 38s\tremaining: 6m\n",
            "206:\tlearn: 122.2582157\ttotal: 1m 39s\tremaining: 6m\n",
            "207:\tlearn: 122.0917791\ttotal: 1m 39s\tremaining: 6m\n",
            "208:\tlearn: 121.2401873\ttotal: 1m 40s\tremaining: 5m 59s\n",
            "209:\tlearn: 121.1007461\ttotal: 1m 40s\tremaining: 5m 59s\n",
            "210:\tlearn: 120.9211091\ttotal: 1m 41s\tremaining: 5m 59s\n",
            "211:\tlearn: 120.1392755\ttotal: 1m 41s\tremaining: 5m 59s\n",
            "212:\tlearn: 119.7199070\ttotal: 1m 42s\tremaining: 5m 58s\n",
            "213:\tlearn: 119.4987585\ttotal: 1m 42s\tremaining: 5m 58s\n",
            "214:\tlearn: 119.2791640\ttotal: 1m 43s\tremaining: 5m 58s\n",
            "215:\tlearn: 119.2488870\ttotal: 1m 44s\tremaining: 5m 57s\n",
            "216:\tlearn: 118.9946303\ttotal: 1m 44s\tremaining: 5m 57s\n",
            "217:\tlearn: 118.4753787\ttotal: 1m 45s\tremaining: 5m 57s\n",
            "218:\tlearn: 118.3888319\ttotal: 1m 45s\tremaining: 5m 57s\n",
            "219:\tlearn: 118.1989185\ttotal: 1m 46s\tremaining: 5m 57s\n",
            "220:\tlearn: 117.9248223\ttotal: 1m 47s\tremaining: 5m 57s\n",
            "221:\tlearn: 117.8775931\ttotal: 1m 47s\tremaining: 5m 57s\n",
            "222:\tlearn: 117.7519811\ttotal: 1m 48s\tremaining: 5m 57s\n",
            "223:\tlearn: 117.3711413\ttotal: 1m 48s\tremaining: 5m 56s\n",
            "224:\tlearn: 117.3701135\ttotal: 1m 49s\tremaining: 5m 56s\n",
            "225:\tlearn: 117.2219281\ttotal: 1m 49s\tremaining: 5m 56s\n",
            "226:\tlearn: 117.2201272\ttotal: 1m 50s\tremaining: 5m 56s\n",
            "227:\tlearn: 116.5979210\ttotal: 1m 50s\tremaining: 5m 55s\n",
            "228:\tlearn: 116.0078667\ttotal: 1m 51s\tremaining: 5m 55s\n",
            "229:\tlearn: 115.5121925\ttotal: 1m 51s\tremaining: 5m 54s\n",
            "230:\tlearn: 115.2521848\ttotal: 1m 52s\tremaining: 5m 54s\n",
            "231:\tlearn: 115.1516725\ttotal: 1m 53s\tremaining: 5m 54s\n",
            "232:\tlearn: 114.9409200\ttotal: 1m 53s\tremaining: 5m 53s\n",
            "233:\tlearn: 114.7871000\ttotal: 1m 54s\tremaining: 5m 53s\n",
            "234:\tlearn: 114.4248529\ttotal: 1m 54s\tremaining: 5m 52s\n",
            "235:\tlearn: 113.6627437\ttotal: 1m 55s\tremaining: 5m 52s\n",
            "236:\tlearn: 113.1710682\ttotal: 1m 55s\tremaining: 5m 52s\n",
            "237:\tlearn: 113.0270491\ttotal: 1m 56s\tremaining: 5m 52s\n",
            "238:\tlearn: 112.8671850\ttotal: 1m 56s\tremaining: 5m 51s\n",
            "239:\tlearn: 112.4472882\ttotal: 1m 57s\tremaining: 5m 51s\n",
            "240:\tlearn: 112.2779648\ttotal: 1m 57s\tremaining: 5m 51s\n",
            "241:\tlearn: 112.0422685\ttotal: 1m 58s\tremaining: 5m 50s\n",
            "242:\tlearn: 111.9047652\ttotal: 1m 58s\tremaining: 5m 50s\n",
            "243:\tlearn: 111.7573287\ttotal: 1m 59s\tremaining: 5m 50s\n",
            "244:\tlearn: 111.2124788\ttotal: 2m\tremaining: 5m 49s\n",
            "245:\tlearn: 111.0345382\ttotal: 2m\tremaining: 5m 49s\n",
            "246:\tlearn: 110.8085064\ttotal: 2m 1s\tremaining: 5m 49s\n",
            "247:\tlearn: 110.4207807\ttotal: 2m 1s\tremaining: 5m 48s\n",
            "248:\tlearn: 110.1177717\ttotal: 2m 2s\tremaining: 5m 48s\n",
            "249:\tlearn: 110.0616954\ttotal: 2m 2s\tremaining: 5m 48s\n",
            "250:\tlearn: 109.6913882\ttotal: 2m 3s\tremaining: 5m 48s\n",
            "251:\tlearn: 109.5313167\ttotal: 2m 4s\tremaining: 5m 48s\n",
            "252:\tlearn: 109.1158481\ttotal: 2m 4s\tremaining: 5m 47s\n",
            "253:\tlearn: 108.7575373\ttotal: 2m 5s\tremaining: 5m 47s\n",
            "254:\tlearn: 108.6368766\ttotal: 2m 5s\tremaining: 5m 47s\n",
            "255:\tlearn: 108.4286071\ttotal: 2m 6s\tremaining: 5m 46s\n",
            "256:\tlearn: 108.3023341\ttotal: 2m 6s\tremaining: 5m 46s\n",
            "257:\tlearn: 108.0188417\ttotal: 2m 7s\tremaining: 5m 45s\n",
            "258:\tlearn: 107.8200377\ttotal: 2m 7s\tremaining: 5m 45s\n",
            "259:\tlearn: 107.6721311\ttotal: 2m 8s\tremaining: 5m 44s\n",
            "260:\tlearn: 107.3646166\ttotal: 2m 8s\tremaining: 5m 44s\n",
            "261:\tlearn: 107.1971925\ttotal: 2m 9s\tremaining: 5m 43s\n",
            "262:\tlearn: 107.0633911\ttotal: 2m 9s\tremaining: 5m 43s\n",
            "263:\tlearn: 106.8467026\ttotal: 2m 10s\tremaining: 5m 42s\n",
            "264:\tlearn: 106.6539204\ttotal: 2m 10s\tremaining: 5m 42s\n",
            "265:\tlearn: 106.5312945\ttotal: 2m 11s\tremaining: 5m 42s\n",
            "266:\tlearn: 106.2807993\ttotal: 2m 11s\tremaining: 5m 41s\n",
            "267:\tlearn: 106.2040173\ttotal: 2m 12s\tremaining: 5m 41s\n",
            "268:\tlearn: 106.0127379\ttotal: 2m 12s\tremaining: 5m 41s\n",
            "269:\tlearn: 105.8046828\ttotal: 2m 13s\tremaining: 5m 40s\n",
            "270:\tlearn: 105.8041650\ttotal: 2m 14s\tremaining: 5m 40s\n",
            "271:\tlearn: 105.5466726\ttotal: 2m 14s\tremaining: 5m 40s\n",
            "272:\tlearn: 105.3458955\ttotal: 2m 15s\tremaining: 5m 39s\n",
            "273:\tlearn: 105.2519848\ttotal: 2m 15s\tremaining: 5m 39s\n",
            "274:\tlearn: 105.2471987\ttotal: 2m 16s\tremaining: 5m 39s\n",
            "275:\tlearn: 105.0212590\ttotal: 2m 16s\tremaining: 5m 38s\n",
            "276:\tlearn: 104.8272197\ttotal: 2m 17s\tremaining: 5m 38s\n",
            "277:\tlearn: 104.7194543\ttotal: 2m 18s\tremaining: 5m 38s\n",
            "278:\tlearn: 104.5855806\ttotal: 2m 18s\tremaining: 5m 37s\n",
            "279:\tlearn: 104.2800689\ttotal: 2m 19s\tremaining: 5m 37s\n",
            "280:\tlearn: 103.8080153\ttotal: 2m 19s\tremaining: 5m 36s\n",
            "281:\tlearn: 103.5668558\ttotal: 2m 20s\tremaining: 5m 36s\n",
            "282:\tlearn: 103.4670299\ttotal: 2m 20s\tremaining: 5m 36s\n",
            "283:\tlearn: 103.4152823\ttotal: 2m 21s\tremaining: 5m 36s\n",
            "284:\tlearn: 103.1655917\ttotal: 2m 21s\tremaining: 5m 35s\n",
            "285:\tlearn: 102.6959734\ttotal: 2m 22s\tremaining: 5m 35s\n",
            "286:\tlearn: 102.4718644\ttotal: 2m 22s\tremaining: 5m 34s\n",
            "287:\tlearn: 102.4015924\ttotal: 2m 23s\tremaining: 5m 34s\n",
            "288:\tlearn: 102.3286021\ttotal: 2m 24s\tremaining: 5m 34s\n",
            "289:\tlearn: 102.0451196\ttotal: 2m 24s\tremaining: 5m 33s\n",
            "290:\tlearn: 101.9744209\ttotal: 2m 25s\tremaining: 5m 33s\n",
            "291:\tlearn: 101.8502029\ttotal: 2m 25s\tremaining: 5m 33s\n",
            "292:\tlearn: 101.3145783\ttotal: 2m 26s\tremaining: 5m 32s\n",
            "293:\tlearn: 101.2505077\ttotal: 2m 26s\tremaining: 5m 32s\n",
            "294:\tlearn: 101.0972887\ttotal: 2m 27s\tremaining: 5m 31s\n",
            "295:\tlearn: 100.9104958\ttotal: 2m 27s\tremaining: 5m 31s\n",
            "296:\tlearn: 100.7677574\ttotal: 2m 28s\tremaining: 5m 31s\n",
            "297:\tlearn: 100.6104758\ttotal: 2m 29s\tremaining: 5m 30s\n",
            "298:\tlearn: 100.3853119\ttotal: 2m 29s\tremaining: 5m 30s\n",
            "299:\tlearn: 100.3846395\ttotal: 2m 30s\tremaining: 5m 29s\n",
            "300:\tlearn: 100.3375789\ttotal: 2m 30s\tremaining: 5m 29s\n",
            "301:\tlearn: 100.2567292\ttotal: 2m 31s\tremaining: 5m 28s\n",
            "302:\tlearn: 100.1246760\ttotal: 2m 31s\tremaining: 5m 28s\n",
            "303:\tlearn: 100.0386757\ttotal: 2m 32s\tremaining: 5m 28s\n",
            "304:\tlearn: 99.8860998\ttotal: 2m 32s\tremaining: 5m 27s\n",
            "305:\tlearn: 99.7691459\ttotal: 2m 33s\tremaining: 5m 27s\n",
            "306:\tlearn: 99.6526177\ttotal: 2m 33s\tremaining: 5m 26s\n",
            "307:\tlearn: 99.6519342\ttotal: 2m 34s\tremaining: 5m 26s\n",
            "308:\tlearn: 99.4922148\ttotal: 2m 35s\tremaining: 5m 26s\n",
            "309:\tlearn: 99.4604637\ttotal: 2m 35s\tremaining: 5m 26s\n",
            "310:\tlearn: 99.3224820\ttotal: 2m 36s\tremaining: 5m 25s\n",
            "311:\tlearn: 99.1791578\ttotal: 2m 36s\tremaining: 5m 25s\n",
            "312:\tlearn: 98.9661257\ttotal: 2m 37s\tremaining: 5m 25s\n",
            "313:\tlearn: 98.6089645\ttotal: 2m 38s\tremaining: 5m 24s\n",
            "314:\tlearn: 98.4499416\ttotal: 2m 38s\tremaining: 5m 24s\n",
            "315:\tlearn: 98.1513188\ttotal: 2m 39s\tremaining: 5m 24s\n",
            "316:\tlearn: 98.0001049\ttotal: 2m 39s\tremaining: 5m 23s\n",
            "317:\tlearn: 97.9201052\ttotal: 2m 40s\tremaining: 5m 23s\n",
            "318:\tlearn: 97.8145330\ttotal: 2m 40s\tremaining: 5m 22s\n",
            "319:\tlearn: 97.7227575\ttotal: 2m 41s\tremaining: 5m 22s\n",
            "320:\tlearn: 97.6766726\ttotal: 2m 41s\tremaining: 5m 21s\n",
            "321:\tlearn: 97.5017738\ttotal: 2m 42s\tremaining: 5m 21s\n",
            "322:\tlearn: 97.3631848\ttotal: 2m 43s\tremaining: 5m 21s\n",
            "323:\tlearn: 97.0030910\ttotal: 2m 43s\tremaining: 5m 20s\n",
            "324:\tlearn: 96.7469777\ttotal: 2m 44s\tremaining: 5m 20s\n",
            "325:\tlearn: 96.4868195\ttotal: 2m 44s\tremaining: 5m 19s\n",
            "326:\tlearn: 96.3837590\ttotal: 2m 45s\tremaining: 5m 18s\n",
            "327:\tlearn: 96.1517525\ttotal: 2m 45s\tremaining: 5m 18s\n",
            "328:\tlearn: 96.0757607\ttotal: 2m 46s\tremaining: 5m 18s\n",
            "329:\tlearn: 95.9550163\ttotal: 2m 46s\tremaining: 5m 17s\n",
            "330:\tlearn: 95.5879336\ttotal: 2m 47s\tremaining: 5m 17s\n",
            "331:\tlearn: 95.3111760\ttotal: 2m 47s\tremaining: 5m 16s\n",
            "332:\tlearn: 95.0346978\ttotal: 2m 48s\tremaining: 5m 16s\n",
            "333:\tlearn: 94.5446753\ttotal: 2m 48s\tremaining: 5m 15s\n",
            "334:\tlearn: 94.4290586\ttotal: 2m 49s\tremaining: 5m 15s\n",
            "335:\tlearn: 94.3262718\ttotal: 2m 49s\tremaining: 5m 15s\n",
            "336:\tlearn: 93.7891803\ttotal: 2m 50s\tremaining: 5m 14s\n",
            "337:\tlearn: 93.6537678\ttotal: 2m 51s\tremaining: 5m 14s\n",
            "338:\tlearn: 93.4936705\ttotal: 2m 51s\tremaining: 5m 14s\n",
            "339:\tlearn: 93.3885108\ttotal: 2m 52s\tremaining: 5m 13s\n",
            "340:\tlearn: 93.1230134\ttotal: 2m 52s\tremaining: 5m 13s\n",
            "341:\tlearn: 93.0153056\ttotal: 2m 53s\tremaining: 5m 13s\n",
            "342:\tlearn: 92.8960412\ttotal: 2m 53s\tremaining: 5m 12s\n",
            "343:\tlearn: 92.8526383\ttotal: 2m 54s\tremaining: 5m 11s\n",
            "344:\tlearn: 92.6819550\ttotal: 2m 55s\tremaining: 5m 11s\n",
            "345:\tlearn: 92.4240523\ttotal: 2m 55s\tremaining: 5m 11s\n",
            "346:\tlearn: 92.3794469\ttotal: 2m 56s\tremaining: 5m 10s\n",
            "347:\tlearn: 92.2333613\ttotal: 2m 56s\tremaining: 5m 10s\n",
            "348:\tlearn: 92.1007708\ttotal: 2m 57s\tremaining: 5m 9s\n",
            "349:\tlearn: 91.9640586\ttotal: 2m 57s\tremaining: 5m 9s\n",
            "350:\tlearn: 91.8978812\ttotal: 2m 58s\tremaining: 5m 8s\n",
            "351:\tlearn: 91.6068747\ttotal: 2m 58s\tremaining: 5m 8s\n",
            "352:\tlearn: 91.3761501\ttotal: 2m 59s\tremaining: 5m 7s\n",
            "353:\tlearn: 91.3351947\ttotal: 2m 59s\tremaining: 5m 7s\n",
            "354:\tlearn: 91.1689557\ttotal: 3m\tremaining: 5m 7s\n",
            "355:\tlearn: 91.1298692\ttotal: 3m 1s\tremaining: 5m 6s\n",
            "356:\tlearn: 90.9082228\ttotal: 3m 1s\tremaining: 5m 6s\n",
            "357:\tlearn: 90.8272418\ttotal: 3m 2s\tremaining: 5m 5s\n",
            "358:\tlearn: 90.7038507\ttotal: 3m 2s\tremaining: 5m 5s\n",
            "359:\tlearn: 90.5222182\ttotal: 3m 3s\tremaining: 5m 4s\n",
            "360:\tlearn: 90.4828169\ttotal: 3m 3s\tremaining: 5m 4s\n",
            "361:\tlearn: 90.3739167\ttotal: 3m 4s\tremaining: 5m 3s\n",
            "362:\tlearn: 90.1382219\ttotal: 3m 4s\tremaining: 5m 3s\n",
            "363:\tlearn: 90.0747880\ttotal: 3m 5s\tremaining: 5m 2s\n",
            "364:\tlearn: 89.9689715\ttotal: 3m 5s\tremaining: 5m 2s\n",
            "365:\tlearn: 89.8499644\ttotal: 3m 6s\tremaining: 5m 2s\n",
            "366:\tlearn: 89.6651438\ttotal: 3m 7s\tremaining: 5m 1s\n",
            "367:\tlearn: 89.6648620\ttotal: 3m 7s\tremaining: 5m 1s\n",
            "368:\tlearn: 89.4962972\ttotal: 3m 8s\tremaining: 5m 1s\n",
            "369:\tlearn: 89.4265616\ttotal: 3m 8s\tremaining: 5m\n",
            "370:\tlearn: 89.3480199\ttotal: 3m 9s\tremaining: 5m\n",
            "371:\tlearn: 89.2722604\ttotal: 3m 9s\tremaining: 4m 59s\n",
            "372:\tlearn: 89.2380172\ttotal: 3m 10s\tremaining: 4m 59s\n",
            "373:\tlearn: 89.1965868\ttotal: 3m 11s\tremaining: 4m 59s\n",
            "374:\tlearn: 89.1440634\ttotal: 3m 11s\tremaining: 4m 58s\n",
            "375:\tlearn: 89.0472870\ttotal: 3m 12s\tremaining: 4m 58s\n",
            "376:\tlearn: 88.8950123\ttotal: 3m 12s\tremaining: 4m 57s\n",
            "377:\tlearn: 88.6777592\ttotal: 3m 13s\tremaining: 4m 57s\n",
            "378:\tlearn: 88.5680420\ttotal: 3m 13s\tremaining: 4m 56s\n",
            "379:\tlearn: 88.2925358\ttotal: 3m 14s\tremaining: 4m 56s\n",
            "380:\tlearn: 88.1623974\ttotal: 3m 15s\tremaining: 4m 55s\n",
            "381:\tlearn: 88.1120453\ttotal: 3m 15s\tremaining: 4m 55s\n",
            "382:\tlearn: 88.0615488\ttotal: 3m 16s\tremaining: 4m 54s\n",
            "383:\tlearn: 88.0611641\ttotal: 3m 16s\tremaining: 4m 54s\n",
            "384:\tlearn: 87.9934158\ttotal: 3m 17s\tremaining: 4m 53s\n",
            "385:\tlearn: 87.7031816\ttotal: 3m 17s\tremaining: 4m 53s\n",
            "386:\tlearn: 87.5157258\ttotal: 3m 18s\tremaining: 4m 52s\n",
            "387:\tlearn: 87.4911556\ttotal: 3m 18s\tremaining: 4m 52s\n",
            "388:\tlearn: 87.3489066\ttotal: 3m 19s\tremaining: 4m 51s\n",
            "389:\tlearn: 87.1811493\ttotal: 3m 19s\tremaining: 4m 51s\n",
            "390:\tlearn: 87.0975191\ttotal: 3m 20s\tremaining: 4m 50s\n",
            "391:\tlearn: 87.0185110\ttotal: 3m 20s\tremaining: 4m 50s\n",
            "392:\tlearn: 86.9700507\ttotal: 3m 21s\tremaining: 4m 50s\n",
            "393:\tlearn: 86.8169799\ttotal: 3m 22s\tremaining: 4m 49s\n",
            "394:\tlearn: 86.7400452\ttotal: 3m 22s\tremaining: 4m 49s\n",
            "395:\tlearn: 86.4079850\ttotal: 3m 23s\tremaining: 4m 49s\n",
            "396:\tlearn: 86.3054021\ttotal: 3m 23s\tremaining: 4m 48s\n",
            "397:\tlearn: 86.1635768\ttotal: 3m 24s\tremaining: 4m 48s\n",
            "398:\tlearn: 86.0726967\ttotal: 3m 24s\tremaining: 4m 47s\n",
            "399:\tlearn: 86.0111417\ttotal: 3m 25s\tremaining: 4m 47s\n",
            "400:\tlearn: 85.9093677\ttotal: 3m 26s\tremaining: 4m 46s\n",
            "401:\tlearn: 85.8075997\ttotal: 3m 26s\tremaining: 4m 46s\n",
            "402:\tlearn: 85.8069534\ttotal: 3m 27s\tremaining: 4m 45s\n",
            "403:\tlearn: 85.7206243\ttotal: 3m 27s\tremaining: 4m 45s\n",
            "404:\tlearn: 85.6923938\ttotal: 3m 28s\tremaining: 4m 44s\n",
            "405:\tlearn: 85.5139692\ttotal: 3m 28s\tremaining: 4m 44s\n",
            "406:\tlearn: 85.0343673\ttotal: 3m 29s\tremaining: 4m 43s\n",
            "407:\tlearn: 84.9220421\ttotal: 3m 29s\tremaining: 4m 43s\n",
            "408:\tlearn: 84.8093441\ttotal: 3m 30s\tremaining: 4m 42s\n",
            "409:\tlearn: 84.6892968\ttotal: 3m 30s\tremaining: 4m 42s\n",
            "410:\tlearn: 84.6257804\ttotal: 3m 31s\tremaining: 4m 41s\n",
            "411:\tlearn: 84.2123270\ttotal: 3m 32s\tremaining: 4m 41s\n",
            "412:\tlearn: 84.0214915\ttotal: 3m 32s\tremaining: 4m 40s\n",
            "413:\tlearn: 83.9331984\ttotal: 3m 33s\tremaining: 4m 40s\n",
            "414:\tlearn: 83.8348958\ttotal: 3m 33s\tremaining: 4m 40s\n",
            "415:\tlearn: 83.7637964\ttotal: 3m 34s\tremaining: 4m 39s\n",
            "416:\tlearn: 83.7107705\ttotal: 3m 34s\tremaining: 4m 39s\n",
            "417:\tlearn: 83.5896444\ttotal: 3m 35s\tremaining: 4m 38s\n",
            "418:\tlearn: 83.4859242\ttotal: 3m 35s\tremaining: 4m 38s\n",
            "419:\tlearn: 83.1845806\ttotal: 3m 36s\tremaining: 4m 37s\n",
            "420:\tlearn: 83.1284789\ttotal: 3m 36s\tremaining: 4m 37s\n",
            "421:\tlearn: 82.9717179\ttotal: 3m 37s\tremaining: 4m 36s\n",
            "422:\tlearn: 82.9182203\ttotal: 3m 38s\tremaining: 4m 36s\n",
            "423:\tlearn: 82.8210399\ttotal: 3m 38s\tremaining: 4m 35s\n",
            "424:\tlearn: 82.7985993\ttotal: 3m 39s\tremaining: 4m 35s\n",
            "425:\tlearn: 82.6810117\ttotal: 3m 39s\tremaining: 4m 34s\n",
            "426:\tlearn: 82.5213449\ttotal: 3m 40s\tremaining: 4m 34s\n",
            "427:\tlearn: 82.4266284\ttotal: 3m 40s\tremaining: 4m 33s\n",
            "428:\tlearn: 82.1447373\ttotal: 3m 41s\tremaining: 4m 33s\n",
            "429:\tlearn: 82.1265181\ttotal: 3m 41s\tremaining: 4m 33s\n",
            "430:\tlearn: 81.9641285\ttotal: 3m 42s\tremaining: 4m 32s\n",
            "431:\tlearn: 81.8751153\ttotal: 3m 43s\tremaining: 4m 32s\n",
            "432:\tlearn: 81.8365070\ttotal: 3m 43s\tremaining: 4m 31s\n",
            "433:\tlearn: 81.7926871\ttotal: 3m 44s\tremaining: 4m 31s\n",
            "434:\tlearn: 81.4711525\ttotal: 3m 44s\tremaining: 4m 30s\n",
            "435:\tlearn: 81.1319099\ttotal: 3m 45s\tremaining: 4m 30s\n",
            "436:\tlearn: 80.8893786\ttotal: 3m 45s\tremaining: 4m 29s\n",
            "437:\tlearn: 80.7715088\ttotal: 3m 46s\tremaining: 4m 29s\n",
            "438:\tlearn: 80.7168593\ttotal: 3m 46s\tremaining: 4m 28s\n",
            "439:\tlearn: 80.6413033\ttotal: 3m 47s\tremaining: 4m 28s\n",
            "440:\tlearn: 80.5742468\ttotal: 3m 47s\tremaining: 4m 27s\n",
            "441:\tlearn: 80.4133785\ttotal: 3m 48s\tremaining: 4m 27s\n",
            "442:\tlearn: 80.3281984\ttotal: 3m 48s\tremaining: 4m 26s\n",
            "443:\tlearn: 80.2432226\ttotal: 3m 49s\tremaining: 4m 26s\n",
            "444:\tlearn: 80.1179456\ttotal: 3m 50s\tremaining: 4m 25s\n",
            "445:\tlearn: 80.0571311\ttotal: 3m 50s\tremaining: 4m 25s\n",
            "446:\tlearn: 80.0089314\ttotal: 3m 51s\tremaining: 4m 24s\n",
            "447:\tlearn: 79.9217744\ttotal: 3m 51s\tremaining: 4m 24s\n",
            "448:\tlearn: 79.7401931\ttotal: 3m 52s\tremaining: 4m 23s\n",
            "449:\tlearn: 79.6817704\ttotal: 3m 52s\tremaining: 4m 23s\n",
            "450:\tlearn: 79.6380370\ttotal: 3m 53s\tremaining: 4m 23s\n",
            "451:\tlearn: 79.4878888\ttotal: 3m 54s\tremaining: 4m 22s\n",
            "452:\tlearn: 79.3845412\ttotal: 3m 54s\tremaining: 4m 22s\n",
            "453:\tlearn: 79.3428200\ttotal: 3m 55s\tremaining: 4m 21s\n",
            "454:\tlearn: 79.1913095\ttotal: 3m 55s\tremaining: 4m 21s\n",
            "455:\tlearn: 78.9132354\ttotal: 3m 56s\tremaining: 4m 20s\n",
            "456:\tlearn: 78.7726202\ttotal: 3m 56s\tremaining: 4m 20s\n",
            "457:\tlearn: 78.6390554\ttotal: 3m 57s\tremaining: 4m 19s\n",
            "458:\tlearn: 78.5762392\ttotal: 3m 58s\tremaining: 4m 19s\n",
            "459:\tlearn: 78.4961521\ttotal: 3m 58s\tremaining: 4m 18s\n",
            "460:\tlearn: 78.4510607\ttotal: 3m 59s\tremaining: 4m 18s\n",
            "461:\tlearn: 78.3822169\ttotal: 3m 59s\tremaining: 4m 17s\n",
            "462:\tlearn: 78.3013831\ttotal: 4m\tremaining: 4m 17s\n",
            "463:\tlearn: 78.2341804\ttotal: 4m\tremaining: 4m 16s\n",
            "464:\tlearn: 78.1555939\ttotal: 4m 1s\tremaining: 4m 16s\n",
            "465:\tlearn: 78.1091851\ttotal: 4m 1s\tremaining: 4m 15s\n",
            "466:\tlearn: 78.0789279\ttotal: 4m 2s\tremaining: 4m 15s\n",
            "467:\tlearn: 78.0757491\ttotal: 4m 2s\tremaining: 4m 14s\n",
            "468:\tlearn: 77.8691570\ttotal: 4m 3s\tremaining: 4m 14s\n",
            "469:\tlearn: 77.7646113\ttotal: 4m 3s\tremaining: 4m 13s\n",
            "470:\tlearn: 77.7193151\ttotal: 4m 4s\tremaining: 4m 13s\n",
            "471:\tlearn: 77.6608383\ttotal: 4m 5s\tremaining: 4m 12s\n",
            "472:\tlearn: 77.4499238\ttotal: 4m 5s\tremaining: 4m 12s\n",
            "473:\tlearn: 77.1723334\ttotal: 4m 6s\tremaining: 4m 11s\n",
            "474:\tlearn: 77.0891400\ttotal: 4m 6s\tremaining: 4m 11s\n",
            "475:\tlearn: 77.0042201\ttotal: 4m 7s\tremaining: 4m 10s\n",
            "476:\tlearn: 76.9753064\ttotal: 4m 7s\tremaining: 4m 10s\n",
            "477:\tlearn: 76.9533040\ttotal: 4m 8s\tremaining: 4m 9s\n",
            "478:\tlearn: 76.9420111\ttotal: 4m 8s\tremaining: 4m 9s\n",
            "479:\tlearn: 76.8847422\ttotal: 4m 9s\tremaining: 4m 9s\n",
            "480:\tlearn: 76.8823198\ttotal: 4m 10s\tremaining: 4m 8s\n",
            "481:\tlearn: 76.8550332\ttotal: 4m 10s\tremaining: 4m 8s\n",
            "482:\tlearn: 76.7667852\ttotal: 4m 11s\tremaining: 4m 7s\n",
            "483:\tlearn: 76.7289138\ttotal: 4m 12s\tremaining: 4m 7s\n",
            "484:\tlearn: 76.6768740\ttotal: 4m 12s\tremaining: 4m 6s\n",
            "485:\tlearn: 76.6422591\ttotal: 4m 13s\tremaining: 4m 6s\n",
            "486:\tlearn: 76.4344111\ttotal: 4m 13s\tremaining: 4m 5s\n",
            "487:\tlearn: 76.3645155\ttotal: 4m 14s\tremaining: 4m 5s\n",
            "488:\tlearn: 76.3245680\ttotal: 4m 14s\tremaining: 4m 4s\n",
            "489:\tlearn: 76.2356463\ttotal: 4m 15s\tremaining: 4m 4s\n",
            "490:\tlearn: 76.1166724\ttotal: 4m 15s\tremaining: 4m 3s\n",
            "491:\tlearn: 76.0565673\ttotal: 4m 16s\tremaining: 4m 3s\n",
            "492:\tlearn: 75.9257390\ttotal: 4m 16s\tremaining: 4m 2s\n",
            "493:\tlearn: 75.8641358\ttotal: 4m 17s\tremaining: 4m 2s\n",
            "494:\tlearn: 75.8584808\ttotal: 4m 18s\tremaining: 4m 1s\n",
            "495:\tlearn: 75.7958416\ttotal: 4m 18s\tremaining: 4m 1s\n",
            "496:\tlearn: 75.6227040\ttotal: 4m 19s\tremaining: 4m\n",
            "497:\tlearn: 75.5092883\ttotal: 4m 19s\tremaining: 4m\n",
            "498:\tlearn: 75.4532081\ttotal: 4m 20s\tremaining: 3m 59s\n",
            "499:\tlearn: 75.2773868\ttotal: 4m 20s\tremaining: 3m 59s\n",
            "500:\tlearn: 75.1572391\ttotal: 4m 21s\tremaining: 3m 58s\n",
            "501:\tlearn: 75.0110273\ttotal: 4m 21s\tremaining: 3m 58s\n",
            "502:\tlearn: 74.7511066\ttotal: 4m 22s\tremaining: 3m 57s\n",
            "503:\tlearn: 74.7069928\ttotal: 4m 22s\tremaining: 3m 57s\n",
            "504:\tlearn: 74.6344798\ttotal: 4m 23s\tremaining: 3m 56s\n",
            "505:\tlearn: 74.5878940\ttotal: 4m 23s\tremaining: 3m 56s\n",
            "506:\tlearn: 74.4662825\ttotal: 4m 24s\tremaining: 3m 55s\n",
            "507:\tlearn: 74.4034103\ttotal: 4m 25s\tremaining: 3m 55s\n",
            "508:\tlearn: 74.2327730\ttotal: 4m 25s\tremaining: 3m 54s\n",
            "509:\tlearn: 74.1846846\ttotal: 4m 26s\tremaining: 3m 54s\n",
            "510:\tlearn: 74.0848211\ttotal: 4m 26s\tremaining: 3m 53s\n",
            "511:\tlearn: 73.9912750\ttotal: 4m 27s\tremaining: 3m 53s\n",
            "512:\tlearn: 73.9445792\ttotal: 4m 27s\tremaining: 3m 52s\n",
            "513:\tlearn: 73.7129489\ttotal: 4m 28s\tremaining: 3m 52s\n",
            "514:\tlearn: 73.6259066\ttotal: 4m 28s\tremaining: 3m 51s\n",
            "515:\tlearn: 73.5663957\ttotal: 4m 29s\tremaining: 3m 51s\n",
            "516:\tlearn: 73.4663755\ttotal: 4m 29s\tremaining: 3m 50s\n",
            "517:\tlearn: 73.3644511\ttotal: 4m 30s\tremaining: 3m 50s\n",
            "518:\tlearn: 73.2148495\ttotal: 4m 30s\tremaining: 3m 49s\n",
            "519:\tlearn: 73.1776563\ttotal: 4m 31s\tremaining: 3m 49s\n",
            "520:\tlearn: 73.1079078\ttotal: 4m 32s\tremaining: 3m 48s\n",
            "521:\tlearn: 73.0629079\ttotal: 4m 32s\tremaining: 3m 48s\n",
            "522:\tlearn: 73.0470123\ttotal: 4m 33s\tremaining: 3m 47s\n",
            "523:\tlearn: 72.8562796\ttotal: 4m 33s\tremaining: 3m 47s\n",
            "524:\tlearn: 72.5887225\ttotal: 4m 34s\tremaining: 3m 46s\n",
            "525:\tlearn: 72.5174247\ttotal: 4m 34s\tremaining: 3m 46s\n",
            "526:\tlearn: 72.3745509\ttotal: 4m 35s\tremaining: 3m 45s\n",
            "527:\tlearn: 72.3219402\ttotal: 4m 35s\tremaining: 3m 45s\n",
            "528:\tlearn: 72.1933439\ttotal: 4m 36s\tremaining: 3m 44s\n",
            "529:\tlearn: 72.1196393\ttotal: 4m 36s\tremaining: 3m 44s\n",
            "530:\tlearn: 72.0164716\ttotal: 4m 37s\tremaining: 3m 43s\n",
            "531:\tlearn: 71.8867910\ttotal: 4m 38s\tremaining: 3m 43s\n",
            "532:\tlearn: 71.8345746\ttotal: 4m 38s\tremaining: 3m 42s\n",
            "533:\tlearn: 71.6918493\ttotal: 4m 39s\tremaining: 3m 42s\n",
            "534:\tlearn: 71.6408301\ttotal: 4m 39s\tremaining: 3m 41s\n",
            "535:\tlearn: 71.5017156\ttotal: 4m 40s\tremaining: 3m 41s\n",
            "536:\tlearn: 71.4213257\ttotal: 4m 40s\tremaining: 3m 40s\n",
            "537:\tlearn: 71.3390308\ttotal: 4m 41s\tremaining: 3m 40s\n",
            "538:\tlearn: 71.3075225\ttotal: 4m 41s\tremaining: 3m 39s\n",
            "539:\tlearn: 71.2243463\ttotal: 4m 42s\tremaining: 3m 39s\n",
            "540:\tlearn: 71.1820704\ttotal: 4m 43s\tremaining: 3m 38s\n",
            "541:\tlearn: 71.1042438\ttotal: 4m 43s\tremaining: 3m 38s\n",
            "542:\tlearn: 71.0683807\ttotal: 4m 44s\tremaining: 3m 37s\n",
            "543:\tlearn: 71.0162879\ttotal: 4m 44s\tremaining: 3m 37s\n",
            "544:\tlearn: 70.9560362\ttotal: 4m 45s\tremaining: 3m 36s\n",
            "545:\tlearn: 70.8259050\ttotal: 4m 45s\tremaining: 3m 36s\n",
            "546:\tlearn: 70.7923865\ttotal: 4m 46s\tremaining: 3m 35s\n",
            "547:\tlearn: 70.7313702\ttotal: 4m 46s\tremaining: 3m 35s\n",
            "548:\tlearn: 70.6978511\ttotal: 4m 47s\tremaining: 3m 34s\n",
            "549:\tlearn: 70.6500216\ttotal: 4m 47s\tremaining: 3m 34s\n",
            "550:\tlearn: 70.5116722\ttotal: 4m 48s\tremaining: 3m 33s\n",
            "551:\tlearn: 70.4772541\ttotal: 4m 49s\tremaining: 3m 33s\n",
            "552:\tlearn: 70.4375783\ttotal: 4m 49s\tremaining: 3m 32s\n",
            "553:\tlearn: 70.3786720\ttotal: 4m 50s\tremaining: 3m 32s\n",
            "554:\tlearn: 70.3730538\ttotal: 4m 50s\tremaining: 3m 31s\n",
            "555:\tlearn: 70.2520917\ttotal: 4m 51s\tremaining: 3m 31s\n",
            "556:\tlearn: 70.0375815\ttotal: 4m 51s\tremaining: 3m 30s\n",
            "557:\tlearn: 69.9942756\ttotal: 4m 52s\tremaining: 3m 30s\n",
            "558:\tlearn: 69.8563328\ttotal: 4m 52s\tremaining: 3m 29s\n",
            "559:\tlearn: 69.8031202\ttotal: 4m 53s\tremaining: 3m 28s\n",
            "560:\tlearn: 69.6638208\ttotal: 4m 53s\tremaining: 3m 28s\n",
            "561:\tlearn: 69.4928729\ttotal: 4m 54s\tremaining: 3m 27s\n",
            "562:\tlearn: 69.3822122\ttotal: 4m 54s\tremaining: 3m 27s\n",
            "563:\tlearn: 69.3417484\ttotal: 4m 55s\tremaining: 3m 26s\n",
            "564:\tlearn: 69.2697813\ttotal: 4m 55s\tremaining: 3m 26s\n",
            "565:\tlearn: 69.2445108\ttotal: 4m 56s\tremaining: 3m 25s\n",
            "566:\tlearn: 69.1725558\ttotal: 4m 57s\tremaining: 3m 25s\n",
            "567:\tlearn: 69.0895626\ttotal: 4m 57s\tremaining: 3m 24s\n",
            "568:\tlearn: 69.0083311\ttotal: 4m 58s\tremaining: 3m 24s\n",
            "569:\tlearn: 68.9607032\ttotal: 4m 58s\tremaining: 3m 23s\n",
            "570:\tlearn: 68.9106525\ttotal: 4m 59s\tremaining: 3m 23s\n",
            "571:\tlearn: 68.8202242\ttotal: 5m\tremaining: 3m 23s\n",
            "572:\tlearn: 68.6531255\ttotal: 5m\tremaining: 3m 22s\n",
            "573:\tlearn: 68.4497345\ttotal: 5m 1s\tremaining: 3m 22s\n",
            "574:\tlearn: 68.4244854\ttotal: 5m 1s\tremaining: 3m 21s\n",
            "575:\tlearn: 68.3421485\ttotal: 5m 2s\tremaining: 3m 21s\n",
            "576:\tlearn: 68.2007596\ttotal: 5m 3s\tremaining: 3m 20s\n",
            "577:\tlearn: 68.1440707\ttotal: 5m 3s\tremaining: 3m 20s\n",
            "578:\tlearn: 68.0128467\ttotal: 5m 4s\tremaining: 3m 19s\n",
            "579:\tlearn: 67.9203080\ttotal: 5m 4s\tremaining: 3m 19s\n",
            "580:\tlearn: 67.8024897\ttotal: 5m 5s\tremaining: 3m 18s\n",
            "581:\tlearn: 67.7516853\ttotal: 5m 5s\tremaining: 3m 18s\n",
            "582:\tlearn: 67.6697596\ttotal: 5m 6s\tremaining: 3m 17s\n",
            "583:\tlearn: 67.6173970\ttotal: 5m 6s\tremaining: 3m 17s\n",
            "584:\tlearn: 67.5321911\ttotal: 5m 7s\tremaining: 3m 16s\n",
            "585:\tlearn: 67.4963538\ttotal: 5m 8s\tremaining: 3m 16s\n",
            "586:\tlearn: 67.3139227\ttotal: 5m 8s\tremaining: 3m 15s\n",
            "587:\tlearn: 67.3080638\ttotal: 5m 9s\tremaining: 3m 15s\n",
            "588:\tlearn: 67.1838494\ttotal: 5m 9s\tremaining: 3m 14s\n",
            "589:\tlearn: 67.1467868\ttotal: 5m 10s\tremaining: 3m 14s\n",
            "590:\tlearn: 67.0110999\ttotal: 5m 10s\tremaining: 3m 13s\n",
            "591:\tlearn: 66.9530280\ttotal: 5m 11s\tremaining: 3m 12s\n",
            "592:\tlearn: 66.9029571\ttotal: 5m 11s\tremaining: 3m 12s\n",
            "593:\tlearn: 66.8630166\ttotal: 5m 12s\tremaining: 3m 11s\n",
            "594:\tlearn: 66.7932809\ttotal: 5m 12s\tremaining: 3m 11s\n",
            "595:\tlearn: 66.7527218\ttotal: 5m 13s\tremaining: 3m 10s\n",
            "596:\tlearn: 66.7042025\ttotal: 5m 14s\tremaining: 3m 10s\n",
            "597:\tlearn: 66.5443404\ttotal: 5m 14s\tremaining: 3m 9s\n",
            "598:\tlearn: 66.4164905\ttotal: 5m 15s\tremaining: 3m 9s\n",
            "599:\tlearn: 66.3387161\ttotal: 5m 15s\tremaining: 3m 8s\n",
            "600:\tlearn: 66.2750592\ttotal: 5m 16s\tremaining: 3m 8s\n",
            "601:\tlearn: 66.1433097\ttotal: 5m 16s\tremaining: 3m 7s\n",
            "602:\tlearn: 66.0850357\ttotal: 5m 17s\tremaining: 3m 7s\n",
            "603:\tlearn: 66.0170039\ttotal: 5m 18s\tremaining: 3m 7s\n",
            "604:\tlearn: 65.9744230\ttotal: 5m 18s\tremaining: 3m 6s\n",
            "605:\tlearn: 65.8598722\ttotal: 5m 19s\tremaining: 3m 5s\n",
            "606:\tlearn: 65.7848032\ttotal: 5m 19s\tremaining: 3m 5s\n",
            "607:\tlearn: 65.6515555\ttotal: 5m 20s\tremaining: 3m 4s\n",
            "608:\tlearn: 65.5802125\ttotal: 5m 20s\tremaining: 3m 4s\n",
            "609:\tlearn: 65.4738596\ttotal: 5m 21s\tremaining: 3m 3s\n",
            "610:\tlearn: 65.4040542\ttotal: 5m 21s\tremaining: 3m 3s\n",
            "611:\tlearn: 65.3348539\ttotal: 5m 22s\tremaining: 3m 2s\n",
            "612:\tlearn: 65.2857303\ttotal: 5m 22s\tremaining: 3m 2s\n",
            "613:\tlearn: 65.2228732\ttotal: 5m 23s\tremaining: 3m 1s\n",
            "614:\tlearn: 65.0803114\ttotal: 5m 23s\tremaining: 3m 1s\n",
            "615:\tlearn: 65.0341262\ttotal: 5m 24s\tremaining: 3m\n",
            "616:\tlearn: 64.8815040\ttotal: 5m 25s\tremaining: 3m\n",
            "617:\tlearn: 64.8159448\ttotal: 5m 25s\tremaining: 2m 59s\n",
            "618:\tlearn: 64.7467872\ttotal: 5m 26s\tremaining: 2m 59s\n",
            "619:\tlearn: 64.6413489\ttotal: 5m 26s\tremaining: 2m 58s\n",
            "620:\tlearn: 64.6151168\ttotal: 5m 27s\tremaining: 2m 58s\n",
            "621:\tlearn: 64.5633506\ttotal: 5m 27s\tremaining: 2m 57s\n",
            "622:\tlearn: 64.4475625\ttotal: 5m 28s\tremaining: 2m 57s\n",
            "623:\tlearn: 64.4033531\ttotal: 5m 28s\tremaining: 2m 56s\n",
            "624:\tlearn: 64.3628297\ttotal: 5m 29s\tremaining: 2m 56s\n",
            "625:\tlearn: 64.3323421\ttotal: 5m 30s\tremaining: 2m 55s\n",
            "626:\tlearn: 64.2637988\ttotal: 5m 30s\tremaining: 2m 55s\n",
            "627:\tlearn: 64.2298186\ttotal: 5m 31s\tremaining: 2m 54s\n",
            "628:\tlearn: 64.0908527\ttotal: 5m 31s\tremaining: 2m 54s\n",
            "629:\tlearn: 63.8800622\ttotal: 5m 32s\tremaining: 2m 53s\n",
            "630:\tlearn: 63.8215136\ttotal: 5m 33s\tremaining: 2m 53s\n",
            "631:\tlearn: 63.7875540\ttotal: 5m 33s\tremaining: 2m 52s\n",
            "632:\tlearn: 63.7705212\ttotal: 5m 34s\tremaining: 2m 52s\n",
            "633:\tlearn: 63.7030815\ttotal: 5m 34s\tremaining: 2m 51s\n",
            "634:\tlearn: 63.6634512\ttotal: 5m 35s\tremaining: 2m 51s\n",
            "635:\tlearn: 63.6192358\ttotal: 5m 36s\tremaining: 2m 50s\n",
            "636:\tlearn: 63.4584540\ttotal: 5m 36s\tremaining: 2m 50s\n",
            "637:\tlearn: 63.2673538\ttotal: 5m 37s\tremaining: 2m 49s\n",
            "638:\tlearn: 63.2210926\ttotal: 5m 37s\tremaining: 2m 49s\n",
            "639:\tlearn: 63.2046896\ttotal: 5m 38s\tremaining: 2m 48s\n",
            "640:\tlearn: 63.1458264\ttotal: 5m 38s\tremaining: 2m 48s\n",
            "641:\tlearn: 63.0509755\ttotal: 5m 39s\tremaining: 2m 47s\n",
            "642:\tlearn: 62.9986593\ttotal: 5m 39s\tremaining: 2m 47s\n",
            "643:\tlearn: 62.9594824\ttotal: 5m 40s\tremaining: 2m 46s\n",
            "644:\tlearn: 62.9226794\ttotal: 5m 40s\tremaining: 2m 45s\n",
            "645:\tlearn: 62.8594082\ttotal: 5m 41s\tremaining: 2m 45s\n",
            "646:\tlearn: 62.8415193\ttotal: 5m 42s\tremaining: 2m 44s\n",
            "647:\tlearn: 62.8407227\ttotal: 5m 42s\tremaining: 2m 44s\n",
            "648:\tlearn: 62.8009729\ttotal: 5m 43s\tremaining: 2m 43s\n",
            "649:\tlearn: 62.7715868\ttotal: 5m 43s\tremaining: 2m 43s\n",
            "650:\tlearn: 62.7419411\ttotal: 5m 44s\tremaining: 2m 42s\n",
            "651:\tlearn: 62.6170520\ttotal: 5m 44s\tremaining: 2m 42s\n",
            "652:\tlearn: 62.5148843\ttotal: 5m 45s\tremaining: 2m 41s\n",
            "653:\tlearn: 62.4830360\ttotal: 5m 45s\tremaining: 2m 41s\n",
            "654:\tlearn: 62.4820451\ttotal: 5m 46s\tremaining: 2m 40s\n",
            "655:\tlearn: 62.4750920\ttotal: 5m 47s\tremaining: 2m 40s\n",
            "656:\tlearn: 62.4524280\ttotal: 5m 47s\tremaining: 2m 39s\n",
            "657:\tlearn: 62.3888188\ttotal: 5m 48s\tremaining: 2m 39s\n",
            "658:\tlearn: 62.3350027\ttotal: 5m 48s\tremaining: 2m 38s\n",
            "659:\tlearn: 62.3150711\ttotal: 5m 49s\tremaining: 2m 38s\n",
            "660:\tlearn: 62.2650514\ttotal: 5m 49s\tremaining: 2m 37s\n",
            "661:\tlearn: 62.2631367\ttotal: 5m 50s\tremaining: 2m 37s\n",
            "662:\tlearn: 62.2400313\ttotal: 5m 51s\tremaining: 2m 36s\n",
            "663:\tlearn: 62.1922742\ttotal: 5m 51s\tremaining: 2m 36s\n",
            "664:\tlearn: 62.1528248\ttotal: 5m 52s\tremaining: 2m 35s\n",
            "665:\tlearn: 62.0344668\ttotal: 5m 52s\tremaining: 2m 35s\n",
            "666:\tlearn: 61.9825772\ttotal: 5m 53s\tremaining: 2m 34s\n",
            "667:\tlearn: 61.8756782\ttotal: 5m 53s\tremaining: 2m 34s\n",
            "668:\tlearn: 61.7868678\ttotal: 5m 54s\tremaining: 2m 33s\n",
            "669:\tlearn: 61.7240339\ttotal: 5m 54s\tremaining: 2m 33s\n",
            "670:\tlearn: 61.7086799\ttotal: 5m 55s\tremaining: 2m 32s\n",
            "671:\tlearn: 61.5908384\ttotal: 5m 55s\tremaining: 2m 32s\n",
            "672:\tlearn: 61.5149081\ttotal: 5m 56s\tremaining: 2m 31s\n",
            "673:\tlearn: 61.4705036\ttotal: 5m 57s\tremaining: 2m 31s\n",
            "674:\tlearn: 61.3886497\ttotal: 5m 57s\tremaining: 2m 30s\n",
            "675:\tlearn: 61.3034544\ttotal: 5m 58s\tremaining: 2m 29s\n",
            "676:\tlearn: 61.2969194\ttotal: 5m 58s\tremaining: 2m 29s\n",
            "677:\tlearn: 61.2187202\ttotal: 5m 59s\tremaining: 2m 28s\n",
            "678:\tlearn: 61.1981972\ttotal: 5m 59s\tremaining: 2m 28s\n",
            "679:\tlearn: 61.0593308\ttotal: 6m\tremaining: 2m 27s\n",
            "680:\tlearn: 60.8962555\ttotal: 6m 1s\tremaining: 2m 27s\n",
            "681:\tlearn: 60.8366227\ttotal: 6m 1s\tremaining: 2m 26s\n",
            "682:\tlearn: 60.7866242\ttotal: 6m 2s\tremaining: 2m 26s\n",
            "683:\tlearn: 60.7480279\ttotal: 6m 2s\tremaining: 2m 25s\n",
            "684:\tlearn: 60.7169165\ttotal: 6m 3s\tremaining: 2m 25s\n",
            "685:\tlearn: 60.6536561\ttotal: 6m 3s\tremaining: 2m 24s\n",
            "686:\tlearn: 60.6325565\ttotal: 6m 4s\tremaining: 2m 24s\n",
            "687:\tlearn: 60.5942275\ttotal: 6m 5s\tremaining: 2m 23s\n",
            "688:\tlearn: 60.5728031\ttotal: 6m 5s\tremaining: 2m 23s\n",
            "689:\tlearn: 60.5337290\ttotal: 6m 6s\tremaining: 2m 22s\n",
            "690:\tlearn: 60.5104123\ttotal: 6m 6s\tremaining: 2m 22s\n",
            "691:\tlearn: 60.4577397\ttotal: 6m 7s\tremaining: 2m 21s\n",
            "692:\tlearn: 60.3734112\ttotal: 6m 7s\tremaining: 2m 21s\n",
            "693:\tlearn: 60.2559179\ttotal: 6m 8s\tremaining: 2m 20s\n",
            "694:\tlearn: 60.2240235\ttotal: 6m 8s\tremaining: 2m 20s\n",
            "695:\tlearn: 60.1818368\ttotal: 6m 9s\tremaining: 2m 19s\n",
            "696:\tlearn: 60.1535536\ttotal: 6m 9s\tremaining: 2m 19s\n",
            "697:\tlearn: 60.1019184\ttotal: 6m 10s\tremaining: 2m 18s\n",
            "698:\tlearn: 60.0694475\ttotal: 6m 11s\tremaining: 2m 18s\n",
            "699:\tlearn: 60.0244118\ttotal: 6m 11s\tremaining: 2m 17s\n",
            "700:\tlearn: 59.9885618\ttotal: 6m 12s\tremaining: 2m 16s\n",
            "701:\tlearn: 59.9470693\ttotal: 6m 12s\tremaining: 2m 16s\n",
            "702:\tlearn: 59.8977140\ttotal: 6m 13s\tremaining: 2m 15s\n",
            "703:\tlearn: 59.8401306\ttotal: 6m 13s\tremaining: 2m 15s\n",
            "704:\tlearn: 59.7089196\ttotal: 6m 14s\tremaining: 2m 14s\n",
            "705:\tlearn: 59.6562428\ttotal: 6m 14s\tremaining: 2m 14s\n",
            "706:\tlearn: 59.6299332\ttotal: 6m 15s\tremaining: 2m 13s\n",
            "707:\tlearn: 59.5021313\ttotal: 6m 15s\tremaining: 2m 13s\n",
            "708:\tlearn: 59.4566095\ttotal: 6m 16s\tremaining: 2m 12s\n",
            "709:\tlearn: 59.3796600\ttotal: 6m 16s\tremaining: 2m 12s\n",
            "710:\tlearn: 59.3378348\ttotal: 6m 17s\tremaining: 2m 11s\n",
            "711:\tlearn: 59.2865215\ttotal: 6m 17s\tremaining: 2m 11s\n",
            "712:\tlearn: 59.1783416\ttotal: 6m 18s\tremaining: 2m 10s\n",
            "713:\tlearn: 59.1543989\ttotal: 6m 19s\tremaining: 2m 10s\n",
            "714:\tlearn: 59.0896500\ttotal: 6m 19s\tremaining: 2m 9s\n",
            "715:\tlearn: 59.0252351\ttotal: 6m 20s\tremaining: 2m 9s\n",
            "716:\tlearn: 58.9713438\ttotal: 6m 20s\tremaining: 2m 8s\n",
            "717:\tlearn: 58.8983418\ttotal: 6m 21s\tremaining: 2m 8s\n",
            "718:\tlearn: 58.8462891\ttotal: 6m 22s\tremaining: 2m 7s\n",
            "719:\tlearn: 58.6804346\ttotal: 6m 22s\tremaining: 2m 6s\n",
            "720:\tlearn: 58.6355987\ttotal: 6m 23s\tremaining: 2m 6s\n",
            "721:\tlearn: 58.6108043\ttotal: 6m 23s\tremaining: 2m 5s\n",
            "722:\tlearn: 58.5082054\ttotal: 6m 24s\tremaining: 2m 5s\n",
            "723:\tlearn: 58.3926006\ttotal: 6m 24s\tremaining: 2m 4s\n",
            "724:\tlearn: 58.3483004\ttotal: 6m 25s\tremaining: 2m 4s\n",
            "725:\tlearn: 58.2935606\ttotal: 6m 25s\tremaining: 2m 3s\n",
            "726:\tlearn: 58.2263822\ttotal: 6m 26s\tremaining: 2m 3s\n",
            "727:\tlearn: 58.2006234\ttotal: 6m 26s\tremaining: 2m 2s\n",
            "728:\tlearn: 58.1563267\ttotal: 6m 27s\tremaining: 2m 2s\n",
            "729:\tlearn: 58.1337750\ttotal: 6m 27s\tremaining: 2m 1s\n",
            "730:\tlearn: 58.0566642\ttotal: 6m 28s\tremaining: 2m 1s\n",
            "731:\tlearn: 58.0334422\ttotal: 6m 28s\tremaining: 2m\n",
            "732:\tlearn: 57.9650787\ttotal: 6m 29s\tremaining: 2m\n",
            "733:\tlearn: 57.8972839\ttotal: 6m 29s\tremaining: 1m 59s\n",
            "734:\tlearn: 57.8719933\ttotal: 6m 30s\tremaining: 1m 59s\n",
            "735:\tlearn: 57.8398414\ttotal: 6m 31s\tremaining: 1m 58s\n",
            "736:\tlearn: 57.7992863\ttotal: 6m 31s\tremaining: 1m 57s\n",
            "737:\tlearn: 57.7685148\ttotal: 6m 32s\tremaining: 1m 57s\n",
            "738:\tlearn: 57.6992521\ttotal: 6m 32s\tremaining: 1m 56s\n",
            "739:\tlearn: 57.6116060\ttotal: 6m 33s\tremaining: 1m 56s\n",
            "740:\tlearn: 57.5072281\ttotal: 6m 33s\tremaining: 1m 55s\n",
            "741:\tlearn: 57.4609934\ttotal: 6m 34s\tremaining: 1m 55s\n",
            "742:\tlearn: 57.3880160\ttotal: 6m 34s\tremaining: 1m 54s\n",
            "743:\tlearn: 57.3358936\ttotal: 6m 35s\tremaining: 1m 54s\n",
            "744:\tlearn: 57.3064333\ttotal: 6m 36s\tremaining: 1m 53s\n",
            "745:\tlearn: 57.2048706\ttotal: 6m 36s\tremaining: 1m 53s\n",
            "746:\tlearn: 57.1495990\ttotal: 6m 37s\tremaining: 1m 52s\n",
            "747:\tlearn: 57.1253221\ttotal: 6m 37s\tremaining: 1m 52s\n",
            "748:\tlearn: 57.0702326\ttotal: 6m 38s\tremaining: 1m 51s\n",
            "749:\tlearn: 57.0374587\ttotal: 6m 38s\tremaining: 1m 51s\n",
            "750:\tlearn: 56.9902067\ttotal: 6m 39s\tremaining: 1m 50s\n",
            "751:\tlearn: 56.9534806\ttotal: 6m 40s\tremaining: 1m 50s\n",
            "752:\tlearn: 56.9177821\ttotal: 6m 40s\tremaining: 1m 49s\n",
            "753:\tlearn: 56.8755873\ttotal: 6m 41s\tremaining: 1m 49s\n",
            "754:\tlearn: 56.8482080\ttotal: 6m 41s\tremaining: 1m 48s\n",
            "755:\tlearn: 56.8089963\ttotal: 6m 42s\tremaining: 1m 47s\n",
            "756:\tlearn: 56.7671581\ttotal: 6m 42s\tremaining: 1m 47s\n",
            "757:\tlearn: 56.7240171\ttotal: 6m 43s\tremaining: 1m 46s\n",
            "758:\tlearn: 56.7198605\ttotal: 6m 43s\tremaining: 1m 46s\n",
            "759:\tlearn: 56.6945786\ttotal: 6m 44s\tremaining: 1m 45s\n",
            "760:\tlearn: 56.6064983\ttotal: 6m 44s\tremaining: 1m 45s\n",
            "761:\tlearn: 56.5227769\ttotal: 6m 45s\tremaining: 1m 44s\n",
            "762:\tlearn: 56.4527133\ttotal: 6m 45s\tremaining: 1m 44s\n",
            "763:\tlearn: 56.4072021\ttotal: 6m 46s\tremaining: 1m 43s\n",
            "764:\tlearn: 56.3797398\ttotal: 6m 46s\tremaining: 1m 43s\n",
            "765:\tlearn: 56.3136810\ttotal: 6m 47s\tremaining: 1m 42s\n",
            "766:\tlearn: 56.2855496\ttotal: 6m 48s\tremaining: 1m 42s\n",
            "767:\tlearn: 56.2607946\ttotal: 6m 48s\tremaining: 1m 41s\n",
            "768:\tlearn: 56.2444935\ttotal: 6m 49s\tremaining: 1m 41s\n",
            "769:\tlearn: 56.1184185\ttotal: 6m 49s\tremaining: 1m 40s\n",
            "770:\tlearn: 56.0752619\ttotal: 6m 50s\tremaining: 1m 40s\n",
            "771:\tlearn: 56.0201576\ttotal: 6m 51s\tremaining: 1m 39s\n",
            "772:\tlearn: 55.9836206\ttotal: 6m 51s\tremaining: 1m 39s\n",
            "773:\tlearn: 55.9554086\ttotal: 6m 52s\tremaining: 1m 38s\n",
            "774:\tlearn: 55.9182582\ttotal: 6m 52s\tremaining: 1m 38s\n",
            "775:\tlearn: 55.8191614\ttotal: 6m 53s\tremaining: 1m 37s\n",
            "776:\tlearn: 55.7508574\ttotal: 6m 53s\tremaining: 1m 36s\n",
            "777:\tlearn: 55.7209558\ttotal: 6m 54s\tremaining: 1m 36s\n",
            "778:\tlearn: 55.6945392\ttotal: 6m 54s\tremaining: 1m 35s\n",
            "779:\tlearn: 55.6570846\ttotal: 6m 55s\tremaining: 1m 35s\n",
            "780:\tlearn: 55.6174292\ttotal: 6m 56s\tremaining: 1m 34s\n",
            "781:\tlearn: 55.5237602\ttotal: 6m 56s\tremaining: 1m 34s\n",
            "782:\tlearn: 55.4192181\ttotal: 6m 57s\tremaining: 1m 33s\n",
            "783:\tlearn: 55.3802243\ttotal: 6m 57s\tremaining: 1m 33s\n",
            "784:\tlearn: 55.3200829\ttotal: 6m 58s\tremaining: 1m 32s\n",
            "785:\tlearn: 55.2373974\ttotal: 6m 58s\tremaining: 1m 32s\n",
            "786:\tlearn: 55.2030163\ttotal: 6m 59s\tremaining: 1m 31s\n",
            "787:\tlearn: 55.1535302\ttotal: 6m 59s\tremaining: 1m 31s\n",
            "788:\tlearn: 55.0692708\ttotal: 7m\tremaining: 1m 30s\n",
            "789:\tlearn: 55.0211799\ttotal: 7m\tremaining: 1m 30s\n",
            "790:\tlearn: 54.9909039\ttotal: 7m 1s\tremaining: 1m 29s\n",
            "791:\tlearn: 54.9459020\ttotal: 7m 1s\tremaining: 1m 28s\n",
            "792:\tlearn: 54.9454417\ttotal: 7m 2s\tremaining: 1m 28s\n",
            "793:\tlearn: 54.9238012\ttotal: 7m 3s\tremaining: 1m 27s\n",
            "794:\tlearn: 54.8079025\ttotal: 7m 3s\tremaining: 1m 27s\n",
            "795:\tlearn: 54.7652005\ttotal: 7m 4s\tremaining: 1m 26s\n",
            "796:\tlearn: 54.7272408\ttotal: 7m 4s\tremaining: 1m 26s\n",
            "797:\tlearn: 54.6924977\ttotal: 7m 5s\tremaining: 1m 25s\n",
            "798:\tlearn: 54.6469337\ttotal: 7m 5s\tremaining: 1m 25s\n",
            "799:\tlearn: 54.5387213\ttotal: 7m 6s\tremaining: 1m 24s\n",
            "800:\tlearn: 54.4638620\ttotal: 7m 6s\tremaining: 1m 24s\n",
            "801:\tlearn: 54.4186189\ttotal: 7m 7s\tremaining: 1m 23s\n",
            "802:\tlearn: 54.3879650\ttotal: 7m 8s\tremaining: 1m 23s\n",
            "803:\tlearn: 54.2866152\ttotal: 7m 8s\tremaining: 1m 22s\n",
            "804:\tlearn: 54.2398837\ttotal: 7m 9s\tremaining: 1m 22s\n",
            "805:\tlearn: 54.2245576\ttotal: 7m 9s\tremaining: 1m 21s\n",
            "806:\tlearn: 54.1819150\ttotal: 7m 10s\tremaining: 1m 21s\n",
            "807:\tlearn: 54.1756538\ttotal: 7m 10s\tremaining: 1m 20s\n",
            "808:\tlearn: 54.0114516\ttotal: 7m 11s\tremaining: 1m 19s\n",
            "809:\tlearn: 53.9870294\ttotal: 7m 12s\tremaining: 1m 19s\n",
            "810:\tlearn: 53.9582178\ttotal: 7m 12s\tremaining: 1m 18s\n",
            "811:\tlearn: 53.9045196\ttotal: 7m 13s\tremaining: 1m 18s\n",
            "812:\tlearn: 53.8945254\ttotal: 7m 13s\tremaining: 1m 17s\n",
            "813:\tlearn: 53.8709197\ttotal: 7m 14s\tremaining: 1m 17s\n",
            "814:\tlearn: 53.8048215\ttotal: 7m 14s\tremaining: 1m 16s\n",
            "815:\tlearn: 53.7677623\ttotal: 7m 15s\tremaining: 1m 16s\n",
            "816:\tlearn: 53.6568878\ttotal: 7m 15s\tremaining: 1m 15s\n",
            "817:\tlearn: 53.6183823\ttotal: 7m 16s\tremaining: 1m 15s\n",
            "818:\tlearn: 53.5855283\ttotal: 7m 16s\tremaining: 1m 14s\n",
            "819:\tlearn: 53.5677247\ttotal: 7m 17s\tremaining: 1m 14s\n",
            "820:\tlearn: 53.5359292\ttotal: 7m 17s\tremaining: 1m 13s\n",
            "821:\tlearn: 53.5162166\ttotal: 7m 18s\tremaining: 1m 13s\n",
            "822:\tlearn: 53.4856170\ttotal: 7m 19s\tremaining: 1m 12s\n",
            "823:\tlearn: 53.4100259\ttotal: 7m 19s\tremaining: 1m 12s\n",
            "824:\tlearn: 53.3740729\ttotal: 7m 20s\tremaining: 1m 11s\n",
            "825:\tlearn: 53.3540754\ttotal: 7m 20s\tremaining: 1m 10s\n",
            "826:\tlearn: 53.3083132\ttotal: 7m 21s\tremaining: 1m 10s\n",
            "827:\tlearn: 53.2674231\ttotal: 7m 21s\tremaining: 1m 9s\n",
            "828:\tlearn: 53.2553672\ttotal: 7m 22s\tremaining: 1m 9s\n",
            "829:\tlearn: 53.1571975\ttotal: 7m 22s\tremaining: 1m 8s\n",
            "830:\tlearn: 53.1249366\ttotal: 7m 23s\tremaining: 1m 8s\n",
            "831:\tlearn: 53.0939941\ttotal: 7m 24s\tremaining: 1m 7s\n",
            "832:\tlearn: 53.0529933\ttotal: 7m 24s\tremaining: 1m 7s\n",
            "833:\tlearn: 53.0342127\ttotal: 7m 25s\tremaining: 1m 6s\n",
            "834:\tlearn: 52.9976689\ttotal: 7m 25s\tremaining: 1m 6s\n",
            "835:\tlearn: 52.9742311\ttotal: 7m 26s\tremaining: 1m 5s\n",
            "836:\tlearn: 52.9384105\ttotal: 7m 26s\tremaining: 1m 5s\n",
            "837:\tlearn: 52.8363619\ttotal: 7m 27s\tremaining: 1m 4s\n",
            "838:\tlearn: 52.7861372\ttotal: 7m 27s\tremaining: 1m 4s\n",
            "839:\tlearn: 52.7646455\ttotal: 7m 28s\tremaining: 1m 3s\n",
            "840:\tlearn: 52.7310622\ttotal: 7m 28s\tremaining: 1m 2s\n",
            "841:\tlearn: 52.6965077\ttotal: 7m 29s\tremaining: 1m 2s\n",
            "842:\tlearn: 52.6818126\ttotal: 7m 30s\tremaining: 1m 1s\n",
            "843:\tlearn: 52.6217344\ttotal: 7m 30s\tremaining: 1m 1s\n",
            "844:\tlearn: 52.5601413\ttotal: 7m 31s\tremaining: 1m\n",
            "845:\tlearn: 52.5295260\ttotal: 7m 31s\tremaining: 1m\n",
            "846:\tlearn: 52.5215430\ttotal: 7m 32s\tremaining: 59.8s\n",
            "847:\tlearn: 52.4903676\ttotal: 7m 32s\tremaining: 59.3s\n",
            "848:\tlearn: 52.4689843\ttotal: 7m 33s\tremaining: 58.7s\n",
            "849:\tlearn: 52.4407390\ttotal: 7m 34s\tremaining: 58.2s\n",
            "850:\tlearn: 52.4079351\ttotal: 7m 34s\tremaining: 57.7s\n",
            "851:\tlearn: 52.3403820\ttotal: 7m 35s\tremaining: 57.2s\n",
            "852:\tlearn: 52.2953581\ttotal: 7m 35s\tremaining: 56.6s\n",
            "853:\tlearn: 52.2951673\ttotal: 7m 36s\tremaining: 56.1s\n",
            "854:\tlearn: 52.2480948\ttotal: 7m 36s\tremaining: 55.6s\n",
            "855:\tlearn: 52.2312447\ttotal: 7m 37s\tremaining: 55s\n",
            "856:\tlearn: 52.1447451\ttotal: 7m 37s\tremaining: 54.5s\n",
            "857:\tlearn: 52.0998280\ttotal: 7m 38s\tremaining: 54s\n",
            "858:\tlearn: 52.0664206\ttotal: 7m 39s\tremaining: 53.4s\n",
            "859:\tlearn: 52.0252588\ttotal: 7m 39s\tremaining: 52.9s\n",
            "860:\tlearn: 51.9702256\ttotal: 7m 40s\tremaining: 52.4s\n",
            "861:\tlearn: 51.9349259\ttotal: 7m 40s\tremaining: 51.9s\n",
            "862:\tlearn: 51.9137791\ttotal: 7m 41s\tremaining: 51.3s\n",
            "863:\tlearn: 51.8349015\ttotal: 7m 41s\tremaining: 50.8s\n",
            "864:\tlearn: 51.7880288\ttotal: 7m 42s\tremaining: 50.3s\n",
            "865:\tlearn: 51.7554927\ttotal: 7m 43s\tremaining: 49.7s\n",
            "866:\tlearn: 51.6721437\ttotal: 7m 43s\tremaining: 49.2s\n",
            "867:\tlearn: 51.6195723\ttotal: 7m 44s\tremaining: 48.7s\n",
            "868:\tlearn: 51.6076951\ttotal: 7m 44s\tremaining: 48.1s\n",
            "869:\tlearn: 51.5961078\ttotal: 7m 45s\tremaining: 47.6s\n",
            "870:\tlearn: 51.5001133\ttotal: 7m 45s\tremaining: 47.1s\n",
            "871:\tlearn: 51.4511761\ttotal: 7m 46s\tremaining: 46.5s\n",
            "872:\tlearn: 51.4398447\ttotal: 7m 46s\tremaining: 46s\n",
            "873:\tlearn: 51.3783130\ttotal: 7m 47s\tremaining: 45.5s\n",
            "874:\tlearn: 51.3348779\ttotal: 7m 48s\tremaining: 44.9s\n",
            "875:\tlearn: 51.2557854\ttotal: 7m 48s\tremaining: 44.4s\n",
            "876:\tlearn: 51.2299085\ttotal: 7m 49s\tremaining: 43.9s\n",
            "877:\tlearn: 51.1803354\ttotal: 7m 49s\tremaining: 43.3s\n",
            "878:\tlearn: 51.1283192\ttotal: 7m 50s\tremaining: 42.8s\n",
            "879:\tlearn: 51.1051467\ttotal: 7m 50s\tremaining: 42.3s\n",
            "880:\tlearn: 51.0635339\ttotal: 7m 51s\tremaining: 41.7s\n",
            "881:\tlearn: 51.0349282\ttotal: 7m 51s\tremaining: 41.2s\n",
            "882:\tlearn: 50.9691543\ttotal: 7m 52s\tremaining: 40.7s\n",
            "883:\tlearn: 50.9532750\ttotal: 7m 52s\tremaining: 40.1s\n",
            "884:\tlearn: 50.9199977\ttotal: 7m 53s\tremaining: 39.6s\n",
            "885:\tlearn: 50.8327475\ttotal: 7m 53s\tremaining: 39.1s\n",
            "886:\tlearn: 50.8088601\ttotal: 7m 54s\tremaining: 38.5s\n",
            "887:\tlearn: 50.7804297\ttotal: 7m 55s\tremaining: 38s\n",
            "888:\tlearn: 50.7517075\ttotal: 7m 55s\tremaining: 37.5s\n",
            "889:\tlearn: 50.7437721\ttotal: 7m 56s\tremaining: 36.9s\n",
            "890:\tlearn: 50.7117835\ttotal: 7m 57s\tremaining: 36.4s\n",
            "891:\tlearn: 50.6514324\ttotal: 7m 57s\tremaining: 35.9s\n",
            "892:\tlearn: 50.5815811\ttotal: 7m 58s\tremaining: 35.3s\n",
            "893:\tlearn: 50.5444211\ttotal: 7m 58s\tremaining: 34.8s\n",
            "894:\tlearn: 50.4836816\ttotal: 7m 59s\tremaining: 34.3s\n",
            "895:\tlearn: 50.4479854\ttotal: 7m 59s\tremaining: 33.7s\n",
            "896:\tlearn: 50.3925495\ttotal: 8m\tremaining: 33.2s\n",
            "897:\tlearn: 50.2485321\ttotal: 8m\tremaining: 32.7s\n",
            "898:\tlearn: 50.2116945\ttotal: 8m 1s\tremaining: 32.1s\n",
            "899:\tlearn: 50.1440765\ttotal: 8m 1s\tremaining: 31.6s\n",
            "900:\tlearn: 50.1029553\ttotal: 8m 2s\tremaining: 31.1s\n",
            "901:\tlearn: 50.0440522\ttotal: 8m 2s\tremaining: 30.5s\n",
            "902:\tlearn: 50.0035988\ttotal: 8m 3s\tremaining: 30s\n",
            "903:\tlearn: 49.9549932\ttotal: 8m 4s\tremaining: 29.5s\n",
            "904:\tlearn: 49.9044756\ttotal: 8m 4s\tremaining: 28.9s\n",
            "905:\tlearn: 49.8769374\ttotal: 8m 5s\tremaining: 28.4s\n",
            "906:\tlearn: 49.8577730\ttotal: 8m 5s\tremaining: 27.8s\n",
            "907:\tlearn: 49.7987082\ttotal: 8m 6s\tremaining: 27.3s\n",
            "908:\tlearn: 49.7596637\ttotal: 8m 6s\tremaining: 26.8s\n",
            "909:\tlearn: 49.7084122\ttotal: 8m 7s\tremaining: 26.2s\n",
            "910:\tlearn: 49.6829891\ttotal: 8m 7s\tremaining: 25.7s\n",
            "911:\tlearn: 49.6461583\ttotal: 8m 8s\tremaining: 25.2s\n",
            "912:\tlearn: 49.6075607\ttotal: 8m 8s\tremaining: 24.6s\n",
            "913:\tlearn: 49.5985769\ttotal: 8m 9s\tremaining: 24.1s\n",
            "914:\tlearn: 49.5423550\ttotal: 8m 9s\tremaining: 23.6s\n",
            "915:\tlearn: 49.5174640\ttotal: 8m 10s\tremaining: 23s\n",
            "916:\tlearn: 49.4707596\ttotal: 8m 11s\tremaining: 22.5s\n",
            "917:\tlearn: 49.4386581\ttotal: 8m 11s\tremaining: 22s\n",
            "918:\tlearn: 49.3699258\ttotal: 8m 12s\tremaining: 21.4s\n",
            "919:\tlearn: 49.3314773\ttotal: 8m 12s\tremaining: 20.9s\n",
            "920:\tlearn: 49.2769377\ttotal: 8m 13s\tremaining: 20.4s\n",
            "921:\tlearn: 49.2049414\ttotal: 8m 13s\tremaining: 19.8s\n",
            "922:\tlearn: 49.1726785\ttotal: 8m 14s\tremaining: 19.3s\n",
            "923:\tlearn: 49.0814385\ttotal: 8m 14s\tremaining: 18.7s\n",
            "924:\tlearn: 49.0544445\ttotal: 8m 15s\tremaining: 18.2s\n",
            "925:\tlearn: 48.9520224\ttotal: 8m 15s\tremaining: 17.7s\n",
            "926:\tlearn: 48.9325674\ttotal: 8m 16s\tremaining: 17.1s\n",
            "927:\tlearn: 48.9025521\ttotal: 8m 16s\tremaining: 16.6s\n",
            "928:\tlearn: 48.8701598\ttotal: 8m 17s\tremaining: 16.1s\n",
            "929:\tlearn: 48.8268723\ttotal: 8m 17s\tremaining: 15.5s\n",
            "930:\tlearn: 48.8178425\ttotal: 8m 18s\tremaining: 15s\n",
            "931:\tlearn: 48.7819047\ttotal: 8m 19s\tremaining: 14.5s\n",
            "932:\tlearn: 48.7389593\ttotal: 8m 19s\tremaining: 13.9s\n",
            "933:\tlearn: 48.7042287\ttotal: 8m 20s\tremaining: 13.4s\n",
            "934:\tlearn: 48.6642053\ttotal: 8m 20s\tremaining: 12.9s\n",
            "935:\tlearn: 48.6132414\ttotal: 8m 21s\tremaining: 12.3s\n",
            "936:\tlearn: 48.5812717\ttotal: 8m 22s\tremaining: 11.8s\n",
            "937:\tlearn: 48.5634437\ttotal: 8m 22s\tremaining: 11.3s\n",
            "938:\tlearn: 48.5445139\ttotal: 8m 23s\tremaining: 10.7s\n",
            "939:\tlearn: 48.5325100\ttotal: 8m 23s\tremaining: 10.2s\n",
            "940:\tlearn: 48.5137430\ttotal: 8m 24s\tremaining: 9.65s\n",
            "941:\tlearn: 48.4882198\ttotal: 8m 24s\tremaining: 9.11s\n",
            "942:\tlearn: 48.4679201\ttotal: 8m 25s\tremaining: 8.58s\n",
            "943:\tlearn: 48.3672744\ttotal: 8m 25s\tremaining: 8.04s\n",
            "944:\tlearn: 48.3257697\ttotal: 8m 26s\tremaining: 7.5s\n",
            "945:\tlearn: 48.2785893\ttotal: 8m 27s\tremaining: 6.97s\n",
            "946:\tlearn: 48.2464626\ttotal: 8m 27s\tremaining: 6.43s\n",
            "947:\tlearn: 48.2153426\ttotal: 8m 28s\tremaining: 5.89s\n",
            "948:\tlearn: 48.1078628\ttotal: 8m 28s\tremaining: 5.36s\n",
            "949:\tlearn: 48.0734037\ttotal: 8m 29s\tremaining: 4.82s\n",
            "950:\tlearn: 48.0516772\ttotal: 8m 29s\tremaining: 4.29s\n",
            "951:\tlearn: 47.9907117\ttotal: 8m 30s\tremaining: 3.75s\n",
            "952:\tlearn: 47.9643227\ttotal: 8m 30s\tremaining: 3.21s\n",
            "953:\tlearn: 47.9541823\ttotal: 8m 31s\tremaining: 2.68s\n",
            "954:\tlearn: 47.9203229\ttotal: 8m 31s\tremaining: 2.14s\n",
            "955:\tlearn: 47.9027695\ttotal: 8m 32s\tremaining: 1.61s\n",
            "956:\tlearn: 47.8120408\ttotal: 8m 32s\tremaining: 1.07s\n",
            "957:\tlearn: 47.7704193\ttotal: 8m 33s\tremaining: 536ms\n",
            "958:\tlearn: 47.7365876\ttotal: 8m 33s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 12:55:42,110] A new study created in memory with name: no-name-9ad5a723-018b-477e-8a18-89bc93ec67dc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 13:06:37,100] Trial 0 finished with value: 58.09895228956435 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 58.09895228956435.\n",
            "[I 2023-10-08 13:10:32,680] Trial 1 finished with value: 40.83763511149624 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:17:17,838] Trial 2 finished with value: 43.121509137115815 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:21:57,624] Trial 3 finished with value: 43.94985455796643 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:25:58,244] Trial 4 finished with value: 41.36720579017578 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:32:51,176] Trial 5 finished with value: 75.5271781349379 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:36:14,227] Trial 6 finished with value: 44.14301991329147 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:53:07,134] Trial 7 finished with value: 52.63030352611256 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:55:08,885] Trial 8 finished with value: 56.27519143102873 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 13:58:42,458] Trial 9 finished with value: 47.60148372071607 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 14:04:39,673] Trial 10 finished with value: 51.363627432964 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 40.83763511149624.\n",
            "[I 2023-10-08 14:08:39,409] Trial 11 finished with value: 39.72644429965318 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 11 with value: 39.72644429965318.\n",
            "[I 2023-10-08 14:12:00,662] Trial 12 finished with value: 44.26732465979449 and parameters: {'iterations': 743, 'learning_rate': 0.7356346127088652, 'depth': 9, 'min_data_in_leaf': 17, 'reg_lambda': 49.16423147603391, 'subsample': 0.5854290622440648, 'random_strength': 31.588734753749215, 'od_wait': 122, 'leaf_estimation_iterations': 4, 'bagging_temperature': 23.993242751388063, 'colsample_bylevel': 0.3642319611435846}. Best is trial 11 with value: 39.72644429965318.\n",
            "[I 2023-10-08 14:14:28,558] Trial 13 finished with value: 51.42050495780325 and parameters: {'iterations': 757, 'learning_rate': 0.7501800009545696, 'depth': 11, 'min_data_in_leaf': 13, 'reg_lambda': 30.221360663173336, 'subsample': 0.4774950955421056, 'random_strength': 30.328584321736876, 'od_wait': 18, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.6071397112449649}. Best is trial 11 with value: 39.72644429965318.\n",
            "[I 2023-10-08 14:18:48,110] Trial 14 finished with value: 39.48495224853988 and parameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}. Best is trial 14 with value: 39.48495224853988.\n",
            "[I 2023-10-08 14:27:28,347] Trial 15 finished with value: 39.85639347584095 and parameters: {'iterations': 951, 'learning_rate': 0.8511066461626123, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 57.12168849740681, 'subsample': 0.6332455121753336, 'random_strength': 42.985904488419436, 'od_wait': 150, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.32936110772192, 'colsample_bylevel': 0.9920572964452485}. Best is trial 14 with value: 39.48495224853988.\n",
            "[I 2023-10-08 14:30:17,514] Trial 16 finished with value: 45.9101317746918 and parameters: {'iterations': 772, 'learning_rate': 0.8503707442954018, 'depth': 9, 'min_data_in_leaf': 22, 'reg_lambda': 45.56988826790886, 'subsample': 0.43361830890771325, 'random_strength': 40.85285355325164, 'od_wait': 10, 'leaf_estimation_iterations': 4, 'bagging_temperature': 48.52697376311865, 'colsample_bylevel': 0.6952854200311266}. Best is trial 14 with value: 39.48495224853988.\n",
            "[I 2023-10-08 14:31:39,197] Trial 17 finished with value: 53.809632287669274 and parameters: {'iterations': 305, 'learning_rate': 0.6940981700259699, 'depth': 7, 'min_data_in_leaf': 13, 'reg_lambda': 59.169649275320815, 'subsample': 0.5588258864286549, 'random_strength': 65.67001229048435, 'od_wait': 81, 'leaf_estimation_iterations': 4, 'bagging_temperature': 94.38598838290035, 'colsample_bylevel': 0.7425321676611494}. Best is trial 14 with value: 39.48495224853988.\n",
            "[I 2023-10-08 14:43:51,855] Trial 18 finished with value: 43.24636959516313 and parameters: {'iterations': 942, 'learning_rate': 0.4590762268369928, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 71.62634725545307, 'subsample': 0.44325233558749344, 'random_strength': 42.23473947881512, 'od_wait': 133, 'leaf_estimation_iterations': 6, 'bagging_temperature': 49.18023756004181, 'colsample_bylevel': 0.8075324887137805}. Best is trial 14 with value: 39.48495224853988.\n",
            "[I 2023-10-08 14:47:20,790] Trial 19 finished with value: 42.03420342110946 and parameters: {'iterations': 671, 'learning_rate': 0.8288728088600124, 'depth': 9, 'min_data_in_leaf': 22, 'reg_lambda': 55.697461616307315, 'subsample': 0.6156215600174659, 'random_strength': 21.01912084832779, 'od_wait': 94, 'leaf_estimation_iterations': 2, 'bagging_temperature': 59.80021622617315, 'colsample_bylevel': 0.5705483795639386}. Best is trial 14 with value: 39.48495224853988.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 5: 39.48495224853988\n",
            "Best hyperparameters for fold 5: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}\n",
            "0:\tlearn: 206.1000708\ttotal: 246ms\tremaining: 3m 54s\n",
            "1:\tlearn: 205.5428784\ttotal: 346ms\tremaining: 2m 44s\n",
            "2:\tlearn: 202.8560873\ttotal: 569ms\tremaining: 3m\n",
            "3:\tlearn: 202.4004134\ttotal: 814ms\tremaining: 3m 13s\n",
            "4:\tlearn: 201.8472516\ttotal: 1.04s\tremaining: 3m 18s\n",
            "5:\tlearn: 201.8470539\ttotal: 1.07s\tremaining: 2m 49s\n",
            "6:\tlearn: 200.6837004\ttotal: 1.33s\tremaining: 3m\n",
            "7:\tlearn: 200.4923458\ttotal: 1.57s\tremaining: 3m 6s\n",
            "8:\tlearn: 199.6000669\ttotal: 1.78s\tremaining: 3m 7s\n",
            "9:\tlearn: 199.1301704\ttotal: 1.99s\tremaining: 3m 8s\n",
            "10:\tlearn: 198.4540247\ttotal: 2.22s\tremaining: 3m 10s\n",
            "11:\tlearn: 198.3782617\ttotal: 2.47s\tremaining: 3m 14s\n",
            "12:\tlearn: 197.6241462\ttotal: 2.66s\tremaining: 3m 12s\n",
            "13:\tlearn: 197.6239752\ttotal: 2.68s\tremaining: 3m\n",
            "14:\tlearn: 197.2313838\ttotal: 2.86s\tremaining: 2m 59s\n",
            "15:\tlearn: 197.1900723\ttotal: 2.96s\tremaining: 2m 54s\n",
            "16:\tlearn: 197.1899779\ttotal: 3s\tremaining: 2m 45s\n",
            "17:\tlearn: 196.9097854\ttotal: 3.25s\tremaining: 2m 49s\n",
            "18:\tlearn: 196.7381888\ttotal: 3.52s\tremaining: 2m 53s\n",
            "19:\tlearn: 196.0580641\ttotal: 3.75s\tremaining: 2m 55s\n",
            "20:\tlearn: 196.0444953\ttotal: 3.8s\tremaining: 2m 49s\n",
            "21:\tlearn: 195.6961440\ttotal: 4.05s\tremaining: 2m 52s\n",
            "22:\tlearn: 194.2280656\ttotal: 4.29s\tremaining: 2m 53s\n",
            "23:\tlearn: 192.9606222\ttotal: 4.54s\tremaining: 2m 56s\n",
            "24:\tlearn: 189.0813046\ttotal: 4.75s\tremaining: 2m 57s\n",
            "25:\tlearn: 188.1846774\ttotal: 5.02s\tremaining: 2m 59s\n",
            "26:\tlearn: 186.0815259\ttotal: 5.23s\tremaining: 2m 59s\n",
            "27:\tlearn: 185.4050338\ttotal: 5.45s\tremaining: 3m\n",
            "28:\tlearn: 184.2431956\ttotal: 5.66s\tremaining: 3m\n",
            "29:\tlearn: 183.9042737\ttotal: 5.89s\tremaining: 3m 1s\n",
            "30:\tlearn: 183.3332421\ttotal: 6.12s\tremaining: 3m 2s\n",
            "31:\tlearn: 182.8054885\ttotal: 6.36s\tremaining: 3m 3s\n",
            "32:\tlearn: 182.3884140\ttotal: 6.57s\tremaining: 3m 3s\n",
            "33:\tlearn: 179.9182388\ttotal: 6.76s\tremaining: 3m 3s\n",
            "34:\tlearn: 179.7191604\ttotal: 7s\tremaining: 3m 4s\n",
            "35:\tlearn: 177.5757271\ttotal: 7.22s\tremaining: 3m 4s\n",
            "36:\tlearn: 174.8658474\ttotal: 7.43s\tremaining: 3m 4s\n",
            "37:\tlearn: 173.7071298\ttotal: 7.63s\tremaining: 3m 4s\n",
            "38:\tlearn: 172.9351441\ttotal: 7.8s\tremaining: 3m 3s\n",
            "39:\tlearn: 171.5152729\ttotal: 8.01s\tremaining: 3m 3s\n",
            "40:\tlearn: 171.2569639\ttotal: 8.23s\tremaining: 3m 3s\n",
            "41:\tlearn: 170.2985867\ttotal: 8.44s\tremaining: 3m 3s\n",
            "42:\tlearn: 169.7934618\ttotal: 8.63s\tremaining: 3m 3s\n",
            "43:\tlearn: 167.7379571\ttotal: 8.88s\tremaining: 3m 4s\n",
            "44:\tlearn: 167.3423255\ttotal: 9.11s\tremaining: 3m 4s\n",
            "45:\tlearn: 167.0372947\ttotal: 9.4s\tremaining: 3m 5s\n",
            "46:\tlearn: 166.4730770\ttotal: 9.61s\tremaining: 3m 5s\n",
            "47:\tlearn: 165.7479934\ttotal: 9.81s\tremaining: 3m 5s\n",
            "48:\tlearn: 163.3688566\ttotal: 10s\tremaining: 3m 5s\n",
            "49:\tlearn: 162.3581816\ttotal: 10.3s\tremaining: 3m 5s\n",
            "50:\tlearn: 162.1336450\ttotal: 10.5s\tremaining: 3m 6s\n",
            "51:\tlearn: 160.2038725\ttotal: 10.7s\tremaining: 3m 6s\n",
            "52:\tlearn: 159.7027211\ttotal: 10.9s\tremaining: 3m 6s\n",
            "53:\tlearn: 159.3647750\ttotal: 11.2s\tremaining: 3m 6s\n",
            "54:\tlearn: 158.8954639\ttotal: 11.4s\tremaining: 3m 6s\n",
            "55:\tlearn: 158.7521100\ttotal: 11.6s\tremaining: 3m 7s\n",
            "56:\tlearn: 158.5512480\ttotal: 11.9s\tremaining: 3m 7s\n",
            "57:\tlearn: 158.4228135\ttotal: 12.1s\tremaining: 3m 8s\n",
            "58:\tlearn: 157.5501048\ttotal: 12.3s\tremaining: 3m 7s\n",
            "59:\tlearn: 157.3736239\ttotal: 12.6s\tremaining: 3m 7s\n",
            "60:\tlearn: 156.2766605\ttotal: 12.8s\tremaining: 3m 8s\n",
            "61:\tlearn: 155.0685671\ttotal: 13s\tremaining: 3m 7s\n",
            "62:\tlearn: 154.0747712\ttotal: 13.3s\tremaining: 3m 8s\n",
            "63:\tlearn: 153.9392662\ttotal: 13.6s\tremaining: 3m 9s\n",
            "64:\tlearn: 152.6943728\ttotal: 13.8s\tremaining: 3m 8s\n",
            "65:\tlearn: 151.8064410\ttotal: 14s\tremaining: 3m 8s\n",
            "66:\tlearn: 151.2509384\ttotal: 14.2s\tremaining: 3m 8s\n",
            "67:\tlearn: 150.9186983\ttotal: 14.4s\tremaining: 3m 8s\n",
            "68:\tlearn: 149.7067015\ttotal: 14.6s\tremaining: 3m 7s\n",
            "69:\tlearn: 149.2951994\ttotal: 14.8s\tremaining: 3m 7s\n",
            "70:\tlearn: 149.2269236\ttotal: 15.1s\tremaining: 3m 8s\n",
            "71:\tlearn: 149.2265094\ttotal: 15.3s\tremaining: 3m 8s\n",
            "72:\tlearn: 148.8506333\ttotal: 15.5s\tremaining: 3m 7s\n",
            "73:\tlearn: 148.6931949\ttotal: 15.8s\tremaining: 3m 7s\n",
            "74:\tlearn: 148.1755241\ttotal: 16s\tremaining: 3m 7s\n",
            "75:\tlearn: 147.8288229\ttotal: 16.2s\tremaining: 3m 7s\n",
            "76:\tlearn: 147.6570206\ttotal: 16.4s\tremaining: 3m 7s\n",
            "77:\tlearn: 146.8277545\ttotal: 16.6s\tremaining: 3m 6s\n",
            "78:\tlearn: 146.3467626\ttotal: 16.8s\tremaining: 3m 6s\n",
            "79:\tlearn: 145.6860784\ttotal: 17s\tremaining: 3m 6s\n",
            "80:\tlearn: 145.3598302\ttotal: 17.2s\tremaining: 3m 6s\n",
            "81:\tlearn: 143.4882653\ttotal: 17.4s\tremaining: 3m 5s\n",
            "82:\tlearn: 142.1420180\ttotal: 17.7s\tremaining: 3m 5s\n",
            "83:\tlearn: 141.6376089\ttotal: 17.9s\tremaining: 3m 5s\n",
            "84:\tlearn: 141.3721373\ttotal: 18.1s\tremaining: 3m 5s\n",
            "85:\tlearn: 140.5537981\ttotal: 18.3s\tremaining: 3m 5s\n",
            "86:\tlearn: 140.2299665\ttotal: 18.5s\tremaining: 3m 4s\n",
            "87:\tlearn: 140.0898620\ttotal: 18.7s\tremaining: 3m 4s\n",
            "88:\tlearn: 139.8473772\ttotal: 19s\tremaining: 3m 4s\n",
            "89:\tlearn: 139.4244349\ttotal: 19.2s\tremaining: 3m 4s\n",
            "90:\tlearn: 139.0848859\ttotal: 19.4s\tremaining: 3m 4s\n",
            "91:\tlearn: 138.9353950\ttotal: 19.6s\tremaining: 3m 3s\n",
            "92:\tlearn: 138.7470572\ttotal: 19.8s\tremaining: 3m 3s\n",
            "93:\tlearn: 137.6407891\ttotal: 20s\tremaining: 3m 3s\n",
            "94:\tlearn: 137.4615091\ttotal: 20.2s\tremaining: 3m 3s\n",
            "95:\tlearn: 136.9918821\ttotal: 20.4s\tremaining: 3m 3s\n",
            "96:\tlearn: 136.7233885\ttotal: 20.6s\tremaining: 3m 2s\n",
            "97:\tlearn: 135.7835503\ttotal: 20.8s\tremaining: 3m 2s\n",
            "98:\tlearn: 135.6033778\ttotal: 21s\tremaining: 3m 2s\n",
            "99:\tlearn: 134.7956942\ttotal: 21.3s\tremaining: 3m 2s\n",
            "100:\tlearn: 134.4976593\ttotal: 21.5s\tremaining: 3m 1s\n",
            "101:\tlearn: 134.3830705\ttotal: 21.7s\tremaining: 3m 1s\n",
            "102:\tlearn: 133.8336848\ttotal: 21.9s\tremaining: 3m 1s\n",
            "103:\tlearn: 133.6354513\ttotal: 22.1s\tremaining: 3m 1s\n",
            "104:\tlearn: 133.2063574\ttotal: 22.4s\tremaining: 3m 1s\n",
            "105:\tlearn: 131.6011531\ttotal: 22.6s\tremaining: 3m 1s\n",
            "106:\tlearn: 130.4130136\ttotal: 22.8s\tremaining: 3m\n",
            "107:\tlearn: 130.3161878\ttotal: 23s\tremaining: 3m\n",
            "108:\tlearn: 129.9624124\ttotal: 23.2s\tremaining: 3m\n",
            "109:\tlearn: 129.6246633\ttotal: 23.4s\tremaining: 3m\n",
            "110:\tlearn: 129.4967296\ttotal: 23.6s\tremaining: 2m 59s\n",
            "111:\tlearn: 129.4081803\ttotal: 23.9s\tremaining: 2m 59s\n",
            "112:\tlearn: 128.8446519\ttotal: 24.1s\tremaining: 2m 59s\n",
            "113:\tlearn: 128.6317202\ttotal: 24.3s\tremaining: 2m 59s\n",
            "114:\tlearn: 128.1650600\ttotal: 24.5s\tremaining: 2m 59s\n",
            "115:\tlearn: 127.6834169\ttotal: 24.7s\tremaining: 2m 58s\n",
            "116:\tlearn: 127.5558610\ttotal: 25s\tremaining: 2m 58s\n",
            "117:\tlearn: 127.2419837\ttotal: 25.2s\tremaining: 2m 59s\n",
            "118:\tlearn: 126.7034323\ttotal: 25.5s\tremaining: 2m 59s\n",
            "119:\tlearn: 126.0247154\ttotal: 25.6s\tremaining: 2m 58s\n",
            "120:\tlearn: 125.9679920\ttotal: 25.9s\tremaining: 2m 58s\n",
            "121:\tlearn: 125.7851587\ttotal: 26.2s\tremaining: 2m 59s\n",
            "122:\tlearn: 125.5270121\ttotal: 26.4s\tremaining: 2m 58s\n",
            "123:\tlearn: 125.3297967\ttotal: 26.7s\tremaining: 2m 58s\n",
            "124:\tlearn: 124.7642365\ttotal: 26.9s\tremaining: 2m 58s\n",
            "125:\tlearn: 123.6539329\ttotal: 27.1s\tremaining: 2m 58s\n",
            "126:\tlearn: 123.4325261\ttotal: 27.3s\tremaining: 2m 58s\n",
            "127:\tlearn: 123.3342039\ttotal: 27.6s\tremaining: 2m 58s\n",
            "128:\tlearn: 123.0609646\ttotal: 27.8s\tremaining: 2m 58s\n",
            "129:\tlearn: 122.6872836\ttotal: 28s\tremaining: 2m 57s\n",
            "130:\tlearn: 122.4582742\ttotal: 28.2s\tremaining: 2m 57s\n",
            "131:\tlearn: 122.4465588\ttotal: 28.5s\tremaining: 2m 57s\n",
            "132:\tlearn: 122.3482433\ttotal: 28.7s\tremaining: 2m 57s\n",
            "133:\tlearn: 122.1914055\ttotal: 29s\tremaining: 2m 57s\n",
            "134:\tlearn: 122.0894057\ttotal: 29.2s\tremaining: 2m 57s\n",
            "135:\tlearn: 121.6828757\ttotal: 29.4s\tremaining: 2m 57s\n",
            "136:\tlearn: 121.0353036\ttotal: 29.6s\tremaining: 2m 57s\n",
            "137:\tlearn: 120.8731637\ttotal: 29.8s\tremaining: 2m 56s\n",
            "138:\tlearn: 120.3858141\ttotal: 30s\tremaining: 2m 56s\n",
            "139:\tlearn: 119.5666429\ttotal: 30.2s\tremaining: 2m 56s\n",
            "140:\tlearn: 119.0840158\ttotal: 30.4s\tremaining: 2m 55s\n",
            "141:\tlearn: 118.9959172\ttotal: 30.6s\tremaining: 2m 55s\n",
            "142:\tlearn: 118.7046895\ttotal: 30.8s\tremaining: 2m 55s\n",
            "143:\tlearn: 118.3016828\ttotal: 31s\tremaining: 2m 54s\n",
            "144:\tlearn: 118.0730994\ttotal: 31.2s\tremaining: 2m 54s\n",
            "145:\tlearn: 117.7628540\ttotal: 31.5s\tremaining: 2m 54s\n",
            "146:\tlearn: 117.5890433\ttotal: 31.7s\tremaining: 2m 54s\n",
            "147:\tlearn: 117.2778434\ttotal: 31.9s\tremaining: 2m 54s\n",
            "148:\tlearn: 116.9380525\ttotal: 32.1s\tremaining: 2m 53s\n",
            "149:\tlearn: 116.8357124\ttotal: 32.4s\tremaining: 2m 54s\n",
            "150:\tlearn: 116.6720558\ttotal: 32.6s\tremaining: 2m 53s\n",
            "151:\tlearn: 116.5659115\ttotal: 32.8s\tremaining: 2m 53s\n",
            "152:\tlearn: 116.2180308\ttotal: 33s\tremaining: 2m 53s\n",
            "153:\tlearn: 116.1254516\ttotal: 33.2s\tremaining: 2m 53s\n",
            "154:\tlearn: 115.5057259\ttotal: 33.4s\tremaining: 2m 52s\n",
            "155:\tlearn: 115.1153129\ttotal: 33.6s\tremaining: 2m 52s\n",
            "156:\tlearn: 114.9984519\ttotal: 33.9s\tremaining: 2m 52s\n",
            "157:\tlearn: 114.7288705\ttotal: 34.1s\tremaining: 2m 52s\n",
            "158:\tlearn: 114.4579284\ttotal: 34.3s\tremaining: 2m 51s\n",
            "159:\tlearn: 113.9534967\ttotal: 34.5s\tremaining: 2m 51s\n",
            "160:\tlearn: 113.6884569\ttotal: 34.7s\tremaining: 2m 51s\n",
            "161:\tlearn: 113.4971422\ttotal: 34.9s\tremaining: 2m 51s\n",
            "162:\tlearn: 113.1861675\ttotal: 35.1s\tremaining: 2m 51s\n",
            "163:\tlearn: 113.0670369\ttotal: 35.3s\tremaining: 2m 50s\n",
            "164:\tlearn: 112.7844030\ttotal: 35.6s\tremaining: 2m 50s\n",
            "165:\tlearn: 112.3823753\ttotal: 35.8s\tremaining: 2m 50s\n",
            "166:\tlearn: 112.2614636\ttotal: 36.1s\tremaining: 2m 50s\n",
            "167:\tlearn: 111.9055862\ttotal: 36.3s\tremaining: 2m 50s\n",
            "168:\tlearn: 111.8051064\ttotal: 36.5s\tremaining: 2m 49s\n",
            "169:\tlearn: 111.7037190\ttotal: 36.7s\tremaining: 2m 49s\n",
            "170:\tlearn: 111.5883575\ttotal: 36.9s\tremaining: 2m 49s\n",
            "171:\tlearn: 111.3359006\ttotal: 37.1s\tremaining: 2m 49s\n",
            "172:\tlearn: 110.6191300\ttotal: 37.3s\tremaining: 2m 49s\n",
            "173:\tlearn: 110.4392386\ttotal: 37.5s\tremaining: 2m 48s\n",
            "174:\tlearn: 110.1948862\ttotal: 37.7s\tremaining: 2m 48s\n",
            "175:\tlearn: 110.0772816\ttotal: 37.9s\tremaining: 2m 48s\n",
            "176:\tlearn: 109.4995441\ttotal: 38.2s\tremaining: 2m 47s\n",
            "177:\tlearn: 109.2437403\ttotal: 38.4s\tremaining: 2m 47s\n",
            "178:\tlearn: 108.8632777\ttotal: 38.6s\tremaining: 2m 47s\n",
            "179:\tlearn: 108.7866648\ttotal: 38.9s\tremaining: 2m 47s\n",
            "180:\tlearn: 108.7028439\ttotal: 39.1s\tremaining: 2m 47s\n",
            "181:\tlearn: 108.5510762\ttotal: 39.4s\tremaining: 2m 47s\n",
            "182:\tlearn: 108.4435394\ttotal: 39.6s\tremaining: 2m 47s\n",
            "183:\tlearn: 108.3764976\ttotal: 39.8s\tremaining: 2m 46s\n",
            "184:\tlearn: 108.2205997\ttotal: 40s\tremaining: 2m 46s\n",
            "185:\tlearn: 108.1695796\ttotal: 40.2s\tremaining: 2m 46s\n",
            "186:\tlearn: 108.0608378\ttotal: 40.4s\tremaining: 2m 46s\n",
            "187:\tlearn: 108.0082317\ttotal: 40.7s\tremaining: 2m 46s\n",
            "188:\tlearn: 107.8927463\ttotal: 40.9s\tremaining: 2m 45s\n",
            "189:\tlearn: 107.8210958\ttotal: 41.1s\tremaining: 2m 45s\n",
            "190:\tlearn: 107.7291646\ttotal: 41.3s\tremaining: 2m 45s\n",
            "191:\tlearn: 107.3816847\ttotal: 41.6s\tremaining: 2m 45s\n",
            "192:\tlearn: 107.3340964\ttotal: 41.8s\tremaining: 2m 45s\n",
            "193:\tlearn: 106.8948329\ttotal: 42s\tremaining: 2m 45s\n",
            "194:\tlearn: 106.5634632\ttotal: 42.3s\tremaining: 2m 45s\n",
            "195:\tlearn: 106.3994863\ttotal: 42.6s\tremaining: 2m 45s\n",
            "196:\tlearn: 106.2259082\ttotal: 42.9s\tremaining: 2m 45s\n",
            "197:\tlearn: 106.0915785\ttotal: 43.1s\tremaining: 2m 44s\n",
            "198:\tlearn: 105.7102702\ttotal: 43.3s\tremaining: 2m 44s\n",
            "199:\tlearn: 105.4117460\ttotal: 43.5s\tremaining: 2m 44s\n",
            "200:\tlearn: 105.1448106\ttotal: 43.8s\tremaining: 2m 44s\n",
            "201:\tlearn: 104.9487496\ttotal: 44s\tremaining: 2m 44s\n",
            "202:\tlearn: 104.8220117\ttotal: 44.2s\tremaining: 2m 44s\n",
            "203:\tlearn: 104.7088298\ttotal: 44.6s\tremaining: 2m 44s\n",
            "204:\tlearn: 104.5363302\ttotal: 44.8s\tremaining: 2m 44s\n",
            "205:\tlearn: 104.4422903\ttotal: 45s\tremaining: 2m 43s\n",
            "206:\tlearn: 104.3967426\ttotal: 45.3s\tremaining: 2m 43s\n",
            "207:\tlearn: 104.1372068\ttotal: 45.5s\tremaining: 2m 43s\n",
            "208:\tlearn: 104.0423044\ttotal: 45.7s\tremaining: 2m 43s\n",
            "209:\tlearn: 103.9760700\ttotal: 45.9s\tremaining: 2m 43s\n",
            "210:\tlearn: 103.9082021\ttotal: 46.3s\tremaining: 2m 43s\n",
            "211:\tlearn: 103.8746019\ttotal: 46.5s\tremaining: 2m 43s\n",
            "212:\tlearn: 103.5665139\ttotal: 46.7s\tremaining: 2m 42s\n",
            "213:\tlearn: 103.4960752\ttotal: 46.9s\tremaining: 2m 42s\n",
            "214:\tlearn: 102.9684881\ttotal: 47.2s\tremaining: 2m 42s\n",
            "215:\tlearn: 102.7048175\ttotal: 47.4s\tremaining: 2m 42s\n",
            "216:\tlearn: 102.5445514\ttotal: 47.6s\tremaining: 2m 42s\n",
            "217:\tlearn: 102.3579247\ttotal: 47.8s\tremaining: 2m 41s\n",
            "218:\tlearn: 102.2036968\ttotal: 48s\tremaining: 2m 41s\n",
            "219:\tlearn: 102.1128553\ttotal: 48.3s\tremaining: 2m 41s\n",
            "220:\tlearn: 101.8876994\ttotal: 48.5s\tremaining: 2m 41s\n",
            "221:\tlearn: 101.3854367\ttotal: 48.7s\tremaining: 2m 40s\n",
            "222:\tlearn: 101.3131119\ttotal: 48.9s\tremaining: 2m 40s\n",
            "223:\tlearn: 101.2265584\ttotal: 49.1s\tremaining: 2m 40s\n",
            "224:\tlearn: 101.1525095\ttotal: 49.3s\tremaining: 2m 40s\n",
            "225:\tlearn: 100.9490885\ttotal: 49.5s\tremaining: 2m 39s\n",
            "226:\tlearn: 100.7702537\ttotal: 49.7s\tremaining: 2m 39s\n",
            "227:\tlearn: 100.6807094\ttotal: 49.9s\tremaining: 2m 39s\n",
            "228:\tlearn: 100.5923357\ttotal: 50.1s\tremaining: 2m 39s\n",
            "229:\tlearn: 100.5275817\ttotal: 50.3s\tremaining: 2m 38s\n",
            "230:\tlearn: 100.4607340\ttotal: 50.6s\tremaining: 2m 38s\n",
            "231:\tlearn: 100.3265329\ttotal: 50.8s\tremaining: 2m 38s\n",
            "232:\tlearn: 100.0939247\ttotal: 50.9s\tremaining: 2m 38s\n",
            "233:\tlearn: 100.0606005\ttotal: 51.2s\tremaining: 2m 37s\n",
            "234:\tlearn: 99.8975642\ttotal: 51.4s\tremaining: 2m 37s\n",
            "235:\tlearn: 99.7696595\ttotal: 51.6s\tremaining: 2m 37s\n",
            "236:\tlearn: 99.4404761\ttotal: 51.8s\tremaining: 2m 37s\n",
            "237:\tlearn: 99.1480355\ttotal: 52s\tremaining: 2m 36s\n",
            "238:\tlearn: 98.9641349\ttotal: 52.2s\tremaining: 2m 36s\n",
            "239:\tlearn: 98.7024721\ttotal: 52.4s\tremaining: 2m 36s\n",
            "240:\tlearn: 98.6262678\ttotal: 52.7s\tremaining: 2m 36s\n",
            "241:\tlearn: 98.5467402\ttotal: 52.8s\tremaining: 2m 35s\n",
            "242:\tlearn: 98.2411737\ttotal: 53s\tremaining: 2m 35s\n",
            "243:\tlearn: 98.0209083\ttotal: 53.2s\tremaining: 2m 35s\n",
            "244:\tlearn: 97.8515922\ttotal: 53.5s\tremaining: 2m 35s\n",
            "245:\tlearn: 97.7768101\ttotal: 53.7s\tremaining: 2m 34s\n",
            "246:\tlearn: 97.6881895\ttotal: 53.9s\tremaining: 2m 34s\n",
            "247:\tlearn: 97.5757494\ttotal: 54.1s\tremaining: 2m 34s\n",
            "248:\tlearn: 97.4489419\ttotal: 54.4s\tremaining: 2m 34s\n",
            "249:\tlearn: 97.3177361\ttotal: 54.6s\tremaining: 2m 34s\n",
            "250:\tlearn: 97.2091013\ttotal: 54.8s\tremaining: 2m 33s\n",
            "251:\tlearn: 97.0346914\ttotal: 55s\tremaining: 2m 33s\n",
            "252:\tlearn: 96.8065994\ttotal: 55.2s\tremaining: 2m 33s\n",
            "253:\tlearn: 96.6391198\ttotal: 55.4s\tremaining: 2m 33s\n",
            "254:\tlearn: 96.5280691\ttotal: 55.7s\tremaining: 2m 33s\n",
            "255:\tlearn: 96.4919742\ttotal: 55.9s\tremaining: 2m 32s\n",
            "256:\tlearn: 96.2790996\ttotal: 56.1s\tremaining: 2m 32s\n",
            "257:\tlearn: 96.0769881\ttotal: 56.4s\tremaining: 2m 32s\n",
            "258:\tlearn: 95.9800993\ttotal: 56.6s\tremaining: 2m 32s\n",
            "259:\tlearn: 95.7885484\ttotal: 56.8s\tremaining: 2m 32s\n",
            "260:\tlearn: 95.5489777\ttotal: 57s\tremaining: 2m 31s\n",
            "261:\tlearn: 95.4808633\ttotal: 57.3s\tremaining: 2m 31s\n",
            "262:\tlearn: 95.4143451\ttotal: 57.5s\tremaining: 2m 31s\n",
            "263:\tlearn: 95.2768545\ttotal: 57.8s\tremaining: 2m 31s\n",
            "264:\tlearn: 95.0596608\ttotal: 58s\tremaining: 2m 31s\n",
            "265:\tlearn: 94.9884603\ttotal: 58.2s\tremaining: 2m 31s\n",
            "266:\tlearn: 94.8329432\ttotal: 58.5s\tremaining: 2m 30s\n",
            "267:\tlearn: 94.6954820\ttotal: 58.7s\tremaining: 2m 30s\n",
            "268:\tlearn: 94.5806689\ttotal: 58.9s\tremaining: 2m 30s\n",
            "269:\tlearn: 94.4606385\ttotal: 59.1s\tremaining: 2m 30s\n",
            "270:\tlearn: 94.3641313\ttotal: 59.4s\tremaining: 2m 30s\n",
            "271:\tlearn: 94.2929548\ttotal: 59.6s\tremaining: 2m 29s\n",
            "272:\tlearn: 94.1929852\ttotal: 59.8s\tremaining: 2m 29s\n",
            "273:\tlearn: 94.0016431\ttotal: 1m\tremaining: 2m 29s\n",
            "274:\tlearn: 93.8285390\ttotal: 1m\tremaining: 2m 29s\n",
            "275:\tlearn: 93.6277079\ttotal: 1m\tremaining: 2m 28s\n",
            "276:\tlearn: 93.4557554\ttotal: 1m\tremaining: 2m 28s\n",
            "277:\tlearn: 93.3240796\ttotal: 1m\tremaining: 2m 28s\n",
            "278:\tlearn: 93.2569683\ttotal: 1m 1s\tremaining: 2m 28s\n",
            "279:\tlearn: 93.2047946\ttotal: 1m 1s\tremaining: 2m 28s\n",
            "280:\tlearn: 93.1424234\ttotal: 1m 1s\tremaining: 2m 28s\n",
            "281:\tlearn: 93.0666559\ttotal: 1m 1s\tremaining: 2m 27s\n",
            "282:\tlearn: 92.8939285\ttotal: 1m 2s\tremaining: 2m 27s\n",
            "283:\tlearn: 92.8345846\ttotal: 1m 2s\tremaining: 2m 27s\n",
            "284:\tlearn: 92.7578798\ttotal: 1m 2s\tremaining: 2m 27s\n",
            "285:\tlearn: 92.6094540\ttotal: 1m 2s\tremaining: 2m 27s\n",
            "286:\tlearn: 92.5516138\ttotal: 1m 3s\tremaining: 2m 27s\n",
            "287:\tlearn: 92.5045984\ttotal: 1m 3s\tremaining: 2m 26s\n",
            "288:\tlearn: 92.4344590\ttotal: 1m 3s\tremaining: 2m 26s\n",
            "289:\tlearn: 92.2111034\ttotal: 1m 3s\tremaining: 2m 26s\n",
            "290:\tlearn: 92.0298546\ttotal: 1m 3s\tremaining: 2m 26s\n",
            "291:\tlearn: 91.8226113\ttotal: 1m 4s\tremaining: 2m 25s\n",
            "292:\tlearn: 91.6993094\ttotal: 1m 4s\tremaining: 2m 25s\n",
            "293:\tlearn: 91.5515791\ttotal: 1m 4s\tremaining: 2m 25s\n",
            "294:\tlearn: 91.4856442\ttotal: 1m 4s\tremaining: 2m 25s\n",
            "295:\tlearn: 91.3403992\ttotal: 1m 5s\tremaining: 2m 24s\n",
            "296:\tlearn: 91.2674702\ttotal: 1m 5s\tremaining: 2m 24s\n",
            "297:\tlearn: 91.1569307\ttotal: 1m 5s\tremaining: 2m 24s\n",
            "298:\tlearn: 91.0808138\ttotal: 1m 5s\tremaining: 2m 24s\n",
            "299:\tlearn: 91.0644798\ttotal: 1m 5s\tremaining: 2m 24s\n",
            "300:\tlearn: 91.0631499\ttotal: 1m 6s\tremaining: 2m 23s\n",
            "301:\tlearn: 90.5626793\ttotal: 1m 6s\tremaining: 2m 23s\n",
            "302:\tlearn: 90.3913651\ttotal: 1m 6s\tremaining: 2m 23s\n",
            "303:\tlearn: 90.3314245\ttotal: 1m 6s\tremaining: 2m 23s\n",
            "304:\tlearn: 90.2143000\ttotal: 1m 6s\tremaining: 2m 22s\n",
            "305:\tlearn: 90.1129836\ttotal: 1m 7s\tremaining: 2m 22s\n",
            "306:\tlearn: 90.0564565\ttotal: 1m 7s\tremaining: 2m 22s\n",
            "307:\tlearn: 89.9589212\ttotal: 1m 7s\tremaining: 2m 22s\n",
            "308:\tlearn: 89.8382728\ttotal: 1m 7s\tremaining: 2m 22s\n",
            "309:\tlearn: 89.7801938\ttotal: 1m 8s\tremaining: 2m 21s\n",
            "310:\tlearn: 89.4924725\ttotal: 1m 8s\tremaining: 2m 21s\n",
            "311:\tlearn: 89.3757024\ttotal: 1m 8s\tremaining: 2m 21s\n",
            "312:\tlearn: 89.3081696\ttotal: 1m 8s\tremaining: 2m 21s\n",
            "313:\tlearn: 89.2816094\ttotal: 1m 8s\tremaining: 2m 20s\n",
            "314:\tlearn: 89.2613800\ttotal: 1m 9s\tremaining: 2m 20s\n",
            "315:\tlearn: 89.2471777\ttotal: 1m 9s\tremaining: 2m 20s\n",
            "316:\tlearn: 89.1533057\ttotal: 1m 9s\tremaining: 2m 20s\n",
            "317:\tlearn: 89.1138768\ttotal: 1m 9s\tremaining: 2m 20s\n",
            "318:\tlearn: 88.8813354\ttotal: 1m 10s\tremaining: 2m 20s\n",
            "319:\tlearn: 88.7911031\ttotal: 1m 10s\tremaining: 2m 19s\n",
            "320:\tlearn: 88.7545777\ttotal: 1m 10s\tremaining: 2m 19s\n",
            "321:\tlearn: 88.6375520\ttotal: 1m 10s\tremaining: 2m 19s\n",
            "322:\tlearn: 88.6112181\ttotal: 1m 11s\tremaining: 2m 19s\n",
            "323:\tlearn: 88.5327252\ttotal: 1m 11s\tremaining: 2m 19s\n",
            "324:\tlearn: 88.4078793\ttotal: 1m 11s\tremaining: 2m 18s\n",
            "325:\tlearn: 88.2253400\ttotal: 1m 11s\tremaining: 2m 18s\n",
            "326:\tlearn: 88.1097423\ttotal: 1m 11s\tremaining: 2m 18s\n",
            "327:\tlearn: 88.0468207\ttotal: 1m 12s\tremaining: 2m 18s\n",
            "328:\tlearn: 87.9849501\ttotal: 1m 12s\tremaining: 2m 17s\n",
            "329:\tlearn: 87.9276382\ttotal: 1m 12s\tremaining: 2m 17s\n",
            "330:\tlearn: 87.8449541\ttotal: 1m 12s\tremaining: 2m 17s\n",
            "331:\tlearn: 87.8057484\ttotal: 1m 13s\tremaining: 2m 17s\n",
            "332:\tlearn: 87.7088566\ttotal: 1m 13s\tremaining: 2m 17s\n",
            "333:\tlearn: 87.6289682\ttotal: 1m 13s\tremaining: 2m 16s\n",
            "334:\tlearn: 87.6005264\ttotal: 1m 13s\tremaining: 2m 16s\n",
            "335:\tlearn: 87.4664389\ttotal: 1m 13s\tremaining: 2m 16s\n",
            "336:\tlearn: 87.3905085\ttotal: 1m 14s\tremaining: 2m 16s\n",
            "337:\tlearn: 87.3253395\ttotal: 1m 14s\tremaining: 2m 16s\n",
            "338:\tlearn: 87.2450862\ttotal: 1m 14s\tremaining: 2m 15s\n",
            "339:\tlearn: 87.1349183\ttotal: 1m 14s\tremaining: 2m 15s\n",
            "340:\tlearn: 87.0697013\ttotal: 1m 15s\tremaining: 2m 15s\n",
            "341:\tlearn: 86.8485499\ttotal: 1m 15s\tremaining: 2m 15s\n",
            "342:\tlearn: 86.7239941\ttotal: 1m 15s\tremaining: 2m 15s\n",
            "343:\tlearn: 86.4969850\ttotal: 1m 15s\tremaining: 2m 14s\n",
            "344:\tlearn: 86.3625167\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "345:\tlearn: 86.0919623\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "346:\tlearn: 86.0427987\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "347:\tlearn: 86.0123759\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "348:\tlearn: 85.9641945\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "349:\tlearn: 85.9364435\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "350:\tlearn: 85.8070334\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "351:\tlearn: 85.6999214\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "352:\tlearn: 85.6155627\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "353:\tlearn: 85.5326624\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "354:\tlearn: 85.2488954\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "355:\tlearn: 85.2065562\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "356:\tlearn: 85.0871719\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "357:\tlearn: 85.0344545\ttotal: 1m 18s\tremaining: 2m 11s\n",
            "358:\tlearn: 84.9269159\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "359:\tlearn: 84.8866538\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "360:\tlearn: 84.8319001\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "361:\tlearn: 84.7464619\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "362:\tlearn: 84.7055503\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "363:\tlearn: 84.5284905\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "364:\tlearn: 84.4563025\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "365:\tlearn: 84.2646242\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "366:\tlearn: 84.1852721\ttotal: 1m 20s\tremaining: 2m 9s\n",
            "367:\tlearn: 83.9985967\ttotal: 1m 21s\tremaining: 2m 9s\n",
            "368:\tlearn: 83.9008434\ttotal: 1m 21s\tremaining: 2m 9s\n",
            "369:\tlearn: 83.8541112\ttotal: 1m 21s\tremaining: 2m 9s\n",
            "370:\tlearn: 83.7914642\ttotal: 1m 21s\tremaining: 2m 8s\n",
            "371:\tlearn: 83.7077288\ttotal: 1m 21s\tremaining: 2m 8s\n",
            "372:\tlearn: 83.6114343\ttotal: 1m 22s\tremaining: 2m 8s\n",
            "373:\tlearn: 83.5641609\ttotal: 1m 22s\tremaining: 2m 8s\n",
            "374:\tlearn: 83.5636913\ttotal: 1m 22s\tremaining: 2m 8s\n",
            "375:\tlearn: 83.4687293\ttotal: 1m 22s\tremaining: 2m 7s\n",
            "376:\tlearn: 83.3097864\ttotal: 1m 23s\tremaining: 2m 7s\n",
            "377:\tlearn: 83.1814158\ttotal: 1m 23s\tremaining: 2m 7s\n",
            "378:\tlearn: 83.1165859\ttotal: 1m 23s\tremaining: 2m 7s\n",
            "379:\tlearn: 83.0850163\ttotal: 1m 23s\tremaining: 2m 6s\n",
            "380:\tlearn: 83.0654371\ttotal: 1m 23s\tremaining: 2m 6s\n",
            "381:\tlearn: 82.9714436\ttotal: 1m 24s\tremaining: 2m 6s\n",
            "382:\tlearn: 82.8856157\ttotal: 1m 24s\tremaining: 2m 6s\n",
            "383:\tlearn: 82.8136691\ttotal: 1m 24s\tremaining: 2m 5s\n",
            "384:\tlearn: 82.7767419\ttotal: 1m 24s\tremaining: 2m 5s\n",
            "385:\tlearn: 82.6787411\ttotal: 1m 24s\tremaining: 2m 5s\n",
            "386:\tlearn: 82.6773679\ttotal: 1m 25s\tremaining: 2m 5s\n",
            "387:\tlearn: 82.6615796\ttotal: 1m 25s\tremaining: 2m 5s\n",
            "388:\tlearn: 82.6223816\ttotal: 1m 25s\tremaining: 2m 5s\n",
            "389:\tlearn: 82.5833869\ttotal: 1m 26s\tremaining: 2m 4s\n",
            "390:\tlearn: 82.4565068\ttotal: 1m 26s\tremaining: 2m 4s\n",
            "391:\tlearn: 82.4552767\ttotal: 1m 26s\tremaining: 2m 4s\n",
            "392:\tlearn: 82.3871006\ttotal: 1m 26s\tremaining: 2m 4s\n",
            "393:\tlearn: 82.3167236\ttotal: 1m 26s\tremaining: 2m 3s\n",
            "394:\tlearn: 82.2572766\ttotal: 1m 27s\tremaining: 2m 3s\n",
            "395:\tlearn: 82.2343798\ttotal: 1m 27s\tremaining: 2m 3s\n",
            "396:\tlearn: 82.0272038\ttotal: 1m 27s\tremaining: 2m 3s\n",
            "397:\tlearn: 81.8586373\ttotal: 1m 27s\tremaining: 2m 3s\n",
            "398:\tlearn: 81.7879476\ttotal: 1m 27s\tremaining: 2m 2s\n",
            "399:\tlearn: 81.7607897\ttotal: 1m 28s\tremaining: 2m 2s\n",
            "400:\tlearn: 81.5754744\ttotal: 1m 28s\tremaining: 2m 2s\n",
            "401:\tlearn: 81.4807029\ttotal: 1m 28s\tremaining: 2m 2s\n",
            "402:\tlearn: 81.4142539\ttotal: 1m 28s\tremaining: 2m 1s\n",
            "403:\tlearn: 81.3928459\ttotal: 1m 29s\tremaining: 2m 1s\n",
            "404:\tlearn: 81.3476326\ttotal: 1m 29s\tremaining: 2m 1s\n",
            "405:\tlearn: 81.2207658\ttotal: 1m 29s\tremaining: 2m 1s\n",
            "406:\tlearn: 81.1265281\ttotal: 1m 29s\tremaining: 2m 1s\n",
            "407:\tlearn: 81.0976845\ttotal: 1m 30s\tremaining: 2m\n",
            "408:\tlearn: 81.0828597\ttotal: 1m 30s\tremaining: 2m\n",
            "409:\tlearn: 81.0259691\ttotal: 1m 30s\tremaining: 2m\n",
            "410:\tlearn: 80.9681874\ttotal: 1m 30s\tremaining: 2m\n",
            "411:\tlearn: 80.8311357\ttotal: 1m 31s\tremaining: 2m\n",
            "412:\tlearn: 80.7590574\ttotal: 1m 31s\tremaining: 2m\n",
            "413:\tlearn: 80.7291349\ttotal: 1m 31s\tremaining: 1m 59s\n",
            "414:\tlearn: 80.5957401\ttotal: 1m 31s\tremaining: 1m 59s\n",
            "415:\tlearn: 80.5781796\ttotal: 1m 32s\tremaining: 1m 59s\n",
            "416:\tlearn: 80.5543837\ttotal: 1m 32s\tremaining: 1m 59s\n",
            "417:\tlearn: 80.5204408\ttotal: 1m 32s\tremaining: 1m 59s\n",
            "418:\tlearn: 80.4555415\ttotal: 1m 32s\tremaining: 1m 58s\n",
            "419:\tlearn: 80.4511181\ttotal: 1m 32s\tremaining: 1m 58s\n",
            "420:\tlearn: 80.3863251\ttotal: 1m 33s\tremaining: 1m 58s\n",
            "421:\tlearn: 80.2440622\ttotal: 1m 33s\tremaining: 1m 58s\n",
            "422:\tlearn: 80.1925780\ttotal: 1m 33s\tremaining: 1m 58s\n",
            "423:\tlearn: 80.1771508\ttotal: 1m 33s\tremaining: 1m 57s\n",
            "424:\tlearn: 80.1756728\ttotal: 1m 34s\tremaining: 1m 57s\n",
            "425:\tlearn: 80.0948918\ttotal: 1m 34s\tremaining: 1m 57s\n",
            "426:\tlearn: 80.0805174\ttotal: 1m 34s\tremaining: 1m 57s\n",
            "427:\tlearn: 79.9159695\ttotal: 1m 34s\tremaining: 1m 56s\n",
            "428:\tlearn: 79.8111281\ttotal: 1m 35s\tremaining: 1m 56s\n",
            "429:\tlearn: 79.7797839\ttotal: 1m 35s\tremaining: 1m 56s\n",
            "430:\tlearn: 79.7663352\ttotal: 1m 35s\tremaining: 1m 56s\n",
            "431:\tlearn: 79.7470810\ttotal: 1m 35s\tremaining: 1m 56s\n",
            "432:\tlearn: 79.4268297\ttotal: 1m 35s\tremaining: 1m 55s\n",
            "433:\tlearn: 79.3465633\ttotal: 1m 36s\tremaining: 1m 55s\n",
            "434:\tlearn: 79.3113766\ttotal: 1m 36s\tremaining: 1m 55s\n",
            "435:\tlearn: 79.2195811\ttotal: 1m 36s\tremaining: 1m 55s\n",
            "436:\tlearn: 79.1864393\ttotal: 1m 36s\tremaining: 1m 54s\n",
            "437:\tlearn: 78.9961262\ttotal: 1m 37s\tremaining: 1m 54s\n",
            "438:\tlearn: 78.9704704\ttotal: 1m 37s\tremaining: 1m 54s\n",
            "439:\tlearn: 78.9188101\ttotal: 1m 37s\tremaining: 1m 54s\n",
            "440:\tlearn: 78.8500096\ttotal: 1m 37s\tremaining: 1m 54s\n",
            "441:\tlearn: 78.6551165\ttotal: 1m 37s\tremaining: 1m 53s\n",
            "442:\tlearn: 78.6519808\ttotal: 1m 38s\tremaining: 1m 53s\n",
            "443:\tlearn: 78.6387123\ttotal: 1m 38s\tremaining: 1m 53s\n",
            "444:\tlearn: 78.5309957\ttotal: 1m 38s\tremaining: 1m 53s\n",
            "445:\tlearn: 78.3970884\ttotal: 1m 38s\tremaining: 1m 53s\n",
            "446:\tlearn: 78.3799636\ttotal: 1m 39s\tremaining: 1m 53s\n",
            "447:\tlearn: 78.3233510\ttotal: 1m 39s\tremaining: 1m 52s\n",
            "448:\tlearn: 78.2944630\ttotal: 1m 39s\tremaining: 1m 52s\n",
            "449:\tlearn: 78.2593085\ttotal: 1m 39s\tremaining: 1m 52s\n",
            "450:\tlearn: 78.2581981\ttotal: 1m 40s\tremaining: 1m 52s\n",
            "451:\tlearn: 78.2081869\ttotal: 1m 40s\tremaining: 1m 51s\n",
            "452:\tlearn: 78.0645398\ttotal: 1m 40s\tremaining: 1m 51s\n",
            "453:\tlearn: 77.9551001\ttotal: 1m 40s\tremaining: 1m 51s\n",
            "454:\tlearn: 77.9545197\ttotal: 1m 40s\tremaining: 1m 51s\n",
            "455:\tlearn: 77.8304362\ttotal: 1m 41s\tremaining: 1m 50s\n",
            "456:\tlearn: 77.7435019\ttotal: 1m 41s\tremaining: 1m 50s\n",
            "457:\tlearn: 77.7026873\ttotal: 1m 41s\tremaining: 1m 50s\n",
            "458:\tlearn: 77.6350355\ttotal: 1m 41s\tremaining: 1m 50s\n",
            "459:\tlearn: 77.5165405\ttotal: 1m 41s\tremaining: 1m 49s\n",
            "460:\tlearn: 77.4504279\ttotal: 1m 42s\tremaining: 1m 49s\n",
            "461:\tlearn: 77.4303159\ttotal: 1m 42s\tremaining: 1m 49s\n",
            "462:\tlearn: 77.3898532\ttotal: 1m 42s\tremaining: 1m 49s\n",
            "463:\tlearn: 77.2678137\ttotal: 1m 42s\tremaining: 1m 49s\n",
            "464:\tlearn: 77.2183246\ttotal: 1m 43s\tremaining: 1m 48s\n",
            "465:\tlearn: 77.1495741\ttotal: 1m 43s\tremaining: 1m 48s\n",
            "466:\tlearn: 77.0718582\ttotal: 1m 43s\tremaining: 1m 48s\n",
            "467:\tlearn: 77.0355267\ttotal: 1m 43s\tremaining: 1m 48s\n",
            "468:\tlearn: 77.0349415\ttotal: 1m 43s\tremaining: 1m 47s\n",
            "469:\tlearn: 76.9611340\ttotal: 1m 44s\tremaining: 1m 47s\n",
            "470:\tlearn: 76.9316440\ttotal: 1m 44s\tremaining: 1m 47s\n",
            "471:\tlearn: 76.9212277\ttotal: 1m 44s\tremaining: 1m 47s\n",
            "472:\tlearn: 76.8943862\ttotal: 1m 44s\tremaining: 1m 47s\n",
            "473:\tlearn: 76.8713617\ttotal: 1m 45s\tremaining: 1m 46s\n",
            "474:\tlearn: 76.8202169\ttotal: 1m 45s\tremaining: 1m 46s\n",
            "475:\tlearn: 76.7462223\ttotal: 1m 45s\tremaining: 1m 46s\n",
            "476:\tlearn: 76.6795961\ttotal: 1m 45s\tremaining: 1m 46s\n",
            "477:\tlearn: 76.5373387\ttotal: 1m 46s\tremaining: 1m 46s\n",
            "478:\tlearn: 76.5135877\ttotal: 1m 46s\tremaining: 1m 45s\n",
            "479:\tlearn: 76.4448203\ttotal: 1m 46s\tremaining: 1m 45s\n",
            "480:\tlearn: 76.3607546\ttotal: 1m 46s\tremaining: 1m 45s\n",
            "481:\tlearn: 76.2529915\ttotal: 1m 47s\tremaining: 1m 45s\n",
            "482:\tlearn: 76.1480293\ttotal: 1m 47s\tremaining: 1m 45s\n",
            "483:\tlearn: 76.0980612\ttotal: 1m 47s\tremaining: 1m 44s\n",
            "484:\tlearn: 76.0288123\ttotal: 1m 47s\tremaining: 1m 44s\n",
            "485:\tlearn: 75.9463234\ttotal: 1m 47s\tremaining: 1m 44s\n",
            "486:\tlearn: 75.8651636\ttotal: 1m 48s\tremaining: 1m 44s\n",
            "487:\tlearn: 75.8650037\ttotal: 1m 48s\tremaining: 1m 43s\n",
            "488:\tlearn: 75.8000485\ttotal: 1m 48s\tremaining: 1m 43s\n",
            "489:\tlearn: 75.6968939\ttotal: 1m 48s\tremaining: 1m 43s\n",
            "490:\tlearn: 75.6241343\ttotal: 1m 49s\tremaining: 1m 43s\n",
            "491:\tlearn: 75.5832717\ttotal: 1m 49s\tremaining: 1m 43s\n",
            "492:\tlearn: 75.5465942\ttotal: 1m 49s\tremaining: 1m 42s\n",
            "493:\tlearn: 75.4598629\ttotal: 1m 49s\tremaining: 1m 42s\n",
            "494:\tlearn: 75.4442053\ttotal: 1m 50s\tremaining: 1m 42s\n",
            "495:\tlearn: 75.4086640\ttotal: 1m 50s\tremaining: 1m 42s\n",
            "496:\tlearn: 75.2774812\ttotal: 1m 50s\tremaining: 1m 42s\n",
            "497:\tlearn: 75.2394145\ttotal: 1m 50s\tremaining: 1m 41s\n",
            "498:\tlearn: 75.2034544\ttotal: 1m 50s\tremaining: 1m 41s\n",
            "499:\tlearn: 75.1444695\ttotal: 1m 51s\tremaining: 1m 41s\n",
            "500:\tlearn: 75.1289485\ttotal: 1m 51s\tremaining: 1m 41s\n",
            "501:\tlearn: 75.1283559\ttotal: 1m 51s\tremaining: 1m 40s\n",
            "502:\tlearn: 75.0795128\ttotal: 1m 51s\tremaining: 1m 40s\n",
            "503:\tlearn: 75.0614811\ttotal: 1m 52s\tremaining: 1m 40s\n",
            "504:\tlearn: 75.0598851\ttotal: 1m 52s\tremaining: 1m 40s\n",
            "505:\tlearn: 74.8500629\ttotal: 1m 52s\tremaining: 1m 40s\n",
            "506:\tlearn: 74.7913541\ttotal: 1m 52s\tremaining: 1m 39s\n",
            "507:\tlearn: 74.7337632\ttotal: 1m 52s\tremaining: 1m 39s\n",
            "508:\tlearn: 74.6545723\ttotal: 1m 53s\tremaining: 1m 39s\n",
            "509:\tlearn: 74.6167682\ttotal: 1m 53s\tremaining: 1m 39s\n",
            "510:\tlearn: 74.5467751\ttotal: 1m 53s\tremaining: 1m 38s\n",
            "511:\tlearn: 74.4548777\ttotal: 1m 53s\tremaining: 1m 38s\n",
            "512:\tlearn: 74.3663068\ttotal: 1m 53s\tremaining: 1m 38s\n",
            "513:\tlearn: 74.2811635\ttotal: 1m 54s\tremaining: 1m 38s\n",
            "514:\tlearn: 74.2339692\ttotal: 1m 54s\tremaining: 1m 37s\n",
            "515:\tlearn: 74.1623891\ttotal: 1m 54s\tremaining: 1m 37s\n",
            "516:\tlearn: 74.1244399\ttotal: 1m 54s\tremaining: 1m 37s\n",
            "517:\tlearn: 74.0797133\ttotal: 1m 55s\tremaining: 1m 37s\n",
            "518:\tlearn: 74.0617419\ttotal: 1m 55s\tremaining: 1m 37s\n",
            "519:\tlearn: 74.0031188\ttotal: 1m 55s\tremaining: 1m 36s\n",
            "520:\tlearn: 73.8928369\ttotal: 1m 55s\tremaining: 1m 36s\n",
            "521:\tlearn: 73.8821385\ttotal: 1m 56s\tremaining: 1m 36s\n",
            "522:\tlearn: 73.8627664\ttotal: 1m 56s\tremaining: 1m 36s\n",
            "523:\tlearn: 73.8620081\ttotal: 1m 56s\tremaining: 1m 36s\n",
            "524:\tlearn: 73.7450095\ttotal: 1m 56s\tremaining: 1m 35s\n",
            "525:\tlearn: 73.6693427\ttotal: 1m 56s\tremaining: 1m 35s\n",
            "526:\tlearn: 73.5203992\ttotal: 1m 57s\tremaining: 1m 35s\n",
            "527:\tlearn: 73.4669270\ttotal: 1m 57s\tremaining: 1m 35s\n",
            "528:\tlearn: 73.4320681\ttotal: 1m 57s\tremaining: 1m 34s\n",
            "529:\tlearn: 73.3996924\ttotal: 1m 57s\tremaining: 1m 34s\n",
            "530:\tlearn: 73.3241208\ttotal: 1m 58s\tremaining: 1m 34s\n",
            "531:\tlearn: 73.2991145\ttotal: 1m 58s\tremaining: 1m 34s\n",
            "532:\tlearn: 73.2782847\ttotal: 1m 58s\tremaining: 1m 34s\n",
            "533:\tlearn: 73.2489507\ttotal: 1m 58s\tremaining: 1m 33s\n",
            "534:\tlearn: 73.1860194\ttotal: 1m 58s\tremaining: 1m 33s\n",
            "535:\tlearn: 73.1694469\ttotal: 1m 59s\tremaining: 1m 33s\n",
            "536:\tlearn: 73.1025574\ttotal: 1m 59s\tremaining: 1m 33s\n",
            "537:\tlearn: 73.0983785\ttotal: 1m 59s\tremaining: 1m 33s\n",
            "538:\tlearn: 73.0748387\ttotal: 2m\tremaining: 1m 32s\n",
            "539:\tlearn: 73.0026267\ttotal: 2m\tremaining: 1m 32s\n",
            "540:\tlearn: 72.9687017\ttotal: 2m\tremaining: 1m 32s\n",
            "541:\tlearn: 72.9428301\ttotal: 2m\tremaining: 1m 32s\n",
            "542:\tlearn: 72.9161954\ttotal: 2m\tremaining: 1m 31s\n",
            "543:\tlearn: 72.8678711\ttotal: 2m 1s\tremaining: 1m 31s\n",
            "544:\tlearn: 72.8012909\ttotal: 2m 1s\tremaining: 1m 31s\n",
            "545:\tlearn: 72.7998939\ttotal: 2m 1s\tremaining: 1m 31s\n",
            "546:\tlearn: 72.7997002\ttotal: 2m 1s\tremaining: 1m 31s\n",
            "547:\tlearn: 72.7597308\ttotal: 2m 1s\tremaining: 1m 30s\n",
            "548:\tlearn: 72.7342085\ttotal: 2m 2s\tremaining: 1m 30s\n",
            "549:\tlearn: 72.6691709\ttotal: 2m 2s\tremaining: 1m 30s\n",
            "550:\tlearn: 72.6156779\ttotal: 2m 2s\tremaining: 1m 30s\n",
            "551:\tlearn: 72.4848464\ttotal: 2m 2s\tremaining: 1m 29s\n",
            "552:\tlearn: 72.4298156\ttotal: 2m 3s\tremaining: 1m 29s\n",
            "553:\tlearn: 72.3887551\ttotal: 2m 3s\tremaining: 1m 29s\n",
            "554:\tlearn: 72.3404573\ttotal: 2m 3s\tremaining: 1m 29s\n",
            "555:\tlearn: 72.3024408\ttotal: 2m 3s\tremaining: 1m 29s\n",
            "556:\tlearn: 72.3019626\ttotal: 2m 4s\tremaining: 1m 28s\n",
            "557:\tlearn: 72.2836908\ttotal: 2m 4s\tremaining: 1m 28s\n",
            "558:\tlearn: 72.2731569\ttotal: 2m 4s\tremaining: 1m 28s\n",
            "559:\tlearn: 72.2448711\ttotal: 2m 4s\tremaining: 1m 28s\n",
            "560:\tlearn: 72.2184062\ttotal: 2m 5s\tremaining: 1m 28s\n",
            "561:\tlearn: 72.1953087\ttotal: 2m 5s\tremaining: 1m 27s\n",
            "562:\tlearn: 72.1934804\ttotal: 2m 5s\tremaining: 1m 27s\n",
            "563:\tlearn: 72.0620344\ttotal: 2m 5s\tremaining: 1m 27s\n",
            "564:\tlearn: 71.9688322\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "565:\tlearn: 71.8381358\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "566:\tlearn: 71.8126865\ttotal: 2m 6s\tremaining: 1m 26s\n",
            "567:\tlearn: 71.8048220\ttotal: 2m 6s\tremaining: 1m 26s\n",
            "568:\tlearn: 71.7801066\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "569:\tlearn: 71.7225803\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "570:\tlearn: 71.6632624\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "571:\tlearn: 71.6423445\ttotal: 2m 7s\tremaining: 1m 25s\n",
            "572:\tlearn: 71.6163078\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "573:\tlearn: 71.5917235\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "574:\tlearn: 71.5613585\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "575:\tlearn: 71.5210385\ttotal: 2m 8s\tremaining: 1m 24s\n",
            "576:\tlearn: 71.4976549\ttotal: 2m 8s\tremaining: 1m 24s\n",
            "577:\tlearn: 71.4611950\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "578:\tlearn: 71.4422815\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "579:\tlearn: 71.4142404\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "580:\tlearn: 71.4137049\ttotal: 2m 9s\tremaining: 1m 23s\n",
            "581:\tlearn: 71.4033762\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "582:\tlearn: 71.3712805\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "583:\tlearn: 71.3380646\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "584:\tlearn: 71.3010960\ttotal: 2m 10s\tremaining: 1m 22s\n",
            "585:\tlearn: 71.3005817\ttotal: 2m 10s\tremaining: 1m 22s\n",
            "586:\tlearn: 71.2998295\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "587:\tlearn: 71.2773896\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "588:\tlearn: 71.2647167\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "589:\tlearn: 71.1959064\ttotal: 2m 11s\tremaining: 1m 21s\n",
            "590:\tlearn: 71.0771178\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "591:\tlearn: 71.0078798\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "592:\tlearn: 70.9772606\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "593:\tlearn: 70.9594159\ttotal: 2m 12s\tremaining: 1m 20s\n",
            "594:\tlearn: 70.9404186\ttotal: 2m 12s\tremaining: 1m 20s\n",
            "595:\tlearn: 70.9212360\ttotal: 2m 13s\tremaining: 1m 20s\n",
            "596:\tlearn: 70.8584688\ttotal: 2m 13s\tremaining: 1m 20s\n",
            "597:\tlearn: 70.8468734\ttotal: 2m 13s\tremaining: 1m 20s\n",
            "598:\tlearn: 70.8077254\ttotal: 2m 13s\tremaining: 1m 19s\n",
            "599:\tlearn: 70.6998383\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "600:\tlearn: 70.5893299\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "601:\tlearn: 70.4964194\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "602:\tlearn: 70.4708372\ttotal: 2m 14s\tremaining: 1m 18s\n",
            "603:\tlearn: 70.4554915\ttotal: 2m 14s\tremaining: 1m 18s\n",
            "604:\tlearn: 70.4247741\ttotal: 2m 15s\tremaining: 1m 18s\n",
            "605:\tlearn: 70.4234977\ttotal: 2m 15s\tremaining: 1m 18s\n",
            "606:\tlearn: 70.3456595\ttotal: 2m 15s\tremaining: 1m 17s\n",
            "607:\tlearn: 70.2082522\ttotal: 2m 15s\tremaining: 1m 17s\n",
            "608:\tlearn: 70.1735774\ttotal: 2m 16s\tremaining: 1m 17s\n",
            "609:\tlearn: 70.1359401\ttotal: 2m 16s\tremaining: 1m 17s\n",
            "610:\tlearn: 70.0863552\ttotal: 2m 16s\tremaining: 1m 17s\n",
            "611:\tlearn: 70.0703588\ttotal: 2m 16s\tremaining: 1m 16s\n",
            "612:\tlearn: 69.8972809\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "613:\tlearn: 69.7905699\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "614:\tlearn: 69.7094442\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "615:\tlearn: 69.5999656\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "616:\tlearn: 69.5068538\ttotal: 2m 17s\tremaining: 1m 15s\n",
            "617:\tlearn: 69.4031117\ttotal: 2m 18s\tremaining: 1m 15s\n",
            "618:\tlearn: 69.3232520\ttotal: 2m 18s\tremaining: 1m 15s\n",
            "619:\tlearn: 69.2734386\ttotal: 2m 18s\tremaining: 1m 15s\n",
            "620:\tlearn: 69.2408000\ttotal: 2m 18s\tremaining: 1m 14s\n",
            "621:\tlearn: 69.2024943\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "622:\tlearn: 69.1005092\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "623:\tlearn: 69.0590160\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "624:\tlearn: 68.8982835\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "625:\tlearn: 68.7636372\ttotal: 2m 20s\tremaining: 1m 13s\n",
            "626:\tlearn: 68.6833354\ttotal: 2m 20s\tremaining: 1m 13s\n",
            "627:\tlearn: 68.6748673\ttotal: 2m 20s\tremaining: 1m 13s\n",
            "628:\tlearn: 68.6698647\ttotal: 2m 21s\tremaining: 1m 13s\n",
            "629:\tlearn: 68.6259187\ttotal: 2m 21s\tremaining: 1m 13s\n",
            "630:\tlearn: 68.5477369\ttotal: 2m 21s\tremaining: 1m 12s\n",
            "631:\tlearn: 68.4651243\ttotal: 2m 21s\tremaining: 1m 12s\n",
            "632:\tlearn: 68.4280691\ttotal: 2m 22s\tremaining: 1m 12s\n",
            "633:\tlearn: 68.4056983\ttotal: 2m 22s\tremaining: 1m 12s\n",
            "634:\tlearn: 68.3713397\ttotal: 2m 22s\tremaining: 1m 12s\n",
            "635:\tlearn: 68.2908672\ttotal: 2m 22s\tremaining: 1m 11s\n",
            "636:\tlearn: 68.2904221\ttotal: 2m 23s\tremaining: 1m 11s\n",
            "637:\tlearn: 68.2244931\ttotal: 2m 23s\tremaining: 1m 11s\n",
            "638:\tlearn: 68.1617659\ttotal: 2m 23s\tremaining: 1m 11s\n",
            "639:\tlearn: 68.0796472\ttotal: 2m 23s\tremaining: 1m 10s\n",
            "640:\tlearn: 68.0525105\ttotal: 2m 23s\tremaining: 1m 10s\n",
            "641:\tlearn: 68.0519293\ttotal: 2m 24s\tremaining: 1m 10s\n",
            "642:\tlearn: 68.0512244\ttotal: 2m 24s\tremaining: 1m 10s\n",
            "643:\tlearn: 68.0139712\ttotal: 2m 24s\tremaining: 1m 10s\n",
            "644:\tlearn: 67.9545999\ttotal: 2m 24s\tremaining: 1m 9s\n",
            "645:\tlearn: 67.9294122\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "646:\tlearn: 67.8992606\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "647:\tlearn: 67.8318065\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "648:\tlearn: 67.8316981\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "649:\tlearn: 67.8316625\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "650:\tlearn: 67.8116168\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "651:\tlearn: 67.7882168\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "652:\tlearn: 67.7777154\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "653:\tlearn: 67.7372430\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "654:\tlearn: 67.7197545\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "655:\tlearn: 67.7195068\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "656:\tlearn: 67.7137081\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "657:\tlearn: 67.7037197\ttotal: 2m 28s\tremaining: 1m 7s\n",
            "658:\tlearn: 67.6905937\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "659:\tlearn: 67.6597425\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "660:\tlearn: 67.6199921\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "661:\tlearn: 67.5908077\ttotal: 2m 29s\tremaining: 1m 6s\n",
            "662:\tlearn: 67.5341161\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "663:\tlearn: 67.4651882\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "664:\tlearn: 67.4084739\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "665:\tlearn: 67.3544023\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "666:\tlearn: 67.3164805\ttotal: 2m 30s\tremaining: 1m 5s\n",
            "667:\tlearn: 67.2415587\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "668:\tlearn: 67.2362009\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "669:\tlearn: 67.2355704\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "670:\tlearn: 67.1910389\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "671:\tlearn: 67.1151729\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "672:\tlearn: 67.0831021\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "673:\tlearn: 67.0817456\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "674:\tlearn: 67.0805728\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "675:\tlearn: 67.0592371\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "676:\tlearn: 67.0586484\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "677:\tlearn: 67.0469822\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "678:\tlearn: 67.0468476\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "679:\tlearn: 66.8995630\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "680:\tlearn: 66.8800037\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "681:\tlearn: 66.8367334\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "682:\tlearn: 66.8154784\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "683:\tlearn: 66.8151924\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "684:\tlearn: 66.7990590\ttotal: 2m 34s\tremaining: 1m\n",
            "685:\tlearn: 66.7988532\ttotal: 2m 34s\tremaining: 1m\n",
            "686:\tlearn: 66.7710099\ttotal: 2m 34s\tremaining: 1m\n",
            "687:\tlearn: 66.7603411\ttotal: 2m 34s\tremaining: 1m\n",
            "688:\tlearn: 66.7202226\ttotal: 2m 35s\tremaining: 1m\n",
            "689:\tlearn: 66.6889896\ttotal: 2m 35s\tremaining: 59.9s\n",
            "690:\tlearn: 66.6889076\ttotal: 2m 35s\tremaining: 59.6s\n",
            "691:\tlearn: 66.6465090\ttotal: 2m 35s\tremaining: 59.4s\n",
            "692:\tlearn: 66.6245519\ttotal: 2m 35s\tremaining: 59.2s\n",
            "693:\tlearn: 66.6242270\ttotal: 2m 36s\tremaining: 59s\n",
            "694:\tlearn: 66.6241272\ttotal: 2m 36s\tremaining: 58.7s\n",
            "695:\tlearn: 66.6226468\ttotal: 2m 36s\tremaining: 58.5s\n",
            "696:\tlearn: 66.6086608\ttotal: 2m 36s\tremaining: 58.3s\n",
            "697:\tlearn: 66.5885746\ttotal: 2m 37s\tremaining: 58.1s\n",
            "698:\tlearn: 66.5173488\ttotal: 2m 37s\tremaining: 57.9s\n",
            "699:\tlearn: 66.5078745\ttotal: 2m 37s\tremaining: 57.6s\n",
            "700:\tlearn: 66.4777284\ttotal: 2m 37s\tremaining: 57.4s\n",
            "701:\tlearn: 66.4652472\ttotal: 2m 38s\tremaining: 57.2s\n",
            "702:\tlearn: 66.4593353\ttotal: 2m 38s\tremaining: 57s\n",
            "703:\tlearn: 66.4591877\ttotal: 2m 38s\tremaining: 56.7s\n",
            "704:\tlearn: 66.4143025\ttotal: 2m 38s\tremaining: 56.5s\n",
            "705:\tlearn: 66.3567914\ttotal: 2m 38s\tremaining: 56.3s\n",
            "706:\tlearn: 66.3564849\ttotal: 2m 39s\tremaining: 56.1s\n",
            "707:\tlearn: 66.3259433\ttotal: 2m 39s\tremaining: 55.9s\n",
            "708:\tlearn: 66.2730155\ttotal: 2m 39s\tremaining: 55.6s\n",
            "709:\tlearn: 66.1707364\ttotal: 2m 39s\tremaining: 55.4s\n",
            "710:\tlearn: 66.1418434\ttotal: 2m 40s\tremaining: 55.2s\n",
            "711:\tlearn: 66.0782886\ttotal: 2m 40s\tremaining: 55s\n",
            "712:\tlearn: 66.0067585\ttotal: 2m 40s\tremaining: 54.7s\n",
            "713:\tlearn: 65.9567074\ttotal: 2m 40s\tremaining: 54.5s\n",
            "714:\tlearn: 65.8920988\ttotal: 2m 40s\tremaining: 54.3s\n",
            "715:\tlearn: 65.8783000\ttotal: 2m 41s\tremaining: 54s\n",
            "716:\tlearn: 65.8460612\ttotal: 2m 41s\tremaining: 53.8s\n",
            "717:\tlearn: 65.8446959\ttotal: 2m 41s\tremaining: 53.6s\n",
            "718:\tlearn: 65.8209912\ttotal: 2m 41s\tremaining: 53.4s\n",
            "719:\tlearn: 65.8198108\ttotal: 2m 42s\tremaining: 53.1s\n",
            "720:\tlearn: 65.7842354\ttotal: 2m 42s\tremaining: 52.9s\n",
            "721:\tlearn: 65.7607489\ttotal: 2m 42s\tremaining: 52.7s\n",
            "722:\tlearn: 65.6900674\ttotal: 2m 42s\tremaining: 52.5s\n",
            "723:\tlearn: 65.6468438\ttotal: 2m 42s\tremaining: 52.2s\n",
            "724:\tlearn: 65.6357838\ttotal: 2m 43s\tremaining: 52s\n",
            "725:\tlearn: 65.6258241\ttotal: 2m 43s\tremaining: 51.8s\n",
            "726:\tlearn: 65.5613716\ttotal: 2m 43s\tremaining: 51.6s\n",
            "727:\tlearn: 65.5528979\ttotal: 2m 43s\tremaining: 51.3s\n",
            "728:\tlearn: 65.5435875\ttotal: 2m 44s\tremaining: 51.1s\n",
            "729:\tlearn: 65.5211502\ttotal: 2m 44s\tremaining: 50.9s\n",
            "730:\tlearn: 65.4819270\ttotal: 2m 44s\tremaining: 50.7s\n",
            "731:\tlearn: 65.3853184\ttotal: 2m 44s\tremaining: 50.4s\n",
            "732:\tlearn: 65.3520054\ttotal: 2m 44s\tremaining: 50.2s\n",
            "733:\tlearn: 65.3055268\ttotal: 2m 45s\tremaining: 50s\n",
            "734:\tlearn: 65.2966446\ttotal: 2m 45s\tremaining: 49.7s\n",
            "735:\tlearn: 65.2297358\ttotal: 2m 45s\tremaining: 49.5s\n",
            "736:\tlearn: 65.1807637\ttotal: 2m 45s\tremaining: 49.3s\n",
            "737:\tlearn: 65.0630006\ttotal: 2m 46s\tremaining: 49s\n",
            "738:\tlearn: 65.0080610\ttotal: 2m 46s\tremaining: 48.8s\n",
            "739:\tlearn: 64.9829095\ttotal: 2m 46s\tremaining: 48.6s\n",
            "740:\tlearn: 64.9689874\ttotal: 2m 46s\tremaining: 48.4s\n",
            "741:\tlearn: 64.9521323\ttotal: 2m 46s\tremaining: 48.1s\n",
            "742:\tlearn: 64.9520411\ttotal: 2m 47s\tremaining: 47.9s\n",
            "743:\tlearn: 64.8822120\ttotal: 2m 47s\tremaining: 47.7s\n",
            "744:\tlearn: 64.8649768\ttotal: 2m 47s\tremaining: 47.5s\n",
            "745:\tlearn: 64.8042443\ttotal: 2m 47s\tremaining: 47.2s\n",
            "746:\tlearn: 64.7567957\ttotal: 2m 48s\tremaining: 47s\n",
            "747:\tlearn: 64.6892376\ttotal: 2m 48s\tremaining: 46.8s\n",
            "748:\tlearn: 64.5882831\ttotal: 2m 48s\tremaining: 46.5s\n",
            "749:\tlearn: 64.5274264\ttotal: 2m 48s\tremaining: 46.3s\n",
            "750:\tlearn: 64.5273421\ttotal: 2m 48s\tremaining: 46.1s\n",
            "751:\tlearn: 64.4351024\ttotal: 2m 49s\tremaining: 45.9s\n",
            "752:\tlearn: 64.3938414\ttotal: 2m 49s\tremaining: 45.6s\n",
            "753:\tlearn: 64.3803603\ttotal: 2m 49s\tremaining: 45.4s\n",
            "754:\tlearn: 64.3146516\ttotal: 2m 49s\tremaining: 45.2s\n",
            "755:\tlearn: 64.2831114\ttotal: 2m 50s\tremaining: 45s\n",
            "756:\tlearn: 64.2278202\ttotal: 2m 50s\tremaining: 44.8s\n",
            "757:\tlearn: 64.2076563\ttotal: 2m 50s\tremaining: 44.6s\n",
            "758:\tlearn: 64.1841932\ttotal: 2m 50s\tremaining: 44.3s\n",
            "759:\tlearn: 64.1725336\ttotal: 2m 51s\tremaining: 44.1s\n",
            "760:\tlearn: 64.1556572\ttotal: 2m 51s\tremaining: 43.9s\n",
            "761:\tlearn: 64.1476821\ttotal: 2m 51s\tremaining: 43.7s\n",
            "762:\tlearn: 64.1216539\ttotal: 2m 52s\tremaining: 43.5s\n",
            "763:\tlearn: 64.0964570\ttotal: 2m 52s\tremaining: 43.3s\n",
            "764:\tlearn: 64.0715885\ttotal: 2m 52s\tremaining: 43.1s\n",
            "765:\tlearn: 64.0338031\ttotal: 2m 52s\tremaining: 42.9s\n",
            "766:\tlearn: 64.0009864\ttotal: 2m 53s\tremaining: 42.6s\n",
            "767:\tlearn: 63.9603947\ttotal: 2m 53s\tremaining: 42.4s\n",
            "768:\tlearn: 63.9388636\ttotal: 2m 53s\tremaining: 42.2s\n",
            "769:\tlearn: 63.8937664\ttotal: 2m 53s\tremaining: 42s\n",
            "770:\tlearn: 63.8912165\ttotal: 2m 53s\tremaining: 41.7s\n",
            "771:\tlearn: 63.8186303\ttotal: 2m 54s\tremaining: 41.5s\n",
            "772:\tlearn: 63.7755637\ttotal: 2m 54s\tremaining: 41.3s\n",
            "773:\tlearn: 63.7480586\ttotal: 2m 54s\tremaining: 41s\n",
            "774:\tlearn: 63.6709954\ttotal: 2m 54s\tremaining: 40.8s\n",
            "775:\tlearn: 63.5826495\ttotal: 2m 55s\tremaining: 40.6s\n",
            "776:\tlearn: 63.5038198\ttotal: 2m 55s\tremaining: 40.4s\n",
            "777:\tlearn: 63.4424728\ttotal: 2m 55s\tremaining: 40.1s\n",
            "778:\tlearn: 63.4091051\ttotal: 2m 55s\tremaining: 39.9s\n",
            "779:\tlearn: 63.3334547\ttotal: 2m 55s\tremaining: 39.7s\n",
            "780:\tlearn: 63.3130831\ttotal: 2m 56s\tremaining: 39.5s\n",
            "781:\tlearn: 63.3044295\ttotal: 2m 56s\tremaining: 39.2s\n",
            "782:\tlearn: 63.2957758\ttotal: 2m 56s\tremaining: 39s\n",
            "783:\tlearn: 63.2363053\ttotal: 2m 56s\tremaining: 38.8s\n",
            "784:\tlearn: 63.2168484\ttotal: 2m 56s\tremaining: 38.5s\n",
            "785:\tlearn: 63.2031395\ttotal: 2m 57s\tremaining: 38.3s\n",
            "786:\tlearn: 63.1675762\ttotal: 2m 57s\tremaining: 38.1s\n",
            "787:\tlearn: 63.1406290\ttotal: 2m 57s\tremaining: 37.9s\n",
            "788:\tlearn: 63.1140532\ttotal: 2m 57s\tremaining: 37.7s\n",
            "789:\tlearn: 63.1128357\ttotal: 2m 58s\tremaining: 37.4s\n",
            "790:\tlearn: 63.1116507\ttotal: 2m 58s\tremaining: 37.2s\n",
            "791:\tlearn: 63.0936262\ttotal: 2m 58s\tremaining: 37s\n",
            "792:\tlearn: 63.0839553\ttotal: 2m 58s\tremaining: 36.8s\n",
            "793:\tlearn: 63.0463761\ttotal: 2m 59s\tremaining: 36.5s\n",
            "794:\tlearn: 63.0174477\ttotal: 2m 59s\tremaining: 36.3s\n",
            "795:\tlearn: 62.9935273\ttotal: 2m 59s\tremaining: 36.1s\n",
            "796:\tlearn: 62.9549539\ttotal: 2m 59s\tremaining: 35.8s\n",
            "797:\tlearn: 62.9321071\ttotal: 2m 59s\tremaining: 35.6s\n",
            "798:\tlearn: 62.9120458\ttotal: 3m\tremaining: 35.4s\n",
            "799:\tlearn: 62.9051934\ttotal: 3m\tremaining: 35.2s\n",
            "800:\tlearn: 62.8729242\ttotal: 3m\tremaining: 35s\n",
            "801:\tlearn: 62.8032375\ttotal: 3m\tremaining: 34.8s\n",
            "802:\tlearn: 62.7860979\ttotal: 3m 1s\tremaining: 34.5s\n",
            "803:\tlearn: 62.7607439\ttotal: 3m 1s\tremaining: 34.3s\n",
            "804:\tlearn: 62.7060491\ttotal: 3m 1s\tremaining: 34.1s\n",
            "805:\tlearn: 62.6178531\ttotal: 3m 1s\tremaining: 33.8s\n",
            "806:\tlearn: 62.5344114\ttotal: 3m 2s\tremaining: 33.6s\n",
            "807:\tlearn: 62.4776165\ttotal: 3m 2s\tremaining: 33.4s\n",
            "808:\tlearn: 62.4638145\ttotal: 3m 2s\tremaining: 33.2s\n",
            "809:\tlearn: 62.4334919\ttotal: 3m 2s\tremaining: 32.9s\n",
            "810:\tlearn: 62.3908375\ttotal: 3m 2s\tremaining: 32.7s\n",
            "811:\tlearn: 62.3247223\ttotal: 3m 3s\tremaining: 32.5s\n",
            "812:\tlearn: 62.2884736\ttotal: 3m 3s\tremaining: 32.2s\n",
            "813:\tlearn: 62.2868966\ttotal: 3m 3s\tremaining: 32s\n",
            "814:\tlearn: 62.2714919\ttotal: 3m 3s\tremaining: 31.8s\n",
            "815:\tlearn: 62.2676063\ttotal: 3m 4s\tremaining: 31.6s\n",
            "816:\tlearn: 62.2580626\ttotal: 3m 4s\tremaining: 31.3s\n",
            "817:\tlearn: 62.2303254\ttotal: 3m 4s\tremaining: 31.1s\n",
            "818:\tlearn: 62.2151452\ttotal: 3m 4s\tremaining: 30.9s\n",
            "819:\tlearn: 62.1965918\ttotal: 3m 5s\tremaining: 30.7s\n",
            "820:\tlearn: 62.1716652\ttotal: 3m 5s\tremaining: 30.5s\n",
            "821:\tlearn: 62.1145203\ttotal: 3m 5s\tremaining: 30.2s\n",
            "822:\tlearn: 62.0946165\ttotal: 3m 5s\tremaining: 30s\n",
            "823:\tlearn: 62.0777087\ttotal: 3m 5s\tremaining: 29.8s\n",
            "824:\tlearn: 62.0541521\ttotal: 3m 6s\tremaining: 29.6s\n",
            "825:\tlearn: 62.0154667\ttotal: 3m 6s\tremaining: 29.4s\n",
            "826:\tlearn: 61.9780779\ttotal: 3m 6s\tremaining: 29.1s\n",
            "827:\tlearn: 61.9604269\ttotal: 3m 6s\tremaining: 28.9s\n",
            "828:\tlearn: 61.9508640\ttotal: 3m 7s\tremaining: 28.7s\n",
            "829:\tlearn: 61.9231600\ttotal: 3m 7s\tremaining: 28.4s\n",
            "830:\tlearn: 61.8878790\ttotal: 3m 7s\tremaining: 28.2s\n",
            "831:\tlearn: 61.8876667\ttotal: 3m 7s\tremaining: 28s\n",
            "832:\tlearn: 61.8862325\ttotal: 3m 8s\tremaining: 27.8s\n",
            "833:\tlearn: 61.8387609\ttotal: 3m 8s\tremaining: 27.5s\n",
            "834:\tlearn: 61.8364075\ttotal: 3m 8s\tremaining: 27.3s\n",
            "835:\tlearn: 61.7398303\ttotal: 3m 8s\tremaining: 27.1s\n",
            "836:\tlearn: 61.7285084\ttotal: 3m 9s\tremaining: 26.9s\n",
            "837:\tlearn: 61.6807683\ttotal: 3m 9s\tremaining: 26.7s\n",
            "838:\tlearn: 61.6679479\ttotal: 3m 9s\tremaining: 26.4s\n",
            "839:\tlearn: 61.5918467\ttotal: 3m 9s\tremaining: 26.2s\n",
            "840:\tlearn: 61.5496824\ttotal: 3m 9s\tremaining: 26s\n",
            "841:\tlearn: 61.5274508\ttotal: 3m 10s\tremaining: 25.8s\n",
            "842:\tlearn: 61.4974433\ttotal: 3m 10s\tremaining: 25.5s\n",
            "843:\tlearn: 61.4813075\ttotal: 3m 10s\tremaining: 25.3s\n",
            "844:\tlearn: 61.4734777\ttotal: 3m 11s\tremaining: 25.1s\n",
            "845:\tlearn: 61.4453325\ttotal: 3m 11s\tremaining: 24.9s\n",
            "846:\tlearn: 61.4186434\ttotal: 3m 11s\tremaining: 24.6s\n",
            "847:\tlearn: 61.3427058\ttotal: 3m 11s\tremaining: 24.4s\n",
            "848:\tlearn: 61.3174031\ttotal: 3m 11s\tremaining: 24.2s\n",
            "849:\tlearn: 61.2265121\ttotal: 3m 12s\tremaining: 24s\n",
            "850:\tlearn: 61.2085604\ttotal: 3m 12s\tremaining: 23.7s\n",
            "851:\tlearn: 61.1770033\ttotal: 3m 12s\tremaining: 23.5s\n",
            "852:\tlearn: 61.1666804\ttotal: 3m 12s\tremaining: 23.3s\n",
            "853:\tlearn: 61.1607232\ttotal: 3m 13s\tremaining: 23.1s\n",
            "854:\tlearn: 61.0991076\ttotal: 3m 13s\tremaining: 22.8s\n",
            "855:\tlearn: 61.0670454\ttotal: 3m 13s\tremaining: 22.6s\n",
            "856:\tlearn: 61.0239141\ttotal: 3m 13s\tremaining: 22.4s\n",
            "857:\tlearn: 61.0159968\ttotal: 3m 14s\tremaining: 22.2s\n",
            "858:\tlearn: 60.9796110\ttotal: 3m 14s\tremaining: 21.9s\n",
            "859:\tlearn: 60.9604374\ttotal: 3m 14s\tremaining: 21.7s\n",
            "860:\tlearn: 60.9047553\ttotal: 3m 14s\tremaining: 21.5s\n",
            "861:\tlearn: 60.8880840\ttotal: 3m 14s\tremaining: 21.3s\n",
            "862:\tlearn: 60.8836129\ttotal: 3m 15s\tremaining: 21s\n",
            "863:\tlearn: 60.8074355\ttotal: 3m 15s\tremaining: 20.8s\n",
            "864:\tlearn: 60.7974519\ttotal: 3m 15s\tremaining: 20.6s\n",
            "865:\tlearn: 60.7969753\ttotal: 3m 15s\tremaining: 20.4s\n",
            "866:\tlearn: 60.7754950\ttotal: 3m 16s\tremaining: 20.1s\n",
            "867:\tlearn: 60.7730014\ttotal: 3m 16s\tremaining: 19.9s\n",
            "868:\tlearn: 60.7496205\ttotal: 3m 16s\tremaining: 19.7s\n",
            "869:\tlearn: 60.7482403\ttotal: 3m 16s\tremaining: 19.4s\n",
            "870:\tlearn: 60.7338838\ttotal: 3m 16s\tremaining: 19.2s\n",
            "871:\tlearn: 60.7334135\ttotal: 3m 17s\tremaining: 19s\n",
            "872:\tlearn: 60.7143724\ttotal: 3m 17s\tremaining: 18.8s\n",
            "873:\tlearn: 60.6851685\ttotal: 3m 17s\tremaining: 18.6s\n",
            "874:\tlearn: 60.6673552\ttotal: 3m 18s\tremaining: 18.3s\n",
            "875:\tlearn: 60.6414984\ttotal: 3m 18s\tremaining: 18.1s\n",
            "876:\tlearn: 60.6225566\ttotal: 3m 18s\tremaining: 17.9s\n",
            "877:\tlearn: 60.6098986\ttotal: 3m 18s\tremaining: 17.7s\n",
            "878:\tlearn: 60.5870192\ttotal: 3m 19s\tremaining: 17.4s\n",
            "879:\tlearn: 60.5275623\ttotal: 3m 19s\tremaining: 17.2s\n",
            "880:\tlearn: 60.4842763\ttotal: 3m 19s\tremaining: 17s\n",
            "881:\tlearn: 60.4679896\ttotal: 3m 19s\tremaining: 16.8s\n",
            "882:\tlearn: 60.4406636\ttotal: 3m 19s\tremaining: 16.5s\n",
            "883:\tlearn: 60.4061459\ttotal: 3m 20s\tremaining: 16.3s\n",
            "884:\tlearn: 60.3126885\ttotal: 3m 20s\tremaining: 16.1s\n",
            "885:\tlearn: 60.2959984\ttotal: 3m 20s\tremaining: 15.8s\n",
            "886:\tlearn: 60.2869838\ttotal: 3m 20s\tremaining: 15.6s\n",
            "887:\tlearn: 60.2093250\ttotal: 3m 21s\tremaining: 15.4s\n",
            "888:\tlearn: 60.1615149\ttotal: 3m 21s\tremaining: 15.2s\n",
            "889:\tlearn: 60.1315699\ttotal: 3m 21s\tremaining: 14.9s\n",
            "890:\tlearn: 60.0938094\ttotal: 3m 21s\tremaining: 14.7s\n",
            "891:\tlearn: 60.0827527\ttotal: 3m 21s\tremaining: 14.5s\n",
            "892:\tlearn: 60.0540287\ttotal: 3m 22s\tremaining: 14.3s\n",
            "893:\tlearn: 60.0333525\ttotal: 3m 22s\tremaining: 14s\n",
            "894:\tlearn: 60.0121185\ttotal: 3m 22s\tremaining: 13.8s\n",
            "895:\tlearn: 59.9680559\ttotal: 3m 22s\tremaining: 13.6s\n",
            "896:\tlearn: 59.9052859\ttotal: 3m 23s\tremaining: 13.4s\n",
            "897:\tlearn: 59.8248836\ttotal: 3m 23s\tremaining: 13.1s\n",
            "898:\tlearn: 59.8110556\ttotal: 3m 23s\tremaining: 12.9s\n",
            "899:\tlearn: 59.7762423\ttotal: 3m 23s\tremaining: 12.7s\n",
            "900:\tlearn: 59.7284941\ttotal: 3m 24s\tremaining: 12.5s\n",
            "901:\tlearn: 59.7148343\ttotal: 3m 24s\tremaining: 12.2s\n",
            "902:\tlearn: 59.6288773\ttotal: 3m 24s\tremaining: 12s\n",
            "903:\tlearn: 59.5907019\ttotal: 3m 24s\tremaining: 11.8s\n",
            "904:\tlearn: 59.5678905\ttotal: 3m 25s\tremaining: 11.6s\n",
            "905:\tlearn: 59.5554432\ttotal: 3m 25s\tremaining: 11.3s\n",
            "906:\tlearn: 59.4983261\ttotal: 3m 25s\tremaining: 11.1s\n",
            "907:\tlearn: 59.4706424\ttotal: 3m 25s\tremaining: 10.9s\n",
            "908:\tlearn: 59.4038884\ttotal: 3m 26s\tremaining: 10.7s\n",
            "909:\tlearn: 59.3588473\ttotal: 3m 26s\tremaining: 10.4s\n",
            "910:\tlearn: 59.2673492\ttotal: 3m 26s\tremaining: 10.2s\n",
            "911:\tlearn: 59.2306610\ttotal: 3m 26s\tremaining: 9.97s\n",
            "912:\tlearn: 59.2209460\ttotal: 3m 26s\tremaining: 9.74s\n",
            "913:\tlearn: 59.1763605\ttotal: 3m 27s\tremaining: 9.51s\n",
            "914:\tlearn: 59.1262000\ttotal: 3m 27s\tremaining: 9.29s\n",
            "915:\tlearn: 59.1258213\ttotal: 3m 27s\tremaining: 9.06s\n",
            "916:\tlearn: 59.1214210\ttotal: 3m 27s\tremaining: 8.83s\n",
            "917:\tlearn: 59.0580874\ttotal: 3m 27s\tremaining: 8.61s\n",
            "918:\tlearn: 58.9998292\ttotal: 3m 28s\tremaining: 8.38s\n",
            "919:\tlearn: 58.9389858\ttotal: 3m 28s\tremaining: 8.15s\n",
            "920:\tlearn: 58.8894838\ttotal: 3m 28s\tremaining: 7.92s\n",
            "921:\tlearn: 58.8689725\ttotal: 3m 28s\tremaining: 7.7s\n",
            "922:\tlearn: 58.8663806\ttotal: 3m 28s\tremaining: 7.47s\n",
            "923:\tlearn: 58.8339848\ttotal: 3m 29s\tremaining: 7.25s\n",
            "924:\tlearn: 58.8260056\ttotal: 3m 29s\tremaining: 7.02s\n",
            "925:\tlearn: 58.8232472\ttotal: 3m 29s\tremaining: 6.8s\n",
            "926:\tlearn: 58.8028380\ttotal: 3m 30s\tremaining: 6.57s\n",
            "927:\tlearn: 58.7965530\ttotal: 3m 30s\tremaining: 6.34s\n",
            "928:\tlearn: 58.7726845\ttotal: 3m 30s\tremaining: 6.12s\n",
            "929:\tlearn: 58.7121796\ttotal: 3m 30s\tremaining: 5.89s\n",
            "930:\tlearn: 58.6922060\ttotal: 3m 30s\tremaining: 5.66s\n",
            "931:\tlearn: 58.6347845\ttotal: 3m 31s\tremaining: 5.44s\n",
            "932:\tlearn: 58.5911577\ttotal: 3m 31s\tremaining: 5.21s\n",
            "933:\tlearn: 58.5559977\ttotal: 3m 31s\tremaining: 4.98s\n",
            "934:\tlearn: 58.5476850\ttotal: 3m 31s\tremaining: 4.76s\n",
            "935:\tlearn: 58.5263636\ttotal: 3m 32s\tremaining: 4.53s\n",
            "936:\tlearn: 58.4790835\ttotal: 3m 32s\tremaining: 4.3s\n",
            "937:\tlearn: 58.4413117\ttotal: 3m 32s\tremaining: 4.08s\n",
            "938:\tlearn: 58.3888457\ttotal: 3m 32s\tremaining: 3.85s\n",
            "939:\tlearn: 58.3710026\ttotal: 3m 32s\tremaining: 3.62s\n",
            "940:\tlearn: 58.3101415\ttotal: 3m 33s\tremaining: 3.4s\n",
            "941:\tlearn: 58.2617449\ttotal: 3m 33s\tremaining: 3.17s\n",
            "942:\tlearn: 58.2322560\ttotal: 3m 33s\tremaining: 2.94s\n",
            "943:\tlearn: 58.2052180\ttotal: 3m 33s\tremaining: 2.72s\n",
            "944:\tlearn: 58.1953863\ttotal: 3m 33s\tremaining: 2.49s\n",
            "945:\tlearn: 58.1660799\ttotal: 3m 34s\tremaining: 2.26s\n",
            "946:\tlearn: 58.1533796\ttotal: 3m 34s\tremaining: 2.04s\n",
            "947:\tlearn: 58.1212362\ttotal: 3m 34s\tremaining: 1.81s\n",
            "948:\tlearn: 58.0918604\ttotal: 3m 34s\tremaining: 1.58s\n",
            "949:\tlearn: 58.0451971\ttotal: 3m 34s\tremaining: 1.36s\n",
            "950:\tlearn: 58.0324805\ttotal: 3m 35s\tremaining: 1.13s\n",
            "951:\tlearn: 57.9741800\ttotal: 3m 35s\tremaining: 905ms\n",
            "952:\tlearn: 57.9537886\ttotal: 3m 35s\tremaining: 679ms\n",
            "953:\tlearn: 57.9137313\ttotal: 3m 35s\tremaining: 453ms\n",
            "954:\tlearn: 57.9010740\ttotal: 3m 36s\tremaining: 226ms\n",
            "955:\tlearn: 57.8708945\ttotal: 3m 36s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 14:51:00,291] A new study created in memory with name: no-name-9ca827fe-fcb4-4b13-816c-d40335aa0e75\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 6...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 15:00:15,564] Trial 0 finished with value: 59.5207275412696 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 59.5207275412696.\n",
            "[I 2023-10-08 15:04:30,731] Trial 1 finished with value: 40.698052925472865 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:11:46,485] Trial 2 finished with value: 43.65033162269285 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:16:43,868] Trial 3 finished with value: 42.89528536565887 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:20:54,036] Trial 4 finished with value: 41.08327386202685 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:29:29,157] Trial 5 finished with value: 76.57507529863395 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:33:04,007] Trial 6 finished with value: 43.788839936383354 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:52:44,581] Trial 7 finished with value: 52.47267110792404 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:54:57,349] Trial 8 finished with value: 56.36059946967275 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 15:58:47,306] Trial 9 finished with value: 46.47900791826814 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:03:21,138] Trial 10 finished with value: 49.9891539015058 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:06:33,073] Trial 11 finished with value: 44.171291100281415 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:09:38,125] Trial 12 finished with value: 47.08459693625009 and parameters: {'iterations': 758, 'learning_rate': 0.983343987565645, 'depth': 8, 'min_data_in_leaf': 3, 'reg_lambda': 72.61431221371632, 'subsample': 0.5757735214983872, 'random_strength': 33.23239395085676, 'od_wait': 108, 'leaf_estimation_iterations': 4, 'bagging_temperature': 24.356631192739087, 'colsample_bylevel': 0.3642319611435846}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:14:33,375] Trial 13 finished with value: 56.82489920813729 and parameters: {'iterations': 971, 'learning_rate': 0.8348832267870809, 'depth': 11, 'min_data_in_leaf': 21, 'reg_lambda': 30.221360663173336, 'subsample': 0.4627605210231004, 'random_strength': 67.17789626310656, 'od_wait': 145, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.9773987892377944}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:17:37,527] Trial 14 finished with value: 43.16150945160905 and parameters: {'iterations': 725, 'learning_rate': 0.7127771672306289, 'depth': 8, 'min_data_in_leaf': 9, 'reg_lambda': 58.113406993512264, 'subsample': 0.65866699801961, 'random_strength': 45.00496191936034, 'od_wait': 126, 'leaf_estimation_iterations': 1, 'bagging_temperature': 14.031025575549046, 'colsample_bylevel': 0.5544022353458152}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:22:31,498] Trial 15 finished with value: 50.69940921381914 and parameters: {'iterations': 811, 'learning_rate': 0.8479230466326824, 'depth': 10, 'min_data_in_leaf': 23, 'reg_lambda': 42.57803780188458, 'subsample': 0.46082857874135125, 'random_strength': 20.618074524594604, 'od_wait': 98, 'leaf_estimation_iterations': 5, 'bagging_temperature': 43.040289080815256, 'colsample_bylevel': 0.7670727501880628}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:26:48,629] Trial 16 finished with value: 43.888114316085094 and parameters: {'iterations': 932, 'learning_rate': 0.991514101642958, 'depth': 7, 'min_data_in_leaf': 7, 'reg_lambda': 75.69602958648913, 'subsample': 0.6232542409041335, 'random_strength': 44.211563241494794, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6017437666088656}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:29:11,930] Trial 17 finished with value: 50.5652933235534 and parameters: {'iterations': 305, 'learning_rate': 0.6379370742580681, 'depth': 12, 'min_data_in_leaf': 17, 'reg_lambda': 96.539811034711, 'subsample': 0.4556846128045943, 'random_strength': 23.28846249783863, 'od_wait': 62, 'leaf_estimation_iterations': 5, 'bagging_temperature': 5.276027833053952, 'colsample_bylevel': 0.2932296239833232}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:35:23,318] Trial 18 finished with value: 49.350729906867095 and parameters: {'iterations': 821, 'learning_rate': 0.8835598505366324, 'depth': 9, 'min_data_in_leaf': 1, 'reg_lambda': 63.060135923122104, 'subsample': 0.7231925383686137, 'random_strength': 36.49046201305777, 'od_wait': 124, 'leaf_estimation_iterations': 18, 'bagging_temperature': 90.74448118331959, 'colsample_bylevel': 0.800712550834435}. Best is trial 1 with value: 40.698052925472865.\n",
            "[I 2023-10-08 16:37:47,016] Trial 19 finished with value: 70.408318995576 and parameters: {'iterations': 572, 'learning_rate': 0.12616080685751307, 'depth': 7, 'min_data_in_leaf': 30, 'reg_lambda': 80.72321673935316, 'subsample': 0.6015892895130246, 'random_strength': 63.99947767145992, 'od_wait': 88, 'leaf_estimation_iterations': 3, 'bagging_temperature': 16.62365082841058, 'colsample_bylevel': 0.6818744770955735}. Best is trial 1 with value: 40.698052925472865.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 6: 40.698052925472865\n",
            "Best hyperparameters for fold 6: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}\n",
            "0:\tlearn: 203.5054253\ttotal: 240ms\tremaining: 3m 22s\n",
            "1:\tlearn: 200.8450096\ttotal: 492ms\tremaining: 3m 28s\n",
            "2:\tlearn: 199.2831662\ttotal: 781ms\tremaining: 3m 40s\n",
            "3:\tlearn: 198.1827801\ttotal: 1.03s\tremaining: 3m 36s\n",
            "4:\tlearn: 196.3865702\ttotal: 1.25s\tremaining: 3m 31s\n",
            "5:\tlearn: 196.3791385\ttotal: 1.36s\tremaining: 3m 10s\n",
            "6:\tlearn: 195.6012441\ttotal: 1.61s\tremaining: 3m 14s\n",
            "7:\tlearn: 195.5073262\ttotal: 1.86s\tremaining: 3m 15s\n",
            "8:\tlearn: 194.6417946\ttotal: 2.12s\tremaining: 3m 17s\n",
            "9:\tlearn: 194.5027203\ttotal: 2.35s\tremaining: 3m 17s\n",
            "10:\tlearn: 194.5000133\ttotal: 2.44s\tremaining: 3m 5s\n",
            "11:\tlearn: 194.3377136\ttotal: 2.69s\tremaining: 3m 7s\n",
            "12:\tlearn: 193.6725784\ttotal: 2.91s\tremaining: 3m 6s\n",
            "13:\tlearn: 193.6725752\ttotal: 2.97s\tremaining: 2m 57s\n",
            "14:\tlearn: 193.3730503\ttotal: 3.23s\tremaining: 2m 59s\n",
            "15:\tlearn: 192.9324906\ttotal: 3.47s\tremaining: 3m\n",
            "16:\tlearn: 192.5109947\ttotal: 3.63s\tremaining: 2m 57s\n",
            "17:\tlearn: 192.5065011\ttotal: 3.77s\tremaining: 2m 54s\n",
            "18:\tlearn: 192.2438526\ttotal: 3.98s\tremaining: 2m 53s\n",
            "19:\tlearn: 191.4105276\ttotal: 4.23s\tremaining: 2m 55s\n",
            "20:\tlearn: 191.2310900\ttotal: 4.41s\tremaining: 2m 53s\n",
            "21:\tlearn: 190.2421431\ttotal: 4.66s\tremaining: 2m 54s\n",
            "22:\tlearn: 189.5099971\ttotal: 4.92s\tremaining: 2m 56s\n",
            "23:\tlearn: 188.8154090\ttotal: 5.17s\tremaining: 2m 57s\n",
            "24:\tlearn: 186.3434408\ttotal: 5.39s\tremaining: 2m 57s\n",
            "25:\tlearn: 184.6627712\ttotal: 5.63s\tremaining: 2m 58s\n",
            "26:\tlearn: 183.4163199\ttotal: 5.87s\tremaining: 2m 58s\n",
            "27:\tlearn: 182.7996109\ttotal: 6.18s\tremaining: 3m\n",
            "28:\tlearn: 181.7572244\ttotal: 6.43s\tremaining: 3m 1s\n",
            "29:\tlearn: 181.0134846\ttotal: 6.68s\tremaining: 3m 2s\n",
            "30:\tlearn: 178.4639432\ttotal: 6.92s\tremaining: 3m 2s\n",
            "31:\tlearn: 178.1988612\ttotal: 7.27s\tremaining: 3m 5s\n",
            "32:\tlearn: 177.5472662\ttotal: 7.55s\tremaining: 3m 6s\n",
            "33:\tlearn: 177.1148467\ttotal: 7.81s\tremaining: 3m 6s\n",
            "34:\tlearn: 173.0090217\ttotal: 8.03s\tremaining: 3m 6s\n",
            "35:\tlearn: 169.9135532\ttotal: 8.26s\tremaining: 3m 6s\n",
            "36:\tlearn: 169.1040641\ttotal: 8.47s\tremaining: 3m 5s\n",
            "37:\tlearn: 168.2700500\ttotal: 8.75s\tremaining: 3m 6s\n",
            "38:\tlearn: 167.9036443\ttotal: 9.02s\tremaining: 3m 7s\n",
            "39:\tlearn: 164.8227335\ttotal: 9.26s\tremaining: 3m 7s\n",
            "40:\tlearn: 163.1648791\ttotal: 9.51s\tremaining: 3m 7s\n",
            "41:\tlearn: 161.7457893\ttotal: 9.73s\tremaining: 3m 6s\n",
            "42:\tlearn: 160.0904408\ttotal: 9.95s\tremaining: 3m 6s\n",
            "43:\tlearn: 157.0472776\ttotal: 10.2s\tremaining: 3m 6s\n",
            "44:\tlearn: 156.7274973\ttotal: 10.5s\tremaining: 3m 6s\n",
            "45:\tlearn: 156.0845385\ttotal: 10.7s\tremaining: 3m 6s\n",
            "46:\tlearn: 155.6372029\ttotal: 11s\tremaining: 3m 6s\n",
            "47:\tlearn: 154.8871602\ttotal: 11.2s\tremaining: 3m 7s\n",
            "48:\tlearn: 154.5049564\ttotal: 11.5s\tremaining: 3m 7s\n",
            "49:\tlearn: 153.6074408\ttotal: 11.7s\tremaining: 3m 7s\n",
            "50:\tlearn: 152.5781560\ttotal: 12s\tremaining: 3m 6s\n",
            "51:\tlearn: 151.4265559\ttotal: 12.2s\tremaining: 3m 6s\n",
            "52:\tlearn: 150.1226139\ttotal: 12.4s\tremaining: 3m 6s\n",
            "53:\tlearn: 149.8299337\ttotal: 12.7s\tremaining: 3m 6s\n",
            "54:\tlearn: 148.9056222\ttotal: 12.9s\tremaining: 3m 6s\n",
            "55:\tlearn: 147.6238557\ttotal: 13.2s\tremaining: 3m 6s\n",
            "56:\tlearn: 147.2544825\ttotal: 13.4s\tremaining: 3m 6s\n",
            "57:\tlearn: 146.8080679\ttotal: 13.6s\tremaining: 3m 5s\n",
            "58:\tlearn: 145.4486307\ttotal: 13.8s\tremaining: 3m 5s\n",
            "59:\tlearn: 144.5495955\ttotal: 14.1s\tremaining: 3m 4s\n",
            "60:\tlearn: 144.1143809\ttotal: 14.3s\tremaining: 3m 4s\n",
            "61:\tlearn: 143.8356862\ttotal: 14.5s\tremaining: 3m 4s\n",
            "62:\tlearn: 142.9475004\ttotal: 14.8s\tremaining: 3m 4s\n",
            "63:\tlearn: 141.9041547\ttotal: 15s\tremaining: 3m 3s\n",
            "64:\tlearn: 141.6800227\ttotal: 15.2s\tremaining: 3m 3s\n",
            "65:\tlearn: 141.0918176\ttotal: 15.5s\tremaining: 3m 3s\n",
            "66:\tlearn: 140.9297431\ttotal: 15.7s\tremaining: 3m 3s\n",
            "67:\tlearn: 137.9053290\ttotal: 16s\tremaining: 3m 3s\n",
            "68:\tlearn: 137.4704507\ttotal: 16.2s\tremaining: 3m 3s\n",
            "69:\tlearn: 136.7847270\ttotal: 16.4s\tremaining: 3m 2s\n",
            "70:\tlearn: 136.5625716\ttotal: 16.7s\tremaining: 3m 2s\n",
            "71:\tlearn: 135.4389000\ttotal: 16.9s\tremaining: 3m 2s\n",
            "72:\tlearn: 135.2146874\ttotal: 17.1s\tremaining: 3m 2s\n",
            "73:\tlearn: 134.0478597\ttotal: 17.4s\tremaining: 3m 1s\n",
            "74:\tlearn: 133.7031976\ttotal: 17.6s\tremaining: 3m 1s\n",
            "75:\tlearn: 133.1793859\ttotal: 17.8s\tremaining: 3m 1s\n",
            "76:\tlearn: 132.9866947\ttotal: 18.1s\tremaining: 3m 1s\n",
            "77:\tlearn: 132.7699755\ttotal: 18.4s\tremaining: 3m 1s\n",
            "78:\tlearn: 130.5548405\ttotal: 18.6s\tremaining: 3m\n",
            "79:\tlearn: 130.0251057\ttotal: 18.8s\tremaining: 3m\n",
            "80:\tlearn: 129.4456207\ttotal: 19.1s\tremaining: 3m\n",
            "81:\tlearn: 129.2822587\ttotal: 19.3s\tremaining: 3m\n",
            "82:\tlearn: 128.7925677\ttotal: 19.5s\tremaining: 2m 59s\n",
            "83:\tlearn: 128.6147078\ttotal: 19.8s\tremaining: 3m\n",
            "84:\tlearn: 127.5854131\ttotal: 20.1s\tremaining: 3m\n",
            "85:\tlearn: 127.1630152\ttotal: 20.3s\tremaining: 2m 59s\n",
            "86:\tlearn: 126.7984249\ttotal: 20.5s\tremaining: 2m 59s\n",
            "87:\tlearn: 126.2534395\ttotal: 20.7s\tremaining: 2m 59s\n",
            "88:\tlearn: 125.9971820\ttotal: 21s\tremaining: 2m 58s\n",
            "89:\tlearn: 125.3984895\ttotal: 21.2s\tremaining: 2m 58s\n",
            "90:\tlearn: 123.7167848\ttotal: 21.4s\tremaining: 2m 58s\n",
            "91:\tlearn: 123.4611866\ttotal: 21.6s\tremaining: 2m 57s\n",
            "92:\tlearn: 122.8132962\ttotal: 21.9s\tremaining: 2m 57s\n",
            "93:\tlearn: 122.3563987\ttotal: 22.1s\tremaining: 2m 57s\n",
            "94:\tlearn: 122.0927175\ttotal: 22.3s\tremaining: 2m 56s\n",
            "95:\tlearn: 121.4791849\ttotal: 22.5s\tremaining: 2m 56s\n",
            "96:\tlearn: 121.0314849\ttotal: 22.8s\tremaining: 2m 56s\n",
            "97:\tlearn: 120.8296287\ttotal: 23s\tremaining: 2m 56s\n",
            "98:\tlearn: 120.7080248\ttotal: 23.3s\tremaining: 2m 56s\n",
            "99:\tlearn: 120.0815565\ttotal: 23.6s\tremaining: 2m 56s\n",
            "100:\tlearn: 119.6437855\ttotal: 23.8s\tremaining: 2m 55s\n",
            "101:\tlearn: 119.4079893\ttotal: 24.1s\tremaining: 2m 56s\n",
            "102:\tlearn: 119.2826932\ttotal: 24.3s\tremaining: 2m 56s\n",
            "103:\tlearn: 118.7539005\ttotal: 24.6s\tremaining: 2m 55s\n",
            "104:\tlearn: 118.4045037\ttotal: 24.9s\tremaining: 2m 55s\n",
            "105:\tlearn: 117.9628618\ttotal: 25.1s\tremaining: 2m 55s\n",
            "106:\tlearn: 117.9588326\ttotal: 25.4s\tremaining: 2m 55s\n",
            "107:\tlearn: 117.4425118\ttotal: 25.6s\tremaining: 2m 55s\n",
            "108:\tlearn: 117.3042468\ttotal: 25.9s\tremaining: 2m 55s\n",
            "109:\tlearn: 117.1876909\ttotal: 26.1s\tremaining: 2m 55s\n",
            "110:\tlearn: 116.3854400\ttotal: 26.4s\tremaining: 2m 55s\n",
            "111:\tlearn: 116.0551019\ttotal: 26.6s\tremaining: 2m 55s\n",
            "112:\tlearn: 116.0520523\ttotal: 26.9s\tremaining: 2m 54s\n",
            "113:\tlearn: 115.7311116\ttotal: 27.1s\tremaining: 2m 54s\n",
            "114:\tlearn: 115.3238616\ttotal: 27.3s\tremaining: 2m 54s\n",
            "115:\tlearn: 114.9137921\ttotal: 27.6s\tremaining: 2m 53s\n",
            "116:\tlearn: 114.7999212\ttotal: 27.8s\tremaining: 2m 53s\n",
            "117:\tlearn: 114.6462458\ttotal: 28.1s\tremaining: 2m 53s\n",
            "118:\tlearn: 114.4985157\ttotal: 28.3s\tremaining: 2m 53s\n",
            "119:\tlearn: 114.0446448\ttotal: 28.6s\tremaining: 2m 53s\n",
            "120:\tlearn: 114.0042417\ttotal: 28.8s\tremaining: 2m 53s\n",
            "121:\tlearn: 112.6470041\ttotal: 29s\tremaining: 2m 52s\n",
            "122:\tlearn: 112.3097636\ttotal: 29.3s\tremaining: 2m 52s\n",
            "123:\tlearn: 111.3917334\ttotal: 29.5s\tremaining: 2m 52s\n",
            "124:\tlearn: 110.7702785\ttotal: 29.7s\tremaining: 2m 51s\n",
            "125:\tlearn: 110.5965206\ttotal: 30s\tremaining: 2m 51s\n",
            "126:\tlearn: 110.1972220\ttotal: 30.3s\tremaining: 2m 51s\n",
            "127:\tlearn: 110.0392179\ttotal: 30.5s\tremaining: 2m 51s\n",
            "128:\tlearn: 109.9051452\ttotal: 30.8s\tremaining: 2m 51s\n",
            "129:\tlearn: 109.7753041\ttotal: 31s\tremaining: 2m 51s\n",
            "130:\tlearn: 109.6885876\ttotal: 31.3s\tremaining: 2m 51s\n",
            "131:\tlearn: 109.4417419\ttotal: 31.5s\tremaining: 2m 50s\n",
            "132:\tlearn: 108.3987092\ttotal: 31.7s\tremaining: 2m 50s\n",
            "133:\tlearn: 107.4934844\ttotal: 31.9s\tremaining: 2m 50s\n",
            "134:\tlearn: 107.1580529\ttotal: 32.2s\tremaining: 2m 49s\n",
            "135:\tlearn: 106.9432822\ttotal: 32.4s\tremaining: 2m 49s\n",
            "136:\tlearn: 106.5741174\ttotal: 32.6s\tremaining: 2m 49s\n",
            "137:\tlearn: 106.4314770\ttotal: 32.8s\tremaining: 2m 48s\n",
            "138:\tlearn: 106.2762071\ttotal: 33.1s\tremaining: 2m 48s\n",
            "139:\tlearn: 105.9865313\ttotal: 33.4s\tremaining: 2m 48s\n",
            "140:\tlearn: 105.8132944\ttotal: 33.6s\tremaining: 2m 48s\n",
            "141:\tlearn: 105.6736314\ttotal: 33.9s\tremaining: 2m 48s\n",
            "142:\tlearn: 105.3506612\ttotal: 34.1s\tremaining: 2m 48s\n",
            "143:\tlearn: 105.1595625\ttotal: 34.3s\tremaining: 2m 47s\n",
            "144:\tlearn: 105.0685339\ttotal: 34.6s\tremaining: 2m 47s\n",
            "145:\tlearn: 105.0010605\ttotal: 34.8s\tremaining: 2m 47s\n",
            "146:\tlearn: 104.8710784\ttotal: 35s\tremaining: 2m 47s\n",
            "147:\tlearn: 103.6766557\ttotal: 35.3s\tremaining: 2m 46s\n",
            "148:\tlearn: 103.4692023\ttotal: 35.5s\tremaining: 2m 46s\n",
            "149:\tlearn: 103.2007746\ttotal: 35.7s\tremaining: 2m 46s\n",
            "150:\tlearn: 102.8917011\ttotal: 35.9s\tremaining: 2m 45s\n",
            "151:\tlearn: 102.8174350\ttotal: 36.2s\tremaining: 2m 45s\n",
            "152:\tlearn: 102.7111509\ttotal: 36.5s\tremaining: 2m 45s\n",
            "153:\tlearn: 102.6330018\ttotal: 36.8s\tremaining: 2m 45s\n",
            "154:\tlearn: 102.3246130\ttotal: 37s\tremaining: 2m 45s\n",
            "155:\tlearn: 101.9902049\ttotal: 37.2s\tremaining: 2m 45s\n",
            "156:\tlearn: 101.5827130\ttotal: 37.5s\tremaining: 2m 44s\n",
            "157:\tlearn: 101.2021435\ttotal: 37.7s\tremaining: 2m 44s\n",
            "158:\tlearn: 100.8918988\ttotal: 37.9s\tremaining: 2m 44s\n",
            "159:\tlearn: 100.1522177\ttotal: 38.1s\tremaining: 2m 43s\n",
            "160:\tlearn: 100.0617035\ttotal: 38.4s\tremaining: 2m 43s\n",
            "161:\tlearn: 99.8728728\ttotal: 38.6s\tremaining: 2m 43s\n",
            "162:\tlearn: 99.4179748\ttotal: 38.8s\tremaining: 2m 43s\n",
            "163:\tlearn: 99.1353750\ttotal: 39.1s\tremaining: 2m 42s\n",
            "164:\tlearn: 98.9851561\ttotal: 39.3s\tremaining: 2m 42s\n",
            "165:\tlearn: 98.4011265\ttotal: 39.6s\tremaining: 2m 42s\n",
            "166:\tlearn: 98.2277521\ttotal: 39.8s\tremaining: 2m 42s\n",
            "167:\tlearn: 97.8712830\ttotal: 40.1s\tremaining: 2m 42s\n",
            "168:\tlearn: 97.7723168\ttotal: 40.3s\tremaining: 2m 42s\n",
            "169:\tlearn: 97.6568140\ttotal: 40.6s\tremaining: 2m 41s\n",
            "170:\tlearn: 97.5233871\ttotal: 40.9s\tremaining: 2m 41s\n",
            "171:\tlearn: 97.4295971\ttotal: 41.2s\tremaining: 2m 41s\n",
            "172:\tlearn: 97.4185680\ttotal: 41.4s\tremaining: 2m 41s\n",
            "173:\tlearn: 96.8661802\ttotal: 41.7s\tremaining: 2m 41s\n",
            "174:\tlearn: 96.6545926\ttotal: 41.9s\tremaining: 2m 41s\n",
            "175:\tlearn: 96.5638358\ttotal: 42.2s\tremaining: 2m 41s\n",
            "176:\tlearn: 96.4420786\ttotal: 42.4s\tremaining: 2m 40s\n",
            "177:\tlearn: 95.9733719\ttotal: 42.7s\tremaining: 2m 40s\n",
            "178:\tlearn: 95.8582775\ttotal: 42.9s\tremaining: 2m 40s\n",
            "179:\tlearn: 95.3038877\ttotal: 43.2s\tremaining: 2m 40s\n",
            "180:\tlearn: 95.2070486\ttotal: 43.4s\tremaining: 2m 39s\n",
            "181:\tlearn: 94.5942262\ttotal: 43.7s\tremaining: 2m 39s\n",
            "182:\tlearn: 94.3731884\ttotal: 43.9s\tremaining: 2m 39s\n",
            "183:\tlearn: 94.0176990\ttotal: 44.1s\tremaining: 2m 39s\n",
            "184:\tlearn: 93.9378520\ttotal: 44.4s\tremaining: 2m 38s\n",
            "185:\tlearn: 93.8325114\ttotal: 44.5s\tremaining: 2m 38s\n",
            "186:\tlearn: 93.2606058\ttotal: 44.8s\tremaining: 2m 38s\n",
            "187:\tlearn: 92.8714902\ttotal: 45s\tremaining: 2m 38s\n",
            "188:\tlearn: 92.7448922\ttotal: 45.3s\tremaining: 2m 37s\n",
            "189:\tlearn: 92.6221699\ttotal: 45.5s\tremaining: 2m 37s\n",
            "190:\tlearn: 92.3354976\ttotal: 45.7s\tremaining: 2m 37s\n",
            "191:\tlearn: 92.2000056\ttotal: 46s\tremaining: 2m 37s\n",
            "192:\tlearn: 92.0022000\ttotal: 46.2s\tremaining: 2m 36s\n",
            "193:\tlearn: 91.8369308\ttotal: 46.4s\tremaining: 2m 36s\n",
            "194:\tlearn: 91.7604731\ttotal: 46.7s\tremaining: 2m 36s\n",
            "195:\tlearn: 91.6520118\ttotal: 46.9s\tremaining: 2m 36s\n",
            "196:\tlearn: 91.4994707\ttotal: 47.2s\tremaining: 2m 35s\n",
            "197:\tlearn: 91.4610135\ttotal: 47.5s\tremaining: 2m 35s\n",
            "198:\tlearn: 91.2037380\ttotal: 47.7s\tremaining: 2m 35s\n",
            "199:\tlearn: 90.9454452\ttotal: 47.9s\tremaining: 2m 35s\n",
            "200:\tlearn: 90.8117253\ttotal: 48.1s\tremaining: 2m 34s\n",
            "201:\tlearn: 90.5904281\ttotal: 48.3s\tremaining: 2m 34s\n",
            "202:\tlearn: 90.2092432\ttotal: 48.6s\tremaining: 2m 34s\n",
            "203:\tlearn: 89.9079468\ttotal: 48.8s\tremaining: 2m 34s\n",
            "204:\tlearn: 89.8419659\ttotal: 49.1s\tremaining: 2m 33s\n",
            "205:\tlearn: 89.6103663\ttotal: 49.3s\tremaining: 2m 33s\n",
            "206:\tlearn: 89.4314484\ttotal: 49.5s\tremaining: 2m 33s\n",
            "207:\tlearn: 89.2675838\ttotal: 49.7s\tremaining: 2m 33s\n",
            "208:\tlearn: 88.9850911\ttotal: 50s\tremaining: 2m 32s\n",
            "209:\tlearn: 88.8302778\ttotal: 50.3s\tremaining: 2m 32s\n",
            "210:\tlearn: 88.7168019\ttotal: 50.5s\tremaining: 2m 32s\n",
            "211:\tlearn: 88.6416767\ttotal: 50.8s\tremaining: 2m 32s\n",
            "212:\tlearn: 88.5810807\ttotal: 51s\tremaining: 2m 32s\n",
            "213:\tlearn: 88.3448542\ttotal: 51.2s\tremaining: 2m 31s\n",
            "214:\tlearn: 88.2251988\ttotal: 51.5s\tremaining: 2m 31s\n",
            "215:\tlearn: 88.1839037\ttotal: 51.8s\tremaining: 2m 31s\n",
            "216:\tlearn: 88.0388189\ttotal: 52s\tremaining: 2m 31s\n",
            "217:\tlearn: 87.7407194\ttotal: 52.3s\tremaining: 2m 31s\n",
            "218:\tlearn: 87.5721260\ttotal: 52.5s\tremaining: 2m 30s\n",
            "219:\tlearn: 87.5118637\ttotal: 52.7s\tremaining: 2m 30s\n",
            "220:\tlearn: 87.4085175\ttotal: 53s\tremaining: 2m 30s\n",
            "221:\tlearn: 87.3384886\ttotal: 53.2s\tremaining: 2m 30s\n",
            "222:\tlearn: 87.0185891\ttotal: 53.4s\tremaining: 2m 29s\n",
            "223:\tlearn: 86.9460242\ttotal: 53.7s\tremaining: 2m 29s\n",
            "224:\tlearn: 86.8486373\ttotal: 53.9s\tremaining: 2m 29s\n",
            "225:\tlearn: 86.6628853\ttotal: 54.2s\tremaining: 2m 29s\n",
            "226:\tlearn: 86.6037132\ttotal: 54.4s\tremaining: 2m 28s\n",
            "227:\tlearn: 86.5889932\ttotal: 54.7s\tremaining: 2m 28s\n",
            "228:\tlearn: 86.5242363\ttotal: 55s\tremaining: 2m 28s\n",
            "229:\tlearn: 86.4082449\ttotal: 55.3s\tremaining: 2m 28s\n",
            "230:\tlearn: 86.3088767\ttotal: 55.6s\tremaining: 2m 28s\n",
            "231:\tlearn: 86.2161820\ttotal: 55.9s\tremaining: 2m 28s\n",
            "232:\tlearn: 85.9217503\ttotal: 56.2s\tremaining: 2m 28s\n",
            "233:\tlearn: 85.6763025\ttotal: 56.4s\tremaining: 2m 28s\n",
            "234:\tlearn: 85.5657573\ttotal: 56.6s\tremaining: 2m 27s\n",
            "235:\tlearn: 85.4234610\ttotal: 56.9s\tremaining: 2m 27s\n",
            "236:\tlearn: 85.3611837\ttotal: 57.1s\tremaining: 2m 27s\n",
            "237:\tlearn: 85.2778757\ttotal: 57.4s\tremaining: 2m 27s\n",
            "238:\tlearn: 85.1735121\ttotal: 57.7s\tremaining: 2m 26s\n",
            "239:\tlearn: 84.9429136\ttotal: 57.9s\tremaining: 2m 26s\n",
            "240:\tlearn: 84.5216807\ttotal: 58.2s\tremaining: 2m 26s\n",
            "241:\tlearn: 84.3688964\ttotal: 58.5s\tremaining: 2m 26s\n",
            "242:\tlearn: 84.1449945\ttotal: 58.7s\tremaining: 2m 26s\n",
            "243:\tlearn: 84.1037402\ttotal: 58.9s\tremaining: 2m 25s\n",
            "244:\tlearn: 84.0174520\ttotal: 59.2s\tremaining: 2m 25s\n",
            "245:\tlearn: 83.9640326\ttotal: 59.4s\tremaining: 2m 25s\n",
            "246:\tlearn: 83.7106301\ttotal: 59.7s\tremaining: 2m 25s\n",
            "247:\tlearn: 83.5876221\ttotal: 59.9s\tremaining: 2m 24s\n",
            "248:\tlearn: 83.5535269\ttotal: 1m\tremaining: 2m 24s\n",
            "249:\tlearn: 83.2236944\ttotal: 1m\tremaining: 2m 24s\n",
            "250:\tlearn: 82.9823496\ttotal: 1m\tremaining: 2m 24s\n",
            "251:\tlearn: 82.9377234\ttotal: 1m\tremaining: 2m 23s\n",
            "252:\tlearn: 82.8615993\ttotal: 1m 1s\tremaining: 2m 23s\n",
            "253:\tlearn: 82.8106924\ttotal: 1m 1s\tremaining: 2m 23s\n",
            "254:\tlearn: 82.6292842\ttotal: 1m 1s\tremaining: 2m 23s\n",
            "255:\tlearn: 82.5585463\ttotal: 1m 1s\tremaining: 2m 22s\n",
            "256:\tlearn: 82.5280082\ttotal: 1m 2s\tremaining: 2m 22s\n",
            "257:\tlearn: 82.4427650\ttotal: 1m 2s\tremaining: 2m 22s\n",
            "258:\tlearn: 82.4421569\ttotal: 1m 2s\tremaining: 2m 21s\n",
            "259:\tlearn: 82.3594021\ttotal: 1m 2s\tremaining: 2m 21s\n",
            "260:\tlearn: 82.3565215\ttotal: 1m 3s\tremaining: 2m 21s\n",
            "261:\tlearn: 82.3067955\ttotal: 1m 3s\tremaining: 2m 21s\n",
            "262:\tlearn: 81.7511666\ttotal: 1m 3s\tremaining: 2m 21s\n",
            "263:\tlearn: 81.5919605\ttotal: 1m 3s\tremaining: 2m 21s\n",
            "264:\tlearn: 81.4097301\ttotal: 1m 4s\tremaining: 2m 20s\n",
            "265:\tlearn: 81.3309960\ttotal: 1m 4s\tremaining: 2m 20s\n",
            "266:\tlearn: 80.9635720\ttotal: 1m 4s\tremaining: 2m 20s\n",
            "267:\tlearn: 80.8358464\ttotal: 1m 4s\tremaining: 2m 20s\n",
            "268:\tlearn: 80.7269952\ttotal: 1m 4s\tremaining: 2m 19s\n",
            "269:\tlearn: 80.6305810\ttotal: 1m 5s\tremaining: 2m 19s\n",
            "270:\tlearn: 80.5778050\ttotal: 1m 5s\tremaining: 2m 19s\n",
            "271:\tlearn: 80.3921196\ttotal: 1m 5s\tremaining: 2m 19s\n",
            "272:\tlearn: 80.3420405\ttotal: 1m 5s\tremaining: 2m 18s\n",
            "273:\tlearn: 80.3402976\ttotal: 1m 6s\tremaining: 2m 18s\n",
            "274:\tlearn: 80.2788257\ttotal: 1m 6s\tremaining: 2m 18s\n",
            "275:\tlearn: 80.1862692\ttotal: 1m 6s\tremaining: 2m 18s\n",
            "276:\tlearn: 79.6920564\ttotal: 1m 6s\tremaining: 2m 17s\n",
            "277:\tlearn: 79.5803908\ttotal: 1m 7s\tremaining: 2m 17s\n",
            "278:\tlearn: 79.5188327\ttotal: 1m 7s\tremaining: 2m 17s\n",
            "279:\tlearn: 79.2467033\ttotal: 1m 7s\tremaining: 2m 17s\n",
            "280:\tlearn: 79.0437194\ttotal: 1m 7s\tremaining: 2m 16s\n",
            "281:\tlearn: 78.9525189\ttotal: 1m 8s\tremaining: 2m 16s\n",
            "282:\tlearn: 78.8846762\ttotal: 1m 8s\tremaining: 2m 16s\n",
            "283:\tlearn: 78.7206261\ttotal: 1m 8s\tremaining: 2m 16s\n",
            "284:\tlearn: 78.6443885\ttotal: 1m 8s\tremaining: 2m 15s\n",
            "285:\tlearn: 78.6313137\ttotal: 1m 9s\tremaining: 2m 15s\n",
            "286:\tlearn: 78.4474356\ttotal: 1m 9s\tremaining: 2m 15s\n",
            "287:\tlearn: 78.2769201\ttotal: 1m 9s\tremaining: 2m 15s\n",
            "288:\tlearn: 77.8452405\ttotal: 1m 9s\tremaining: 2m 14s\n",
            "289:\tlearn: 77.7636025\ttotal: 1m 9s\tremaining: 2m 14s\n",
            "290:\tlearn: 77.6965794\ttotal: 1m 10s\tremaining: 2m 14s\n",
            "291:\tlearn: 77.4869539\ttotal: 1m 10s\tremaining: 2m 14s\n",
            "292:\tlearn: 77.4120709\ttotal: 1m 10s\tremaining: 2m 14s\n",
            "293:\tlearn: 77.2550490\ttotal: 1m 10s\tremaining: 2m 13s\n",
            "294:\tlearn: 77.2278315\ttotal: 1m 11s\tremaining: 2m 13s\n",
            "295:\tlearn: 77.0889836\ttotal: 1m 11s\tremaining: 2m 13s\n",
            "296:\tlearn: 76.9148764\ttotal: 1m 11s\tremaining: 2m 13s\n",
            "297:\tlearn: 76.8633203\ttotal: 1m 12s\tremaining: 2m 12s\n",
            "298:\tlearn: 76.6484371\ttotal: 1m 12s\tremaining: 2m 12s\n",
            "299:\tlearn: 76.5661698\ttotal: 1m 12s\tremaining: 2m 12s\n",
            "300:\tlearn: 76.5360372\ttotal: 1m 12s\tremaining: 2m 12s\n",
            "301:\tlearn: 76.4220611\ttotal: 1m 13s\tremaining: 2m 12s\n",
            "302:\tlearn: 76.2450430\ttotal: 1m 13s\tremaining: 2m 12s\n",
            "303:\tlearn: 76.1900978\ttotal: 1m 13s\tremaining: 2m 11s\n",
            "304:\tlearn: 75.9194138\ttotal: 1m 13s\tremaining: 2m 11s\n",
            "305:\tlearn: 75.6968068\ttotal: 1m 14s\tremaining: 2m 11s\n",
            "306:\tlearn: 75.6466267\ttotal: 1m 14s\tremaining: 2m 11s\n",
            "307:\tlearn: 75.5389752\ttotal: 1m 14s\tremaining: 2m 11s\n",
            "308:\tlearn: 75.3486753\ttotal: 1m 15s\tremaining: 2m 10s\n",
            "309:\tlearn: 75.1783283\ttotal: 1m 15s\tremaining: 2m 10s\n",
            "310:\tlearn: 75.0177316\ttotal: 1m 15s\tremaining: 2m 10s\n",
            "311:\tlearn: 74.9588135\ttotal: 1m 15s\tremaining: 2m 10s\n",
            "312:\tlearn: 74.9565825\ttotal: 1m 15s\tremaining: 2m 9s\n",
            "313:\tlearn: 74.8925703\ttotal: 1m 16s\tremaining: 2m 9s\n",
            "314:\tlearn: 74.8408223\ttotal: 1m 16s\tremaining: 2m 9s\n",
            "315:\tlearn: 74.8008693\ttotal: 1m 16s\tremaining: 2m 9s\n",
            "316:\tlearn: 74.7572740\ttotal: 1m 16s\tremaining: 2m 8s\n",
            "317:\tlearn: 74.4299336\ttotal: 1m 17s\tremaining: 2m 8s\n",
            "318:\tlearn: 74.3712810\ttotal: 1m 17s\tremaining: 2m 8s\n",
            "319:\tlearn: 74.2694011\ttotal: 1m 17s\tremaining: 2m 8s\n",
            "320:\tlearn: 74.2021975\ttotal: 1m 17s\tremaining: 2m 7s\n",
            "321:\tlearn: 74.0553710\ttotal: 1m 18s\tremaining: 2m 7s\n",
            "322:\tlearn: 73.9714699\ttotal: 1m 18s\tremaining: 2m 7s\n",
            "323:\tlearn: 73.8613758\ttotal: 1m 18s\tremaining: 2m 7s\n",
            "324:\tlearn: 73.5681611\ttotal: 1m 18s\tremaining: 2m 6s\n",
            "325:\tlearn: 73.4962879\ttotal: 1m 19s\tremaining: 2m 6s\n",
            "326:\tlearn: 73.4197508\ttotal: 1m 19s\tremaining: 2m 6s\n",
            "327:\tlearn: 73.3755250\ttotal: 1m 19s\tremaining: 2m 6s\n",
            "328:\tlearn: 73.0868707\ttotal: 1m 19s\tremaining: 2m 5s\n",
            "329:\tlearn: 72.9551684\ttotal: 1m 20s\tremaining: 2m 5s\n",
            "330:\tlearn: 72.8408238\ttotal: 1m 20s\tremaining: 2m 5s\n",
            "331:\tlearn: 72.7687134\ttotal: 1m 20s\tremaining: 2m 5s\n",
            "332:\tlearn: 72.6924947\ttotal: 1m 20s\tremaining: 2m 4s\n",
            "333:\tlearn: 72.4976422\ttotal: 1m 21s\tremaining: 2m 4s\n",
            "334:\tlearn: 72.3058427\ttotal: 1m 21s\tremaining: 2m 4s\n",
            "335:\tlearn: 72.2482015\ttotal: 1m 21s\tremaining: 2m 4s\n",
            "336:\tlearn: 71.9527647\ttotal: 1m 21s\tremaining: 2m 3s\n",
            "337:\tlearn: 71.7555891\ttotal: 1m 21s\tremaining: 2m 3s\n",
            "338:\tlearn: 71.5822635\ttotal: 1m 22s\tremaining: 2m 3s\n",
            "339:\tlearn: 71.5468427\ttotal: 1m 22s\tremaining: 2m 3s\n",
            "340:\tlearn: 71.4835791\ttotal: 1m 22s\tremaining: 2m 2s\n",
            "341:\tlearn: 71.4472936\ttotal: 1m 22s\tremaining: 2m 2s\n",
            "342:\tlearn: 71.3148466\ttotal: 1m 23s\tremaining: 2m 2s\n",
            "343:\tlearn: 71.3136507\ttotal: 1m 23s\tremaining: 2m 2s\n",
            "344:\tlearn: 71.2702621\ttotal: 1m 23s\tremaining: 2m 2s\n",
            "345:\tlearn: 71.1745699\ttotal: 1m 23s\tremaining: 2m 1s\n",
            "346:\tlearn: 71.0842737\ttotal: 1m 24s\tremaining: 2m 1s\n",
            "347:\tlearn: 70.9115478\ttotal: 1m 24s\tremaining: 2m 1s\n",
            "348:\tlearn: 70.8352537\ttotal: 1m 24s\tremaining: 2m 1s\n",
            "349:\tlearn: 70.5345003\ttotal: 1m 25s\tremaining: 2m\n",
            "350:\tlearn: 70.3151096\ttotal: 1m 25s\tremaining: 2m\n",
            "351:\tlearn: 70.2168672\ttotal: 1m 25s\tremaining: 2m\n",
            "352:\tlearn: 70.1487935\ttotal: 1m 25s\tremaining: 2m\n",
            "353:\tlearn: 70.1203531\ttotal: 1m 25s\tremaining: 2m\n",
            "354:\tlearn: 69.9542516\ttotal: 1m 26s\tremaining: 1m 59s\n",
            "355:\tlearn: 69.8433694\ttotal: 1m 26s\tremaining: 1m 59s\n",
            "356:\tlearn: 69.7762140\ttotal: 1m 26s\tremaining: 1m 59s\n",
            "357:\tlearn: 69.6404501\ttotal: 1m 27s\tremaining: 1m 59s\n",
            "358:\tlearn: 69.5986099\ttotal: 1m 27s\tremaining: 1m 58s\n",
            "359:\tlearn: 69.4778421\ttotal: 1m 27s\tremaining: 1m 58s\n",
            "360:\tlearn: 69.3996426\ttotal: 1m 27s\tremaining: 1m 58s\n",
            "361:\tlearn: 69.3443067\ttotal: 1m 28s\tremaining: 1m 58s\n",
            "362:\tlearn: 69.2049351\ttotal: 1m 28s\tremaining: 1m 57s\n",
            "363:\tlearn: 69.1598723\ttotal: 1m 28s\tremaining: 1m 57s\n",
            "364:\tlearn: 69.0601268\ttotal: 1m 28s\tremaining: 1m 57s\n",
            "365:\tlearn: 68.8103498\ttotal: 1m 28s\tremaining: 1m 57s\n",
            "366:\tlearn: 68.7323645\ttotal: 1m 29s\tremaining: 1m 56s\n",
            "367:\tlearn: 68.6663158\ttotal: 1m 29s\tremaining: 1m 56s\n",
            "368:\tlearn: 68.3596948\ttotal: 1m 29s\tremaining: 1m 56s\n",
            "369:\tlearn: 68.2509074\ttotal: 1m 29s\tremaining: 1m 56s\n",
            "370:\tlearn: 68.1748362\ttotal: 1m 30s\tremaining: 1m 55s\n",
            "371:\tlearn: 68.1177871\ttotal: 1m 30s\tremaining: 1m 55s\n",
            "372:\tlearn: 68.1133980\ttotal: 1m 30s\tremaining: 1m 55s\n",
            "373:\tlearn: 67.9402195\ttotal: 1m 30s\tremaining: 1m 55s\n",
            "374:\tlearn: 67.7618722\ttotal: 1m 31s\tremaining: 1m 55s\n",
            "375:\tlearn: 67.6808747\ttotal: 1m 31s\tremaining: 1m 54s\n",
            "376:\tlearn: 67.5129411\ttotal: 1m 31s\tremaining: 1m 54s\n",
            "377:\tlearn: 67.4725023\ttotal: 1m 31s\tremaining: 1m 54s\n",
            "378:\tlearn: 67.3826765\ttotal: 1m 32s\tremaining: 1m 54s\n",
            "379:\tlearn: 67.2388786\ttotal: 1m 32s\tremaining: 1m 53s\n",
            "380:\tlearn: 67.2034827\ttotal: 1m 32s\tremaining: 1m 53s\n",
            "381:\tlearn: 67.1436227\ttotal: 1m 32s\tremaining: 1m 53s\n",
            "382:\tlearn: 66.9654975\ttotal: 1m 33s\tremaining: 1m 53s\n",
            "383:\tlearn: 66.8711357\ttotal: 1m 33s\tremaining: 1m 52s\n",
            "384:\tlearn: 66.7849653\ttotal: 1m 33s\tremaining: 1m 52s\n",
            "385:\tlearn: 66.7460545\ttotal: 1m 33s\tremaining: 1m 52s\n",
            "386:\tlearn: 66.7088289\ttotal: 1m 34s\tremaining: 1m 52s\n",
            "387:\tlearn: 66.6688038\ttotal: 1m 34s\tremaining: 1m 51s\n",
            "388:\tlearn: 66.5928084\ttotal: 1m 34s\tremaining: 1m 51s\n",
            "389:\tlearn: 66.4992960\ttotal: 1m 34s\tremaining: 1m 51s\n",
            "390:\tlearn: 66.4469507\ttotal: 1m 35s\tremaining: 1m 51s\n",
            "391:\tlearn: 66.3981780\ttotal: 1m 35s\tremaining: 1m 50s\n",
            "392:\tlearn: 66.3948390\ttotal: 1m 35s\tremaining: 1m 50s\n",
            "393:\tlearn: 66.3722732\ttotal: 1m 36s\tremaining: 1m 50s\n",
            "394:\tlearn: 66.3476228\ttotal: 1m 36s\tremaining: 1m 50s\n",
            "395:\tlearn: 66.3250231\ttotal: 1m 36s\tremaining: 1m 50s\n",
            "396:\tlearn: 66.2435232\ttotal: 1m 36s\tremaining: 1m 49s\n",
            "397:\tlearn: 65.9975001\ttotal: 1m 37s\tremaining: 1m 49s\n",
            "398:\tlearn: 65.7327939\ttotal: 1m 37s\tremaining: 1m 49s\n",
            "399:\tlearn: 65.6839889\ttotal: 1m 37s\tremaining: 1m 49s\n",
            "400:\tlearn: 65.5368507\ttotal: 1m 37s\tremaining: 1m 48s\n",
            "401:\tlearn: 65.4920141\ttotal: 1m 37s\tremaining: 1m 48s\n",
            "402:\tlearn: 65.4022781\ttotal: 1m 38s\tremaining: 1m 48s\n",
            "403:\tlearn: 65.1983911\ttotal: 1m 38s\tremaining: 1m 48s\n",
            "404:\tlearn: 65.1617615\ttotal: 1m 38s\tremaining: 1m 47s\n",
            "405:\tlearn: 65.0891664\ttotal: 1m 38s\tremaining: 1m 47s\n",
            "406:\tlearn: 65.0160396\ttotal: 1m 39s\tremaining: 1m 47s\n",
            "407:\tlearn: 64.8731319\ttotal: 1m 39s\tremaining: 1m 47s\n",
            "408:\tlearn: 64.8202953\ttotal: 1m 39s\tremaining: 1m 46s\n",
            "409:\tlearn: 64.7452909\ttotal: 1m 39s\tremaining: 1m 46s\n",
            "410:\tlearn: 64.7186581\ttotal: 1m 40s\tremaining: 1m 46s\n",
            "411:\tlearn: 64.6926104\ttotal: 1m 40s\tremaining: 1m 46s\n",
            "412:\tlearn: 64.5941660\ttotal: 1m 40s\tremaining: 1m 46s\n",
            "413:\tlearn: 64.5612350\ttotal: 1m 40s\tremaining: 1m 45s\n",
            "414:\tlearn: 64.5078033\ttotal: 1m 41s\tremaining: 1m 45s\n",
            "415:\tlearn: 64.4766740\ttotal: 1m 41s\tremaining: 1m 45s\n",
            "416:\tlearn: 64.4156542\ttotal: 1m 41s\tremaining: 1m 45s\n",
            "417:\tlearn: 64.3365957\ttotal: 1m 41s\tremaining: 1m 44s\n",
            "418:\tlearn: 64.3230712\ttotal: 1m 42s\tremaining: 1m 44s\n",
            "419:\tlearn: 64.2692755\ttotal: 1m 42s\tremaining: 1m 44s\n",
            "420:\tlearn: 64.1260513\ttotal: 1m 42s\tremaining: 1m 44s\n",
            "421:\tlearn: 64.1047620\ttotal: 1m 42s\tremaining: 1m 43s\n",
            "422:\tlearn: 64.0670536\ttotal: 1m 43s\tremaining: 1m 43s\n",
            "423:\tlearn: 64.0066046\ttotal: 1m 43s\tremaining: 1m 43s\n",
            "424:\tlearn: 63.8385450\ttotal: 1m 43s\tremaining: 1m 43s\n",
            "425:\tlearn: 63.7644294\ttotal: 1m 43s\tremaining: 1m 42s\n",
            "426:\tlearn: 63.6666961\ttotal: 1m 44s\tremaining: 1m 42s\n",
            "427:\tlearn: 63.6228937\ttotal: 1m 44s\tremaining: 1m 42s\n",
            "428:\tlearn: 63.5402360\ttotal: 1m 44s\tremaining: 1m 42s\n",
            "429:\tlearn: 63.4650953\ttotal: 1m 45s\tremaining: 1m 42s\n",
            "430:\tlearn: 63.3905192\ttotal: 1m 45s\tremaining: 1m 41s\n",
            "431:\tlearn: 63.3458878\ttotal: 1m 45s\tremaining: 1m 41s\n",
            "432:\tlearn: 63.2623044\ttotal: 1m 45s\tremaining: 1m 41s\n",
            "433:\tlearn: 63.1372965\ttotal: 1m 46s\tremaining: 1m 41s\n",
            "434:\tlearn: 63.0072898\ttotal: 1m 46s\tremaining: 1m 41s\n",
            "435:\tlearn: 62.9090884\ttotal: 1m 46s\tremaining: 1m 40s\n",
            "436:\tlearn: 62.7758030\ttotal: 1m 46s\tremaining: 1m 40s\n",
            "437:\tlearn: 62.7273388\ttotal: 1m 47s\tremaining: 1m 40s\n",
            "438:\tlearn: 62.5867306\ttotal: 1m 47s\tremaining: 1m 40s\n",
            "439:\tlearn: 62.5169999\ttotal: 1m 47s\tremaining: 1m 39s\n",
            "440:\tlearn: 62.4147195\ttotal: 1m 47s\tremaining: 1m 39s\n",
            "441:\tlearn: 62.3942481\ttotal: 1m 48s\tremaining: 1m 39s\n",
            "442:\tlearn: 62.3154283\ttotal: 1m 48s\tremaining: 1m 39s\n",
            "443:\tlearn: 62.2435844\ttotal: 1m 48s\tremaining: 1m 38s\n",
            "444:\tlearn: 62.1918233\ttotal: 1m 48s\tremaining: 1m 38s\n",
            "445:\tlearn: 62.1561459\ttotal: 1m 49s\tremaining: 1m 38s\n",
            "446:\tlearn: 62.0803625\ttotal: 1m 49s\tremaining: 1m 38s\n",
            "447:\tlearn: 61.9476875\ttotal: 1m 49s\tremaining: 1m 37s\n",
            "448:\tlearn: 61.8743901\ttotal: 1m 49s\tremaining: 1m 37s\n",
            "449:\tlearn: 61.8204054\ttotal: 1m 50s\tremaining: 1m 37s\n",
            "450:\tlearn: 61.7937663\ttotal: 1m 50s\tremaining: 1m 37s\n",
            "451:\tlearn: 61.7639948\ttotal: 1m 50s\tremaining: 1m 36s\n",
            "452:\tlearn: 61.6913232\ttotal: 1m 50s\tremaining: 1m 36s\n",
            "453:\tlearn: 61.6097335\ttotal: 1m 51s\tremaining: 1m 36s\n",
            "454:\tlearn: 61.4817270\ttotal: 1m 51s\tremaining: 1m 36s\n",
            "455:\tlearn: 61.4019123\ttotal: 1m 51s\tremaining: 1m 36s\n",
            "456:\tlearn: 61.3796905\ttotal: 1m 51s\tremaining: 1m 35s\n",
            "457:\tlearn: 61.3664677\ttotal: 1m 52s\tremaining: 1m 35s\n",
            "458:\tlearn: 61.2802104\ttotal: 1m 52s\tremaining: 1m 35s\n",
            "459:\tlearn: 61.1935088\ttotal: 1m 52s\tremaining: 1m 35s\n",
            "460:\tlearn: 61.0954417\ttotal: 1m 52s\tremaining: 1m 34s\n",
            "461:\tlearn: 61.0264827\ttotal: 1m 53s\tremaining: 1m 34s\n",
            "462:\tlearn: 60.8760650\ttotal: 1m 53s\tremaining: 1m 34s\n",
            "463:\tlearn: 60.7929454\ttotal: 1m 53s\tremaining: 1m 34s\n",
            "464:\tlearn: 60.6933258\ttotal: 1m 53s\tremaining: 1m 33s\n",
            "465:\tlearn: 60.6131687\ttotal: 1m 54s\tremaining: 1m 33s\n",
            "466:\tlearn: 60.5479844\ttotal: 1m 54s\tremaining: 1m 33s\n",
            "467:\tlearn: 60.5160769\ttotal: 1m 54s\tremaining: 1m 33s\n",
            "468:\tlearn: 60.4200754\ttotal: 1m 54s\tremaining: 1m 32s\n",
            "469:\tlearn: 60.3619738\ttotal: 1m 55s\tremaining: 1m 32s\n",
            "470:\tlearn: 60.3141648\ttotal: 1m 55s\tremaining: 1m 32s\n",
            "471:\tlearn: 60.2637917\ttotal: 1m 55s\tremaining: 1m 32s\n",
            "472:\tlearn: 60.1408113\ttotal: 1m 55s\tremaining: 1m 31s\n",
            "473:\tlearn: 60.0642241\ttotal: 1m 56s\tremaining: 1m 31s\n",
            "474:\tlearn: 60.0200046\ttotal: 1m 56s\tremaining: 1m 31s\n",
            "475:\tlearn: 59.9457572\ttotal: 1m 56s\tremaining: 1m 31s\n",
            "476:\tlearn: 59.8487892\ttotal: 1m 56s\tremaining: 1m 30s\n",
            "477:\tlearn: 59.8243280\ttotal: 1m 56s\tremaining: 1m 30s\n",
            "478:\tlearn: 59.7676910\ttotal: 1m 57s\tremaining: 1m 30s\n",
            "479:\tlearn: 59.7166430\ttotal: 1m 57s\tremaining: 1m 30s\n",
            "480:\tlearn: 59.6343634\ttotal: 1m 57s\tremaining: 1m 29s\n",
            "481:\tlearn: 59.5942477\ttotal: 1m 57s\tremaining: 1m 29s\n",
            "482:\tlearn: 59.5650453\ttotal: 1m 58s\tremaining: 1m 29s\n",
            "483:\tlearn: 59.3936756\ttotal: 1m 58s\tremaining: 1m 29s\n",
            "484:\tlearn: 59.3696684\ttotal: 1m 58s\tremaining: 1m 28s\n",
            "485:\tlearn: 59.2918061\ttotal: 1m 58s\tremaining: 1m 28s\n",
            "486:\tlearn: 59.1997205\ttotal: 1m 59s\tremaining: 1m 28s\n",
            "487:\tlearn: 59.1772464\ttotal: 1m 59s\tremaining: 1m 28s\n",
            "488:\tlearn: 59.1106496\ttotal: 1m 59s\tremaining: 1m 27s\n",
            "489:\tlearn: 59.0254765\ttotal: 1m 59s\tremaining: 1m 27s\n",
            "490:\tlearn: 58.9976672\ttotal: 2m\tremaining: 1m 27s\n",
            "491:\tlearn: 58.9805463\ttotal: 2m\tremaining: 1m 27s\n",
            "492:\tlearn: 58.9598763\ttotal: 2m\tremaining: 1m 26s\n",
            "493:\tlearn: 58.9192294\ttotal: 2m 1s\tremaining: 1m 26s\n",
            "494:\tlearn: 58.9114466\ttotal: 2m 1s\tremaining: 1m 26s\n",
            "495:\tlearn: 58.8200299\ttotal: 2m 1s\tremaining: 1m 26s\n",
            "496:\tlearn: 58.7294912\ttotal: 2m 1s\tremaining: 1m 26s\n",
            "497:\tlearn: 58.7103354\ttotal: 2m 2s\tremaining: 1m 25s\n",
            "498:\tlearn: 58.6148483\ttotal: 2m 2s\tremaining: 1m 25s\n",
            "499:\tlearn: 58.5909717\ttotal: 2m 2s\tremaining: 1m 25s\n",
            "500:\tlearn: 58.5131591\ttotal: 2m 2s\tremaining: 1m 25s\n",
            "501:\tlearn: 58.4918050\ttotal: 2m 3s\tremaining: 1m 24s\n",
            "502:\tlearn: 58.3141974\ttotal: 2m 3s\tremaining: 1m 24s\n",
            "503:\tlearn: 58.2543115\ttotal: 2m 3s\tremaining: 1m 24s\n",
            "504:\tlearn: 58.2124727\ttotal: 2m 3s\tremaining: 1m 24s\n",
            "505:\tlearn: 58.1757840\ttotal: 2m 4s\tremaining: 1m 23s\n",
            "506:\tlearn: 58.0684375\ttotal: 2m 4s\tremaining: 1m 23s\n",
            "507:\tlearn: 58.0382862\ttotal: 2m 4s\tremaining: 1m 23s\n",
            "508:\tlearn: 58.0289969\ttotal: 2m 4s\tremaining: 1m 23s\n",
            "509:\tlearn: 58.0264906\ttotal: 2m 5s\tremaining: 1m 22s\n",
            "510:\tlearn: 58.0016774\ttotal: 2m 5s\tremaining: 1m 22s\n",
            "511:\tlearn: 57.9536401\ttotal: 2m 5s\tremaining: 1m 22s\n",
            "512:\tlearn: 57.8582872\ttotal: 2m 5s\tremaining: 1m 22s\n",
            "513:\tlearn: 57.7422787\ttotal: 2m 5s\tremaining: 1m 21s\n",
            "514:\tlearn: 57.6926339\ttotal: 2m 6s\tremaining: 1m 21s\n",
            "515:\tlearn: 57.6742017\ttotal: 2m 6s\tremaining: 1m 21s\n",
            "516:\tlearn: 57.5973663\ttotal: 2m 6s\tremaining: 1m 21s\n",
            "517:\tlearn: 57.4985651\ttotal: 2m 6s\tremaining: 1m 20s\n",
            "518:\tlearn: 57.4713850\ttotal: 2m 7s\tremaining: 1m 20s\n",
            "519:\tlearn: 57.3692820\ttotal: 2m 7s\tremaining: 1m 20s\n",
            "520:\tlearn: 57.2969008\ttotal: 2m 7s\tremaining: 1m 20s\n",
            "521:\tlearn: 57.2694485\ttotal: 2m 7s\tremaining: 1m 19s\n",
            "522:\tlearn: 57.1793550\ttotal: 2m 8s\tremaining: 1m 19s\n",
            "523:\tlearn: 57.1485258\ttotal: 2m 8s\tremaining: 1m 19s\n",
            "524:\tlearn: 57.0992545\ttotal: 2m 8s\tremaining: 1m 19s\n",
            "525:\tlearn: 57.0625462\ttotal: 2m 8s\tremaining: 1m 18s\n",
            "526:\tlearn: 57.0357357\ttotal: 2m 9s\tremaining: 1m 18s\n",
            "527:\tlearn: 56.8410755\ttotal: 2m 9s\tremaining: 1m 18s\n",
            "528:\tlearn: 56.7947002\ttotal: 2m 9s\tremaining: 1m 18s\n",
            "529:\tlearn: 56.7858454\ttotal: 2m 9s\tremaining: 1m 17s\n",
            "530:\tlearn: 56.6443441\ttotal: 2m 10s\tremaining: 1m 17s\n",
            "531:\tlearn: 56.5992404\ttotal: 2m 10s\tremaining: 1m 17s\n",
            "532:\tlearn: 56.5630942\ttotal: 2m 10s\tremaining: 1m 17s\n",
            "533:\tlearn: 56.4901432\ttotal: 2m 10s\tremaining: 1m 16s\n",
            "534:\tlearn: 56.4322595\ttotal: 2m 11s\tremaining: 1m 16s\n",
            "535:\tlearn: 56.4048618\ttotal: 2m 11s\tremaining: 1m 16s\n",
            "536:\tlearn: 56.3995298\ttotal: 2m 11s\tremaining: 1m 16s\n",
            "537:\tlearn: 56.3893071\ttotal: 2m 11s\tremaining: 1m 15s\n",
            "538:\tlearn: 56.3424914\ttotal: 2m 12s\tremaining: 1m 15s\n",
            "539:\tlearn: 56.2020530\ttotal: 2m 12s\tremaining: 1m 15s\n",
            "540:\tlearn: 56.1740292\ttotal: 2m 12s\tremaining: 1m 15s\n",
            "541:\tlearn: 56.1120721\ttotal: 2m 12s\tremaining: 1m 15s\n",
            "542:\tlearn: 56.1103525\ttotal: 2m 13s\tremaining: 1m 14s\n",
            "543:\tlearn: 56.0091763\ttotal: 2m 13s\tremaining: 1m 14s\n",
            "544:\tlearn: 55.9836641\ttotal: 2m 13s\tremaining: 1m 14s\n",
            "545:\tlearn: 55.8993153\ttotal: 2m 13s\tremaining: 1m 14s\n",
            "546:\tlearn: 55.7950348\ttotal: 2m 14s\tremaining: 1m 13s\n",
            "547:\tlearn: 55.7687618\ttotal: 2m 14s\tremaining: 1m 13s\n",
            "548:\tlearn: 55.7593154\ttotal: 2m 14s\tremaining: 1m 13s\n",
            "549:\tlearn: 55.7353763\ttotal: 2m 14s\tremaining: 1m 13s\n",
            "550:\tlearn: 55.6489374\ttotal: 2m 15s\tremaining: 1m 12s\n",
            "551:\tlearn: 55.6116626\ttotal: 2m 15s\tremaining: 1m 12s\n",
            "552:\tlearn: 55.5585218\ttotal: 2m 15s\tremaining: 1m 12s\n",
            "553:\tlearn: 55.5537186\ttotal: 2m 15s\tremaining: 1m 12s\n",
            "554:\tlearn: 55.4021203\ttotal: 2m 16s\tremaining: 1m 11s\n",
            "555:\tlearn: 55.3675603\ttotal: 2m 16s\tremaining: 1m 11s\n",
            "556:\tlearn: 55.3042042\ttotal: 2m 16s\tremaining: 1m 11s\n",
            "557:\tlearn: 55.2516130\ttotal: 2m 16s\tremaining: 1m 11s\n",
            "558:\tlearn: 55.2268980\ttotal: 2m 17s\tremaining: 1m 10s\n",
            "559:\tlearn: 55.1844305\ttotal: 2m 17s\tremaining: 1m 10s\n",
            "560:\tlearn: 55.1048020\ttotal: 2m 17s\tremaining: 1m 10s\n",
            "561:\tlearn: 55.0896879\ttotal: 2m 17s\tremaining: 1m 10s\n",
            "562:\tlearn: 55.0263262\ttotal: 2m 18s\tremaining: 1m 9s\n",
            "563:\tlearn: 54.9867759\ttotal: 2m 18s\tremaining: 1m 9s\n",
            "564:\tlearn: 54.9734002\ttotal: 2m 18s\tremaining: 1m 9s\n",
            "565:\tlearn: 54.9470743\ttotal: 2m 19s\tremaining: 1m 9s\n",
            "566:\tlearn: 54.9153625\ttotal: 2m 19s\tremaining: 1m 9s\n",
            "567:\tlearn: 54.8518773\ttotal: 2m 19s\tremaining: 1m 8s\n",
            "568:\tlearn: 54.8083280\ttotal: 2m 19s\tremaining: 1m 8s\n",
            "569:\tlearn: 54.7796188\ttotal: 2m 20s\tremaining: 1m 8s\n",
            "570:\tlearn: 54.7467127\ttotal: 2m 20s\tremaining: 1m 8s\n",
            "571:\tlearn: 54.6874634\ttotal: 2m 20s\tremaining: 1m 7s\n",
            "572:\tlearn: 54.5925164\ttotal: 2m 20s\tremaining: 1m 7s\n",
            "573:\tlearn: 54.5517218\ttotal: 2m 21s\tremaining: 1m 7s\n",
            "574:\tlearn: 54.4625685\ttotal: 2m 21s\tremaining: 1m 7s\n",
            "575:\tlearn: 54.3455511\ttotal: 2m 21s\tremaining: 1m 6s\n",
            "576:\tlearn: 54.3019174\ttotal: 2m 21s\tremaining: 1m 6s\n",
            "577:\tlearn: 54.2144953\ttotal: 2m 22s\tremaining: 1m 6s\n",
            "578:\tlearn: 54.1441980\ttotal: 2m 22s\tremaining: 1m 6s\n",
            "579:\tlearn: 54.0772825\ttotal: 2m 22s\tremaining: 1m 5s\n",
            "580:\tlearn: 54.0008834\ttotal: 2m 22s\tremaining: 1m 5s\n",
            "581:\tlearn: 53.9561150\ttotal: 2m 22s\tremaining: 1m 5s\n",
            "582:\tlearn: 53.9251735\ttotal: 2m 23s\tremaining: 1m 5s\n",
            "583:\tlearn: 53.8937816\ttotal: 2m 23s\tremaining: 1m 4s\n",
            "584:\tlearn: 53.8771607\ttotal: 2m 23s\tremaining: 1m 4s\n",
            "585:\tlearn: 53.8745657\ttotal: 2m 23s\tremaining: 1m 4s\n",
            "586:\tlearn: 53.8357147\ttotal: 2m 24s\tremaining: 1m 4s\n",
            "587:\tlearn: 53.7964067\ttotal: 2m 24s\tremaining: 1m 3s\n",
            "588:\tlearn: 53.7701541\ttotal: 2m 24s\tremaining: 1m 3s\n",
            "589:\tlearn: 53.7177894\ttotal: 2m 24s\tremaining: 1m 3s\n",
            "590:\tlearn: 53.6445946\ttotal: 2m 25s\tremaining: 1m 3s\n",
            "591:\tlearn: 53.6147271\ttotal: 2m 25s\tremaining: 1m 2s\n",
            "592:\tlearn: 53.5810833\ttotal: 2m 25s\tremaining: 1m 2s\n",
            "593:\tlearn: 53.5076677\ttotal: 2m 25s\tremaining: 1m 2s\n",
            "594:\tlearn: 53.4614876\ttotal: 2m 26s\tremaining: 1m 2s\n",
            "595:\tlearn: 53.4121242\ttotal: 2m 26s\tremaining: 1m 1s\n",
            "596:\tlearn: 53.3777356\ttotal: 2m 26s\tremaining: 1m 1s\n",
            "597:\tlearn: 53.3536128\ttotal: 2m 27s\tremaining: 1m 1s\n",
            "598:\tlearn: 53.2846116\ttotal: 2m 27s\tremaining: 1m 1s\n",
            "599:\tlearn: 53.2652172\ttotal: 2m 27s\tremaining: 1m\n",
            "600:\tlearn: 53.2327494\ttotal: 2m 27s\tremaining: 1m\n",
            "601:\tlearn: 53.2029285\ttotal: 2m 28s\tremaining: 1m\n",
            "602:\tlearn: 53.1133686\ttotal: 2m 28s\tremaining: 1m\n",
            "603:\tlearn: 53.0911541\ttotal: 2m 28s\tremaining: 60s\n",
            "604:\tlearn: 53.0521973\ttotal: 2m 28s\tremaining: 59.8s\n",
            "605:\tlearn: 53.0269703\ttotal: 2m 28s\tremaining: 59.5s\n",
            "606:\tlearn: 52.9809167\ttotal: 2m 29s\tremaining: 59.3s\n",
            "607:\tlearn: 52.9493859\ttotal: 2m 29s\tremaining: 59s\n",
            "608:\tlearn: 52.8938693\ttotal: 2m 29s\tremaining: 58.8s\n",
            "609:\tlearn: 52.8537636\ttotal: 2m 29s\tremaining: 58.5s\n",
            "610:\tlearn: 52.8401901\ttotal: 2m 30s\tremaining: 58.2s\n",
            "611:\tlearn: 52.8112820\ttotal: 2m 30s\tremaining: 58s\n",
            "612:\tlearn: 52.7678755\ttotal: 2m 30s\tremaining: 57.8s\n",
            "613:\tlearn: 52.6633510\ttotal: 2m 30s\tremaining: 57.5s\n",
            "614:\tlearn: 52.6154229\ttotal: 2m 31s\tremaining: 57.3s\n",
            "615:\tlearn: 52.5199407\ttotal: 2m 31s\tremaining: 57s\n",
            "616:\tlearn: 52.4753574\ttotal: 2m 31s\tremaining: 56.8s\n",
            "617:\tlearn: 52.4317658\ttotal: 2m 32s\tremaining: 56.6s\n",
            "618:\tlearn: 52.3975377\ttotal: 2m 32s\tremaining: 56.4s\n",
            "619:\tlearn: 52.3649731\ttotal: 2m 32s\tremaining: 56.1s\n",
            "620:\tlearn: 52.3383902\ttotal: 2m 32s\tremaining: 55.9s\n",
            "621:\tlearn: 52.2840789\ttotal: 2m 33s\tremaining: 55.7s\n",
            "622:\tlearn: 52.2103749\ttotal: 2m 33s\tremaining: 55.4s\n",
            "623:\tlearn: 52.1749421\ttotal: 2m 33s\tremaining: 55.2s\n",
            "624:\tlearn: 52.0970209\ttotal: 2m 33s\tremaining: 54.9s\n",
            "625:\tlearn: 52.0663790\ttotal: 2m 34s\tremaining: 54.7s\n",
            "626:\tlearn: 52.0378491\ttotal: 2m 34s\tremaining: 54.4s\n",
            "627:\tlearn: 52.0142442\ttotal: 2m 34s\tremaining: 54.2s\n",
            "628:\tlearn: 51.9369484\ttotal: 2m 35s\tremaining: 54s\n",
            "629:\tlearn: 51.9082227\ttotal: 2m 35s\tremaining: 53.7s\n",
            "630:\tlearn: 51.8686536\ttotal: 2m 35s\tremaining: 53.5s\n",
            "631:\tlearn: 51.7892708\ttotal: 2m 35s\tremaining: 53.2s\n",
            "632:\tlearn: 51.7824584\ttotal: 2m 35s\tremaining: 53s\n",
            "633:\tlearn: 51.7261328\ttotal: 2m 36s\tremaining: 52.7s\n",
            "634:\tlearn: 51.5847259\ttotal: 2m 36s\tremaining: 52.5s\n",
            "635:\tlearn: 51.5691607\ttotal: 2m 36s\tremaining: 52.2s\n",
            "636:\tlearn: 51.5657780\ttotal: 2m 36s\tremaining: 52s\n",
            "637:\tlearn: 51.5258160\ttotal: 2m 37s\tremaining: 51.7s\n",
            "638:\tlearn: 51.4856713\ttotal: 2m 37s\tremaining: 51.5s\n",
            "639:\tlearn: 51.4027442\ttotal: 2m 37s\tremaining: 51.2s\n",
            "640:\tlearn: 51.2899862\ttotal: 2m 37s\tremaining: 51s\n",
            "641:\tlearn: 51.2488011\ttotal: 2m 38s\tremaining: 50.7s\n",
            "642:\tlearn: 51.2138354\ttotal: 2m 38s\tremaining: 50.5s\n",
            "643:\tlearn: 51.1606379\ttotal: 2m 38s\tremaining: 50.2s\n",
            "644:\tlearn: 51.0821847\ttotal: 2m 38s\tremaining: 50s\n",
            "645:\tlearn: 51.0378552\ttotal: 2m 39s\tremaining: 49.7s\n",
            "646:\tlearn: 51.0068564\ttotal: 2m 39s\tremaining: 49.5s\n",
            "647:\tlearn: 50.9441920\ttotal: 2m 39s\tremaining: 49.2s\n",
            "648:\tlearn: 50.8654615\ttotal: 2m 39s\tremaining: 49s\n",
            "649:\tlearn: 50.7816489\ttotal: 2m 40s\tremaining: 48.8s\n",
            "650:\tlearn: 50.7394057\ttotal: 2m 40s\tremaining: 48.5s\n",
            "651:\tlearn: 50.7000181\ttotal: 2m 40s\tremaining: 48.2s\n",
            "652:\tlearn: 50.6653702\ttotal: 2m 40s\tremaining: 48s\n",
            "653:\tlearn: 50.5986203\ttotal: 2m 40s\tremaining: 47.7s\n",
            "654:\tlearn: 50.5546655\ttotal: 2m 41s\tremaining: 47.5s\n",
            "655:\tlearn: 50.4769017\ttotal: 2m 41s\tremaining: 47.2s\n",
            "656:\tlearn: 50.4265405\ttotal: 2m 41s\tremaining: 47s\n",
            "657:\tlearn: 50.3973853\ttotal: 2m 41s\tremaining: 46.8s\n",
            "658:\tlearn: 50.3716491\ttotal: 2m 42s\tremaining: 46.5s\n",
            "659:\tlearn: 50.3217168\ttotal: 2m 42s\tremaining: 46.3s\n",
            "660:\tlearn: 50.3065635\ttotal: 2m 42s\tremaining: 46s\n",
            "661:\tlearn: 50.2785913\ttotal: 2m 42s\tremaining: 45.8s\n",
            "662:\tlearn: 50.1807111\ttotal: 2m 43s\tremaining: 45.5s\n",
            "663:\tlearn: 50.1485564\ttotal: 2m 43s\tremaining: 45.3s\n",
            "664:\tlearn: 50.1159334\ttotal: 2m 43s\tremaining: 45s\n",
            "665:\tlearn: 50.0910798\ttotal: 2m 43s\tremaining: 44.8s\n",
            "666:\tlearn: 50.0825304\ttotal: 2m 44s\tremaining: 44.6s\n",
            "667:\tlearn: 50.0598553\ttotal: 2m 44s\tremaining: 44.3s\n",
            "668:\tlearn: 50.0184114\ttotal: 2m 44s\tremaining: 44.1s\n",
            "669:\tlearn: 49.9667467\ttotal: 2m 44s\tremaining: 43.8s\n",
            "670:\tlearn: 49.8655450\ttotal: 2m 45s\tremaining: 43.6s\n",
            "671:\tlearn: 49.8098164\ttotal: 2m 45s\tremaining: 43.3s\n",
            "672:\tlearn: 49.7915039\ttotal: 2m 45s\tremaining: 43.1s\n",
            "673:\tlearn: 49.7644085\ttotal: 2m 45s\tremaining: 42.8s\n",
            "674:\tlearn: 49.7337075\ttotal: 2m 46s\tremaining: 42.6s\n",
            "675:\tlearn: 49.7026592\ttotal: 2m 46s\tremaining: 42.3s\n",
            "676:\tlearn: 49.6790489\ttotal: 2m 46s\tremaining: 42.1s\n",
            "677:\tlearn: 49.6773010\ttotal: 2m 46s\tremaining: 41.8s\n",
            "678:\tlearn: 49.5990281\ttotal: 2m 47s\tremaining: 41.6s\n",
            "679:\tlearn: 49.5472153\ttotal: 2m 47s\tremaining: 41.4s\n",
            "680:\tlearn: 49.5015762\ttotal: 2m 47s\tremaining: 41.1s\n",
            "681:\tlearn: 49.4369021\ttotal: 2m 47s\tremaining: 40.9s\n",
            "682:\tlearn: 49.3354446\ttotal: 2m 48s\tremaining: 40.6s\n",
            "683:\tlearn: 49.2931831\ttotal: 2m 48s\tremaining: 40.4s\n",
            "684:\tlearn: 49.2526088\ttotal: 2m 48s\tremaining: 40.2s\n",
            "685:\tlearn: 49.2361172\ttotal: 2m 49s\tremaining: 39.9s\n",
            "686:\tlearn: 49.1893213\ttotal: 2m 49s\tremaining: 39.7s\n",
            "687:\tlearn: 49.1380300\ttotal: 2m 49s\tremaining: 39.4s\n",
            "688:\tlearn: 49.1173403\ttotal: 2m 49s\tremaining: 39.2s\n",
            "689:\tlearn: 49.0585093\ttotal: 2m 50s\tremaining: 39s\n",
            "690:\tlearn: 49.0388507\ttotal: 2m 50s\tremaining: 38.7s\n",
            "691:\tlearn: 49.0288789\ttotal: 2m 50s\tremaining: 38.5s\n",
            "692:\tlearn: 48.9791091\ttotal: 2m 50s\tremaining: 38.2s\n",
            "693:\tlearn: 48.9503166\ttotal: 2m 51s\tremaining: 38s\n",
            "694:\tlearn: 48.9157407\ttotal: 2m 51s\tremaining: 37.8s\n",
            "695:\tlearn: 48.8838182\ttotal: 2m 51s\tremaining: 37.5s\n",
            "696:\tlearn: 48.8597304\ttotal: 2m 52s\tremaining: 37.3s\n",
            "697:\tlearn: 48.8193316\ttotal: 2m 52s\tremaining: 37s\n",
            "698:\tlearn: 48.8040516\ttotal: 2m 52s\tremaining: 36.8s\n",
            "699:\tlearn: 48.7757833\ttotal: 2m 52s\tremaining: 36.5s\n",
            "700:\tlearn: 48.7490721\ttotal: 2m 53s\tremaining: 36.3s\n",
            "701:\tlearn: 48.7198668\ttotal: 2m 53s\tremaining: 36s\n",
            "702:\tlearn: 48.6917671\ttotal: 2m 53s\tremaining: 35.8s\n",
            "703:\tlearn: 48.6587608\ttotal: 2m 53s\tremaining: 35.5s\n",
            "704:\tlearn: 48.6320886\ttotal: 2m 53s\tremaining: 35.3s\n",
            "705:\tlearn: 48.6163314\ttotal: 2m 54s\tremaining: 35.1s\n",
            "706:\tlearn: 48.5678655\ttotal: 2m 54s\tremaining: 34.8s\n",
            "707:\tlearn: 48.5432962\ttotal: 2m 54s\tremaining: 34.6s\n",
            "708:\tlearn: 48.4841823\ttotal: 2m 55s\tremaining: 34.3s\n",
            "709:\tlearn: 48.4619540\ttotal: 2m 55s\tremaining: 34.1s\n",
            "710:\tlearn: 48.4472725\ttotal: 2m 55s\tremaining: 33.8s\n",
            "711:\tlearn: 48.4417140\ttotal: 2m 55s\tremaining: 33.6s\n",
            "712:\tlearn: 48.4198389\ttotal: 2m 56s\tremaining: 33.3s\n",
            "713:\tlearn: 48.4068173\ttotal: 2m 56s\tremaining: 33.1s\n",
            "714:\tlearn: 48.3805935\ttotal: 2m 56s\tremaining: 32.9s\n",
            "715:\tlearn: 48.3424437\ttotal: 2m 56s\tremaining: 32.6s\n",
            "716:\tlearn: 48.3008233\ttotal: 2m 57s\tremaining: 32.4s\n",
            "717:\tlearn: 48.2901254\ttotal: 2m 57s\tremaining: 32.1s\n",
            "718:\tlearn: 48.2698673\ttotal: 2m 57s\tremaining: 31.9s\n",
            "719:\tlearn: 48.2389691\ttotal: 2m 57s\tremaining: 31.6s\n",
            "720:\tlearn: 48.2109601\ttotal: 2m 58s\tremaining: 31.4s\n",
            "721:\tlearn: 48.1761958\ttotal: 2m 58s\tremaining: 31.1s\n",
            "722:\tlearn: 48.1635224\ttotal: 2m 58s\tremaining: 30.9s\n",
            "723:\tlearn: 48.1460920\ttotal: 2m 58s\tremaining: 30.7s\n",
            "724:\tlearn: 48.1078245\ttotal: 2m 59s\tremaining: 30.4s\n",
            "725:\tlearn: 48.0753517\ttotal: 2m 59s\tremaining: 30.2s\n",
            "726:\tlearn: 48.0337224\ttotal: 2m 59s\tremaining: 29.9s\n",
            "727:\tlearn: 48.0262774\ttotal: 3m\tremaining: 29.7s\n",
            "728:\tlearn: 47.9791442\ttotal: 3m\tremaining: 29.4s\n",
            "729:\tlearn: 47.9654024\ttotal: 3m\tremaining: 29.2s\n",
            "730:\tlearn: 47.8757502\ttotal: 3m\tremaining: 28.9s\n",
            "731:\tlearn: 47.8293182\ttotal: 3m\tremaining: 28.7s\n",
            "732:\tlearn: 47.7678131\ttotal: 3m 1s\tremaining: 28.4s\n",
            "733:\tlearn: 47.7622526\ttotal: 3m 1s\tremaining: 28.2s\n",
            "734:\tlearn: 47.7403021\ttotal: 3m 1s\tremaining: 27.9s\n",
            "735:\tlearn: 47.7021377\ttotal: 3m 1s\tremaining: 27.7s\n",
            "736:\tlearn: 47.6837565\ttotal: 3m 2s\tremaining: 27.5s\n",
            "737:\tlearn: 47.6643773\ttotal: 3m 2s\tremaining: 27.2s\n",
            "738:\tlearn: 47.6393182\ttotal: 3m 2s\tremaining: 27s\n",
            "739:\tlearn: 47.6236621\ttotal: 3m 3s\tremaining: 26.7s\n",
            "740:\tlearn: 47.5959344\ttotal: 3m 3s\tremaining: 26.5s\n",
            "741:\tlearn: 47.5750113\ttotal: 3m 3s\tremaining: 26.2s\n",
            "742:\tlearn: 47.5154243\ttotal: 3m 3s\tremaining: 26s\n",
            "743:\tlearn: 47.4995277\ttotal: 3m 4s\tremaining: 25.7s\n",
            "744:\tlearn: 47.4920400\ttotal: 3m 4s\tremaining: 25.5s\n",
            "745:\tlearn: 47.4785988\ttotal: 3m 4s\tremaining: 25.3s\n",
            "746:\tlearn: 47.4141011\ttotal: 3m 5s\tremaining: 25s\n",
            "747:\tlearn: 47.3983893\ttotal: 3m 5s\tremaining: 24.8s\n",
            "748:\tlearn: 47.3713081\ttotal: 3m 5s\tremaining: 24.5s\n",
            "749:\tlearn: 47.2572546\ttotal: 3m 5s\tremaining: 24.3s\n",
            "750:\tlearn: 47.2358826\ttotal: 3m 6s\tremaining: 24s\n",
            "751:\tlearn: 47.2150320\ttotal: 3m 6s\tremaining: 23.8s\n",
            "752:\tlearn: 47.1560906\ttotal: 3m 6s\tremaining: 23.5s\n",
            "753:\tlearn: 47.1440191\ttotal: 3m 6s\tremaining: 23.3s\n",
            "754:\tlearn: 47.1128855\ttotal: 3m 7s\tremaining: 23s\n",
            "755:\tlearn: 47.0793242\ttotal: 3m 7s\tremaining: 22.8s\n",
            "756:\tlearn: 47.0646448\ttotal: 3m 7s\tremaining: 22.6s\n",
            "757:\tlearn: 47.0039101\ttotal: 3m 7s\tremaining: 22.3s\n",
            "758:\tlearn: 46.9696118\ttotal: 3m 8s\tremaining: 22.1s\n",
            "759:\tlearn: 46.9157382\ttotal: 3m 8s\tremaining: 21.8s\n",
            "760:\tlearn: 46.9025463\ttotal: 3m 8s\tremaining: 21.6s\n",
            "761:\tlearn: 46.8877248\ttotal: 3m 8s\tremaining: 21.3s\n",
            "762:\tlearn: 46.8574025\ttotal: 3m 9s\tremaining: 21.1s\n",
            "763:\tlearn: 46.8308013\ttotal: 3m 9s\tremaining: 20.8s\n",
            "764:\tlearn: 46.8061689\ttotal: 3m 9s\tremaining: 20.6s\n",
            "765:\tlearn: 46.7836298\ttotal: 3m 9s\tremaining: 20.3s\n",
            "766:\tlearn: 46.7810839\ttotal: 3m 10s\tremaining: 20.1s\n",
            "767:\tlearn: 46.7532102\ttotal: 3m 10s\tremaining: 19.8s\n",
            "768:\tlearn: 46.7511369\ttotal: 3m 10s\tremaining: 19.6s\n",
            "769:\tlearn: 46.7396766\ttotal: 3m 11s\tremaining: 19.3s\n",
            "770:\tlearn: 46.6734193\ttotal: 3m 11s\tremaining: 19.1s\n",
            "771:\tlearn: 46.6215354\ttotal: 3m 11s\tremaining: 18.8s\n",
            "772:\tlearn: 46.5903804\ttotal: 3m 11s\tremaining: 18.6s\n",
            "773:\tlearn: 46.5759639\ttotal: 3m 11s\tremaining: 18.4s\n",
            "774:\tlearn: 46.5542191\ttotal: 3m 12s\tremaining: 18.1s\n",
            "775:\tlearn: 46.5268703\ttotal: 3m 12s\tremaining: 17.9s\n",
            "776:\tlearn: 46.4923031\ttotal: 3m 12s\tremaining: 17.6s\n",
            "777:\tlearn: 46.4594989\ttotal: 3m 13s\tremaining: 17.4s\n",
            "778:\tlearn: 46.4372722\ttotal: 3m 13s\tremaining: 17.1s\n",
            "779:\tlearn: 46.4125099\ttotal: 3m 13s\tremaining: 16.9s\n",
            "780:\tlearn: 46.3793078\ttotal: 3m 13s\tremaining: 16.6s\n",
            "781:\tlearn: 46.3201860\ttotal: 3m 14s\tremaining: 16.4s\n",
            "782:\tlearn: 46.2717264\ttotal: 3m 14s\tremaining: 16.1s\n",
            "783:\tlearn: 46.1924826\ttotal: 3m 14s\tremaining: 15.9s\n",
            "784:\tlearn: 46.1555849\ttotal: 3m 14s\tremaining: 15.6s\n",
            "785:\tlearn: 46.0905796\ttotal: 3m 15s\tremaining: 15.4s\n",
            "786:\tlearn: 46.0291988\ttotal: 3m 15s\tremaining: 15.1s\n",
            "787:\tlearn: 46.0016096\ttotal: 3m 15s\tremaining: 14.9s\n",
            "788:\tlearn: 45.9876500\ttotal: 3m 15s\tremaining: 14.6s\n",
            "789:\tlearn: 45.9670268\ttotal: 3m 16s\tremaining: 14.4s\n",
            "790:\tlearn: 45.9204744\ttotal: 3m 16s\tremaining: 14.1s\n",
            "791:\tlearn: 45.8823900\ttotal: 3m 16s\tremaining: 13.9s\n",
            "792:\tlearn: 45.8547681\ttotal: 3m 16s\tremaining: 13.6s\n",
            "793:\tlearn: 45.8367953\ttotal: 3m 17s\tremaining: 13.4s\n",
            "794:\tlearn: 45.8246410\ttotal: 3m 17s\tremaining: 13.2s\n",
            "795:\tlearn: 45.8086342\ttotal: 3m 17s\tremaining: 12.9s\n",
            "796:\tlearn: 45.7814754\ttotal: 3m 17s\tremaining: 12.7s\n",
            "797:\tlearn: 45.7678479\ttotal: 3m 18s\tremaining: 12.4s\n",
            "798:\tlearn: 45.7631092\ttotal: 3m 18s\tremaining: 12.2s\n",
            "799:\tlearn: 45.7197994\ttotal: 3m 18s\tremaining: 11.9s\n",
            "800:\tlearn: 45.7105160\ttotal: 3m 18s\tremaining: 11.7s\n",
            "801:\tlearn: 45.6831823\ttotal: 3m 19s\tremaining: 11.4s\n",
            "802:\tlearn: 45.6595795\ttotal: 3m 19s\tremaining: 11.2s\n",
            "803:\tlearn: 45.6590074\ttotal: 3m 19s\tremaining: 10.9s\n",
            "804:\tlearn: 45.6378722\ttotal: 3m 19s\tremaining: 10.7s\n",
            "805:\tlearn: 45.5621618\ttotal: 3m 20s\tremaining: 10.4s\n",
            "806:\tlearn: 45.5176608\ttotal: 3m 20s\tremaining: 10.2s\n",
            "807:\tlearn: 45.4938184\ttotal: 3m 20s\tremaining: 9.93s\n",
            "808:\tlearn: 45.4595321\ttotal: 3m 20s\tremaining: 9.69s\n",
            "809:\tlearn: 45.4131796\ttotal: 3m 21s\tremaining: 9.44s\n",
            "810:\tlearn: 45.4010228\ttotal: 3m 21s\tremaining: 9.19s\n",
            "811:\tlearn: 45.3585383\ttotal: 3m 21s\tremaining: 8.94s\n",
            "812:\tlearn: 45.3433711\ttotal: 3m 21s\tremaining: 8.7s\n",
            "813:\tlearn: 45.3401099\ttotal: 3m 22s\tremaining: 8.45s\n",
            "814:\tlearn: 45.3172753\ttotal: 3m 22s\tremaining: 8.2s\n",
            "815:\tlearn: 45.2600203\ttotal: 3m 22s\tremaining: 7.95s\n",
            "816:\tlearn: 45.2377383\ttotal: 3m 23s\tremaining: 7.71s\n",
            "817:\tlearn: 45.2210529\ttotal: 3m 23s\tremaining: 7.46s\n",
            "818:\tlearn: 45.1937171\ttotal: 3m 23s\tremaining: 7.21s\n",
            "819:\tlearn: 45.1754992\ttotal: 3m 23s\tremaining: 6.96s\n",
            "820:\tlearn: 45.1410242\ttotal: 3m 24s\tremaining: 6.71s\n",
            "821:\tlearn: 45.1168900\ttotal: 3m 24s\tremaining: 6.46s\n",
            "822:\tlearn: 45.0915255\ttotal: 3m 24s\tremaining: 6.22s\n",
            "823:\tlearn: 45.0850011\ttotal: 3m 24s\tremaining: 5.97s\n",
            "824:\tlearn: 45.0814546\ttotal: 3m 25s\tremaining: 5.72s\n",
            "825:\tlearn: 45.0612811\ttotal: 3m 25s\tremaining: 5.47s\n",
            "826:\tlearn: 45.0298492\ttotal: 3m 25s\tremaining: 5.22s\n",
            "827:\tlearn: 45.0071518\ttotal: 3m 25s\tremaining: 4.97s\n",
            "828:\tlearn: 44.9760153\ttotal: 3m 26s\tremaining: 4.72s\n",
            "829:\tlearn: 44.9519180\ttotal: 3m 26s\tremaining: 4.48s\n",
            "830:\tlearn: 44.9435583\ttotal: 3m 26s\tremaining: 4.23s\n",
            "831:\tlearn: 44.9064731\ttotal: 3m 26s\tremaining: 3.98s\n",
            "832:\tlearn: 44.8645608\ttotal: 3m 27s\tremaining: 3.73s\n",
            "833:\tlearn: 44.8425968\ttotal: 3m 27s\tremaining: 3.48s\n",
            "834:\tlearn: 44.8029716\ttotal: 3m 27s\tremaining: 3.23s\n",
            "835:\tlearn: 44.7572408\ttotal: 3m 27s\tremaining: 2.98s\n",
            "836:\tlearn: 44.7413541\ttotal: 3m 28s\tremaining: 2.74s\n",
            "837:\tlearn: 44.6714569\ttotal: 3m 28s\tremaining: 2.49s\n",
            "838:\tlearn: 44.6480780\ttotal: 3m 28s\tremaining: 2.24s\n",
            "839:\tlearn: 44.6308000\ttotal: 3m 28s\tremaining: 1.99s\n",
            "840:\tlearn: 44.5768392\ttotal: 3m 29s\tremaining: 1.74s\n",
            "841:\tlearn: 44.5614471\ttotal: 3m 29s\tremaining: 1.49s\n",
            "842:\tlearn: 44.5480813\ttotal: 3m 29s\tremaining: 1.24s\n",
            "843:\tlearn: 44.5007197\ttotal: 3m 30s\tremaining: 995ms\n",
            "844:\tlearn: 44.4786327\ttotal: 3m 30s\tremaining: 747ms\n",
            "845:\tlearn: 44.4590321\ttotal: 3m 30s\tremaining: 498ms\n",
            "846:\tlearn: 44.4183229\ttotal: 3m 30s\tremaining: 249ms\n",
            "847:\tlearn: 44.3884252\ttotal: 3m 30s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 16:41:20,683] A new study created in memory with name: no-name-b50509ba-7538-491f-9c0e-26a8c01db643\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 7...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 16:51:27,508] Trial 0 finished with value: 58.480345356517965 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 58.480345356517965.\n",
            "[I 2023-10-08 16:55:36,760] Trial 1 finished with value: 39.260980484980614 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:02:34,344] Trial 2 finished with value: 42.58765841634913 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:07:22,197] Trial 3 finished with value: 41.93999705168678 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:11:30,213] Trial 4 finished with value: 40.78445756160866 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:19:53,385] Trial 5 finished with value: 75.8469724439131 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:23:33,273] Trial 6 finished with value: 43.310068346047906 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:36:32,228] Trial 7 finished with value: 54.58894512475406 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:38:44,359] Trial 8 finished with value: 55.94245943685882 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:42:33,428] Trial 9 finished with value: 47.42586447014333 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:49:16,596] Trial 10 finished with value: 50.59395788332883 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:53:27,767] Trial 11 finished with value: 40.63575013641491 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:56:58,779] Trial 12 finished with value: 43.586013960216164 and parameters: {'iterations': 743, 'learning_rate': 0.7356346127088652, 'depth': 9, 'min_data_in_leaf': 17, 'reg_lambda': 49.16423147603391, 'subsample': 0.5854290622440648, 'random_strength': 31.588734753749215, 'od_wait': 122, 'leaf_estimation_iterations': 4, 'bagging_temperature': 23.993242751388063, 'colsample_bylevel': 0.3642319611435846}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 17:59:40,483] Trial 13 finished with value: 53.366973550703484 and parameters: {'iterations': 757, 'learning_rate': 0.7501800009545696, 'depth': 11, 'min_data_in_leaf': 13, 'reg_lambda': 30.221360663173336, 'subsample': 0.4774950955421056, 'random_strength': 30.328584321736876, 'od_wait': 18, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.6071397112449649}. Best is trial 1 with value: 39.260980484980614.\n",
            "[I 2023-10-08 18:04:13,641] Trial 14 finished with value: 38.5779982043375 and parameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}. Best is trial 14 with value: 38.5779982043375.\n",
            "[I 2023-10-08 18:08:12,008] Trial 15 finished with value: 40.216272315788075 and parameters: {'iterations': 951, 'learning_rate': 0.378680852488155, 'depth': 7, 'min_data_in_leaf': 23, 'reg_lambda': 39.94535569998841, 'subsample': 0.6610237845565569, 'random_strength': 20.618074524594604, 'od_wait': 150, 'leaf_estimation_iterations': 5, 'bagging_temperature': 19.000173884005733, 'colsample_bylevel': 0.5429293784058689}. Best is trial 14 with value: 38.5779982043375.\n",
            "[I 2023-10-08 18:18:57,841] Trial 16 finished with value: 38.804291143979064 and parameters: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}. Best is trial 14 with value: 38.5779982043375.\n",
            "[I 2023-10-08 18:22:35,794] Trial 17 finished with value: 64.92607913471696 and parameters: {'iterations': 305, 'learning_rate': 0.10410309839558177, 'depth': 12, 'min_data_in_leaf': 22, 'reg_lambda': 60.12480416835778, 'subsample': 0.6075212384252984, 'random_strength': 43.37074078621255, 'od_wait': 134, 'leaf_estimation_iterations': 4, 'bagging_temperature': 54.04160477504041, 'colsample_bylevel': 0.971827627097186}. Best is trial 14 with value: 38.5779982043375.\n",
            "[I 2023-10-08 18:32:12,681] Trial 18 finished with value: 40.26231248224973 and parameters: {'iterations': 983, 'learning_rate': 0.13599792999265126, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 71.62634725545307, 'subsample': 0.44325233558749344, 'random_strength': 43.37894494169863, 'od_wait': 100, 'leaf_estimation_iterations': 9, 'bagging_temperature': 47.18463446341313, 'colsample_bylevel': 0.7741293449152455}. Best is trial 14 with value: 38.5779982043375.\n",
            "[I 2023-10-08 18:50:35,939] Trial 19 finished with value: 41.56761584633365 and parameters: {'iterations': 926, 'learning_rate': 0.3539878936286308, 'depth': 13, 'min_data_in_leaf': 30, 'reg_lambda': 62.712706465179814, 'subsample': 0.6182530441170837, 'random_strength': 64.9229596867476, 'od_wait': 130, 'leaf_estimation_iterations': 6, 'bagging_temperature': 98.66240341348602, 'colsample_bylevel': 0.9066266381241248}. Best is trial 14 with value: 38.5779982043375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 7: 38.5779982043375\n",
            "Best hyperparameters for fold 7: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}\n",
            "0:\tlearn: 206.7421182\ttotal: 225ms\tremaining: 3m 34s\n",
            "1:\tlearn: 204.1805734\ttotal: 474ms\tremaining: 3m 45s\n",
            "2:\tlearn: 203.7859599\ttotal: 708ms\tremaining: 3m 45s\n",
            "3:\tlearn: 202.9108379\ttotal: 952ms\tremaining: 3m 46s\n",
            "4:\tlearn: 202.0970721\ttotal: 1.19s\tremaining: 3m 46s\n",
            "5:\tlearn: 200.9898690\ttotal: 1.42s\tremaining: 3m 45s\n",
            "6:\tlearn: 200.8197113\ttotal: 1.67s\tremaining: 3m 46s\n",
            "7:\tlearn: 200.8149168\ttotal: 1.71s\tremaining: 3m 22s\n",
            "8:\tlearn: 200.2143650\ttotal: 1.93s\tremaining: 3m 22s\n",
            "9:\tlearn: 199.6479592\ttotal: 2.15s\tremaining: 3m 23s\n",
            "10:\tlearn: 199.2336244\ttotal: 2.38s\tremaining: 3m 24s\n",
            "11:\tlearn: 199.2072198\ttotal: 2.56s\tremaining: 3m 21s\n",
            "12:\tlearn: 198.7634299\ttotal: 2.84s\tremaining: 3m 25s\n",
            "13:\tlearn: 198.4335696\ttotal: 3.09s\tremaining: 3m 27s\n",
            "14:\tlearn: 198.3498704\ttotal: 3.22s\tremaining: 3m 21s\n",
            "15:\tlearn: 198.2369869\ttotal: 3.44s\tremaining: 3m 22s\n",
            "16:\tlearn: 198.2245692\ttotal: 3.51s\tremaining: 3m 13s\n",
            "17:\tlearn: 196.8890278\ttotal: 3.75s\tremaining: 3m 15s\n",
            "18:\tlearn: 196.7046272\ttotal: 4s\tremaining: 3m 17s\n",
            "19:\tlearn: 196.2843334\ttotal: 4.2s\tremaining: 3m 16s\n",
            "20:\tlearn: 195.9068790\ttotal: 4.42s\tremaining: 3m 16s\n",
            "21:\tlearn: 194.9088128\ttotal: 4.65s\tremaining: 3m 17s\n",
            "22:\tlearn: 193.9970034\ttotal: 4.91s\tremaining: 3m 19s\n",
            "23:\tlearn: 192.6206594\ttotal: 5.16s\tremaining: 3m 20s\n",
            "24:\tlearn: 191.8765180\ttotal: 5.39s\tremaining: 3m 20s\n",
            "25:\tlearn: 191.2121512\ttotal: 5.6s\tremaining: 3m 20s\n",
            "26:\tlearn: 190.5170234\ttotal: 5.89s\tremaining: 3m 22s\n",
            "27:\tlearn: 189.6169496\ttotal: 6.13s\tremaining: 3m 23s\n",
            "28:\tlearn: 185.7209599\ttotal: 6.33s\tremaining: 3m 22s\n",
            "29:\tlearn: 182.9632112\ttotal: 6.58s\tremaining: 3m 22s\n",
            "30:\tlearn: 181.3511494\ttotal: 6.8s\tremaining: 3m 22s\n",
            "31:\tlearn: 177.6479488\ttotal: 7.07s\tremaining: 3m 24s\n",
            "32:\tlearn: 175.8570681\ttotal: 7.29s\tremaining: 3m 24s\n",
            "33:\tlearn: 173.8724018\ttotal: 7.54s\tremaining: 3m 24s\n",
            "34:\tlearn: 173.4783928\ttotal: 7.75s\tremaining: 3m 23s\n",
            "35:\tlearn: 173.1717497\ttotal: 8s\tremaining: 3m 24s\n",
            "36:\tlearn: 172.1664412\ttotal: 8.23s\tremaining: 3m 24s\n",
            "37:\tlearn: 171.7998929\ttotal: 8.48s\tremaining: 3m 24s\n",
            "38:\tlearn: 170.6371584\ttotal: 8.73s\tremaining: 3m 25s\n",
            "39:\tlearn: 170.2865785\ttotal: 8.99s\tremaining: 3m 25s\n",
            "40:\tlearn: 167.3589840\ttotal: 9.23s\tremaining: 3m 25s\n",
            "41:\tlearn: 166.1214226\ttotal: 9.45s\tremaining: 3m 25s\n",
            "42:\tlearn: 165.7147003\ttotal: 9.73s\tremaining: 3m 26s\n",
            "43:\tlearn: 165.4876916\ttotal: 10s\tremaining: 3m 27s\n",
            "44:\tlearn: 165.1452649\ttotal: 10.3s\tremaining: 3m 27s\n",
            "45:\tlearn: 164.0508248\ttotal: 10.5s\tremaining: 3m 27s\n",
            "46:\tlearn: 163.5960652\ttotal: 10.7s\tremaining: 3m 26s\n",
            "47:\tlearn: 162.6909252\ttotal: 10.9s\tremaining: 3m 26s\n",
            "48:\tlearn: 161.8071510\ttotal: 11.1s\tremaining: 3m 26s\n",
            "49:\tlearn: 161.4938037\ttotal: 11.4s\tremaining: 3m 26s\n",
            "50:\tlearn: 160.5956529\ttotal: 11.6s\tremaining: 3m 25s\n",
            "51:\tlearn: 159.7431010\ttotal: 11.8s\tremaining: 3m 25s\n",
            "52:\tlearn: 158.5962541\ttotal: 12s\tremaining: 3m 25s\n",
            "53:\tlearn: 158.3948471\ttotal: 12.3s\tremaining: 3m 25s\n",
            "54:\tlearn: 157.4663919\ttotal: 12.5s\tremaining: 3m 24s\n",
            "55:\tlearn: 156.5451860\ttotal: 12.7s\tremaining: 3m 23s\n",
            "56:\tlearn: 155.9654961\ttotal: 12.9s\tremaining: 3m 23s\n",
            "57:\tlearn: 155.5462781\ttotal: 13.1s\tremaining: 3m 22s\n",
            "58:\tlearn: 154.1858196\ttotal: 13.3s\tremaining: 3m 22s\n",
            "59:\tlearn: 154.0407494\ttotal: 13.5s\tremaining: 3m 21s\n",
            "60:\tlearn: 153.2747697\ttotal: 13.7s\tremaining: 3m 21s\n",
            "61:\tlearn: 152.7836659\ttotal: 13.9s\tremaining: 3m 20s\n",
            "62:\tlearn: 152.2814180\ttotal: 14.1s\tremaining: 3m 20s\n",
            "63:\tlearn: 151.8564020\ttotal: 14.4s\tremaining: 3m 20s\n",
            "64:\tlearn: 151.2165362\ttotal: 14.6s\tremaining: 3m 20s\n",
            "65:\tlearn: 150.1734002\ttotal: 14.8s\tremaining: 3m 19s\n",
            "66:\tlearn: 149.2849587\ttotal: 15s\tremaining: 3m 19s\n",
            "67:\tlearn: 148.2272019\ttotal: 15.2s\tremaining: 3m 18s\n",
            "68:\tlearn: 147.8479881\ttotal: 15.4s\tremaining: 3m 18s\n",
            "69:\tlearn: 147.7076527\ttotal: 15.6s\tremaining: 3m 17s\n",
            "70:\tlearn: 147.3859875\ttotal: 15.8s\tremaining: 3m 16s\n",
            "71:\tlearn: 147.1931738\ttotal: 16s\tremaining: 3m 16s\n",
            "72:\tlearn: 146.6740806\ttotal: 16.2s\tremaining: 3m 15s\n",
            "73:\tlearn: 145.5450998\ttotal: 16.4s\tremaining: 3m 15s\n",
            "74:\tlearn: 145.1685516\ttotal: 16.6s\tremaining: 3m 15s\n",
            "75:\tlearn: 144.1643117\ttotal: 16.8s\tremaining: 3m 14s\n",
            "76:\tlearn: 143.1098376\ttotal: 17.1s\tremaining: 3m 14s\n",
            "77:\tlearn: 142.9655493\ttotal: 17.3s\tremaining: 3m 14s\n",
            "78:\tlearn: 142.6441360\ttotal: 17.5s\tremaining: 3m 13s\n",
            "79:\tlearn: 142.2655888\ttotal: 17.8s\tremaining: 3m 14s\n",
            "80:\tlearn: 142.1409799\ttotal: 18s\tremaining: 3m 14s\n",
            "81:\tlearn: 141.7677727\ttotal: 18.2s\tremaining: 3m 14s\n",
            "82:\tlearn: 140.8780594\ttotal: 18.4s\tremaining: 3m 14s\n",
            "83:\tlearn: 140.5886090\ttotal: 18.7s\tremaining: 3m 13s\n",
            "84:\tlearn: 140.1703637\ttotal: 18.9s\tremaining: 3m 13s\n",
            "85:\tlearn: 139.7489739\ttotal: 19.1s\tremaining: 3m 13s\n",
            "86:\tlearn: 139.5518242\ttotal: 19.3s\tremaining: 3m 13s\n",
            "87:\tlearn: 139.3493984\ttotal: 19.5s\tremaining: 3m 12s\n",
            "88:\tlearn: 139.2390408\ttotal: 19.8s\tremaining: 3m 12s\n",
            "89:\tlearn: 139.0811858\ttotal: 20s\tremaining: 3m 12s\n",
            "90:\tlearn: 138.8799268\ttotal: 20.2s\tremaining: 3m 11s\n",
            "91:\tlearn: 138.0028052\ttotal: 20.4s\tremaining: 3m 11s\n",
            "92:\tlearn: 137.8899328\ttotal: 20.7s\tremaining: 3m 12s\n",
            "93:\tlearn: 137.3484621\ttotal: 20.9s\tremaining: 3m 11s\n",
            "94:\tlearn: 136.6826379\ttotal: 21.1s\tremaining: 3m 11s\n",
            "95:\tlearn: 136.4655096\ttotal: 21.3s\tremaining: 3m 10s\n",
            "96:\tlearn: 136.2963757\ttotal: 21.6s\tremaining: 3m 10s\n",
            "97:\tlearn: 136.0898912\ttotal: 21.8s\tremaining: 3m 10s\n",
            "98:\tlearn: 135.8283654\ttotal: 22s\tremaining: 3m 10s\n",
            "99:\tlearn: 135.4706481\ttotal: 22.2s\tremaining: 3m 10s\n",
            "100:\tlearn: 134.5272666\ttotal: 22.4s\tremaining: 3m 10s\n",
            "101:\tlearn: 134.2782462\ttotal: 22.6s\tremaining: 3m 9s\n",
            "102:\tlearn: 134.0767228\ttotal: 22.9s\tremaining: 3m 9s\n",
            "103:\tlearn: 132.8703378\ttotal: 23.1s\tremaining: 3m 9s\n",
            "104:\tlearn: 132.3775612\ttotal: 23.4s\tremaining: 3m 9s\n",
            "105:\tlearn: 132.1077546\ttotal: 23.6s\tremaining: 3m 9s\n",
            "106:\tlearn: 131.9502354\ttotal: 24s\tremaining: 3m 10s\n",
            "107:\tlearn: 131.8326045\ttotal: 24.3s\tremaining: 3m 10s\n",
            "108:\tlearn: 131.1588777\ttotal: 24.5s\tremaining: 3m 10s\n",
            "109:\tlearn: 130.4915996\ttotal: 24.7s\tremaining: 3m 10s\n",
            "110:\tlearn: 130.2769653\ttotal: 25s\tremaining: 3m 10s\n",
            "111:\tlearn: 129.8310124\ttotal: 25.2s\tremaining: 3m 9s\n",
            "112:\tlearn: 128.8743104\ttotal: 25.4s\tremaining: 3m 9s\n",
            "113:\tlearn: 128.5857159\ttotal: 25.6s\tremaining: 3m 9s\n",
            "114:\tlearn: 128.0532442\ttotal: 25.8s\tremaining: 3m 9s\n",
            "115:\tlearn: 127.5537032\ttotal: 26.1s\tremaining: 3m 8s\n",
            "116:\tlearn: 127.2797839\ttotal: 26.3s\tremaining: 3m 8s\n",
            "117:\tlearn: 126.8368614\ttotal: 26.6s\tremaining: 3m 8s\n",
            "118:\tlearn: 126.7301365\ttotal: 26.8s\tremaining: 3m 8s\n",
            "119:\tlearn: 126.1089879\ttotal: 27s\tremaining: 3m 8s\n",
            "120:\tlearn: 125.6986580\ttotal: 27.2s\tremaining: 3m 7s\n",
            "121:\tlearn: 125.4725616\ttotal: 27.4s\tremaining: 3m 7s\n",
            "122:\tlearn: 125.3771255\ttotal: 27.7s\tremaining: 3m 7s\n",
            "123:\tlearn: 124.3836661\ttotal: 27.9s\tremaining: 3m 7s\n",
            "124:\tlearn: 124.0661513\ttotal: 28.1s\tremaining: 3m 6s\n",
            "125:\tlearn: 123.9798296\ttotal: 28.3s\tremaining: 3m 6s\n",
            "126:\tlearn: 123.8331638\ttotal: 28.6s\tremaining: 3m 6s\n",
            "127:\tlearn: 123.7634836\ttotal: 28.8s\tremaining: 3m 6s\n",
            "128:\tlearn: 123.0787069\ttotal: 29s\tremaining: 3m 5s\n",
            "129:\tlearn: 122.4772148\ttotal: 29.2s\tremaining: 3m 5s\n",
            "130:\tlearn: 122.1703855\ttotal: 29.4s\tremaining: 3m 5s\n",
            "131:\tlearn: 122.0616812\ttotal: 29.7s\tremaining: 3m 5s\n",
            "132:\tlearn: 121.4276027\ttotal: 29.9s\tremaining: 3m 4s\n",
            "133:\tlearn: 121.2545369\ttotal: 30.1s\tremaining: 3m 4s\n",
            "134:\tlearn: 121.1108203\ttotal: 30.3s\tremaining: 3m 4s\n",
            "135:\tlearn: 120.9845219\ttotal: 30.6s\tremaining: 3m 4s\n",
            "136:\tlearn: 120.7567549\ttotal: 30.8s\tremaining: 3m 3s\n",
            "137:\tlearn: 120.2268801\ttotal: 31s\tremaining: 3m 3s\n",
            "138:\tlearn: 119.9444942\ttotal: 31.2s\tremaining: 3m 3s\n",
            "139:\tlearn: 119.8345044\ttotal: 31.4s\tremaining: 3m 2s\n",
            "140:\tlearn: 118.6117322\ttotal: 31.6s\tremaining: 3m 2s\n",
            "141:\tlearn: 118.2226642\ttotal: 31.8s\tremaining: 3m 2s\n",
            "142:\tlearn: 117.6274868\ttotal: 32s\tremaining: 3m 1s\n",
            "143:\tlearn: 117.5658186\ttotal: 32.3s\tremaining: 3m 1s\n",
            "144:\tlearn: 117.2216586\ttotal: 32.5s\tremaining: 3m 1s\n",
            "145:\tlearn: 117.1184075\ttotal: 32.7s\tremaining: 3m 1s\n",
            "146:\tlearn: 116.9154714\ttotal: 32.9s\tremaining: 3m 1s\n",
            "147:\tlearn: 116.8249032\ttotal: 33.2s\tremaining: 3m 1s\n",
            "148:\tlearn: 116.6476759\ttotal: 33.4s\tremaining: 3m\n",
            "149:\tlearn: 116.4394942\ttotal: 33.6s\tremaining: 3m\n",
            "150:\tlearn: 116.2194267\ttotal: 33.8s\tremaining: 3m\n",
            "151:\tlearn: 116.0988903\ttotal: 34s\tremaining: 2m 59s\n",
            "152:\tlearn: 116.0218248\ttotal: 34.2s\tremaining: 2m 59s\n",
            "153:\tlearn: 115.9645545\ttotal: 34.5s\tremaining: 2m 59s\n",
            "154:\tlearn: 115.8237554\ttotal: 34.7s\tremaining: 2m 59s\n",
            "155:\tlearn: 115.5476717\ttotal: 34.9s\tremaining: 2m 58s\n",
            "156:\tlearn: 115.3608278\ttotal: 35.1s\tremaining: 2m 58s\n",
            "157:\tlearn: 115.2757586\ttotal: 35.3s\tremaining: 2m 58s\n",
            "158:\tlearn: 115.2247712\ttotal: 35.5s\tremaining: 2m 57s\n",
            "159:\tlearn: 115.0597464\ttotal: 35.7s\tremaining: 2m 57s\n",
            "160:\tlearn: 114.7983841\ttotal: 35.9s\tremaining: 2m 57s\n",
            "161:\tlearn: 114.6226175\ttotal: 36.1s\tremaining: 2m 56s\n",
            "162:\tlearn: 114.4310866\ttotal: 36.3s\tremaining: 2m 56s\n",
            "163:\tlearn: 114.2918510\ttotal: 36.5s\tremaining: 2m 56s\n",
            "164:\tlearn: 114.2601805\ttotal: 36.7s\tremaining: 2m 55s\n",
            "165:\tlearn: 114.2583242\ttotal: 36.9s\tremaining: 2m 55s\n",
            "166:\tlearn: 114.2554864\ttotal: 37.1s\tremaining: 2m 55s\n",
            "167:\tlearn: 114.1930384\ttotal: 37.4s\tremaining: 2m 55s\n",
            "168:\tlearn: 114.1032568\ttotal: 37.6s\tremaining: 2m 54s\n",
            "169:\tlearn: 114.0219157\ttotal: 37.9s\tremaining: 2m 55s\n",
            "170:\tlearn: 114.0085686\ttotal: 38.1s\tremaining: 2m 55s\n",
            "171:\tlearn: 113.7379128\ttotal: 38.3s\tremaining: 2m 54s\n",
            "172:\tlearn: 113.5036207\ttotal: 38.5s\tremaining: 2m 54s\n",
            "173:\tlearn: 113.4429677\ttotal: 38.8s\tremaining: 2m 54s\n",
            "174:\tlearn: 113.2942530\ttotal: 39s\tremaining: 2m 54s\n",
            "175:\tlearn: 113.1103377\ttotal: 39.2s\tremaining: 2m 53s\n",
            "176:\tlearn: 112.8728321\ttotal: 39.4s\tremaining: 2m 53s\n",
            "177:\tlearn: 112.7739152\ttotal: 39.6s\tremaining: 2m 53s\n",
            "178:\tlearn: 112.1167899\ttotal: 39.9s\tremaining: 2m 53s\n",
            "179:\tlearn: 112.0089679\ttotal: 40.1s\tremaining: 2m 52s\n",
            "180:\tlearn: 111.8622687\ttotal: 40.3s\tremaining: 2m 52s\n",
            "181:\tlearn: 111.7028521\ttotal: 40.6s\tremaining: 2m 52s\n",
            "182:\tlearn: 111.4628982\ttotal: 40.8s\tremaining: 2m 52s\n",
            "183:\tlearn: 110.8175726\ttotal: 41s\tremaining: 2m 52s\n",
            "184:\tlearn: 110.7649915\ttotal: 41.3s\tremaining: 2m 52s\n",
            "185:\tlearn: 110.5422283\ttotal: 41.5s\tremaining: 2m 51s\n",
            "186:\tlearn: 110.1975302\ttotal: 41.8s\tremaining: 2m 51s\n",
            "187:\tlearn: 109.8674611\ttotal: 41.9s\tremaining: 2m 51s\n",
            "188:\tlearn: 109.4350071\ttotal: 42.2s\tremaining: 2m 51s\n",
            "189:\tlearn: 109.2313521\ttotal: 42.4s\tremaining: 2m 51s\n",
            "190:\tlearn: 108.9851268\ttotal: 42.7s\tremaining: 2m 50s\n",
            "191:\tlearn: 108.8724150\ttotal: 42.9s\tremaining: 2m 50s\n",
            "192:\tlearn: 108.6875003\ttotal: 43.2s\tremaining: 2m 50s\n",
            "193:\tlearn: 108.4522311\ttotal: 43.4s\tremaining: 2m 50s\n",
            "194:\tlearn: 108.2666121\ttotal: 43.7s\tremaining: 2m 50s\n",
            "195:\tlearn: 107.9635539\ttotal: 43.9s\tremaining: 2m 50s\n",
            "196:\tlearn: 107.7350120\ttotal: 44.1s\tremaining: 2m 49s\n",
            "197:\tlearn: 107.5930562\ttotal: 44.3s\tremaining: 2m 49s\n",
            "198:\tlearn: 107.5218022\ttotal: 44.5s\tremaining: 2m 49s\n",
            "199:\tlearn: 107.2142049\ttotal: 44.7s\tremaining: 2m 48s\n",
            "200:\tlearn: 106.8765238\ttotal: 44.9s\tremaining: 2m 48s\n",
            "201:\tlearn: 106.7567431\ttotal: 45.1s\tremaining: 2m 48s\n",
            "202:\tlearn: 106.6208356\ttotal: 45.4s\tremaining: 2m 48s\n",
            "203:\tlearn: 106.6194689\ttotal: 45.6s\tremaining: 2m 48s\n",
            "204:\tlearn: 106.6103713\ttotal: 45.9s\tremaining: 2m 47s\n",
            "205:\tlearn: 106.5388342\ttotal: 46.1s\tremaining: 2m 47s\n",
            "206:\tlearn: 106.2862825\ttotal: 46.3s\tremaining: 2m 47s\n",
            "207:\tlearn: 106.0902624\ttotal: 46.5s\tremaining: 2m 47s\n",
            "208:\tlearn: 106.0625982\ttotal: 46.7s\tremaining: 2m 46s\n",
            "209:\tlearn: 106.0613895\ttotal: 46.9s\tremaining: 2m 46s\n",
            "210:\tlearn: 105.6925001\ttotal: 47.1s\tremaining: 2m 46s\n",
            "211:\tlearn: 105.5823233\ttotal: 47.3s\tremaining: 2m 45s\n",
            "212:\tlearn: 105.2644483\ttotal: 47.5s\tremaining: 2m 45s\n",
            "213:\tlearn: 104.9027238\ttotal: 47.7s\tremaining: 2m 45s\n",
            "214:\tlearn: 104.7519787\ttotal: 47.9s\tremaining: 2m 45s\n",
            "215:\tlearn: 104.5954957\ttotal: 48.1s\tremaining: 2m 44s\n",
            "216:\tlearn: 104.5210530\ttotal: 48.3s\tremaining: 2m 44s\n",
            "217:\tlearn: 104.4055719\ttotal: 48.5s\tremaining: 2m 44s\n",
            "218:\tlearn: 103.8952683\ttotal: 48.7s\tremaining: 2m 43s\n",
            "219:\tlearn: 103.7410527\ttotal: 48.9s\tremaining: 2m 43s\n",
            "220:\tlearn: 103.6238566\ttotal: 49.1s\tremaining: 2m 43s\n",
            "221:\tlearn: 103.3853500\ttotal: 49.4s\tremaining: 2m 43s\n",
            "222:\tlearn: 103.2658440\ttotal: 49.6s\tremaining: 2m 42s\n",
            "223:\tlearn: 103.0714208\ttotal: 49.8s\tremaining: 2m 42s\n",
            "224:\tlearn: 102.6402796\ttotal: 50s\tremaining: 2m 42s\n",
            "225:\tlearn: 102.5662311\ttotal: 50.2s\tremaining: 2m 42s\n",
            "226:\tlearn: 102.4431642\ttotal: 50.4s\tremaining: 2m 41s\n",
            "227:\tlearn: 102.2031654\ttotal: 50.6s\tremaining: 2m 41s\n",
            "228:\tlearn: 102.0674815\ttotal: 50.8s\tremaining: 2m 41s\n",
            "229:\tlearn: 102.0257906\ttotal: 51s\tremaining: 2m 41s\n",
            "230:\tlearn: 101.9698907\ttotal: 51.3s\tremaining: 2m 40s\n",
            "231:\tlearn: 101.6649311\ttotal: 51.5s\tremaining: 2m 40s\n",
            "232:\tlearn: 101.5663909\ttotal: 51.7s\tremaining: 2m 40s\n",
            "233:\tlearn: 101.5653060\ttotal: 51.9s\tremaining: 2m 40s\n",
            "234:\tlearn: 101.5623159\ttotal: 52.1s\tremaining: 2m 39s\n",
            "235:\tlearn: 101.4942886\ttotal: 52.4s\tremaining: 2m 39s\n",
            "236:\tlearn: 101.4022064\ttotal: 52.6s\tremaining: 2m 39s\n",
            "237:\tlearn: 101.2689659\ttotal: 52.8s\tremaining: 2m 39s\n",
            "238:\tlearn: 101.1675925\ttotal: 53s\tremaining: 2m 38s\n",
            "239:\tlearn: 101.1664057\ttotal: 53.2s\tremaining: 2m 38s\n",
            "240:\tlearn: 101.1654551\ttotal: 53.4s\tremaining: 2m 38s\n",
            "241:\tlearn: 101.0225309\ttotal: 53.6s\tremaining: 2m 38s\n",
            "242:\tlearn: 100.8716841\ttotal: 53.9s\tremaining: 2m 38s\n",
            "243:\tlearn: 100.8654002\ttotal: 54.1s\tremaining: 2m 37s\n",
            "244:\tlearn: 100.7958556\ttotal: 54.3s\tremaining: 2m 37s\n",
            "245:\tlearn: 100.7545458\ttotal: 54.5s\tremaining: 2m 37s\n",
            "246:\tlearn: 100.7528317\ttotal: 54.7s\tremaining: 2m 37s\n",
            "247:\tlearn: 100.7518858\ttotal: 54.9s\tremaining: 2m 36s\n",
            "248:\tlearn: 100.5160027\ttotal: 55.1s\tremaining: 2m 36s\n",
            "249:\tlearn: 100.3230305\ttotal: 55.4s\tremaining: 2m 36s\n",
            "250:\tlearn: 100.2401384\ttotal: 55.6s\tremaining: 2m 36s\n",
            "251:\tlearn: 100.2013010\ttotal: 55.9s\tremaining: 2m 36s\n",
            "252:\tlearn: 99.7354642\ttotal: 56.2s\tremaining: 2m 36s\n",
            "253:\tlearn: 99.5730660\ttotal: 56.4s\tremaining: 2m 35s\n",
            "254:\tlearn: 99.5073722\ttotal: 56.6s\tremaining: 2m 35s\n",
            "255:\tlearn: 99.1233547\ttotal: 56.9s\tremaining: 2m 35s\n",
            "256:\tlearn: 98.9913315\ttotal: 57.1s\tremaining: 2m 35s\n",
            "257:\tlearn: 98.6310987\ttotal: 57.3s\tremaining: 2m 34s\n",
            "258:\tlearn: 98.3575254\ttotal: 57.5s\tremaining: 2m 34s\n",
            "259:\tlearn: 98.1415904\ttotal: 57.7s\tremaining: 2m 34s\n",
            "260:\tlearn: 98.0269867\ttotal: 57.9s\tremaining: 2m 34s\n",
            "261:\tlearn: 97.7323068\ttotal: 58.1s\tremaining: 2m 33s\n",
            "262:\tlearn: 97.6837292\ttotal: 58.4s\tremaining: 2m 33s\n",
            "263:\tlearn: 97.6416054\ttotal: 58.6s\tremaining: 2m 33s\n",
            "264:\tlearn: 97.5387254\ttotal: 58.9s\tremaining: 2m 33s\n",
            "265:\tlearn: 97.4736887\ttotal: 59.1s\tremaining: 2m 33s\n",
            "266:\tlearn: 97.3225365\ttotal: 59.3s\tremaining: 2m 33s\n",
            "267:\tlearn: 97.2448953\ttotal: 59.5s\tremaining: 2m 32s\n",
            "268:\tlearn: 97.1729526\ttotal: 59.7s\tremaining: 2m 32s\n",
            "269:\tlearn: 97.1720063\ttotal: 60s\tremaining: 2m 32s\n",
            "270:\tlearn: 97.1033543\ttotal: 1m\tremaining: 2m 32s\n",
            "271:\tlearn: 96.9557539\ttotal: 1m\tremaining: 2m 31s\n",
            "272:\tlearn: 96.9141489\ttotal: 1m\tremaining: 2m 31s\n",
            "273:\tlearn: 96.8394318\ttotal: 1m\tremaining: 2m 31s\n",
            "274:\tlearn: 96.6444793\ttotal: 1m 1s\tremaining: 2m 31s\n",
            "275:\tlearn: 96.6385465\ttotal: 1m 1s\tremaining: 2m 30s\n",
            "276:\tlearn: 96.5199701\ttotal: 1m 1s\tremaining: 2m 30s\n",
            "277:\tlearn: 96.4090754\ttotal: 1m 1s\tremaining: 2m 30s\n",
            "278:\tlearn: 96.3030137\ttotal: 1m 1s\tremaining: 2m 30s\n",
            "279:\tlearn: 95.9921308\ttotal: 1m 2s\tremaining: 2m 29s\n",
            "280:\tlearn: 95.8752761\ttotal: 1m 2s\tremaining: 2m 29s\n",
            "281:\tlearn: 95.7276325\ttotal: 1m 2s\tremaining: 2m 29s\n",
            "282:\tlearn: 95.5361298\ttotal: 1m 2s\tremaining: 2m 29s\n",
            "283:\tlearn: 95.5062055\ttotal: 1m 2s\tremaining: 2m 28s\n",
            "284:\tlearn: 95.3726836\ttotal: 1m 3s\tremaining: 2m 28s\n",
            "285:\tlearn: 95.3025028\ttotal: 1m 3s\tremaining: 2m 28s\n",
            "286:\tlearn: 95.2361134\ttotal: 1m 3s\tremaining: 2m 28s\n",
            "287:\tlearn: 95.1803999\ttotal: 1m 3s\tremaining: 2m 27s\n",
            "288:\tlearn: 95.0824032\ttotal: 1m 3s\tremaining: 2m 27s\n",
            "289:\tlearn: 94.8899431\ttotal: 1m 4s\tremaining: 2m 27s\n",
            "290:\tlearn: 94.6709465\ttotal: 1m 4s\tremaining: 2m 27s\n",
            "291:\tlearn: 94.4326152\ttotal: 1m 4s\tremaining: 2m 26s\n",
            "292:\tlearn: 94.1919551\ttotal: 1m 4s\tremaining: 2m 26s\n",
            "293:\tlearn: 94.0707828\ttotal: 1m 5s\tremaining: 2m 26s\n",
            "294:\tlearn: 93.9895703\ttotal: 1m 5s\tremaining: 2m 26s\n",
            "295:\tlearn: 93.8758785\ttotal: 1m 5s\tremaining: 2m 25s\n",
            "296:\tlearn: 93.7149184\ttotal: 1m 5s\tremaining: 2m 25s\n",
            "297:\tlearn: 93.5844245\ttotal: 1m 5s\tremaining: 2m 25s\n",
            "298:\tlearn: 93.5450567\ttotal: 1m 6s\tremaining: 2m 25s\n",
            "299:\tlearn: 93.4969074\ttotal: 1m 6s\tremaining: 2m 24s\n",
            "300:\tlearn: 93.4139647\ttotal: 1m 6s\tremaining: 2m 24s\n",
            "301:\tlearn: 93.3709875\ttotal: 1m 6s\tremaining: 2m 24s\n",
            "302:\tlearn: 93.2845142\ttotal: 1m 6s\tremaining: 2m 24s\n",
            "303:\tlearn: 93.2171689\ttotal: 1m 7s\tremaining: 2m 24s\n",
            "304:\tlearn: 93.1786625\ttotal: 1m 7s\tremaining: 2m 23s\n",
            "305:\tlearn: 92.8944265\ttotal: 1m 7s\tremaining: 2m 23s\n",
            "306:\tlearn: 92.8191226\ttotal: 1m 7s\tremaining: 2m 23s\n",
            "307:\tlearn: 92.5430284\ttotal: 1m 8s\tremaining: 2m 23s\n",
            "308:\tlearn: 92.4727503\ttotal: 1m 8s\tremaining: 2m 22s\n",
            "309:\tlearn: 92.3672286\ttotal: 1m 8s\tremaining: 2m 22s\n",
            "310:\tlearn: 92.1690628\ttotal: 1m 8s\tremaining: 2m 22s\n",
            "311:\tlearn: 92.1424451\ttotal: 1m 8s\tremaining: 2m 22s\n",
            "312:\tlearn: 91.9351075\ttotal: 1m 9s\tremaining: 2m 21s\n",
            "313:\tlearn: 91.6857411\ttotal: 1m 9s\tremaining: 2m 21s\n",
            "314:\tlearn: 91.5931573\ttotal: 1m 9s\tremaining: 2m 21s\n",
            "315:\tlearn: 91.4377806\ttotal: 1m 9s\tremaining: 2m 21s\n",
            "316:\tlearn: 91.3916148\ttotal: 1m 9s\tremaining: 2m 20s\n",
            "317:\tlearn: 91.3904792\ttotal: 1m 10s\tremaining: 2m 20s\n",
            "318:\tlearn: 91.3302189\ttotal: 1m 10s\tremaining: 2m 20s\n",
            "319:\tlearn: 91.3291717\ttotal: 1m 10s\tremaining: 2m 20s\n",
            "320:\tlearn: 91.2345257\ttotal: 1m 10s\tremaining: 2m 20s\n",
            "321:\tlearn: 91.1906774\ttotal: 1m 11s\tremaining: 2m 20s\n",
            "322:\tlearn: 91.1893137\ttotal: 1m 11s\tremaining: 2m 19s\n",
            "323:\tlearn: 91.0464455\ttotal: 1m 11s\tremaining: 2m 19s\n",
            "324:\tlearn: 91.0459674\ttotal: 1m 11s\tremaining: 2m 19s\n",
            "325:\tlearn: 90.9421076\ttotal: 1m 12s\tremaining: 2m 19s\n",
            "326:\tlearn: 90.8064039\ttotal: 1m 12s\tremaining: 2m 18s\n",
            "327:\tlearn: 90.6867314\ttotal: 1m 12s\tremaining: 2m 18s\n",
            "328:\tlearn: 90.5393327\ttotal: 1m 12s\tremaining: 2m 18s\n",
            "329:\tlearn: 90.4757184\ttotal: 1m 12s\tremaining: 2m 18s\n",
            "330:\tlearn: 90.4271947\ttotal: 1m 13s\tremaining: 2m 18s\n",
            "331:\tlearn: 90.3672596\ttotal: 1m 13s\tremaining: 2m 17s\n",
            "332:\tlearn: 90.3033956\ttotal: 1m 13s\tremaining: 2m 17s\n",
            "333:\tlearn: 90.2232340\ttotal: 1m 13s\tremaining: 2m 17s\n",
            "334:\tlearn: 90.0603607\ttotal: 1m 14s\tremaining: 2m 17s\n",
            "335:\tlearn: 90.0076378\ttotal: 1m 14s\tremaining: 2m 17s\n",
            "336:\tlearn: 89.9446346\ttotal: 1m 14s\tremaining: 2m 16s\n",
            "337:\tlearn: 89.7999855\ttotal: 1m 14s\tremaining: 2m 16s\n",
            "338:\tlearn: 89.6904534\ttotal: 1m 15s\tremaining: 2m 16s\n",
            "339:\tlearn: 89.6896473\ttotal: 1m 15s\tremaining: 2m 16s\n",
            "340:\tlearn: 89.6067597\ttotal: 1m 15s\tremaining: 2m 16s\n",
            "341:\tlearn: 89.3207757\ttotal: 1m 15s\tremaining: 2m 15s\n",
            "342:\tlearn: 89.2446159\ttotal: 1m 15s\tremaining: 2m 15s\n",
            "343:\tlearn: 89.1486941\ttotal: 1m 16s\tremaining: 2m 15s\n",
            "344:\tlearn: 88.9938519\ttotal: 1m 16s\tremaining: 2m 15s\n",
            "345:\tlearn: 88.9533372\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "346:\tlearn: 88.7769851\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "347:\tlearn: 88.7422349\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "348:\tlearn: 88.6332193\ttotal: 1m 17s\tremaining: 2m 14s\n",
            "349:\tlearn: 88.5814545\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "350:\tlearn: 88.4494590\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "351:\tlearn: 88.4200046\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "352:\tlearn: 88.0429009\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "353:\tlearn: 87.9993517\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "354:\tlearn: 87.8844378\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "355:\tlearn: 87.8507498\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "356:\tlearn: 87.8500674\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "357:\tlearn: 87.8071228\ttotal: 1m 19s\tremaining: 2m 12s\n",
            "358:\tlearn: 87.7318879\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "359:\tlearn: 87.7023342\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "360:\tlearn: 87.5091121\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "361:\tlearn: 87.3314845\ttotal: 1m 19s\tremaining: 2m 11s\n",
            "362:\tlearn: 87.1092550\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "363:\tlearn: 86.9926941\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "364:\tlearn: 86.8938313\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "365:\tlearn: 86.7812918\ttotal: 1m 20s\tremaining: 2m 10s\n",
            "366:\tlearn: 86.6770072\ttotal: 1m 20s\tremaining: 2m 9s\n",
            "367:\tlearn: 86.4358760\ttotal: 1m 21s\tremaining: 2m 9s\n",
            "368:\tlearn: 86.2056653\ttotal: 1m 21s\tremaining: 2m 9s\n",
            "369:\tlearn: 86.0029036\ttotal: 1m 21s\tremaining: 2m 9s\n",
            "370:\tlearn: 85.8987822\ttotal: 1m 21s\tremaining: 2m 9s\n",
            "371:\tlearn: 85.8330724\ttotal: 1m 22s\tremaining: 2m 8s\n",
            "372:\tlearn: 85.7865628\ttotal: 1m 22s\tremaining: 2m 8s\n",
            "373:\tlearn: 85.6839879\ttotal: 1m 22s\tremaining: 2m 8s\n",
            "374:\tlearn: 85.6449631\ttotal: 1m 22s\tremaining: 2m 8s\n",
            "375:\tlearn: 85.5476055\ttotal: 1m 22s\tremaining: 2m 7s\n",
            "376:\tlearn: 85.4284388\ttotal: 1m 23s\tremaining: 2m 7s\n",
            "377:\tlearn: 85.3792396\ttotal: 1m 23s\tremaining: 2m 7s\n",
            "378:\tlearn: 85.3026955\ttotal: 1m 23s\tremaining: 2m 7s\n",
            "379:\tlearn: 85.2754862\ttotal: 1m 23s\tremaining: 2m 7s\n",
            "380:\tlearn: 85.1535828\ttotal: 1m 24s\tremaining: 2m 6s\n",
            "381:\tlearn: 84.8481653\ttotal: 1m 24s\tremaining: 2m 6s\n",
            "382:\tlearn: 84.7947002\ttotal: 1m 24s\tremaining: 2m 6s\n",
            "383:\tlearn: 84.7441877\ttotal: 1m 24s\tremaining: 2m 6s\n",
            "384:\tlearn: 84.7052064\ttotal: 1m 25s\tremaining: 2m 6s\n",
            "385:\tlearn: 84.6587649\ttotal: 1m 25s\tremaining: 2m 5s\n",
            "386:\tlearn: 84.5390270\ttotal: 1m 25s\tremaining: 2m 5s\n",
            "387:\tlearn: 84.5375122\ttotal: 1m 25s\tremaining: 2m 5s\n",
            "388:\tlearn: 84.4981408\ttotal: 1m 25s\tremaining: 2m 5s\n",
            "389:\tlearn: 84.4318253\ttotal: 1m 26s\tremaining: 2m 5s\n",
            "390:\tlearn: 84.4303241\ttotal: 1m 26s\tremaining: 2m 4s\n",
            "391:\tlearn: 84.3818099\ttotal: 1m 26s\tremaining: 2m 4s\n",
            "392:\tlearn: 84.2595900\ttotal: 1m 26s\tremaining: 2m 4s\n",
            "393:\tlearn: 84.1749492\ttotal: 1m 27s\tremaining: 2m 4s\n",
            "394:\tlearn: 84.0996448\ttotal: 1m 27s\tremaining: 2m 4s\n",
            "395:\tlearn: 83.9465994\ttotal: 1m 27s\tremaining: 2m 3s\n",
            "396:\tlearn: 83.9156774\ttotal: 1m 27s\tremaining: 2m 3s\n",
            "397:\tlearn: 83.8579188\ttotal: 1m 28s\tremaining: 2m 3s\n",
            "398:\tlearn: 83.6630754\ttotal: 1m 28s\tremaining: 2m 3s\n",
            "399:\tlearn: 83.5956742\ttotal: 1m 28s\tremaining: 2m 3s\n",
            "400:\tlearn: 83.5048209\ttotal: 1m 28s\tremaining: 2m 2s\n",
            "401:\tlearn: 83.4090214\ttotal: 1m 29s\tremaining: 2m 2s\n",
            "402:\tlearn: 83.3137160\ttotal: 1m 29s\tremaining: 2m 2s\n",
            "403:\tlearn: 83.2638533\ttotal: 1m 29s\tremaining: 2m 2s\n",
            "404:\tlearn: 83.1435532\ttotal: 1m 29s\tremaining: 2m 2s\n",
            "405:\tlearn: 83.0739309\ttotal: 1m 29s\tremaining: 2m 1s\n",
            "406:\tlearn: 82.9666071\ttotal: 1m 30s\tremaining: 2m 1s\n",
            "407:\tlearn: 82.8277471\ttotal: 1m 30s\tremaining: 2m 1s\n",
            "408:\tlearn: 82.6268368\ttotal: 1m 30s\tremaining: 2m 1s\n",
            "409:\tlearn: 82.5481303\ttotal: 1m 30s\tremaining: 2m\n",
            "410:\tlearn: 82.3943835\ttotal: 1m 31s\tremaining: 2m\n",
            "411:\tlearn: 82.2421566\ttotal: 1m 31s\tremaining: 2m\n",
            "412:\tlearn: 82.1428259\ttotal: 1m 31s\tremaining: 2m\n",
            "413:\tlearn: 82.1009790\ttotal: 1m 31s\tremaining: 2m\n",
            "414:\tlearn: 82.0505828\ttotal: 1m 31s\tremaining: 1m 59s\n",
            "415:\tlearn: 82.0020625\ttotal: 1m 32s\tremaining: 1m 59s\n",
            "416:\tlearn: 81.9608270\ttotal: 1m 32s\tremaining: 1m 59s\n",
            "417:\tlearn: 81.8787618\ttotal: 1m 32s\tremaining: 1m 59s\n",
            "418:\tlearn: 81.8173562\ttotal: 1m 32s\tremaining: 1m 58s\n",
            "419:\tlearn: 81.8166478\ttotal: 1m 33s\tremaining: 1m 58s\n",
            "420:\tlearn: 81.7058812\ttotal: 1m 33s\tremaining: 1m 58s\n",
            "421:\tlearn: 81.6318428\ttotal: 1m 33s\tremaining: 1m 58s\n",
            "422:\tlearn: 81.5394198\ttotal: 1m 33s\tremaining: 1m 57s\n",
            "423:\tlearn: 81.4499603\ttotal: 1m 33s\tremaining: 1m 57s\n",
            "424:\tlearn: 81.3686915\ttotal: 1m 33s\tremaining: 1m 57s\n",
            "425:\tlearn: 81.2640307\ttotal: 1m 34s\tremaining: 1m 57s\n",
            "426:\tlearn: 81.2000900\ttotal: 1m 34s\tremaining: 1m 56s\n",
            "427:\tlearn: 81.1275541\ttotal: 1m 34s\tremaining: 1m 56s\n",
            "428:\tlearn: 81.0388446\ttotal: 1m 34s\tremaining: 1m 56s\n",
            "429:\tlearn: 80.8911388\ttotal: 1m 35s\tremaining: 1m 56s\n",
            "430:\tlearn: 80.7857339\ttotal: 1m 35s\tremaining: 1m 55s\n",
            "431:\tlearn: 80.7042334\ttotal: 1m 35s\tremaining: 1m 55s\n",
            "432:\tlearn: 80.6453373\ttotal: 1m 35s\tremaining: 1m 55s\n",
            "433:\tlearn: 80.6149079\ttotal: 1m 35s\tremaining: 1m 55s\n",
            "434:\tlearn: 80.5691030\ttotal: 1m 36s\tremaining: 1m 55s\n",
            "435:\tlearn: 80.5360400\ttotal: 1m 36s\tremaining: 1m 54s\n",
            "436:\tlearn: 80.3979244\ttotal: 1m 36s\tremaining: 1m 54s\n",
            "437:\tlearn: 80.3731198\ttotal: 1m 36s\tremaining: 1m 54s\n",
            "438:\tlearn: 80.2241558\ttotal: 1m 36s\tremaining: 1m 54s\n",
            "439:\tlearn: 80.1463552\ttotal: 1m 37s\tremaining: 1m 53s\n",
            "440:\tlearn: 80.0568652\ttotal: 1m 37s\tremaining: 1m 53s\n",
            "441:\tlearn: 80.0160928\ttotal: 1m 37s\tremaining: 1m 53s\n",
            "442:\tlearn: 79.9947901\ttotal: 1m 37s\tremaining: 1m 53s\n",
            "443:\tlearn: 79.9939634\ttotal: 1m 37s\tremaining: 1m 52s\n",
            "444:\tlearn: 79.9928165\ttotal: 1m 38s\tremaining: 1m 52s\n",
            "445:\tlearn: 79.9170209\ttotal: 1m 38s\tremaining: 1m 52s\n",
            "446:\tlearn: 79.9139933\ttotal: 1m 38s\tremaining: 1m 52s\n",
            "447:\tlearn: 79.8744070\ttotal: 1m 38s\tremaining: 1m 52s\n",
            "448:\tlearn: 79.8734719\ttotal: 1m 39s\tremaining: 1m 51s\n",
            "449:\tlearn: 79.8724439\ttotal: 1m 39s\tremaining: 1m 51s\n",
            "450:\tlearn: 79.8316177\ttotal: 1m 39s\tremaining: 1m 51s\n",
            "451:\tlearn: 79.7242499\ttotal: 1m 39s\tremaining: 1m 51s\n",
            "452:\tlearn: 79.6129033\ttotal: 1m 39s\tremaining: 1m 50s\n",
            "453:\tlearn: 79.5773742\ttotal: 1m 40s\tremaining: 1m 50s\n",
            "454:\tlearn: 79.5766693\ttotal: 1m 40s\tremaining: 1m 50s\n",
            "455:\tlearn: 79.4514040\ttotal: 1m 40s\tremaining: 1m 50s\n",
            "456:\tlearn: 79.4505923\ttotal: 1m 40s\tremaining: 1m 49s\n",
            "457:\tlearn: 79.2904990\ttotal: 1m 40s\tremaining: 1m 49s\n",
            "458:\tlearn: 79.2248139\ttotal: 1m 41s\tremaining: 1m 49s\n",
            "459:\tlearn: 79.1964510\ttotal: 1m 41s\tremaining: 1m 49s\n",
            "460:\tlearn: 79.0712749\ttotal: 1m 41s\tremaining: 1m 48s\n",
            "461:\tlearn: 78.9805961\ttotal: 1m 41s\tremaining: 1m 48s\n",
            "462:\tlearn: 78.8062024\ttotal: 1m 41s\tremaining: 1m 48s\n",
            "463:\tlearn: 78.7397532\ttotal: 1m 42s\tremaining: 1m 48s\n",
            "464:\tlearn: 78.5719132\ttotal: 1m 42s\tremaining: 1m 48s\n",
            "465:\tlearn: 78.4665994\ttotal: 1m 42s\tremaining: 1m 47s\n",
            "466:\tlearn: 78.4186284\ttotal: 1m 42s\tremaining: 1m 47s\n",
            "467:\tlearn: 78.3128366\ttotal: 1m 43s\tremaining: 1m 47s\n",
            "468:\tlearn: 78.2454632\ttotal: 1m 43s\tremaining: 1m 47s\n",
            "469:\tlearn: 78.2290531\ttotal: 1m 43s\tremaining: 1m 47s\n",
            "470:\tlearn: 78.1884013\ttotal: 1m 43s\tremaining: 1m 46s\n",
            "471:\tlearn: 78.1450403\ttotal: 1m 43s\tremaining: 1m 46s\n",
            "472:\tlearn: 78.1042783\ttotal: 1m 44s\tremaining: 1m 46s\n",
            "473:\tlearn: 78.0883608\ttotal: 1m 44s\tremaining: 1m 46s\n",
            "474:\tlearn: 78.0872946\ttotal: 1m 44s\tremaining: 1m 45s\n",
            "475:\tlearn: 77.9905438\ttotal: 1m 44s\tremaining: 1m 45s\n",
            "476:\tlearn: 77.9887948\ttotal: 1m 45s\tremaining: 1m 45s\n",
            "477:\tlearn: 77.9455995\ttotal: 1m 45s\tremaining: 1m 45s\n",
            "478:\tlearn: 77.9039105\ttotal: 1m 45s\tremaining: 1m 45s\n",
            "479:\tlearn: 77.8181126\ttotal: 1m 45s\tremaining: 1m 45s\n",
            "480:\tlearn: 77.7661298\ttotal: 1m 46s\tremaining: 1m 44s\n",
            "481:\tlearn: 77.6862226\ttotal: 1m 46s\tremaining: 1m 44s\n",
            "482:\tlearn: 77.6547292\ttotal: 1m 46s\tremaining: 1m 44s\n",
            "483:\tlearn: 77.6306617\ttotal: 1m 46s\tremaining: 1m 44s\n",
            "484:\tlearn: 77.6299854\ttotal: 1m 47s\tremaining: 1m 44s\n",
            "485:\tlearn: 77.6291022\ttotal: 1m 47s\tremaining: 1m 43s\n",
            "486:\tlearn: 77.6284054\ttotal: 1m 47s\tremaining: 1m 43s\n",
            "487:\tlearn: 77.5280795\ttotal: 1m 47s\tremaining: 1m 43s\n",
            "488:\tlearn: 77.4601849\ttotal: 1m 47s\tremaining: 1m 43s\n",
            "489:\tlearn: 77.4296051\ttotal: 1m 48s\tremaining: 1m 42s\n",
            "490:\tlearn: 77.3208669\ttotal: 1m 48s\tremaining: 1m 42s\n",
            "491:\tlearn: 77.2350530\ttotal: 1m 48s\tremaining: 1m 42s\n",
            "492:\tlearn: 77.1738971\ttotal: 1m 48s\tremaining: 1m 42s\n",
            "493:\tlearn: 77.1312097\ttotal: 1m 49s\tremaining: 1m 41s\n",
            "494:\tlearn: 77.0615145\ttotal: 1m 49s\tremaining: 1m 41s\n",
            "495:\tlearn: 76.9915958\ttotal: 1m 49s\tremaining: 1m 41s\n",
            "496:\tlearn: 76.9691237\ttotal: 1m 49s\tremaining: 1m 41s\n",
            "497:\tlearn: 76.9365480\ttotal: 1m 49s\tremaining: 1m 41s\n",
            "498:\tlearn: 76.8356673\ttotal: 1m 50s\tremaining: 1m 40s\n",
            "499:\tlearn: 76.7079686\ttotal: 1m 50s\tremaining: 1m 40s\n",
            "500:\tlearn: 76.6706338\ttotal: 1m 50s\tremaining: 1m 40s\n",
            "501:\tlearn: 76.6187119\ttotal: 1m 50s\tremaining: 1m 40s\n",
            "502:\tlearn: 76.5933908\ttotal: 1m 51s\tremaining: 1m 39s\n",
            "503:\tlearn: 76.5899469\ttotal: 1m 51s\tremaining: 1m 39s\n",
            "504:\tlearn: 76.5614449\ttotal: 1m 51s\tremaining: 1m 39s\n",
            "505:\tlearn: 76.5385830\ttotal: 1m 51s\tremaining: 1m 39s\n",
            "506:\tlearn: 76.4437695\ttotal: 1m 51s\tremaining: 1m 39s\n",
            "507:\tlearn: 76.2961095\ttotal: 1m 52s\tremaining: 1m 38s\n",
            "508:\tlearn: 76.1649692\ttotal: 1m 52s\tremaining: 1m 38s\n",
            "509:\tlearn: 76.0411443\ttotal: 1m 52s\tremaining: 1m 38s\n",
            "510:\tlearn: 75.9601386\ttotal: 1m 52s\tremaining: 1m 38s\n",
            "511:\tlearn: 75.8560666\ttotal: 1m 52s\tremaining: 1m 37s\n",
            "512:\tlearn: 75.8207282\ttotal: 1m 53s\tremaining: 1m 37s\n",
            "513:\tlearn: 75.7518034\ttotal: 1m 53s\tremaining: 1m 37s\n",
            "514:\tlearn: 75.7060459\ttotal: 1m 53s\tremaining: 1m 37s\n",
            "515:\tlearn: 75.6898267\ttotal: 1m 53s\tremaining: 1m 36s\n",
            "516:\tlearn: 75.6446274\ttotal: 1m 53s\tremaining: 1m 36s\n",
            "517:\tlearn: 75.5981878\ttotal: 1m 54s\tremaining: 1m 36s\n",
            "518:\tlearn: 75.5784161\ttotal: 1m 54s\tremaining: 1m 36s\n",
            "519:\tlearn: 75.5286359\ttotal: 1m 54s\tremaining: 1m 36s\n",
            "520:\tlearn: 75.4694332\ttotal: 1m 54s\tremaining: 1m 35s\n",
            "521:\tlearn: 75.3061306\ttotal: 1m 55s\tremaining: 1m 35s\n",
            "522:\tlearn: 75.1981982\ttotal: 1m 55s\tremaining: 1m 35s\n",
            "523:\tlearn: 75.1230721\ttotal: 1m 55s\tremaining: 1m 35s\n",
            "524:\tlearn: 75.0938012\ttotal: 1m 55s\tremaining: 1m 35s\n",
            "525:\tlearn: 74.8419712\ttotal: 1m 56s\tremaining: 1m 34s\n",
            "526:\tlearn: 74.7458717\ttotal: 1m 56s\tremaining: 1m 34s\n",
            "527:\tlearn: 74.6777902\ttotal: 1m 56s\tremaining: 1m 34s\n",
            "528:\tlearn: 74.5588053\ttotal: 1m 56s\tremaining: 1m 34s\n",
            "529:\tlearn: 74.4826480\ttotal: 1m 56s\tremaining: 1m 33s\n",
            "530:\tlearn: 74.4283848\ttotal: 1m 57s\tremaining: 1m 33s\n",
            "531:\tlearn: 74.3993791\ttotal: 1m 57s\tremaining: 1m 33s\n",
            "532:\tlearn: 74.3780235\ttotal: 1m 57s\tremaining: 1m 33s\n",
            "533:\tlearn: 74.3513138\ttotal: 1m 57s\tremaining: 1m 33s\n",
            "534:\tlearn: 74.3507070\ttotal: 1m 58s\tremaining: 1m 32s\n",
            "535:\tlearn: 74.3219741\ttotal: 1m 58s\tremaining: 1m 32s\n",
            "536:\tlearn: 74.3195471\ttotal: 1m 58s\tremaining: 1m 32s\n",
            "537:\tlearn: 74.3188620\ttotal: 1m 58s\tremaining: 1m 32s\n",
            "538:\tlearn: 74.3066737\ttotal: 1m 59s\tremaining: 1m 32s\n",
            "539:\tlearn: 74.1978065\ttotal: 1m 59s\tremaining: 1m 31s\n",
            "540:\tlearn: 73.9910018\ttotal: 1m 59s\tremaining: 1m 31s\n",
            "541:\tlearn: 73.9438800\ttotal: 1m 59s\tremaining: 1m 31s\n",
            "542:\tlearn: 73.9211055\ttotal: 1m 59s\tremaining: 1m 31s\n",
            "543:\tlearn: 73.8513223\ttotal: 2m\tremaining: 1m 31s\n",
            "544:\tlearn: 73.7788516\ttotal: 2m\tremaining: 1m 30s\n",
            "545:\tlearn: 73.7192308\ttotal: 2m\tremaining: 1m 30s\n",
            "546:\tlearn: 73.6423853\ttotal: 2m\tremaining: 1m 30s\n",
            "547:\tlearn: 73.6183837\ttotal: 2m 1s\tremaining: 1m 30s\n",
            "548:\tlearn: 73.5949692\ttotal: 2m 1s\tremaining: 1m 29s\n",
            "549:\tlearn: 73.5757593\ttotal: 2m 1s\tremaining: 1m 29s\n",
            "550:\tlearn: 73.5747430\ttotal: 2m 1s\tremaining: 1m 29s\n",
            "551:\tlearn: 73.5366628\ttotal: 2m 2s\tremaining: 1m 29s\n",
            "552:\tlearn: 73.5105763\ttotal: 2m 2s\tremaining: 1m 29s\n",
            "553:\tlearn: 73.4536835\ttotal: 2m 2s\tremaining: 1m 28s\n",
            "554:\tlearn: 73.4053806\ttotal: 2m 2s\tremaining: 1m 28s\n",
            "555:\tlearn: 73.3236646\ttotal: 2m 3s\tremaining: 1m 28s\n",
            "556:\tlearn: 73.2553005\ttotal: 2m 3s\tremaining: 1m 28s\n",
            "557:\tlearn: 73.2218809\ttotal: 2m 3s\tremaining: 1m 28s\n",
            "558:\tlearn: 73.1633296\ttotal: 2m 3s\tremaining: 1m 27s\n",
            "559:\tlearn: 73.1351412\ttotal: 2m 4s\tremaining: 1m 27s\n",
            "560:\tlearn: 73.0819262\ttotal: 2m 4s\tremaining: 1m 27s\n",
            "561:\tlearn: 73.0665222\ttotal: 2m 4s\tremaining: 1m 27s\n",
            "562:\tlearn: 72.9260279\ttotal: 2m 4s\tremaining: 1m 27s\n",
            "563:\tlearn: 72.9145705\ttotal: 2m 4s\tremaining: 1m 26s\n",
            "564:\tlearn: 72.9103682\ttotal: 2m 5s\tremaining: 1m 26s\n",
            "565:\tlearn: 72.8878351\ttotal: 2m 5s\tremaining: 1m 26s\n",
            "566:\tlearn: 72.8264950\ttotal: 2m 5s\tremaining: 1m 26s\n",
            "567:\tlearn: 72.8261519\ttotal: 2m 5s\tremaining: 1m 25s\n",
            "568:\tlearn: 72.7029187\ttotal: 2m 6s\tremaining: 1m 25s\n",
            "569:\tlearn: 72.6666077\ttotal: 2m 6s\tremaining: 1m 25s\n",
            "570:\tlearn: 72.6349826\ttotal: 2m 6s\tremaining: 1m 25s\n",
            "571:\tlearn: 72.6021628\ttotal: 2m 6s\tremaining: 1m 25s\n",
            "572:\tlearn: 72.5614555\ttotal: 2m 6s\tremaining: 1m 24s\n",
            "573:\tlearn: 72.5609448\ttotal: 2m 7s\tremaining: 1m 24s\n",
            "574:\tlearn: 72.5255878\ttotal: 2m 7s\tremaining: 1m 24s\n",
            "575:\tlearn: 72.4995047\ttotal: 2m 7s\tremaining: 1m 24s\n",
            "576:\tlearn: 72.4728816\ttotal: 2m 7s\tremaining: 1m 23s\n",
            "577:\tlearn: 72.3949535\ttotal: 2m 7s\tremaining: 1m 23s\n",
            "578:\tlearn: 72.3648830\ttotal: 2m 8s\tremaining: 1m 23s\n",
            "579:\tlearn: 72.3639197\ttotal: 2m 8s\tremaining: 1m 23s\n",
            "580:\tlearn: 72.3485474\ttotal: 2m 8s\tremaining: 1m 23s\n",
            "581:\tlearn: 72.2427744\ttotal: 2m 8s\tremaining: 1m 22s\n",
            "582:\tlearn: 72.2263117\ttotal: 2m 9s\tremaining: 1m 22s\n",
            "583:\tlearn: 72.1794658\ttotal: 2m 9s\tremaining: 1m 22s\n",
            "584:\tlearn: 72.1426299\ttotal: 2m 9s\tremaining: 1m 22s\n",
            "585:\tlearn: 72.0754687\ttotal: 2m 9s\tremaining: 1m 21s\n",
            "586:\tlearn: 71.9933416\ttotal: 2m 9s\tremaining: 1m 21s\n",
            "587:\tlearn: 71.8335469\ttotal: 2m 10s\tremaining: 1m 21s\n",
            "588:\tlearn: 71.8128336\ttotal: 2m 10s\tremaining: 1m 21s\n",
            "589:\tlearn: 71.7460799\ttotal: 2m 10s\tremaining: 1m 20s\n",
            "590:\tlearn: 71.6716666\ttotal: 2m 10s\tremaining: 1m 20s\n",
            "591:\tlearn: 71.6204201\ttotal: 2m 10s\tremaining: 1m 20s\n",
            "592:\tlearn: 71.5475783\ttotal: 2m 11s\tremaining: 1m 20s\n",
            "593:\tlearn: 71.3903978\ttotal: 2m 11s\tremaining: 1m 20s\n",
            "594:\tlearn: 71.3473312\ttotal: 2m 11s\tremaining: 1m 19s\n",
            "595:\tlearn: 71.2460402\ttotal: 2m 11s\tremaining: 1m 19s\n",
            "596:\tlearn: 71.1204047\ttotal: 2m 11s\tremaining: 1m 19s\n",
            "597:\tlearn: 71.1011136\ttotal: 2m 12s\tremaining: 1m 19s\n",
            "598:\tlearn: 71.0637861\ttotal: 2m 12s\tremaining: 1m 18s\n",
            "599:\tlearn: 71.0130441\ttotal: 2m 12s\tremaining: 1m 18s\n",
            "600:\tlearn: 70.9829665\ttotal: 2m 12s\tremaining: 1m 18s\n",
            "601:\tlearn: 70.9420513\ttotal: 2m 13s\tremaining: 1m 18s\n",
            "602:\tlearn: 70.9348167\ttotal: 2m 13s\tremaining: 1m 18s\n",
            "603:\tlearn: 70.9188570\ttotal: 2m 13s\tremaining: 1m 17s\n",
            "604:\tlearn: 70.8491378\ttotal: 2m 13s\tremaining: 1m 17s\n",
            "605:\tlearn: 70.8038226\ttotal: 2m 13s\tremaining: 1m 17s\n",
            "606:\tlearn: 70.7163658\ttotal: 2m 14s\tremaining: 1m 17s\n",
            "607:\tlearn: 70.6603663\ttotal: 2m 14s\tremaining: 1m 16s\n",
            "608:\tlearn: 70.6059616\ttotal: 2m 14s\tremaining: 1m 16s\n",
            "609:\tlearn: 70.4213035\ttotal: 2m 14s\tremaining: 1m 16s\n",
            "610:\tlearn: 70.3782659\ttotal: 2m 15s\tremaining: 1m 16s\n",
            "611:\tlearn: 70.3192410\ttotal: 2m 15s\tremaining: 1m 16s\n",
            "612:\tlearn: 70.3191139\ttotal: 2m 15s\tremaining: 1m 15s\n",
            "613:\tlearn: 70.2562276\ttotal: 2m 15s\tremaining: 1m 15s\n",
            "614:\tlearn: 70.2021262\ttotal: 2m 15s\tremaining: 1m 15s\n",
            "615:\tlearn: 70.0256571\ttotal: 2m 16s\tremaining: 1m 15s\n",
            "616:\tlearn: 70.0058019\ttotal: 2m 16s\tremaining: 1m 14s\n",
            "617:\tlearn: 69.8665968\ttotal: 2m 16s\tremaining: 1m 14s\n",
            "618:\tlearn: 69.8495966\ttotal: 2m 16s\tremaining: 1m 14s\n",
            "619:\tlearn: 69.8242947\ttotal: 2m 17s\tremaining: 1m 14s\n",
            "620:\tlearn: 69.8027986\ttotal: 2m 17s\tremaining: 1m 14s\n",
            "621:\tlearn: 69.7821893\ttotal: 2m 17s\tremaining: 1m 13s\n",
            "622:\tlearn: 69.6984849\ttotal: 2m 18s\tremaining: 1m 13s\n",
            "623:\tlearn: 69.6361409\ttotal: 2m 18s\tremaining: 1m 13s\n",
            "624:\tlearn: 69.5923893\ttotal: 2m 18s\tremaining: 1m 13s\n",
            "625:\tlearn: 69.5533304\ttotal: 2m 18s\tremaining: 1m 13s\n",
            "626:\tlearn: 69.5529249\ttotal: 2m 18s\tremaining: 1m 12s\n",
            "627:\tlearn: 69.5374168\ttotal: 2m 19s\tremaining: 1m 12s\n",
            "628:\tlearn: 69.5224537\ttotal: 2m 19s\tremaining: 1m 12s\n",
            "629:\tlearn: 69.5100283\ttotal: 2m 19s\tremaining: 1m 12s\n",
            "630:\tlearn: 69.4517290\ttotal: 2m 19s\tremaining: 1m 12s\n",
            "631:\tlearn: 69.4016681\ttotal: 2m 20s\tremaining: 1m 11s\n",
            "632:\tlearn: 69.3763814\ttotal: 2m 20s\tremaining: 1m 11s\n",
            "633:\tlearn: 69.3438368\ttotal: 2m 20s\tremaining: 1m 11s\n",
            "634:\tlearn: 69.2867683\ttotal: 2m 20s\tremaining: 1m 11s\n",
            "635:\tlearn: 69.2519816\ttotal: 2m 20s\tremaining: 1m 10s\n",
            "636:\tlearn: 69.2518836\ttotal: 2m 21s\tremaining: 1m 10s\n",
            "637:\tlearn: 69.2236312\ttotal: 2m 21s\tremaining: 1m 10s\n",
            "638:\tlearn: 69.1962475\ttotal: 2m 21s\tremaining: 1m 10s\n",
            "639:\tlearn: 69.1408489\ttotal: 2m 21s\tremaining: 1m 9s\n",
            "640:\tlearn: 69.0015154\ttotal: 2m 21s\tremaining: 1m 9s\n",
            "641:\tlearn: 68.9583721\ttotal: 2m 22s\tremaining: 1m 9s\n",
            "642:\tlearn: 68.9213031\ttotal: 2m 22s\tremaining: 1m 9s\n",
            "643:\tlearn: 68.9179758\ttotal: 2m 22s\tremaining: 1m 9s\n",
            "644:\tlearn: 68.8829462\ttotal: 2m 22s\tremaining: 1m 8s\n",
            "645:\tlearn: 68.8494695\ttotal: 2m 23s\tremaining: 1m 8s\n",
            "646:\tlearn: 68.8232748\ttotal: 2m 23s\tremaining: 1m 8s\n",
            "647:\tlearn: 68.7821331\ttotal: 2m 23s\tremaining: 1m 8s\n",
            "648:\tlearn: 68.7635280\ttotal: 2m 23s\tremaining: 1m 8s\n",
            "649:\tlearn: 68.7426307\ttotal: 2m 24s\tremaining: 1m 7s\n",
            "650:\tlearn: 68.6778137\ttotal: 2m 24s\tremaining: 1m 7s\n",
            "651:\tlearn: 68.6371104\ttotal: 2m 24s\tremaining: 1m 7s\n",
            "652:\tlearn: 68.5371705\ttotal: 2m 24s\tremaining: 1m 7s\n",
            "653:\tlearn: 68.4995837\ttotal: 2m 24s\tremaining: 1m 6s\n",
            "654:\tlearn: 68.3976345\ttotal: 2m 25s\tremaining: 1m 6s\n",
            "655:\tlearn: 68.3732968\ttotal: 2m 25s\tremaining: 1m 6s\n",
            "656:\tlearn: 68.2809591\ttotal: 2m 25s\tremaining: 1m 6s\n",
            "657:\tlearn: 68.2601085\ttotal: 2m 25s\tremaining: 1m 5s\n",
            "658:\tlearn: 68.2091159\ttotal: 2m 25s\tremaining: 1m 5s\n",
            "659:\tlearn: 68.1407697\ttotal: 2m 26s\tremaining: 1m 5s\n",
            "660:\tlearn: 68.1212659\ttotal: 2m 26s\tremaining: 1m 5s\n",
            "661:\tlearn: 68.0850028\ttotal: 2m 26s\tremaining: 1m 5s\n",
            "662:\tlearn: 68.0613695\ttotal: 2m 26s\tremaining: 1m 4s\n",
            "663:\tlearn: 67.9977355\ttotal: 2m 27s\tremaining: 1m 4s\n",
            "664:\tlearn: 67.9325906\ttotal: 2m 27s\tremaining: 1m 4s\n",
            "665:\tlearn: 67.8857704\ttotal: 2m 27s\tremaining: 1m 4s\n",
            "666:\tlearn: 67.8318187\ttotal: 2m 27s\tremaining: 1m 4s\n",
            "667:\tlearn: 67.7555764\ttotal: 2m 27s\tremaining: 1m 3s\n",
            "668:\tlearn: 67.7240776\ttotal: 2m 28s\tremaining: 1m 3s\n",
            "669:\tlearn: 67.6844733\ttotal: 2m 28s\tremaining: 1m 3s\n",
            "670:\tlearn: 67.6834983\ttotal: 2m 28s\tremaining: 1m 3s\n",
            "671:\tlearn: 67.6826730\ttotal: 2m 28s\tremaining: 1m 2s\n",
            "672:\tlearn: 67.6817859\ttotal: 2m 29s\tremaining: 1m 2s\n",
            "673:\tlearn: 67.5687122\ttotal: 2m 29s\tremaining: 1m 2s\n",
            "674:\tlearn: 67.5265984\ttotal: 2m 29s\tremaining: 1m 2s\n",
            "675:\tlearn: 67.4888041\ttotal: 2m 29s\tremaining: 1m 1s\n",
            "676:\tlearn: 67.4729122\ttotal: 2m 29s\tremaining: 1m 1s\n",
            "677:\tlearn: 67.4623493\ttotal: 2m 30s\tremaining: 1m 1s\n",
            "678:\tlearn: 67.3919956\ttotal: 2m 30s\tremaining: 1m 1s\n",
            "679:\tlearn: 67.3679892\ttotal: 2m 30s\tremaining: 1m 1s\n",
            "680:\tlearn: 67.3020685\ttotal: 2m 30s\tremaining: 1m\n",
            "681:\tlearn: 67.2591924\ttotal: 2m 31s\tremaining: 1m\n",
            "682:\tlearn: 67.2422396\ttotal: 2m 31s\tremaining: 1m\n",
            "683:\tlearn: 67.2396751\ttotal: 2m 31s\tremaining: 1m\n",
            "684:\tlearn: 67.2360570\ttotal: 2m 31s\tremaining: 1m\n",
            "685:\tlearn: 67.2057738\ttotal: 2m 32s\tremaining: 59.9s\n",
            "686:\tlearn: 67.1561197\ttotal: 2m 32s\tremaining: 59.7s\n",
            "687:\tlearn: 67.1555094\ttotal: 2m 32s\tremaining: 59.4s\n",
            "688:\tlearn: 67.1271833\ttotal: 2m 32s\tremaining: 59.2s\n",
            "689:\tlearn: 67.0420586\ttotal: 2m 33s\tremaining: 59s\n",
            "690:\tlearn: 66.9834583\ttotal: 2m 33s\tremaining: 58.8s\n",
            "691:\tlearn: 66.9504082\ttotal: 2m 33s\tremaining: 58.6s\n",
            "692:\tlearn: 66.9330267\ttotal: 2m 33s\tremaining: 58.3s\n",
            "693:\tlearn: 66.9226483\ttotal: 2m 33s\tremaining: 58.1s\n",
            "694:\tlearn: 66.9223034\ttotal: 2m 34s\tremaining: 57.9s\n",
            "695:\tlearn: 66.9057759\ttotal: 2m 34s\tremaining: 57.7s\n",
            "696:\tlearn: 66.9044329\ttotal: 2m 34s\tremaining: 57.5s\n",
            "697:\tlearn: 66.8854884\ttotal: 2m 35s\tremaining: 57.3s\n",
            "698:\tlearn: 66.8631039\ttotal: 2m 35s\tremaining: 57.1s\n",
            "699:\tlearn: 66.8622956\ttotal: 2m 35s\tremaining: 56.8s\n",
            "700:\tlearn: 66.8508735\ttotal: 2m 35s\tremaining: 56.6s\n",
            "701:\tlearn: 66.8440207\ttotal: 2m 35s\tremaining: 56.4s\n",
            "702:\tlearn: 66.8397572\ttotal: 2m 36s\tremaining: 56.2s\n",
            "703:\tlearn: 66.8017341\ttotal: 2m 36s\tremaining: 56s\n",
            "704:\tlearn: 66.8009202\ttotal: 2m 36s\tremaining: 55.7s\n",
            "705:\tlearn: 66.6870240\ttotal: 2m 36s\tremaining: 55.5s\n",
            "706:\tlearn: 66.6601549\ttotal: 2m 36s\tremaining: 55.3s\n",
            "707:\tlearn: 66.5585212\ttotal: 2m 37s\tremaining: 55.1s\n",
            "708:\tlearn: 66.4704386\ttotal: 2m 37s\tremaining: 54.8s\n",
            "709:\tlearn: 66.4337827\ttotal: 2m 37s\tremaining: 54.6s\n",
            "710:\tlearn: 66.3453694\ttotal: 2m 37s\tremaining: 54.4s\n",
            "711:\tlearn: 66.2399599\ttotal: 2m 37s\tremaining: 54.1s\n",
            "712:\tlearn: 66.1967838\ttotal: 2m 38s\tremaining: 53.9s\n",
            "713:\tlearn: 66.0742448\ttotal: 2m 38s\tremaining: 53.7s\n",
            "714:\tlearn: 66.0600347\ttotal: 2m 38s\tremaining: 53.5s\n",
            "715:\tlearn: 66.0234314\ttotal: 2m 38s\tremaining: 53.2s\n",
            "716:\tlearn: 65.9935738\ttotal: 2m 39s\tremaining: 53s\n",
            "717:\tlearn: 65.9397605\ttotal: 2m 39s\tremaining: 52.8s\n",
            "718:\tlearn: 65.8622391\ttotal: 2m 39s\tremaining: 52.5s\n",
            "719:\tlearn: 65.7743270\ttotal: 2m 39s\tremaining: 52.3s\n",
            "720:\tlearn: 65.7233331\ttotal: 2m 39s\tremaining: 52.1s\n",
            "721:\tlearn: 65.7140601\ttotal: 2m 40s\tremaining: 51.9s\n",
            "722:\tlearn: 65.6738238\ttotal: 2m 40s\tremaining: 51.7s\n",
            "723:\tlearn: 65.6410968\ttotal: 2m 40s\tremaining: 51.4s\n",
            "724:\tlearn: 65.6112999\ttotal: 2m 40s\tremaining: 51.2s\n",
            "725:\tlearn: 65.5884665\ttotal: 2m 40s\tremaining: 51s\n",
            "726:\tlearn: 65.5873664\ttotal: 2m 41s\tremaining: 50.8s\n",
            "727:\tlearn: 65.5573378\ttotal: 2m 41s\tremaining: 50.6s\n",
            "728:\tlearn: 65.5377051\ttotal: 2m 41s\tremaining: 50.3s\n",
            "729:\tlearn: 65.5368029\ttotal: 2m 41s\tremaining: 50.1s\n",
            "730:\tlearn: 65.5184518\ttotal: 2m 42s\tremaining: 49.9s\n",
            "731:\tlearn: 65.4911038\ttotal: 2m 42s\tremaining: 49.7s\n",
            "732:\tlearn: 65.4907238\ttotal: 2m 42s\tremaining: 49.4s\n",
            "733:\tlearn: 65.4813297\ttotal: 2m 42s\tremaining: 49.2s\n",
            "734:\tlearn: 65.4666186\ttotal: 2m 43s\tremaining: 49s\n",
            "735:\tlearn: 65.3737760\ttotal: 2m 43s\tremaining: 48.8s\n",
            "736:\tlearn: 65.2855569\ttotal: 2m 43s\tremaining: 48.6s\n",
            "737:\tlearn: 65.2694126\ttotal: 2m 43s\tremaining: 48.3s\n",
            "738:\tlearn: 65.2341867\ttotal: 2m 43s\tremaining: 48.1s\n",
            "739:\tlearn: 65.1988035\ttotal: 2m 44s\tremaining: 47.9s\n",
            "740:\tlearn: 65.1143123\ttotal: 2m 44s\tremaining: 47.7s\n",
            "741:\tlearn: 64.9927439\ttotal: 2m 44s\tremaining: 47.5s\n",
            "742:\tlearn: 64.9619278\ttotal: 2m 44s\tremaining: 47.2s\n",
            "743:\tlearn: 64.9353993\ttotal: 2m 44s\tremaining: 47s\n",
            "744:\tlearn: 64.9084287\ttotal: 2m 45s\tremaining: 46.8s\n",
            "745:\tlearn: 64.8702088\ttotal: 2m 45s\tremaining: 46.6s\n",
            "746:\tlearn: 64.8102726\ttotal: 2m 45s\tremaining: 46.3s\n",
            "747:\tlearn: 64.7691774\ttotal: 2m 45s\tremaining: 46.1s\n",
            "748:\tlearn: 64.7066352\ttotal: 2m 46s\tremaining: 45.9s\n",
            "749:\tlearn: 64.6691900\ttotal: 2m 46s\tremaining: 45.7s\n",
            "750:\tlearn: 64.6468866\ttotal: 2m 46s\tremaining: 45.5s\n",
            "751:\tlearn: 64.5846530\ttotal: 2m 46s\tremaining: 45.2s\n",
            "752:\tlearn: 64.5613009\ttotal: 2m 46s\tremaining: 45s\n",
            "753:\tlearn: 64.5199581\ttotal: 2m 47s\tremaining: 44.8s\n",
            "754:\tlearn: 64.5194274\ttotal: 2m 47s\tremaining: 44.6s\n",
            "755:\tlearn: 64.5178125\ttotal: 2m 47s\tremaining: 44.4s\n",
            "756:\tlearn: 64.4756808\ttotal: 2m 47s\tremaining: 44.2s\n",
            "757:\tlearn: 64.4376423\ttotal: 2m 48s\tremaining: 43.9s\n",
            "758:\tlearn: 64.4371688\ttotal: 2m 48s\tremaining: 43.7s\n",
            "759:\tlearn: 64.4204580\ttotal: 2m 48s\tremaining: 43.5s\n",
            "760:\tlearn: 64.3875865\ttotal: 2m 48s\tremaining: 43.3s\n",
            "761:\tlearn: 64.3661034\ttotal: 2m 49s\tremaining: 43.1s\n",
            "762:\tlearn: 64.3441427\ttotal: 2m 49s\tremaining: 42.9s\n",
            "763:\tlearn: 64.2634343\ttotal: 2m 49s\tremaining: 42.6s\n",
            "764:\tlearn: 64.1780941\ttotal: 2m 49s\tremaining: 42.4s\n",
            "765:\tlearn: 64.1006701\ttotal: 2m 50s\tremaining: 42.2s\n",
            "766:\tlearn: 64.0889101\ttotal: 2m 50s\tremaining: 42s\n",
            "767:\tlearn: 64.0533579\ttotal: 2m 50s\tremaining: 41.8s\n",
            "768:\tlearn: 63.9938654\ttotal: 2m 50s\tremaining: 41.5s\n",
            "769:\tlearn: 63.9299651\ttotal: 2m 50s\tremaining: 41.3s\n",
            "770:\tlearn: 63.9272758\ttotal: 2m 51s\tremaining: 41.1s\n",
            "771:\tlearn: 63.8721276\ttotal: 2m 51s\tremaining: 40.8s\n",
            "772:\tlearn: 63.8581109\ttotal: 2m 51s\tremaining: 40.6s\n",
            "773:\tlearn: 63.7931433\ttotal: 2m 51s\tremaining: 40.4s\n",
            "774:\tlearn: 63.7563999\ttotal: 2m 52s\tremaining: 40.2s\n",
            "775:\tlearn: 63.6879291\ttotal: 2m 52s\tremaining: 40s\n",
            "776:\tlearn: 63.5742928\ttotal: 2m 52s\tremaining: 39.7s\n",
            "777:\tlearn: 63.5396598\ttotal: 2m 52s\tremaining: 39.5s\n",
            "778:\tlearn: 63.4978714\ttotal: 2m 52s\tremaining: 39.3s\n",
            "779:\tlearn: 63.4648750\ttotal: 2m 53s\tremaining: 39.1s\n",
            "780:\tlearn: 63.4359406\ttotal: 2m 53s\tremaining: 38.8s\n",
            "781:\tlearn: 63.3845232\ttotal: 2m 53s\tremaining: 38.6s\n",
            "782:\tlearn: 63.3104535\ttotal: 2m 53s\tremaining: 38.4s\n",
            "783:\tlearn: 63.2952234\ttotal: 2m 53s\tremaining: 38.2s\n",
            "784:\tlearn: 63.2943018\ttotal: 2m 54s\tremaining: 37.9s\n",
            "785:\tlearn: 63.2566433\ttotal: 2m 54s\tremaining: 37.7s\n",
            "786:\tlearn: 63.2276162\ttotal: 2m 54s\tremaining: 37.5s\n",
            "787:\tlearn: 63.2034396\ttotal: 2m 54s\tremaining: 37.3s\n",
            "788:\tlearn: 63.1609044\ttotal: 2m 55s\tremaining: 37s\n",
            "789:\tlearn: 63.1313702\ttotal: 2m 55s\tremaining: 36.8s\n",
            "790:\tlearn: 63.0527097\ttotal: 2m 55s\tremaining: 36.6s\n",
            "791:\tlearn: 63.0261871\ttotal: 2m 55s\tremaining: 36.4s\n",
            "792:\tlearn: 62.9979400\ttotal: 2m 55s\tremaining: 36.2s\n",
            "793:\tlearn: 62.9842423\ttotal: 2m 56s\tremaining: 35.9s\n",
            "794:\tlearn: 62.9801215\ttotal: 2m 56s\tremaining: 35.7s\n",
            "795:\tlearn: 62.9484712\ttotal: 2m 56s\tremaining: 35.5s\n",
            "796:\tlearn: 62.9126912\ttotal: 2m 56s\tremaining: 35.3s\n",
            "797:\tlearn: 62.8715141\ttotal: 2m 56s\tremaining: 35s\n",
            "798:\tlearn: 62.8693784\ttotal: 2m 57s\tremaining: 34.8s\n",
            "799:\tlearn: 62.8605844\ttotal: 2m 57s\tremaining: 34.6s\n",
            "800:\tlearn: 62.8511356\ttotal: 2m 57s\tremaining: 34.4s\n",
            "801:\tlearn: 62.8179848\ttotal: 2m 57s\tremaining: 34.2s\n",
            "802:\tlearn: 62.7798045\ttotal: 2m 58s\tremaining: 33.9s\n",
            "803:\tlearn: 62.7411283\ttotal: 2m 58s\tremaining: 33.7s\n",
            "804:\tlearn: 62.7328156\ttotal: 2m 58s\tremaining: 33.5s\n",
            "805:\tlearn: 62.6821771\ttotal: 2m 58s\tremaining: 33.3s\n",
            "806:\tlearn: 62.6552440\ttotal: 2m 58s\tremaining: 33s\n",
            "807:\tlearn: 62.6414493\ttotal: 2m 59s\tremaining: 32.8s\n",
            "808:\tlearn: 62.6404060\ttotal: 2m 59s\tremaining: 32.6s\n",
            "809:\tlearn: 62.6264715\ttotal: 2m 59s\tremaining: 32.4s\n",
            "810:\tlearn: 62.6200055\ttotal: 2m 59s\tremaining: 32.2s\n",
            "811:\tlearn: 62.6197373\ttotal: 3m\tremaining: 31.9s\n",
            "812:\tlearn: 62.6152862\ttotal: 3m\tremaining: 31.7s\n",
            "813:\tlearn: 62.6146847\ttotal: 3m\tremaining: 31.5s\n",
            "814:\tlearn: 62.5946935\ttotal: 3m\tremaining: 31.3s\n",
            "815:\tlearn: 62.5741001\ttotal: 3m\tremaining: 31.1s\n",
            "816:\tlearn: 62.5575609\ttotal: 3m 1s\tremaining: 30.8s\n",
            "817:\tlearn: 62.5093522\ttotal: 3m 1s\tremaining: 30.6s\n",
            "818:\tlearn: 62.4444211\ttotal: 3m 1s\tremaining: 30.4s\n",
            "819:\tlearn: 62.3905602\ttotal: 3m 1s\tremaining: 30.2s\n",
            "820:\tlearn: 62.3616142\ttotal: 3m 2s\tremaining: 29.9s\n",
            "821:\tlearn: 62.3054105\ttotal: 3m 2s\tremaining: 29.7s\n",
            "822:\tlearn: 62.2714683\ttotal: 3m 2s\tremaining: 29.5s\n",
            "823:\tlearn: 62.2470323\ttotal: 3m 2s\tremaining: 29.3s\n",
            "824:\tlearn: 62.1640022\ttotal: 3m 2s\tremaining: 29s\n",
            "825:\tlearn: 62.1635540\ttotal: 3m 3s\tremaining: 28.8s\n",
            "826:\tlearn: 62.0884250\ttotal: 3m 3s\tremaining: 28.6s\n",
            "827:\tlearn: 62.0107901\ttotal: 3m 3s\tremaining: 28.4s\n",
            "828:\tlearn: 61.9881819\ttotal: 3m 3s\tremaining: 28.2s\n",
            "829:\tlearn: 61.9839923\ttotal: 3m 4s\tremaining: 28s\n",
            "830:\tlearn: 61.9107678\ttotal: 3m 4s\tremaining: 27.7s\n",
            "831:\tlearn: 61.8445083\ttotal: 3m 4s\tremaining: 27.5s\n",
            "832:\tlearn: 61.7683969\ttotal: 3m 4s\tremaining: 27.3s\n",
            "833:\tlearn: 61.7551385\ttotal: 3m 5s\tremaining: 27.1s\n",
            "834:\tlearn: 61.7286101\ttotal: 3m 5s\tremaining: 26.9s\n",
            "835:\tlearn: 61.6974350\ttotal: 3m 5s\tremaining: 26.6s\n",
            "836:\tlearn: 61.6567845\ttotal: 3m 5s\tremaining: 26.4s\n",
            "837:\tlearn: 61.6357966\ttotal: 3m 6s\tremaining: 26.2s\n",
            "838:\tlearn: 61.6346977\ttotal: 3m 6s\tremaining: 26s\n",
            "839:\tlearn: 61.6177698\ttotal: 3m 6s\tremaining: 25.8s\n",
            "840:\tlearn: 61.6171216\ttotal: 3m 6s\tremaining: 25.5s\n",
            "841:\tlearn: 61.6159960\ttotal: 3m 7s\tremaining: 25.3s\n",
            "842:\tlearn: 61.6156081\ttotal: 3m 7s\tremaining: 25.1s\n",
            "843:\tlearn: 61.5920196\ttotal: 3m 7s\tremaining: 24.9s\n",
            "844:\tlearn: 61.5405364\ttotal: 3m 7s\tremaining: 24.7s\n",
            "845:\tlearn: 61.5067022\ttotal: 3m 7s\tremaining: 24.4s\n",
            "846:\tlearn: 61.4766441\ttotal: 3m 8s\tremaining: 24.2s\n",
            "847:\tlearn: 61.4580780\ttotal: 3m 8s\tremaining: 24s\n",
            "848:\tlearn: 61.4567882\ttotal: 3m 8s\tremaining: 23.8s\n",
            "849:\tlearn: 61.4405131\ttotal: 3m 8s\tremaining: 23.5s\n",
            "850:\tlearn: 61.3938780\ttotal: 3m 9s\tremaining: 23.3s\n",
            "851:\tlearn: 61.3603129\ttotal: 3m 9s\tremaining: 23.1s\n",
            "852:\tlearn: 61.2983760\ttotal: 3m 9s\tremaining: 22.9s\n",
            "853:\tlearn: 61.2750388\ttotal: 3m 9s\tremaining: 22.7s\n",
            "854:\tlearn: 61.2527586\ttotal: 3m 9s\tremaining: 22.4s\n",
            "855:\tlearn: 61.2060550\ttotal: 3m 10s\tremaining: 22.2s\n",
            "856:\tlearn: 61.1647126\ttotal: 3m 10s\tremaining: 22s\n",
            "857:\tlearn: 61.1409229\ttotal: 3m 10s\tremaining: 21.8s\n",
            "858:\tlearn: 61.1084192\ttotal: 3m 10s\tremaining: 21.5s\n",
            "859:\tlearn: 61.0768356\ttotal: 3m 11s\tremaining: 21.3s\n",
            "860:\tlearn: 61.0657964\ttotal: 3m 11s\tremaining: 21.1s\n",
            "861:\tlearn: 61.0447273\ttotal: 3m 11s\tremaining: 20.9s\n",
            "862:\tlearn: 60.9897446\ttotal: 3m 11s\tremaining: 20.7s\n",
            "863:\tlearn: 60.9788469\ttotal: 3m 11s\tremaining: 20.4s\n",
            "864:\tlearn: 60.9771520\ttotal: 3m 12s\tremaining: 20.2s\n",
            "865:\tlearn: 60.9540259\ttotal: 3m 12s\tremaining: 20s\n",
            "866:\tlearn: 60.9076852\ttotal: 3m 12s\tremaining: 19.8s\n",
            "867:\tlearn: 60.8896760\ttotal: 3m 12s\tremaining: 19.6s\n",
            "868:\tlearn: 60.8704960\ttotal: 3m 13s\tremaining: 19.3s\n",
            "869:\tlearn: 60.8592038\ttotal: 3m 13s\tremaining: 19.1s\n",
            "870:\tlearn: 60.8382992\ttotal: 3m 13s\tremaining: 18.9s\n",
            "871:\tlearn: 60.8325886\ttotal: 3m 13s\tremaining: 18.7s\n",
            "872:\tlearn: 60.7897595\ttotal: 3m 13s\tremaining: 18.4s\n",
            "873:\tlearn: 60.7722387\ttotal: 3m 14s\tremaining: 18.2s\n",
            "874:\tlearn: 60.7599227\ttotal: 3m 14s\tremaining: 18s\n",
            "875:\tlearn: 60.7415403\ttotal: 3m 14s\tremaining: 17.8s\n",
            "876:\tlearn: 60.7402623\ttotal: 3m 14s\tremaining: 17.6s\n",
            "877:\tlearn: 60.7092323\ttotal: 3m 15s\tremaining: 17.3s\n",
            "878:\tlearn: 60.6637386\ttotal: 3m 15s\tremaining: 17.1s\n",
            "879:\tlearn: 60.6268989\ttotal: 3m 15s\tremaining: 16.9s\n",
            "880:\tlearn: 60.6056452\ttotal: 3m 15s\tremaining: 16.7s\n",
            "881:\tlearn: 60.5517399\ttotal: 3m 15s\tremaining: 16.4s\n",
            "882:\tlearn: 60.5503586\ttotal: 3m 16s\tremaining: 16.2s\n",
            "883:\tlearn: 60.4991700\ttotal: 3m 16s\tremaining: 16s\n",
            "884:\tlearn: 60.4810419\ttotal: 3m 16s\tremaining: 15.8s\n",
            "885:\tlearn: 60.4747500\ttotal: 3m 16s\tremaining: 15.6s\n",
            "886:\tlearn: 60.4339092\ttotal: 3m 17s\tremaining: 15.3s\n",
            "887:\tlearn: 60.3997046\ttotal: 3m 17s\tremaining: 15.1s\n",
            "888:\tlearn: 60.3867371\ttotal: 3m 17s\tremaining: 14.9s\n",
            "889:\tlearn: 60.3721313\ttotal: 3m 17s\tremaining: 14.7s\n",
            "890:\tlearn: 60.3623926\ttotal: 3m 18s\tremaining: 14.4s\n",
            "891:\tlearn: 60.3066712\ttotal: 3m 18s\tremaining: 14.2s\n",
            "892:\tlearn: 60.2929188\ttotal: 3m 18s\tremaining: 14s\n",
            "893:\tlearn: 60.2602315\ttotal: 3m 18s\tremaining: 13.8s\n",
            "894:\tlearn: 60.2505780\ttotal: 3m 18s\tremaining: 13.6s\n",
            "895:\tlearn: 60.2407571\ttotal: 3m 19s\tremaining: 13.3s\n",
            "896:\tlearn: 60.2228032\ttotal: 3m 19s\tremaining: 13.1s\n",
            "897:\tlearn: 60.2109557\ttotal: 3m 19s\tremaining: 12.9s\n",
            "898:\tlearn: 60.1942432\ttotal: 3m 20s\tremaining: 12.7s\n",
            "899:\tlearn: 60.1941649\ttotal: 3m 20s\tremaining: 12.5s\n",
            "900:\tlearn: 60.1877404\ttotal: 3m 20s\tremaining: 12.2s\n",
            "901:\tlearn: 60.1203636\ttotal: 3m 20s\tremaining: 12s\n",
            "902:\tlearn: 60.1198955\ttotal: 3m 21s\tremaining: 11.8s\n",
            "903:\tlearn: 60.0881039\ttotal: 3m 21s\tremaining: 11.6s\n",
            "904:\tlearn: 60.0048333\ttotal: 3m 21s\tremaining: 11.4s\n",
            "905:\tlearn: 59.9950900\ttotal: 3m 21s\tremaining: 11.1s\n",
            "906:\tlearn: 59.9731142\ttotal: 3m 22s\tremaining: 10.9s\n",
            "907:\tlearn: 59.9231669\ttotal: 3m 22s\tremaining: 10.7s\n",
            "908:\tlearn: 59.9166078\ttotal: 3m 22s\tremaining: 10.5s\n",
            "909:\tlearn: 59.9069609\ttotal: 3m 22s\tremaining: 10.2s\n",
            "910:\tlearn: 59.8980097\ttotal: 3m 23s\tremaining: 10s\n",
            "911:\tlearn: 59.8765561\ttotal: 3m 23s\tremaining: 9.81s\n",
            "912:\tlearn: 59.8710482\ttotal: 3m 23s\tremaining: 9.59s\n",
            "913:\tlearn: 59.8709305\ttotal: 3m 23s\tremaining: 9.37s\n",
            "914:\tlearn: 59.8705763\ttotal: 3m 24s\tremaining: 9.14s\n",
            "915:\tlearn: 59.8402103\ttotal: 3m 24s\tremaining: 8.92s\n",
            "916:\tlearn: 59.8190681\ttotal: 3m 24s\tremaining: 8.7s\n",
            "917:\tlearn: 59.7998077\ttotal: 3m 24s\tremaining: 8.47s\n",
            "918:\tlearn: 59.7353163\ttotal: 3m 24s\tremaining: 8.25s\n",
            "919:\tlearn: 59.6812412\ttotal: 3m 25s\tremaining: 8.03s\n",
            "920:\tlearn: 59.6366595\ttotal: 3m 25s\tremaining: 7.81s\n",
            "921:\tlearn: 59.5991867\ttotal: 3m 25s\tremaining: 7.58s\n",
            "922:\tlearn: 59.5948829\ttotal: 3m 25s\tremaining: 7.36s\n",
            "923:\tlearn: 59.5707344\ttotal: 3m 26s\tremaining: 7.14s\n",
            "924:\tlearn: 59.5170869\ttotal: 3m 26s\tremaining: 6.92s\n",
            "925:\tlearn: 59.4935395\ttotal: 3m 26s\tremaining: 6.69s\n",
            "926:\tlearn: 59.4705832\ttotal: 3m 26s\tremaining: 6.47s\n",
            "927:\tlearn: 59.4215968\ttotal: 3m 27s\tremaining: 6.25s\n",
            "928:\tlearn: 59.3924121\ttotal: 3m 27s\tremaining: 6.02s\n",
            "929:\tlearn: 59.3921924\ttotal: 3m 27s\tremaining: 5.8s\n",
            "930:\tlearn: 59.3811388\ttotal: 3m 27s\tremaining: 5.58s\n",
            "931:\tlearn: 59.3243783\ttotal: 3m 27s\tremaining: 5.35s\n",
            "932:\tlearn: 59.3113586\ttotal: 3m 28s\tremaining: 5.13s\n",
            "933:\tlearn: 59.2872435\ttotal: 3m 28s\tremaining: 4.91s\n",
            "934:\tlearn: 59.2842160\ttotal: 3m 28s\tremaining: 4.68s\n",
            "935:\tlearn: 59.2840431\ttotal: 3m 28s\tremaining: 4.46s\n",
            "936:\tlearn: 59.2455881\ttotal: 3m 29s\tremaining: 4.24s\n",
            "937:\tlearn: 59.2272597\ttotal: 3m 29s\tremaining: 4.01s\n",
            "938:\tlearn: 59.1775568\ttotal: 3m 29s\tremaining: 3.79s\n",
            "939:\tlearn: 59.1631323\ttotal: 3m 29s\tremaining: 3.57s\n",
            "940:\tlearn: 59.1506107\ttotal: 3m 29s\tremaining: 3.35s\n",
            "941:\tlearn: 59.1360810\ttotal: 3m 30s\tremaining: 3.12s\n",
            "942:\tlearn: 59.1110802\ttotal: 3m 30s\tremaining: 2.9s\n",
            "943:\tlearn: 59.0936743\ttotal: 3m 30s\tremaining: 2.68s\n",
            "944:\tlearn: 59.0458591\ttotal: 3m 30s\tremaining: 2.45s\n",
            "945:\tlearn: 59.0192678\ttotal: 3m 31s\tremaining: 2.23s\n",
            "946:\tlearn: 58.9901518\ttotal: 3m 31s\tremaining: 2.01s\n",
            "947:\tlearn: 58.9630029\ttotal: 3m 31s\tremaining: 1.78s\n",
            "948:\tlearn: 58.9301192\ttotal: 3m 31s\tremaining: 1.56s\n",
            "949:\tlearn: 58.8805108\ttotal: 3m 31s\tremaining: 1.34s\n",
            "950:\tlearn: 58.7640545\ttotal: 3m 32s\tremaining: 1.11s\n",
            "951:\tlearn: 58.7332548\ttotal: 3m 32s\tremaining: 892ms\n",
            "952:\tlearn: 58.7330004\ttotal: 3m 32s\tremaining: 669ms\n",
            "953:\tlearn: 58.7169492\ttotal: 3m 32s\tremaining: 446ms\n",
            "954:\tlearn: 58.6830465\ttotal: 3m 32s\tremaining: 223ms\n",
            "955:\tlearn: 58.6017391\ttotal: 3m 33s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 18:54:12,252] A new study created in memory with name: no-name-79de77e7-1b0a-4cc6-a213-7ef5ce6ee8c8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 8...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 19:08:16,059] Trial 0 finished with value: 60.421724005912594 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 60.421724005912594.\n",
            "[I 2023-10-08 19:12:32,613] Trial 1 finished with value: 41.15381890867699 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 19:19:51,399] Trial 2 finished with value: 44.992022716654546 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 19:24:48,585] Trial 3 finished with value: 44.81604669998172 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 19:28:58,073] Trial 4 finished with value: 41.78738276809415 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 19:38:04,674] Trial 5 finished with value: 77.31435455776983 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 19:41:43,394] Trial 6 finished with value: 44.275752304224824 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 19:57:25,683] Trial 7 finished with value: 57.28862806822793 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 19:59:43,665] Trial 8 finished with value: 56.7648481441448 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:03:42,087] Trial 9 finished with value: 47.72644872234853 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:09:33,951] Trial 10 finished with value: 51.74377558638698 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:13:45,805] Trial 11 finished with value: 41.810324017504676 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:16:50,327] Trial 12 finished with value: 46.367201140872005 and parameters: {'iterations': 758, 'learning_rate': 0.983343987565645, 'depth': 8, 'min_data_in_leaf': 3, 'reg_lambda': 72.61431221371632, 'subsample': 0.5757735214983872, 'random_strength': 33.23239395085676, 'od_wait': 108, 'leaf_estimation_iterations': 4, 'bagging_temperature': 24.356631192739087, 'colsample_bylevel': 0.3642319611435846}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:21:22,704] Trial 13 finished with value: 57.26479580674675 and parameters: {'iterations': 971, 'learning_rate': 0.8348832267870809, 'depth': 11, 'min_data_in_leaf': 21, 'reg_lambda': 30.221360663173336, 'subsample': 0.4627605210231004, 'random_strength': 67.17789626310656, 'od_wait': 145, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.9773987892377944}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:24:28,761] Trial 14 finished with value: 42.74932801954172 and parameters: {'iterations': 725, 'learning_rate': 0.7127771672306289, 'depth': 8, 'min_data_in_leaf': 9, 'reg_lambda': 58.113406993512264, 'subsample': 0.65866699801961, 'random_strength': 45.00496191936034, 'od_wait': 126, 'leaf_estimation_iterations': 1, 'bagging_temperature': 14.031025575549046, 'colsample_bylevel': 0.5544022353458152}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:29:20,780] Trial 15 finished with value: 50.51761916959778 and parameters: {'iterations': 811, 'learning_rate': 0.8479230466326824, 'depth': 10, 'min_data_in_leaf': 23, 'reg_lambda': 42.57803780188458, 'subsample': 0.46082857874135125, 'random_strength': 20.618074524594604, 'od_wait': 98, 'leaf_estimation_iterations': 5, 'bagging_temperature': 43.040289080815256, 'colsample_bylevel': 0.7670727501880628}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:33:45,467] Trial 16 finished with value: 46.05016503194005 and parameters: {'iterations': 932, 'learning_rate': 0.991514101642958, 'depth': 7, 'min_data_in_leaf': 7, 'reg_lambda': 75.69602958648913, 'subsample': 0.6232542409041335, 'random_strength': 44.211563241494794, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6017437666088656}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:36:16,113] Trial 17 finished with value: 51.38035756239824 and parameters: {'iterations': 305, 'learning_rate': 0.6379370742580681, 'depth': 12, 'min_data_in_leaf': 17, 'reg_lambda': 96.539811034711, 'subsample': 0.4556846128045943, 'random_strength': 23.28846249783863, 'od_wait': 62, 'leaf_estimation_iterations': 5, 'bagging_temperature': 5.276027833053952, 'colsample_bylevel': 0.2932296239833232}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:41:50,151] Trial 18 finished with value: 47.879625482600005 and parameters: {'iterations': 821, 'learning_rate': 0.8835598505366324, 'depth': 9, 'min_data_in_leaf': 1, 'reg_lambda': 63.060135923122104, 'subsample': 0.7231925383686137, 'random_strength': 36.49046201305777, 'od_wait': 124, 'leaf_estimation_iterations': 18, 'bagging_temperature': 90.74448118331959, 'colsample_bylevel': 0.800712550834435}. Best is trial 1 with value: 41.15381890867699.\n",
            "[I 2023-10-08 20:44:15,341] Trial 19 finished with value: 70.72442693108611 and parameters: {'iterations': 572, 'learning_rate': 0.12616080685751307, 'depth': 7, 'min_data_in_leaf': 30, 'reg_lambda': 80.72321673935316, 'subsample': 0.6015892895130246, 'random_strength': 63.99947767145992, 'od_wait': 88, 'leaf_estimation_iterations': 3, 'bagging_temperature': 16.62365082841058, 'colsample_bylevel': 0.6818744770955735}. Best is trial 1 with value: 41.15381890867699.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 8: 41.15381890867699\n",
            "Best hyperparameters for fold 8: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}\n",
            "0:\tlearn: 203.0357809\ttotal: 243ms\tremaining: 3m 25s\n",
            "1:\tlearn: 200.3458518\ttotal: 512ms\tremaining: 3m 36s\n",
            "2:\tlearn: 198.8372284\ttotal: 783ms\tremaining: 3m 40s\n",
            "3:\tlearn: 197.6923737\ttotal: 1.03s\tremaining: 3m 38s\n",
            "4:\tlearn: 195.8173874\ttotal: 1.27s\tremaining: 3m 33s\n",
            "5:\tlearn: 195.8106808\ttotal: 1.39s\tremaining: 3m 14s\n",
            "6:\tlearn: 195.5682868\ttotal: 1.64s\tremaining: 3m 17s\n",
            "7:\tlearn: 195.0243531\ttotal: 1.96s\tremaining: 3m 25s\n",
            "8:\tlearn: 194.6056390\ttotal: 2.25s\tremaining: 3m 29s\n",
            "9:\tlearn: 194.0994827\ttotal: 2.52s\tremaining: 3m 31s\n",
            "10:\tlearn: 193.1539461\ttotal: 2.81s\tremaining: 3m 34s\n",
            "11:\tlearn: 193.0734900\ttotal: 3.02s\tremaining: 3m 30s\n",
            "12:\tlearn: 192.7772558\ttotal: 3.32s\tremaining: 3m 33s\n",
            "13:\tlearn: 192.7771152\ttotal: 3.44s\tremaining: 3m 25s\n",
            "14:\tlearn: 192.7751513\ttotal: 3.55s\tremaining: 3m 17s\n",
            "15:\tlearn: 192.7751513\ttotal: 3.65s\tremaining: 3m 9s\n",
            "16:\tlearn: 192.7623720\ttotal: 3.78s\tremaining: 3m 4s\n",
            "17:\tlearn: 192.5804932\ttotal: 4.13s\tremaining: 3m 10s\n",
            "18:\tlearn: 192.5313156\ttotal: 4.33s\tremaining: 3m 8s\n",
            "19:\tlearn: 192.3165704\ttotal: 4.61s\tremaining: 3m 10s\n",
            "20:\tlearn: 191.8482118\ttotal: 4.92s\tremaining: 3m 13s\n",
            "21:\tlearn: 191.5683555\ttotal: 5.23s\tremaining: 3m 16s\n",
            "22:\tlearn: 190.2399386\ttotal: 5.52s\tremaining: 3m 18s\n",
            "23:\tlearn: 189.6773597\ttotal: 5.8s\tremaining: 3m 19s\n",
            "24:\tlearn: 185.4425800\ttotal: 6.04s\tremaining: 3m 18s\n",
            "25:\tlearn: 184.3455824\ttotal: 6.29s\tremaining: 3m 18s\n",
            "26:\tlearn: 183.5152839\ttotal: 6.63s\tremaining: 3m 21s\n",
            "27:\tlearn: 181.6987210\ttotal: 6.87s\tremaining: 3m 21s\n",
            "28:\tlearn: 176.7253891\ttotal: 7.11s\tremaining: 3m 20s\n",
            "29:\tlearn: 175.8392318\ttotal: 7.36s\tremaining: 3m 20s\n",
            "30:\tlearn: 175.4417513\ttotal: 7.66s\tremaining: 3m 21s\n",
            "31:\tlearn: 175.0316609\ttotal: 7.98s\tremaining: 3m 23s\n",
            "32:\tlearn: 174.6770527\ttotal: 8.25s\tremaining: 3m 23s\n",
            "33:\tlearn: 174.2535870\ttotal: 8.5s\tremaining: 3m 23s\n",
            "34:\tlearn: 173.9155683\ttotal: 8.78s\tremaining: 3m 24s\n",
            "35:\tlearn: 170.6993093\ttotal: 9.01s\tremaining: 3m 23s\n",
            "36:\tlearn: 168.7671439\ttotal: 9.24s\tremaining: 3m 22s\n",
            "37:\tlearn: 168.5140880\ttotal: 9.48s\tremaining: 3m 22s\n",
            "38:\tlearn: 168.3130415\ttotal: 9.73s\tremaining: 3m 21s\n",
            "39:\tlearn: 165.9203144\ttotal: 9.97s\tremaining: 3m 21s\n",
            "40:\tlearn: 165.2442393\ttotal: 10.2s\tremaining: 3m 21s\n",
            "41:\tlearn: 162.2181009\ttotal: 10.5s\tremaining: 3m 21s\n",
            "42:\tlearn: 161.8009599\ttotal: 10.7s\tremaining: 3m 21s\n",
            "43:\tlearn: 159.9801503\ttotal: 11s\tremaining: 3m 21s\n",
            "44:\tlearn: 159.0458966\ttotal: 11.3s\tremaining: 3m 20s\n",
            "45:\tlearn: 157.7765476\ttotal: 11.5s\tremaining: 3m 20s\n",
            "46:\tlearn: 153.3468665\ttotal: 11.7s\tremaining: 3m 20s\n",
            "47:\tlearn: 151.9403550\ttotal: 12s\tremaining: 3m 19s\n",
            "48:\tlearn: 151.1104058\ttotal: 12.2s\tremaining: 3m 19s\n",
            "49:\tlearn: 150.7426826\ttotal: 12.5s\tremaining: 3m 18s\n",
            "50:\tlearn: 149.4428701\ttotal: 12.7s\tremaining: 3m 18s\n",
            "51:\tlearn: 148.1744922\ttotal: 12.9s\tremaining: 3m 18s\n",
            "52:\tlearn: 147.9559574\ttotal: 13.2s\tremaining: 3m 18s\n",
            "53:\tlearn: 147.1774104\ttotal: 13.4s\tremaining: 3m 17s\n",
            "54:\tlearn: 145.4825167\ttotal: 13.7s\tremaining: 3m 17s\n",
            "55:\tlearn: 143.5622669\ttotal: 13.9s\tremaining: 3m 16s\n",
            "56:\tlearn: 142.2620972\ttotal: 14.2s\tremaining: 3m 16s\n",
            "57:\tlearn: 141.8746335\ttotal: 14.4s\tremaining: 3m 16s\n",
            "58:\tlearn: 141.8731126\ttotal: 14.7s\tremaining: 3m 16s\n",
            "59:\tlearn: 141.3865158\ttotal: 14.9s\tremaining: 3m 16s\n",
            "60:\tlearn: 140.9813728\ttotal: 15.1s\tremaining: 3m 15s\n",
            "61:\tlearn: 139.5744703\ttotal: 15.4s\tremaining: 3m 14s\n",
            "62:\tlearn: 138.8841959\ttotal: 15.6s\tremaining: 3m 14s\n",
            "63:\tlearn: 138.6191856\ttotal: 15.9s\tremaining: 3m 14s\n",
            "64:\tlearn: 138.0609206\ttotal: 16.1s\tremaining: 3m 13s\n",
            "65:\tlearn: 137.8302125\ttotal: 16.3s\tremaining: 3m 13s\n",
            "66:\tlearn: 136.3495884\ttotal: 16.6s\tremaining: 3m 13s\n",
            "67:\tlearn: 135.9967370\ttotal: 16.8s\tremaining: 3m 13s\n",
            "68:\tlearn: 135.6228852\ttotal: 17.1s\tremaining: 3m 12s\n",
            "69:\tlearn: 134.6051353\ttotal: 17.3s\tremaining: 3m 12s\n",
            "70:\tlearn: 134.0894748\ttotal: 17.6s\tremaining: 3m 12s\n",
            "71:\tlearn: 133.3463444\ttotal: 17.8s\tremaining: 3m 12s\n",
            "72:\tlearn: 132.1656769\ttotal: 18.1s\tremaining: 3m 11s\n",
            "73:\tlearn: 131.1230915\ttotal: 18.3s\tremaining: 3m 11s\n",
            "74:\tlearn: 130.6929803\ttotal: 18.6s\tremaining: 3m 11s\n",
            "75:\tlearn: 130.2140298\ttotal: 18.8s\tremaining: 3m 11s\n",
            "76:\tlearn: 130.0224801\ttotal: 19.1s\tremaining: 3m 11s\n",
            "77:\tlearn: 129.5955229\ttotal: 19.4s\tremaining: 3m 11s\n",
            "78:\tlearn: 129.3283491\ttotal: 19.6s\tremaining: 3m 11s\n",
            "79:\tlearn: 129.1036182\ttotal: 19.9s\tremaining: 3m 10s\n",
            "80:\tlearn: 128.7480113\ttotal: 20.1s\tremaining: 3m 10s\n",
            "81:\tlearn: 128.3291721\ttotal: 20.4s\tremaining: 3m 10s\n",
            "82:\tlearn: 125.9994541\ttotal: 20.6s\tremaining: 3m 9s\n",
            "83:\tlearn: 125.7257259\ttotal: 20.9s\tremaining: 3m 10s\n",
            "84:\tlearn: 125.3241611\ttotal: 21.2s\tremaining: 3m 9s\n",
            "85:\tlearn: 125.1407308\ttotal: 21.4s\tremaining: 3m 9s\n",
            "86:\tlearn: 124.5980187\ttotal: 21.6s\tremaining: 3m 9s\n",
            "87:\tlearn: 124.1549385\ttotal: 21.9s\tremaining: 3m 9s\n",
            "88:\tlearn: 124.0179585\ttotal: 22.2s\tremaining: 3m 9s\n",
            "89:\tlearn: 123.3767987\ttotal: 22.5s\tremaining: 3m 9s\n",
            "90:\tlearn: 122.8542555\ttotal: 22.7s\tremaining: 3m 9s\n",
            "91:\tlearn: 122.6923157\ttotal: 23s\tremaining: 3m 9s\n",
            "92:\tlearn: 121.3810469\ttotal: 23.3s\tremaining: 3m 9s\n",
            "93:\tlearn: 120.7166366\ttotal: 23.5s\tremaining: 3m 8s\n",
            "94:\tlearn: 120.5646063\ttotal: 23.8s\tremaining: 3m 8s\n",
            "95:\tlearn: 120.5625234\ttotal: 24.1s\tremaining: 3m 8s\n",
            "96:\tlearn: 120.0292930\ttotal: 24.3s\tremaining: 3m 8s\n",
            "97:\tlearn: 119.9010977\ttotal: 24.6s\tremaining: 3m 8s\n",
            "98:\tlearn: 119.6816348\ttotal: 24.8s\tremaining: 3m 7s\n",
            "99:\tlearn: 119.2691330\ttotal: 25.1s\tremaining: 3m 7s\n",
            "100:\tlearn: 118.3607587\ttotal: 25.3s\tremaining: 3m 7s\n",
            "101:\tlearn: 117.4580162\ttotal: 25.5s\tremaining: 3m 6s\n",
            "102:\tlearn: 116.9489679\ttotal: 25.8s\tremaining: 3m 6s\n",
            "103:\tlearn: 116.6473275\ttotal: 26s\tremaining: 3m 6s\n",
            "104:\tlearn: 116.4557646\ttotal: 26.3s\tremaining: 3m 6s\n",
            "105:\tlearn: 116.3306408\ttotal: 26.6s\tremaining: 3m 6s\n",
            "106:\tlearn: 116.1310671\ttotal: 26.9s\tremaining: 3m 6s\n",
            "107:\tlearn: 115.9704689\ttotal: 27.1s\tremaining: 3m 5s\n",
            "108:\tlearn: 115.8381712\ttotal: 27.4s\tremaining: 3m 5s\n",
            "109:\tlearn: 115.6107216\ttotal: 27.7s\tremaining: 3m 5s\n",
            "110:\tlearn: 115.5494825\ttotal: 28s\tremaining: 3m 5s\n",
            "111:\tlearn: 115.4269536\ttotal: 28.3s\tremaining: 3m 5s\n",
            "112:\tlearn: 115.2334772\ttotal: 28.5s\tremaining: 3m 5s\n",
            "113:\tlearn: 114.7624875\ttotal: 28.8s\tremaining: 3m 5s\n",
            "114:\tlearn: 114.2760084\ttotal: 29s\tremaining: 3m 4s\n",
            "115:\tlearn: 114.1177916\ttotal: 29.3s\tremaining: 3m 4s\n",
            "116:\tlearn: 113.9217473\ttotal: 29.6s\tremaining: 3m 4s\n",
            "117:\tlearn: 113.1275343\ttotal: 29.8s\tremaining: 3m 4s\n",
            "118:\tlearn: 112.6810801\ttotal: 30.1s\tremaining: 3m 4s\n",
            "119:\tlearn: 112.3767460\ttotal: 30.3s\tremaining: 3m 3s\n",
            "120:\tlearn: 111.2037204\ttotal: 30.6s\tremaining: 3m 3s\n",
            "121:\tlearn: 110.7492054\ttotal: 30.8s\tremaining: 3m 3s\n",
            "122:\tlearn: 110.6239228\ttotal: 31.1s\tremaining: 3m 3s\n",
            "123:\tlearn: 110.1237524\ttotal: 31.4s\tremaining: 3m 3s\n",
            "124:\tlearn: 109.6448797\ttotal: 31.6s\tremaining: 3m 2s\n",
            "125:\tlearn: 109.2610808\ttotal: 31.8s\tremaining: 3m 2s\n",
            "126:\tlearn: 108.8353547\ttotal: 32.1s\tremaining: 3m 2s\n",
            "127:\tlearn: 108.6244227\ttotal: 32.4s\tremaining: 3m 2s\n",
            "128:\tlearn: 108.4323614\ttotal: 32.6s\tremaining: 3m 1s\n",
            "129:\tlearn: 108.1812476\ttotal: 32.8s\tremaining: 3m 1s\n",
            "130:\tlearn: 107.9695797\ttotal: 33.2s\tremaining: 3m 1s\n",
            "131:\tlearn: 107.8596708\ttotal: 33.4s\tremaining: 3m 1s\n",
            "132:\tlearn: 107.4323435\ttotal: 33.6s\tremaining: 3m\n",
            "133:\tlearn: 107.1649477\ttotal: 33.9s\tremaining: 3m\n",
            "134:\tlearn: 106.3227024\ttotal: 34.1s\tremaining: 3m\n",
            "135:\tlearn: 106.0297588\ttotal: 34.3s\tremaining: 2m 59s\n",
            "136:\tlearn: 105.8922858\ttotal: 34.6s\tremaining: 2m 59s\n",
            "137:\tlearn: 104.8831717\ttotal: 34.8s\tremaining: 2m 59s\n",
            "138:\tlearn: 104.7127624\ttotal: 35.1s\tremaining: 2m 58s\n",
            "139:\tlearn: 104.6040253\ttotal: 35.4s\tremaining: 2m 58s\n",
            "140:\tlearn: 104.4912517\ttotal: 35.7s\tremaining: 2m 58s\n",
            "141:\tlearn: 104.3062416\ttotal: 36s\tremaining: 2m 58s\n",
            "142:\tlearn: 103.7067303\ttotal: 36.2s\tremaining: 2m 58s\n",
            "143:\tlearn: 103.4753790\ttotal: 36.5s\tremaining: 2m 58s\n",
            "144:\tlearn: 103.2681577\ttotal: 36.7s\tremaining: 2m 57s\n",
            "145:\tlearn: 103.1271222\ttotal: 36.9s\tremaining: 2m 57s\n",
            "146:\tlearn: 102.9966808\ttotal: 37.2s\tremaining: 2m 57s\n",
            "147:\tlearn: 102.9068480\ttotal: 37.5s\tremaining: 2m 57s\n",
            "148:\tlearn: 102.8417724\ttotal: 37.8s\tremaining: 2m 57s\n",
            "149:\tlearn: 102.7743918\ttotal: 38s\tremaining: 2m 56s\n",
            "150:\tlearn: 102.0937477\ttotal: 38.3s\tremaining: 2m 56s\n",
            "151:\tlearn: 101.7151218\ttotal: 38.6s\tremaining: 2m 56s\n",
            "152:\tlearn: 101.6197379\ttotal: 38.9s\tremaining: 2m 56s\n",
            "153:\tlearn: 101.5895138\ttotal: 39.2s\tremaining: 2m 56s\n",
            "154:\tlearn: 101.4702215\ttotal: 39.5s\tremaining: 2m 56s\n",
            "155:\tlearn: 100.6915697\ttotal: 39.8s\tremaining: 2m 56s\n",
            "156:\tlearn: 100.5108111\ttotal: 40s\tremaining: 2m 56s\n",
            "157:\tlearn: 100.1127416\ttotal: 40.2s\tremaining: 2m 55s\n",
            "158:\tlearn: 99.9443880\ttotal: 40.5s\tremaining: 2m 55s\n",
            "159:\tlearn: 99.9434777\ttotal: 40.7s\tremaining: 2m 55s\n",
            "160:\tlearn: 99.6863539\ttotal: 41s\tremaining: 2m 54s\n",
            "161:\tlearn: 99.5539819\ttotal: 41.3s\tremaining: 2m 55s\n",
            "162:\tlearn: 99.4607087\ttotal: 41.6s\tremaining: 2m 55s\n",
            "163:\tlearn: 98.8925134\ttotal: 41.9s\tremaining: 2m 54s\n",
            "164:\tlearn: 98.6245420\ttotal: 42.2s\tremaining: 2m 54s\n",
            "165:\tlearn: 98.5242544\ttotal: 42.4s\tremaining: 2m 54s\n",
            "166:\tlearn: 98.2823683\ttotal: 42.7s\tremaining: 2m 53s\n",
            "167:\tlearn: 97.9407103\ttotal: 42.9s\tremaining: 2m 53s\n",
            "168:\tlearn: 97.5988403\ttotal: 43.1s\tremaining: 2m 53s\n",
            "169:\tlearn: 97.2822189\ttotal: 43.4s\tremaining: 2m 52s\n",
            "170:\tlearn: 97.1837369\ttotal: 43.6s\tremaining: 2m 52s\n",
            "171:\tlearn: 97.0299437\ttotal: 43.8s\tremaining: 2m 52s\n",
            "172:\tlearn: 96.7784457\ttotal: 44.1s\tremaining: 2m 52s\n",
            "173:\tlearn: 96.5559248\ttotal: 44.4s\tremaining: 2m 51s\n",
            "174:\tlearn: 96.4118320\ttotal: 44.6s\tremaining: 2m 51s\n",
            "175:\tlearn: 96.3553346\ttotal: 44.9s\tremaining: 2m 51s\n",
            "176:\tlearn: 96.2975882\ttotal: 45.2s\tremaining: 2m 51s\n",
            "177:\tlearn: 96.1718017\ttotal: 45.5s\tremaining: 2m 51s\n",
            "178:\tlearn: 96.0116227\ttotal: 45.7s\tremaining: 2m 50s\n",
            "179:\tlearn: 95.9559403\ttotal: 46.1s\tremaining: 2m 50s\n",
            "180:\tlearn: 95.8449481\ttotal: 46.4s\tremaining: 2m 50s\n",
            "181:\tlearn: 95.8434599\ttotal: 46.6s\tremaining: 2m 50s\n",
            "182:\tlearn: 95.7845200\ttotal: 46.9s\tremaining: 2m 50s\n",
            "183:\tlearn: 95.6744405\ttotal: 47.2s\tremaining: 2m 50s\n",
            "184:\tlearn: 95.3888529\ttotal: 47.5s\tremaining: 2m 50s\n",
            "185:\tlearn: 95.2572657\ttotal: 47.7s\tremaining: 2m 49s\n",
            "186:\tlearn: 94.7221809\ttotal: 47.9s\tremaining: 2m 49s\n",
            "187:\tlearn: 94.5693115\ttotal: 48.2s\tremaining: 2m 49s\n",
            "188:\tlearn: 94.2750689\ttotal: 48.5s\tremaining: 2m 48s\n",
            "189:\tlearn: 94.1934726\ttotal: 48.8s\tremaining: 2m 48s\n",
            "190:\tlearn: 94.0951871\ttotal: 49s\tremaining: 2m 48s\n",
            "191:\tlearn: 93.3834946\ttotal: 49.3s\tremaining: 2m 48s\n",
            "192:\tlearn: 93.1013757\ttotal: 49.5s\tremaining: 2m 48s\n",
            "193:\tlearn: 92.9841658\ttotal: 49.8s\tremaining: 2m 47s\n",
            "194:\tlearn: 92.9769494\ttotal: 50s\tremaining: 2m 47s\n",
            "195:\tlearn: 92.6701386\ttotal: 50.3s\tremaining: 2m 47s\n",
            "196:\tlearn: 92.4176842\ttotal: 50.5s\tremaining: 2m 46s\n",
            "197:\tlearn: 92.3381898\ttotal: 50.8s\tremaining: 2m 46s\n",
            "198:\tlearn: 91.8750110\ttotal: 51s\tremaining: 2m 46s\n",
            "199:\tlearn: 91.5523559\ttotal: 51.3s\tremaining: 2m 46s\n",
            "200:\tlearn: 91.1082737\ttotal: 51.5s\tremaining: 2m 45s\n",
            "201:\tlearn: 91.0105780\ttotal: 51.8s\tremaining: 2m 45s\n",
            "202:\tlearn: 90.6300978\ttotal: 52s\tremaining: 2m 45s\n",
            "203:\tlearn: 90.5241793\ttotal: 52.3s\tremaining: 2m 45s\n",
            "204:\tlearn: 90.4108019\ttotal: 52.6s\tremaining: 2m 44s\n",
            "205:\tlearn: 90.3092370\ttotal: 52.9s\tremaining: 2m 44s\n",
            "206:\tlearn: 89.9571038\ttotal: 53.1s\tremaining: 2m 44s\n",
            "207:\tlearn: 89.8201828\ttotal: 53.4s\tremaining: 2m 44s\n",
            "208:\tlearn: 89.7244380\ttotal: 53.7s\tremaining: 2m 44s\n",
            "209:\tlearn: 89.4946546\ttotal: 53.9s\tremaining: 2m 43s\n",
            "210:\tlearn: 89.3983117\ttotal: 54.2s\tremaining: 2m 43s\n",
            "211:\tlearn: 89.1524604\ttotal: 54.5s\tremaining: 2m 43s\n",
            "212:\tlearn: 88.7929184\ttotal: 54.8s\tremaining: 2m 43s\n",
            "213:\tlearn: 88.7147303\ttotal: 55s\tremaining: 2m 42s\n",
            "214:\tlearn: 88.6020342\ttotal: 55.3s\tremaining: 2m 42s\n",
            "215:\tlearn: 88.5279724\ttotal: 55.5s\tremaining: 2m 42s\n",
            "216:\tlearn: 88.3240624\ttotal: 55.8s\tremaining: 2m 42s\n",
            "217:\tlearn: 88.0133061\ttotal: 56s\tremaining: 2m 41s\n",
            "218:\tlearn: 87.7415356\ttotal: 56.3s\tremaining: 2m 41s\n",
            "219:\tlearn: 87.6610960\ttotal: 56.5s\tremaining: 2m 41s\n",
            "220:\tlearn: 87.5630746\ttotal: 56.8s\tremaining: 2m 41s\n",
            "221:\tlearn: 87.5068761\ttotal: 57s\tremaining: 2m 40s\n",
            "222:\tlearn: 86.9989429\ttotal: 57.2s\tremaining: 2m 40s\n",
            "223:\tlearn: 86.9085498\ttotal: 57.5s\tremaining: 2m 40s\n",
            "224:\tlearn: 86.4968770\ttotal: 57.8s\tremaining: 2m 39s\n",
            "225:\tlearn: 86.4242822\ttotal: 58s\tremaining: 2m 39s\n",
            "226:\tlearn: 86.2384391\ttotal: 58.3s\tremaining: 2m 39s\n",
            "227:\tlearn: 86.0175507\ttotal: 58.6s\tremaining: 2m 39s\n",
            "228:\tlearn: 85.8988676\ttotal: 58.8s\tremaining: 2m 38s\n",
            "229:\tlearn: 85.7938489\ttotal: 59.1s\tremaining: 2m 38s\n",
            "230:\tlearn: 85.7184908\ttotal: 59.3s\tremaining: 2m 38s\n",
            "231:\tlearn: 85.4015883\ttotal: 59.6s\tremaining: 2m 38s\n",
            "232:\tlearn: 85.3457300\ttotal: 59.9s\tremaining: 2m 38s\n",
            "233:\tlearn: 85.2623872\ttotal: 1m\tremaining: 2m 37s\n",
            "234:\tlearn: 85.1449251\ttotal: 1m\tremaining: 2m 37s\n",
            "235:\tlearn: 85.0748743\ttotal: 1m\tremaining: 2m 37s\n",
            "236:\tlearn: 84.9984472\ttotal: 1m\tremaining: 2m 37s\n",
            "237:\tlearn: 84.9779758\ttotal: 1m 1s\tremaining: 2m 36s\n",
            "238:\tlearn: 84.8181776\ttotal: 1m 1s\tremaining: 2m 36s\n",
            "239:\tlearn: 84.7551846\ttotal: 1m 1s\tremaining: 2m 36s\n",
            "240:\tlearn: 84.5934515\ttotal: 1m 1s\tremaining: 2m 36s\n",
            "241:\tlearn: 84.3668321\ttotal: 1m 2s\tremaining: 2m 35s\n",
            "242:\tlearn: 84.2390535\ttotal: 1m 2s\tremaining: 2m 35s\n",
            "243:\tlearn: 83.9831492\ttotal: 1m 2s\tremaining: 2m 35s\n",
            "244:\tlearn: 83.9349768\ttotal: 1m 3s\tremaining: 2m 35s\n",
            "245:\tlearn: 83.7803372\ttotal: 1m 3s\tremaining: 2m 34s\n",
            "246:\tlearn: 83.7084287\ttotal: 1m 3s\tremaining: 2m 34s\n",
            "247:\tlearn: 83.4400854\ttotal: 1m 3s\tremaining: 2m 34s\n",
            "248:\tlearn: 83.1415426\ttotal: 1m 4s\tremaining: 2m 33s\n",
            "249:\tlearn: 83.0042127\ttotal: 1m 4s\tremaining: 2m 33s\n",
            "250:\tlearn: 82.7733554\ttotal: 1m 4s\tremaining: 2m 33s\n",
            "251:\tlearn: 82.7024159\ttotal: 1m 4s\tremaining: 2m 33s\n",
            "252:\tlearn: 82.6636905\ttotal: 1m 5s\tremaining: 2m 32s\n",
            "253:\tlearn: 82.5290655\ttotal: 1m 5s\tremaining: 2m 32s\n",
            "254:\tlearn: 82.2725939\ttotal: 1m 5s\tremaining: 2m 32s\n",
            "255:\tlearn: 82.1607763\ttotal: 1m 5s\tremaining: 2m 32s\n",
            "256:\tlearn: 82.1063663\ttotal: 1m 6s\tremaining: 2m 31s\n",
            "257:\tlearn: 81.8773921\ttotal: 1m 6s\tremaining: 2m 31s\n",
            "258:\tlearn: 81.6441278\ttotal: 1m 6s\tremaining: 2m 31s\n",
            "259:\tlearn: 81.3789861\ttotal: 1m 6s\tremaining: 2m 30s\n",
            "260:\tlearn: 81.2360127\ttotal: 1m 6s\tremaining: 2m 30s\n",
            "261:\tlearn: 81.1667959\ttotal: 1m 7s\tremaining: 2m 30s\n",
            "262:\tlearn: 81.0310877\ttotal: 1m 7s\tremaining: 2m 29s\n",
            "263:\tlearn: 80.8195936\ttotal: 1m 7s\tremaining: 2m 29s\n",
            "264:\tlearn: 80.7027162\ttotal: 1m 7s\tremaining: 2m 29s\n",
            "265:\tlearn: 80.5622927\ttotal: 1m 8s\tremaining: 2m 29s\n",
            "266:\tlearn: 80.2244168\ttotal: 1m 8s\tremaining: 2m 28s\n",
            "267:\tlearn: 80.1148185\ttotal: 1m 8s\tremaining: 2m 28s\n",
            "268:\tlearn: 79.9990692\ttotal: 1m 8s\tremaining: 2m 28s\n",
            "269:\tlearn: 79.9517352\ttotal: 1m 9s\tremaining: 2m 28s\n",
            "270:\tlearn: 79.8792664\ttotal: 1m 9s\tremaining: 2m 27s\n",
            "271:\tlearn: 79.6247551\ttotal: 1m 9s\tremaining: 2m 27s\n",
            "272:\tlearn: 79.4407078\ttotal: 1m 9s\tremaining: 2m 27s\n",
            "273:\tlearn: 79.3631972\ttotal: 1m 10s\tremaining: 2m 27s\n",
            "274:\tlearn: 79.2418067\ttotal: 1m 10s\tremaining: 2m 26s\n",
            "275:\tlearn: 79.0261842\ttotal: 1m 10s\tremaining: 2m 26s\n",
            "276:\tlearn: 78.9546715\ttotal: 1m 11s\tremaining: 2m 26s\n",
            "277:\tlearn: 78.7058595\ttotal: 1m 11s\tremaining: 2m 26s\n",
            "278:\tlearn: 78.6192297\ttotal: 1m 11s\tremaining: 2m 25s\n",
            "279:\tlearn: 78.3476720\ttotal: 1m 11s\tremaining: 2m 25s\n",
            "280:\tlearn: 78.1771702\ttotal: 1m 12s\tremaining: 2m 25s\n",
            "281:\tlearn: 78.1212183\ttotal: 1m 12s\tremaining: 2m 25s\n",
            "282:\tlearn: 77.8882948\ttotal: 1m 12s\tremaining: 2m 24s\n",
            "283:\tlearn: 77.6960495\ttotal: 1m 12s\tremaining: 2m 24s\n",
            "284:\tlearn: 77.6056485\ttotal: 1m 13s\tremaining: 2m 24s\n",
            "285:\tlearn: 77.5491829\ttotal: 1m 13s\tremaining: 2m 24s\n",
            "286:\tlearn: 77.4816538\ttotal: 1m 13s\tremaining: 2m 23s\n",
            "287:\tlearn: 77.3041241\ttotal: 1m 13s\tremaining: 2m 23s\n",
            "288:\tlearn: 77.1976105\ttotal: 1m 14s\tremaining: 2m 23s\n",
            "289:\tlearn: 77.0071994\ttotal: 1m 14s\tremaining: 2m 23s\n",
            "290:\tlearn: 76.9153817\ttotal: 1m 14s\tremaining: 2m 22s\n",
            "291:\tlearn: 76.6444475\ttotal: 1m 14s\tremaining: 2m 22s\n",
            "292:\tlearn: 76.5306255\ttotal: 1m 15s\tremaining: 2m 22s\n",
            "293:\tlearn: 76.4267391\ttotal: 1m 15s\tremaining: 2m 22s\n",
            "294:\tlearn: 76.3701773\ttotal: 1m 15s\tremaining: 2m 21s\n",
            "295:\tlearn: 76.2622830\ttotal: 1m 15s\tremaining: 2m 21s\n",
            "296:\tlearn: 76.1608670\ttotal: 1m 16s\tremaining: 2m 21s\n",
            "297:\tlearn: 76.0892460\ttotal: 1m 16s\tremaining: 2m 21s\n",
            "298:\tlearn: 76.0300488\ttotal: 1m 16s\tremaining: 2m 20s\n",
            "299:\tlearn: 75.7798332\ttotal: 1m 16s\tremaining: 2m 20s\n",
            "300:\tlearn: 75.6647539\ttotal: 1m 17s\tremaining: 2m 20s\n",
            "301:\tlearn: 75.6336408\ttotal: 1m 17s\tremaining: 2m 19s\n",
            "302:\tlearn: 75.5521454\ttotal: 1m 17s\tremaining: 2m 19s\n",
            "303:\tlearn: 75.4835694\ttotal: 1m 17s\tremaining: 2m 19s\n",
            "304:\tlearn: 75.4630124\ttotal: 1m 18s\tremaining: 2m 19s\n",
            "305:\tlearn: 75.3814513\ttotal: 1m 18s\tremaining: 2m 18s\n",
            "306:\tlearn: 75.3279246\ttotal: 1m 18s\tremaining: 2m 18s\n",
            "307:\tlearn: 75.1177264\ttotal: 1m 18s\tremaining: 2m 18s\n",
            "308:\tlearn: 74.9526619\ttotal: 1m 19s\tremaining: 2m 18s\n",
            "309:\tlearn: 74.8803838\ttotal: 1m 19s\tremaining: 2m 17s\n",
            "310:\tlearn: 74.8431005\ttotal: 1m 19s\tremaining: 2m 17s\n",
            "311:\tlearn: 74.7425285\ttotal: 1m 19s\tremaining: 2m 17s\n",
            "312:\tlearn: 74.6399216\ttotal: 1m 20s\tremaining: 2m 17s\n",
            "313:\tlearn: 74.5258314\ttotal: 1m 20s\tremaining: 2m 16s\n",
            "314:\tlearn: 74.4380487\ttotal: 1m 20s\tremaining: 2m 16s\n",
            "315:\tlearn: 74.3838482\ttotal: 1m 20s\tremaining: 2m 16s\n",
            "316:\tlearn: 74.2822567\ttotal: 1m 21s\tremaining: 2m 16s\n",
            "317:\tlearn: 74.2121665\ttotal: 1m 21s\tremaining: 2m 15s\n",
            "318:\tlearn: 74.1452856\ttotal: 1m 21s\tremaining: 2m 15s\n",
            "319:\tlearn: 73.8242807\ttotal: 1m 21s\tremaining: 2m 15s\n",
            "320:\tlearn: 73.7893408\ttotal: 1m 22s\tremaining: 2m 15s\n",
            "321:\tlearn: 73.5345843\ttotal: 1m 22s\tremaining: 2m 14s\n",
            "322:\tlearn: 73.4002738\ttotal: 1m 22s\tremaining: 2m 14s\n",
            "323:\tlearn: 73.3991521\ttotal: 1m 22s\tremaining: 2m 14s\n",
            "324:\tlearn: 73.3548044\ttotal: 1m 23s\tremaining: 2m 14s\n",
            "325:\tlearn: 73.3054908\ttotal: 1m 23s\tremaining: 2m 13s\n",
            "326:\tlearn: 73.2476742\ttotal: 1m 23s\tremaining: 2m 13s\n",
            "327:\tlearn: 73.1523570\ttotal: 1m 24s\tremaining: 2m 13s\n",
            "328:\tlearn: 73.1311260\ttotal: 1m 24s\tremaining: 2m 13s\n",
            "329:\tlearn: 73.1283522\ttotal: 1m 24s\tremaining: 2m 13s\n",
            "330:\tlearn: 73.0113594\ttotal: 1m 25s\tremaining: 2m 12s\n",
            "331:\tlearn: 72.9293920\ttotal: 1m 25s\tremaining: 2m 12s\n",
            "332:\tlearn: 72.8462000\ttotal: 1m 25s\tremaining: 2m 12s\n",
            "333:\tlearn: 72.8449773\ttotal: 1m 26s\tremaining: 2m 12s\n",
            "334:\tlearn: 72.6614055\ttotal: 1m 26s\tremaining: 2m 12s\n",
            "335:\tlearn: 72.6597238\ttotal: 1m 26s\tremaining: 2m 11s\n",
            "336:\tlearn: 72.4900410\ttotal: 1m 26s\tremaining: 2m 11s\n",
            "337:\tlearn: 72.4142417\ttotal: 1m 27s\tremaining: 2m 11s\n",
            "338:\tlearn: 72.4042168\ttotal: 1m 27s\tremaining: 2m 11s\n",
            "339:\tlearn: 72.3188025\ttotal: 1m 27s\tremaining: 2m 11s\n",
            "340:\tlearn: 72.1570258\ttotal: 1m 27s\tremaining: 2m 10s\n",
            "341:\tlearn: 71.9620781\ttotal: 1m 28s\tremaining: 2m 10s\n",
            "342:\tlearn: 71.8533666\ttotal: 1m 28s\tremaining: 2m 10s\n",
            "343:\tlearn: 71.6769882\ttotal: 1m 28s\tremaining: 2m 9s\n",
            "344:\tlearn: 71.6323701\ttotal: 1m 28s\tremaining: 2m 9s\n",
            "345:\tlearn: 71.5955814\ttotal: 1m 29s\tremaining: 2m 9s\n",
            "346:\tlearn: 71.5476782\ttotal: 1m 29s\tremaining: 2m 9s\n",
            "347:\tlearn: 71.4333083\ttotal: 1m 29s\tremaining: 2m 8s\n",
            "348:\tlearn: 71.2918192\ttotal: 1m 29s\tremaining: 2m 8s\n",
            "349:\tlearn: 71.1202774\ttotal: 1m 30s\tremaining: 2m 8s\n",
            "350:\tlearn: 71.0699084\ttotal: 1m 30s\tremaining: 2m 7s\n",
            "351:\tlearn: 71.0417052\ttotal: 1m 30s\tremaining: 2m 7s\n",
            "352:\tlearn: 70.9599494\ttotal: 1m 30s\tremaining: 2m 7s\n",
            "353:\tlearn: 70.8135612\ttotal: 1m 31s\tremaining: 2m 7s\n",
            "354:\tlearn: 70.7698547\ttotal: 1m 31s\tremaining: 2m 6s\n",
            "355:\tlearn: 70.7167077\ttotal: 1m 31s\tremaining: 2m 6s\n",
            "356:\tlearn: 70.5158351\ttotal: 1m 31s\tremaining: 2m 6s\n",
            "357:\tlearn: 70.4644627\ttotal: 1m 32s\tremaining: 2m 5s\n",
            "358:\tlearn: 70.3717659\ttotal: 1m 32s\tremaining: 2m 5s\n",
            "359:\tlearn: 70.3176420\ttotal: 1m 32s\tremaining: 2m 5s\n",
            "360:\tlearn: 70.1911950\ttotal: 1m 32s\tremaining: 2m 5s\n",
            "361:\tlearn: 70.1206714\ttotal: 1m 33s\tremaining: 2m 4s\n",
            "362:\tlearn: 70.0564291\ttotal: 1m 33s\tremaining: 2m 4s\n",
            "363:\tlearn: 69.8833926\ttotal: 1m 33s\tremaining: 2m 4s\n",
            "364:\tlearn: 69.8678987\ttotal: 1m 33s\tremaining: 2m 4s\n",
            "365:\tlearn: 69.8495061\ttotal: 1m 34s\tremaining: 2m 3s\n",
            "366:\tlearn: 69.8005546\ttotal: 1m 34s\tremaining: 2m 3s\n",
            "367:\tlearn: 69.7671311\ttotal: 1m 34s\tremaining: 2m 3s\n",
            "368:\tlearn: 69.5445316\ttotal: 1m 34s\tremaining: 2m 3s\n",
            "369:\tlearn: 69.4330407\ttotal: 1m 35s\tremaining: 2m 2s\n",
            "370:\tlearn: 69.3866719\ttotal: 1m 35s\tremaining: 2m 2s\n",
            "371:\tlearn: 69.1894525\ttotal: 1m 35s\tremaining: 2m 2s\n",
            "372:\tlearn: 69.1019033\ttotal: 1m 35s\tremaining: 2m 2s\n",
            "373:\tlearn: 68.8267701\ttotal: 1m 36s\tremaining: 2m 1s\n",
            "374:\tlearn: 68.7705353\ttotal: 1m 36s\tremaining: 2m 1s\n",
            "375:\tlearn: 68.7168269\ttotal: 1m 36s\tremaining: 2m 1s\n",
            "376:\tlearn: 68.6663239\ttotal: 1m 36s\tremaining: 2m 1s\n",
            "377:\tlearn: 68.4903808\ttotal: 1m 37s\tremaining: 2m\n",
            "378:\tlearn: 68.3738340\ttotal: 1m 37s\tremaining: 2m\n",
            "379:\tlearn: 68.2964768\ttotal: 1m 37s\tremaining: 2m\n",
            "380:\tlearn: 68.2343454\ttotal: 1m 37s\tremaining: 1m 59s\n",
            "381:\tlearn: 68.0872074\ttotal: 1m 38s\tremaining: 1m 59s\n",
            "382:\tlearn: 67.9771283\ttotal: 1m 38s\tremaining: 1m 59s\n",
            "383:\tlearn: 67.9069325\ttotal: 1m 38s\tremaining: 1m 59s\n",
            "384:\tlearn: 67.8774529\ttotal: 1m 38s\tremaining: 1m 58s\n",
            "385:\tlearn: 67.7376303\ttotal: 1m 39s\tremaining: 1m 58s\n",
            "386:\tlearn: 67.7062772\ttotal: 1m 39s\tremaining: 1m 58s\n",
            "387:\tlearn: 67.6940195\ttotal: 1m 39s\tremaining: 1m 58s\n",
            "388:\tlearn: 67.6533772\ttotal: 1m 40s\tremaining: 1m 58s\n",
            "389:\tlearn: 67.6032450\ttotal: 1m 40s\tremaining: 1m 57s\n",
            "390:\tlearn: 67.5429855\ttotal: 1m 40s\tremaining: 1m 57s\n",
            "391:\tlearn: 67.4624121\ttotal: 1m 40s\tremaining: 1m 57s\n",
            "392:\tlearn: 67.4036514\ttotal: 1m 41s\tremaining: 1m 57s\n",
            "393:\tlearn: 67.3769689\ttotal: 1m 41s\tremaining: 1m 56s\n",
            "394:\tlearn: 67.2262326\ttotal: 1m 41s\tremaining: 1m 56s\n",
            "395:\tlearn: 67.1552111\ttotal: 1m 41s\tremaining: 1m 56s\n",
            "396:\tlearn: 67.0696563\ttotal: 1m 42s\tremaining: 1m 56s\n",
            "397:\tlearn: 67.0333361\ttotal: 1m 42s\tremaining: 1m 55s\n",
            "398:\tlearn: 66.9426973\ttotal: 1m 42s\tremaining: 1m 55s\n",
            "399:\tlearn: 66.9092359\ttotal: 1m 43s\tremaining: 1m 55s\n",
            "400:\tlearn: 66.8030889\ttotal: 1m 43s\tremaining: 1m 55s\n",
            "401:\tlearn: 66.6053607\ttotal: 1m 43s\tremaining: 1m 54s\n",
            "402:\tlearn: 66.4444232\ttotal: 1m 43s\tremaining: 1m 54s\n",
            "403:\tlearn: 66.4151734\ttotal: 1m 44s\tremaining: 1m 54s\n",
            "404:\tlearn: 66.2931233\ttotal: 1m 44s\tremaining: 1m 54s\n",
            "405:\tlearn: 66.2854103\ttotal: 1m 44s\tremaining: 1m 53s\n",
            "406:\tlearn: 66.2340914\ttotal: 1m 44s\tremaining: 1m 53s\n",
            "407:\tlearn: 66.1944871\ttotal: 1m 45s\tremaining: 1m 53s\n",
            "408:\tlearn: 66.0684924\ttotal: 1m 45s\tremaining: 1m 53s\n",
            "409:\tlearn: 65.9181427\ttotal: 1m 45s\tremaining: 1m 52s\n",
            "410:\tlearn: 65.8043677\ttotal: 1m 45s\tremaining: 1m 52s\n",
            "411:\tlearn: 65.7650782\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "412:\tlearn: 65.6849918\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "413:\tlearn: 65.6234501\ttotal: 1m 46s\tremaining: 1m 51s\n",
            "414:\tlearn: 65.4747245\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "415:\tlearn: 65.3978491\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "416:\tlearn: 65.3010741\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "417:\tlearn: 65.2615983\ttotal: 1m 47s\tremaining: 1m 50s\n",
            "418:\tlearn: 65.2151682\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "419:\tlearn: 65.1415626\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "420:\tlearn: 65.1058911\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "421:\tlearn: 65.0812105\ttotal: 1m 48s\tremaining: 1m 49s\n",
            "422:\tlearn: 64.9447303\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "423:\tlearn: 64.7684723\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "424:\tlearn: 64.7037207\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "425:\tlearn: 64.6261034\ttotal: 1m 49s\tremaining: 1m 48s\n",
            "426:\tlearn: 64.4590824\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "427:\tlearn: 64.3544107\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "428:\tlearn: 64.3146662\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "429:\tlearn: 64.2892851\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "430:\tlearn: 64.2095874\ttotal: 1m 51s\tremaining: 1m 47s\n",
            "431:\tlearn: 64.0664317\ttotal: 1m 51s\tremaining: 1m 47s\n",
            "432:\tlearn: 64.0019491\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "433:\tlearn: 63.9677303\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "434:\tlearn: 63.9410226\ttotal: 1m 52s\tremaining: 1m 46s\n",
            "435:\tlearn: 63.8699904\ttotal: 1m 52s\tremaining: 1m 46s\n",
            "436:\tlearn: 63.8326423\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "437:\tlearn: 63.7645264\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "438:\tlearn: 63.7373691\ttotal: 1m 53s\tremaining: 1m 45s\n",
            "439:\tlearn: 63.6963425\ttotal: 1m 53s\tremaining: 1m 45s\n",
            "440:\tlearn: 63.6536799\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "441:\tlearn: 63.2659550\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "442:\tlearn: 63.2140397\ttotal: 1m 54s\tremaining: 1m 44s\n",
            "443:\tlearn: 63.1190978\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "444:\tlearn: 63.0705311\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "445:\tlearn: 62.9916333\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "446:\tlearn: 62.8912277\ttotal: 1m 55s\tremaining: 1m 43s\n",
            "447:\tlearn: 62.8168155\ttotal: 1m 55s\tremaining: 1m 42s\n",
            "448:\tlearn: 62.7593254\ttotal: 1m 55s\tremaining: 1m 42s\n",
            "449:\tlearn: 62.7176367\ttotal: 1m 55s\tremaining: 1m 42s\n",
            "450:\tlearn: 62.5404144\ttotal: 1m 56s\tremaining: 1m 42s\n",
            "451:\tlearn: 62.4991056\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "452:\tlearn: 62.3887791\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "453:\tlearn: 62.1964361\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "454:\tlearn: 62.1036222\ttotal: 1m 57s\tremaining: 1m 41s\n",
            "455:\tlearn: 62.0531811\ttotal: 1m 57s\tremaining: 1m 41s\n",
            "456:\tlearn: 62.0312889\ttotal: 1m 57s\tremaining: 1m 40s\n",
            "457:\tlearn: 61.7237400\ttotal: 1m 58s\tremaining: 1m 40s\n",
            "458:\tlearn: 61.6494294\ttotal: 1m 58s\tremaining: 1m 40s\n",
            "459:\tlearn: 61.4597886\ttotal: 1m 58s\tremaining: 1m 40s\n",
            "460:\tlearn: 61.4185239\ttotal: 1m 58s\tremaining: 1m 39s\n",
            "461:\tlearn: 61.3639578\ttotal: 1m 59s\tremaining: 1m 39s\n",
            "462:\tlearn: 61.2165440\ttotal: 1m 59s\tremaining: 1m 39s\n",
            "463:\tlearn: 61.0439334\ttotal: 1m 59s\tremaining: 1m 38s\n",
            "464:\tlearn: 60.9970718\ttotal: 1m 59s\tremaining: 1m 38s\n",
            "465:\tlearn: 60.9594743\ttotal: 2m\tremaining: 1m 38s\n",
            "466:\tlearn: 60.9500881\ttotal: 2m\tremaining: 1m 38s\n",
            "467:\tlearn: 60.8329003\ttotal: 2m\tremaining: 1m 37s\n",
            "468:\tlearn: 60.6788887\ttotal: 2m\tremaining: 1m 37s\n",
            "469:\tlearn: 60.5914019\ttotal: 2m 1s\tremaining: 1m 37s\n",
            "470:\tlearn: 60.5893653\ttotal: 2m 1s\tremaining: 1m 37s\n",
            "471:\tlearn: 60.4716189\ttotal: 2m 1s\tremaining: 1m 36s\n",
            "472:\tlearn: 60.4418681\ttotal: 2m 1s\tremaining: 1m 36s\n",
            "473:\tlearn: 60.3003083\ttotal: 2m 2s\tremaining: 1m 36s\n",
            "474:\tlearn: 60.2567501\ttotal: 2m 2s\tremaining: 1m 36s\n",
            "475:\tlearn: 60.1803830\ttotal: 2m 2s\tremaining: 1m 35s\n",
            "476:\tlearn: 60.1543731\ttotal: 2m 3s\tremaining: 1m 35s\n",
            "477:\tlearn: 60.1007568\ttotal: 2m 3s\tremaining: 1m 35s\n",
            "478:\tlearn: 60.0373760\ttotal: 2m 3s\tremaining: 1m 35s\n",
            "479:\tlearn: 60.0108362\ttotal: 2m 3s\tremaining: 1m 34s\n",
            "480:\tlearn: 59.9813280\ttotal: 2m 4s\tremaining: 1m 34s\n",
            "481:\tlearn: 59.8908735\ttotal: 2m 4s\tremaining: 1m 34s\n",
            "482:\tlearn: 59.8626352\ttotal: 2m 4s\tremaining: 1m 34s\n",
            "483:\tlearn: 59.8476935\ttotal: 2m 4s\tremaining: 1m 33s\n",
            "484:\tlearn: 59.8135642\ttotal: 2m 5s\tremaining: 1m 33s\n",
            "485:\tlearn: 59.8001073\ttotal: 2m 5s\tremaining: 1m 33s\n",
            "486:\tlearn: 59.7872936\ttotal: 2m 5s\tremaining: 1m 33s\n",
            "487:\tlearn: 59.6695027\ttotal: 2m 5s\tremaining: 1m 32s\n",
            "488:\tlearn: 59.6306828\ttotal: 2m 6s\tremaining: 1m 32s\n",
            "489:\tlearn: 59.5333002\ttotal: 2m 6s\tremaining: 1m 32s\n",
            "490:\tlearn: 59.4154934\ttotal: 2m 6s\tremaining: 1m 32s\n",
            "491:\tlearn: 59.3687935\ttotal: 2m 6s\tremaining: 1m 31s\n",
            "492:\tlearn: 59.3226186\ttotal: 2m 7s\tremaining: 1m 31s\n",
            "493:\tlearn: 59.2615130\ttotal: 2m 7s\tremaining: 1m 31s\n",
            "494:\tlearn: 59.2019931\ttotal: 2m 7s\tremaining: 1m 31s\n",
            "495:\tlearn: 59.1567785\ttotal: 2m 7s\tremaining: 1m 30s\n",
            "496:\tlearn: 59.1172936\ttotal: 2m 8s\tremaining: 1m 30s\n",
            "497:\tlearn: 59.0600486\ttotal: 2m 8s\tremaining: 1m 30s\n",
            "498:\tlearn: 58.9669864\ttotal: 2m 8s\tremaining: 1m 30s\n",
            "499:\tlearn: 58.9128626\ttotal: 2m 9s\tremaining: 1m 29s\n",
            "500:\tlearn: 58.8027059\ttotal: 2m 9s\tremaining: 1m 29s\n",
            "501:\tlearn: 58.6940260\ttotal: 2m 9s\tremaining: 1m 29s\n",
            "502:\tlearn: 58.6697316\ttotal: 2m 9s\tremaining: 1m 28s\n",
            "503:\tlearn: 58.6359538\ttotal: 2m 9s\tremaining: 1m 28s\n",
            "504:\tlearn: 58.6182323\ttotal: 2m 10s\tremaining: 1m 28s\n",
            "505:\tlearn: 58.5186995\ttotal: 2m 10s\tremaining: 1m 28s\n",
            "506:\tlearn: 58.3845208\ttotal: 2m 10s\tremaining: 1m 27s\n",
            "507:\tlearn: 58.2924758\ttotal: 2m 10s\tremaining: 1m 27s\n",
            "508:\tlearn: 58.2699138\ttotal: 2m 11s\tremaining: 1m 27s\n",
            "509:\tlearn: 58.2012464\ttotal: 2m 11s\tremaining: 1m 27s\n",
            "510:\tlearn: 58.2002396\ttotal: 2m 11s\tremaining: 1m 26s\n",
            "511:\tlearn: 58.1214077\ttotal: 2m 12s\tremaining: 1m 26s\n",
            "512:\tlearn: 58.0691445\ttotal: 2m 12s\tremaining: 1m 26s\n",
            "513:\tlearn: 58.0489098\ttotal: 2m 12s\tremaining: 1m 26s\n",
            "514:\tlearn: 58.0115081\ttotal: 2m 13s\tremaining: 1m 26s\n",
            "515:\tlearn: 57.9671925\ttotal: 2m 13s\tremaining: 1m 25s\n",
            "516:\tlearn: 57.9337824\ttotal: 2m 13s\tremaining: 1m 25s\n",
            "517:\tlearn: 57.8397776\ttotal: 2m 13s\tremaining: 1m 25s\n",
            "518:\tlearn: 57.8208884\ttotal: 2m 14s\tremaining: 1m 25s\n",
            "519:\tlearn: 57.7807999\ttotal: 2m 14s\tremaining: 1m 24s\n",
            "520:\tlearn: 57.7627981\ttotal: 2m 14s\tremaining: 1m 24s\n",
            "521:\tlearn: 57.7203903\ttotal: 2m 14s\tremaining: 1m 24s\n",
            "522:\tlearn: 57.6833714\ttotal: 2m 15s\tremaining: 1m 24s\n",
            "523:\tlearn: 57.5357687\ttotal: 2m 15s\tremaining: 1m 23s\n",
            "524:\tlearn: 57.4947669\ttotal: 2m 15s\tremaining: 1m 23s\n",
            "525:\tlearn: 57.4264263\ttotal: 2m 16s\tremaining: 1m 23s\n",
            "526:\tlearn: 57.4236037\ttotal: 2m 16s\tremaining: 1m 23s\n",
            "527:\tlearn: 57.4066636\ttotal: 2m 16s\tremaining: 1m 22s\n",
            "528:\tlearn: 57.3834427\ttotal: 2m 16s\tremaining: 1m 22s\n",
            "529:\tlearn: 57.3434321\ttotal: 2m 17s\tremaining: 1m 22s\n",
            "530:\tlearn: 57.2336945\ttotal: 2m 17s\tremaining: 1m 22s\n",
            "531:\tlearn: 57.1823187\ttotal: 2m 17s\tremaining: 1m 21s\n",
            "532:\tlearn: 57.1628315\ttotal: 2m 17s\tremaining: 1m 21s\n",
            "533:\tlearn: 57.0448896\ttotal: 2m 18s\tremaining: 1m 21s\n",
            "534:\tlearn: 56.9882264\ttotal: 2m 18s\tremaining: 1m 21s\n",
            "535:\tlearn: 56.9655785\ttotal: 2m 18s\tremaining: 1m 20s\n",
            "536:\tlearn: 56.8839609\ttotal: 2m 19s\tremaining: 1m 20s\n",
            "537:\tlearn: 56.8448567\ttotal: 2m 19s\tremaining: 1m 20s\n",
            "538:\tlearn: 56.7956280\ttotal: 2m 19s\tremaining: 1m 20s\n",
            "539:\tlearn: 56.7865759\ttotal: 2m 19s\tremaining: 1m 19s\n",
            "540:\tlearn: 56.7688346\ttotal: 2m 20s\tremaining: 1m 19s\n",
            "541:\tlearn: 56.7359594\ttotal: 2m 20s\tremaining: 1m 19s\n",
            "542:\tlearn: 56.6929141\ttotal: 2m 20s\tremaining: 1m 19s\n",
            "543:\tlearn: 56.6626149\ttotal: 2m 20s\tremaining: 1m 18s\n",
            "544:\tlearn: 56.6367774\ttotal: 2m 21s\tremaining: 1m 18s\n",
            "545:\tlearn: 56.5478576\ttotal: 2m 21s\tremaining: 1m 18s\n",
            "546:\tlearn: 56.5265801\ttotal: 2m 21s\tremaining: 1m 17s\n",
            "547:\tlearn: 56.4351584\ttotal: 2m 21s\tremaining: 1m 17s\n",
            "548:\tlearn: 56.3520034\ttotal: 2m 22s\tremaining: 1m 17s\n",
            "549:\tlearn: 56.2488172\ttotal: 2m 22s\tremaining: 1m 17s\n",
            "550:\tlearn: 56.2151479\ttotal: 2m 22s\tremaining: 1m 16s\n",
            "551:\tlearn: 56.1358944\ttotal: 2m 22s\tremaining: 1m 16s\n",
            "552:\tlearn: 56.0415641\ttotal: 2m 23s\tremaining: 1m 16s\n",
            "553:\tlearn: 56.0281744\ttotal: 2m 23s\tremaining: 1m 16s\n",
            "554:\tlearn: 56.0131476\ttotal: 2m 23s\tremaining: 1m 15s\n",
            "555:\tlearn: 55.9602968\ttotal: 2m 24s\tremaining: 1m 15s\n",
            "556:\tlearn: 55.8696425\ttotal: 2m 24s\tremaining: 1m 15s\n",
            "557:\tlearn: 55.8343767\ttotal: 2m 24s\tremaining: 1m 15s\n",
            "558:\tlearn: 55.8286703\ttotal: 2m 24s\tremaining: 1m 14s\n",
            "559:\tlearn: 55.7580070\ttotal: 2m 25s\tremaining: 1m 14s\n",
            "560:\tlearn: 55.7305928\ttotal: 2m 25s\tremaining: 1m 14s\n",
            "561:\tlearn: 55.6878320\ttotal: 2m 25s\tremaining: 1m 14s\n",
            "562:\tlearn: 55.6350976\ttotal: 2m 25s\tremaining: 1m 13s\n",
            "563:\tlearn: 55.6010548\ttotal: 2m 26s\tremaining: 1m 13s\n",
            "564:\tlearn: 55.5666216\ttotal: 2m 26s\tremaining: 1m 13s\n",
            "565:\tlearn: 55.4966389\ttotal: 2m 26s\tremaining: 1m 13s\n",
            "566:\tlearn: 55.4805552\ttotal: 2m 26s\tremaining: 1m 12s\n",
            "567:\tlearn: 55.4780113\ttotal: 2m 27s\tremaining: 1m 12s\n",
            "568:\tlearn: 55.4387430\ttotal: 2m 27s\tremaining: 1m 12s\n",
            "569:\tlearn: 55.4374916\ttotal: 2m 27s\tremaining: 1m 12s\n",
            "570:\tlearn: 55.3544887\ttotal: 2m 28s\tremaining: 1m 11s\n",
            "571:\tlearn: 55.3255916\ttotal: 2m 28s\tremaining: 1m 11s\n",
            "572:\tlearn: 55.2489600\ttotal: 2m 28s\tremaining: 1m 11s\n",
            "573:\tlearn: 55.1601173\ttotal: 2m 28s\tremaining: 1m 11s\n",
            "574:\tlearn: 55.1107965\ttotal: 2m 29s\tremaining: 1m 10s\n",
            "575:\tlearn: 55.0489970\ttotal: 2m 29s\tremaining: 1m 10s\n",
            "576:\tlearn: 54.9622374\ttotal: 2m 29s\tremaining: 1m 10s\n",
            "577:\tlearn: 54.9287621\ttotal: 2m 29s\tremaining: 1m 10s\n",
            "578:\tlearn: 54.7867976\ttotal: 2m 30s\tremaining: 1m 9s\n",
            "579:\tlearn: 54.7449016\ttotal: 2m 30s\tremaining: 1m 9s\n",
            "580:\tlearn: 54.6332783\ttotal: 2m 30s\tremaining: 1m 9s\n",
            "581:\tlearn: 54.5341214\ttotal: 2m 30s\tremaining: 1m 8s\n",
            "582:\tlearn: 54.5015623\ttotal: 2m 31s\tremaining: 1m 8s\n",
            "583:\tlearn: 54.4615582\ttotal: 2m 31s\tremaining: 1m 8s\n",
            "584:\tlearn: 54.4191996\ttotal: 2m 31s\tremaining: 1m 8s\n",
            "585:\tlearn: 54.3444684\ttotal: 2m 31s\tremaining: 1m 7s\n",
            "586:\tlearn: 54.3015119\ttotal: 2m 32s\tremaining: 1m 7s\n",
            "587:\tlearn: 54.2935144\ttotal: 2m 32s\tremaining: 1m 7s\n",
            "588:\tlearn: 54.2721397\ttotal: 2m 32s\tremaining: 1m 7s\n",
            "589:\tlearn: 54.2406842\ttotal: 2m 33s\tremaining: 1m 6s\n",
            "590:\tlearn: 54.1778607\ttotal: 2m 33s\tremaining: 1m 6s\n",
            "591:\tlearn: 54.1163357\ttotal: 2m 33s\tremaining: 1m 6s\n",
            "592:\tlearn: 54.0707474\ttotal: 2m 33s\tremaining: 1m 6s\n",
            "593:\tlearn: 54.0319426\ttotal: 2m 34s\tremaining: 1m 5s\n",
            "594:\tlearn: 54.0019274\ttotal: 2m 34s\tremaining: 1m 5s\n",
            "595:\tlearn: 53.9689059\ttotal: 2m 34s\tremaining: 1m 5s\n",
            "596:\tlearn: 53.9239006\ttotal: 2m 34s\tremaining: 1m 5s\n",
            "597:\tlearn: 53.8265103\ttotal: 2m 35s\tremaining: 1m 4s\n",
            "598:\tlearn: 53.7717340\ttotal: 2m 35s\tremaining: 1m 4s\n",
            "599:\tlearn: 53.7063666\ttotal: 2m 35s\tremaining: 1m 4s\n",
            "600:\tlearn: 53.6045415\ttotal: 2m 35s\tremaining: 1m 4s\n",
            "601:\tlearn: 53.5690251\ttotal: 2m 36s\tremaining: 1m 3s\n",
            "602:\tlearn: 53.5488583\ttotal: 2m 36s\tremaining: 1m 3s\n",
            "603:\tlearn: 53.5230026\ttotal: 2m 36s\tremaining: 1m 3s\n",
            "604:\tlearn: 53.4855263\ttotal: 2m 36s\tremaining: 1m 2s\n",
            "605:\tlearn: 53.4435410\ttotal: 2m 37s\tremaining: 1m 2s\n",
            "606:\tlearn: 53.3510359\ttotal: 2m 37s\tremaining: 1m 2s\n",
            "607:\tlearn: 53.3102444\ttotal: 2m 37s\tremaining: 1m 2s\n",
            "608:\tlearn: 53.1567666\ttotal: 2m 37s\tremaining: 1m 1s\n",
            "609:\tlearn: 53.0848004\ttotal: 2m 38s\tremaining: 1m 1s\n",
            "610:\tlearn: 53.0727030\ttotal: 2m 38s\tremaining: 1m 1s\n",
            "611:\tlearn: 53.0531214\ttotal: 2m 38s\tremaining: 1m 1s\n",
            "612:\tlearn: 52.9909758\ttotal: 2m 38s\tremaining: 1m\n",
            "613:\tlearn: 52.9458943\ttotal: 2m 39s\tremaining: 1m\n",
            "614:\tlearn: 52.8965521\ttotal: 2m 39s\tremaining: 1m\n",
            "615:\tlearn: 52.8642885\ttotal: 2m 39s\tremaining: 1m\n",
            "616:\tlearn: 52.8204516\ttotal: 2m 39s\tremaining: 59.9s\n",
            "617:\tlearn: 52.7084308\ttotal: 2m 40s\tremaining: 59.6s\n",
            "618:\tlearn: 52.6531571\ttotal: 2m 40s\tremaining: 59.4s\n",
            "619:\tlearn: 52.5633195\ttotal: 2m 40s\tremaining: 59.1s\n",
            "620:\tlearn: 52.4756511\ttotal: 2m 40s\tremaining: 58.8s\n",
            "621:\tlearn: 52.4080157\ttotal: 2m 41s\tremaining: 58.5s\n",
            "622:\tlearn: 52.3882499\ttotal: 2m 41s\tremaining: 58.3s\n",
            "623:\tlearn: 52.3650570\ttotal: 2m 41s\tremaining: 58s\n",
            "624:\tlearn: 52.3040885\ttotal: 2m 41s\tremaining: 57.8s\n",
            "625:\tlearn: 52.1736390\ttotal: 2m 42s\tremaining: 57.5s\n",
            "626:\tlearn: 52.1217610\ttotal: 2m 42s\tremaining: 57.2s\n",
            "627:\tlearn: 52.0764272\ttotal: 2m 42s\tremaining: 57s\n",
            "628:\tlearn: 52.0450736\ttotal: 2m 42s\tremaining: 56.7s\n",
            "629:\tlearn: 51.9435148\ttotal: 2m 43s\tremaining: 56.5s\n",
            "630:\tlearn: 51.8430780\ttotal: 2m 43s\tremaining: 56.2s\n",
            "631:\tlearn: 51.8164585\ttotal: 2m 43s\tremaining: 56s\n",
            "632:\tlearn: 51.7886501\ttotal: 2m 44s\tremaining: 55.7s\n",
            "633:\tlearn: 51.7536077\ttotal: 2m 44s\tremaining: 55.5s\n",
            "634:\tlearn: 51.6990529\ttotal: 2m 44s\tremaining: 55.2s\n",
            "635:\tlearn: 51.6595880\ttotal: 2m 44s\tremaining: 55s\n",
            "636:\tlearn: 51.6367082\ttotal: 2m 45s\tremaining: 54.7s\n",
            "637:\tlearn: 51.6289088\ttotal: 2m 45s\tremaining: 54.5s\n",
            "638:\tlearn: 51.5466613\ttotal: 2m 45s\tremaining: 54.2s\n",
            "639:\tlearn: 51.5082946\ttotal: 2m 46s\tremaining: 54s\n",
            "640:\tlearn: 51.4073479\ttotal: 2m 46s\tremaining: 53.7s\n",
            "641:\tlearn: 51.3632306\ttotal: 2m 46s\tremaining: 53.5s\n",
            "642:\tlearn: 51.3484529\ttotal: 2m 46s\tremaining: 53.2s\n",
            "643:\tlearn: 51.2851788\ttotal: 2m 47s\tremaining: 52.9s\n",
            "644:\tlearn: 51.2608403\ttotal: 2m 47s\tremaining: 52.7s\n",
            "645:\tlearn: 51.1931702\ttotal: 2m 47s\tremaining: 52.4s\n",
            "646:\tlearn: 51.1585939\ttotal: 2m 47s\tremaining: 52.2s\n",
            "647:\tlearn: 51.1291662\ttotal: 2m 48s\tremaining: 51.9s\n",
            "648:\tlearn: 51.1146005\ttotal: 2m 48s\tremaining: 51.6s\n",
            "649:\tlearn: 51.0798990\ttotal: 2m 48s\tremaining: 51.4s\n",
            "650:\tlearn: 51.0478587\ttotal: 2m 48s\tremaining: 51.1s\n",
            "651:\tlearn: 51.0034885\ttotal: 2m 49s\tremaining: 50.9s\n",
            "652:\tlearn: 50.9861456\ttotal: 2m 49s\tremaining: 50.6s\n",
            "653:\tlearn: 50.9477512\ttotal: 2m 49s\tremaining: 50.3s\n",
            "654:\tlearn: 50.8927077\ttotal: 2m 49s\tremaining: 50.1s\n",
            "655:\tlearn: 50.8317230\ttotal: 2m 50s\tremaining: 49.8s\n",
            "656:\tlearn: 50.8085347\ttotal: 2m 50s\tremaining: 49.6s\n",
            "657:\tlearn: 50.7647953\ttotal: 2m 50s\tremaining: 49.3s\n",
            "658:\tlearn: 50.7202980\ttotal: 2m 51s\tremaining: 49s\n",
            "659:\tlearn: 50.6769754\ttotal: 2m 51s\tremaining: 48.8s\n",
            "660:\tlearn: 50.6103721\ttotal: 2m 51s\tremaining: 48.5s\n",
            "661:\tlearn: 50.5864068\ttotal: 2m 51s\tremaining: 48.2s\n",
            "662:\tlearn: 50.4836168\ttotal: 2m 51s\tremaining: 48s\n",
            "663:\tlearn: 50.4257042\ttotal: 2m 52s\tremaining: 47.7s\n",
            "664:\tlearn: 50.4070569\ttotal: 2m 52s\tremaining: 47.5s\n",
            "665:\tlearn: 50.3528745\ttotal: 2m 52s\tremaining: 47.2s\n",
            "666:\tlearn: 50.3125536\ttotal: 2m 53s\tremaining: 47s\n",
            "667:\tlearn: 50.3093613\ttotal: 2m 53s\tremaining: 46.7s\n",
            "668:\tlearn: 50.2814915\ttotal: 2m 53s\tremaining: 46.4s\n",
            "669:\tlearn: 50.2515158\ttotal: 2m 53s\tremaining: 46.2s\n",
            "670:\tlearn: 50.2195143\ttotal: 2m 54s\tremaining: 45.9s\n",
            "671:\tlearn: 50.1973194\ttotal: 2m 54s\tremaining: 45.7s\n",
            "672:\tlearn: 50.1413403\ttotal: 2m 54s\tremaining: 45.4s\n",
            "673:\tlearn: 50.1132818\ttotal: 2m 54s\tremaining: 45.1s\n",
            "674:\tlearn: 50.0872556\ttotal: 2m 55s\tremaining: 44.9s\n",
            "675:\tlearn: 50.0845186\ttotal: 2m 55s\tremaining: 44.6s\n",
            "676:\tlearn: 50.0377651\ttotal: 2m 55s\tremaining: 44.4s\n",
            "677:\tlearn: 49.9907303\ttotal: 2m 55s\tremaining: 44.1s\n",
            "678:\tlearn: 49.9041092\ttotal: 2m 56s\tremaining: 43.9s\n",
            "679:\tlearn: 49.8461209\ttotal: 2m 56s\tremaining: 43.6s\n",
            "680:\tlearn: 49.7713173\ttotal: 2m 56s\tremaining: 43.3s\n",
            "681:\tlearn: 49.7202379\ttotal: 2m 56s\tremaining: 43.1s\n",
            "682:\tlearn: 49.6663410\ttotal: 2m 57s\tremaining: 42.8s\n",
            "683:\tlearn: 49.5843847\ttotal: 2m 57s\tremaining: 42.5s\n",
            "684:\tlearn: 49.5500697\ttotal: 2m 57s\tremaining: 42.3s\n",
            "685:\tlearn: 49.5054485\ttotal: 2m 57s\tremaining: 42s\n",
            "686:\tlearn: 49.4676560\ttotal: 2m 58s\tremaining: 41.8s\n",
            "687:\tlearn: 49.4545883\ttotal: 2m 58s\tremaining: 41.5s\n",
            "688:\tlearn: 49.4033475\ttotal: 2m 58s\tremaining: 41.2s\n",
            "689:\tlearn: 49.3907375\ttotal: 2m 58s\tremaining: 41s\n",
            "690:\tlearn: 49.3816855\ttotal: 2m 59s\tremaining: 40.7s\n",
            "691:\tlearn: 49.3415080\ttotal: 2m 59s\tremaining: 40.5s\n",
            "692:\tlearn: 49.2916737\ttotal: 2m 59s\tremaining: 40.2s\n",
            "693:\tlearn: 49.2405527\ttotal: 3m\tremaining: 39.9s\n",
            "694:\tlearn: 49.2108665\ttotal: 3m\tremaining: 39.7s\n",
            "695:\tlearn: 49.1807402\ttotal: 3m\tremaining: 39.4s\n",
            "696:\tlearn: 49.1572276\ttotal: 3m\tremaining: 39.2s\n",
            "697:\tlearn: 49.1243311\ttotal: 3m 1s\tremaining: 38.9s\n",
            "698:\tlearn: 49.1075884\ttotal: 3m 1s\tremaining: 38.7s\n",
            "699:\tlearn: 49.0796078\ttotal: 3m 1s\tremaining: 38.4s\n",
            "700:\tlearn: 49.0747026\ttotal: 3m 2s\tremaining: 38.2s\n",
            "701:\tlearn: 49.0033491\ttotal: 3m 2s\tremaining: 37.9s\n",
            "702:\tlearn: 48.9622903\ttotal: 3m 2s\tremaining: 37.7s\n",
            "703:\tlearn: 48.9305419\ttotal: 3m 2s\tremaining: 37.4s\n",
            "704:\tlearn: 48.8791957\ttotal: 3m 3s\tremaining: 37.1s\n",
            "705:\tlearn: 48.8202572\ttotal: 3m 3s\tremaining: 36.9s\n",
            "706:\tlearn: 48.7786263\ttotal: 3m 3s\tremaining: 36.6s\n",
            "707:\tlearn: 48.7466518\ttotal: 3m 4s\tremaining: 36.4s\n",
            "708:\tlearn: 48.7264022\ttotal: 3m 4s\tremaining: 36.1s\n",
            "709:\tlearn: 48.6932954\ttotal: 3m 4s\tremaining: 35.9s\n",
            "710:\tlearn: 48.6486595\ttotal: 3m 4s\tremaining: 35.6s\n",
            "711:\tlearn: 48.6107316\ttotal: 3m 5s\tremaining: 35.4s\n",
            "712:\tlearn: 48.5912687\ttotal: 3m 5s\tremaining: 35.1s\n",
            "713:\tlearn: 48.5322928\ttotal: 3m 5s\tremaining: 34.8s\n",
            "714:\tlearn: 48.5052006\ttotal: 3m 5s\tremaining: 34.6s\n",
            "715:\tlearn: 48.4680958\ttotal: 3m 6s\tremaining: 34.3s\n",
            "716:\tlearn: 48.4336724\ttotal: 3m 6s\tremaining: 34.1s\n",
            "717:\tlearn: 48.3973411\ttotal: 3m 6s\tremaining: 33.8s\n",
            "718:\tlearn: 48.2907403\ttotal: 3m 6s\tremaining: 33.5s\n",
            "719:\tlearn: 48.2895528\ttotal: 3m 7s\tremaining: 33.3s\n",
            "720:\tlearn: 48.2685789\ttotal: 3m 7s\tremaining: 33s\n",
            "721:\tlearn: 48.2617058\ttotal: 3m 7s\tremaining: 32.8s\n",
            "722:\tlearn: 48.2287416\ttotal: 3m 8s\tremaining: 32.5s\n",
            "723:\tlearn: 48.2156339\ttotal: 3m 8s\tremaining: 32.2s\n",
            "724:\tlearn: 48.1911355\ttotal: 3m 8s\tremaining: 32s\n",
            "725:\tlearn: 48.1664614\ttotal: 3m 8s\tremaining: 31.7s\n",
            "726:\tlearn: 48.1436182\ttotal: 3m 9s\tremaining: 31.5s\n",
            "727:\tlearn: 48.0810658\ttotal: 3m 9s\tremaining: 31.2s\n",
            "728:\tlearn: 48.0560743\ttotal: 3m 9s\tremaining: 30.9s\n",
            "729:\tlearn: 48.0326007\ttotal: 3m 9s\tremaining: 30.7s\n",
            "730:\tlearn: 48.0053862\ttotal: 3m 10s\tremaining: 30.4s\n",
            "731:\tlearn: 47.9696472\ttotal: 3m 10s\tremaining: 30.2s\n",
            "732:\tlearn: 47.8683802\ttotal: 3m 10s\tremaining: 29.9s\n",
            "733:\tlearn: 47.8242314\ttotal: 3m 10s\tremaining: 29.6s\n",
            "734:\tlearn: 47.7678051\ttotal: 3m 11s\tremaining: 29.4s\n",
            "735:\tlearn: 47.7574368\ttotal: 3m 11s\tremaining: 29.1s\n",
            "736:\tlearn: 47.6851279\ttotal: 3m 11s\tremaining: 28.8s\n",
            "737:\tlearn: 47.6449881\ttotal: 3m 11s\tremaining: 28.6s\n",
            "738:\tlearn: 47.6165146\ttotal: 3m 12s\tremaining: 28.3s\n",
            "739:\tlearn: 47.5978381\ttotal: 3m 12s\tremaining: 28.1s\n",
            "740:\tlearn: 47.5800710\ttotal: 3m 12s\tremaining: 27.8s\n",
            "741:\tlearn: 47.5625849\ttotal: 3m 12s\tremaining: 27.6s\n",
            "742:\tlearn: 47.5206429\ttotal: 3m 13s\tremaining: 27.3s\n",
            "743:\tlearn: 47.4834577\ttotal: 3m 13s\tremaining: 27s\n",
            "744:\tlearn: 47.4032338\ttotal: 3m 13s\tremaining: 26.8s\n",
            "745:\tlearn: 47.3721363\ttotal: 3m 13s\tremaining: 26.5s\n",
            "746:\tlearn: 47.3434578\ttotal: 3m 14s\tremaining: 26.2s\n",
            "747:\tlearn: 47.3374814\ttotal: 3m 14s\tremaining: 26s\n",
            "748:\tlearn: 47.3122021\ttotal: 3m 14s\tremaining: 25.7s\n",
            "749:\tlearn: 47.2957879\ttotal: 3m 14s\tremaining: 25.5s\n",
            "750:\tlearn: 47.2542844\ttotal: 3m 15s\tremaining: 25.2s\n",
            "751:\tlearn: 47.2237112\ttotal: 3m 15s\tremaining: 24.9s\n",
            "752:\tlearn: 47.2229146\ttotal: 3m 15s\tremaining: 24.7s\n",
            "753:\tlearn: 47.2028253\ttotal: 3m 16s\tremaining: 24.4s\n",
            "754:\tlearn: 47.1493974\ttotal: 3m 16s\tremaining: 24.2s\n",
            "755:\tlearn: 47.1354438\ttotal: 3m 16s\tremaining: 23.9s\n",
            "756:\tlearn: 47.0477667\ttotal: 3m 16s\tremaining: 23.7s\n",
            "757:\tlearn: 47.0459133\ttotal: 3m 17s\tremaining: 23.4s\n",
            "758:\tlearn: 47.0408652\ttotal: 3m 17s\tremaining: 23.1s\n",
            "759:\tlearn: 47.0272363\ttotal: 3m 17s\tremaining: 22.9s\n",
            "760:\tlearn: 47.0159331\ttotal: 3m 17s\tremaining: 22.6s\n",
            "761:\tlearn: 46.9799405\ttotal: 3m 18s\tremaining: 22.4s\n",
            "762:\tlearn: 46.9769458\ttotal: 3m 18s\tremaining: 22.1s\n",
            "763:\tlearn: 46.9638597\ttotal: 3m 18s\tremaining: 21.9s\n",
            "764:\tlearn: 46.9439621\ttotal: 3m 19s\tremaining: 21.6s\n",
            "765:\tlearn: 46.9157145\ttotal: 3m 19s\tremaining: 21.3s\n",
            "766:\tlearn: 46.8946688\ttotal: 3m 19s\tremaining: 21.1s\n",
            "767:\tlearn: 46.8634822\ttotal: 3m 19s\tremaining: 20.8s\n",
            "768:\tlearn: 46.8489216\ttotal: 3m 20s\tremaining: 20.6s\n",
            "769:\tlearn: 46.8277745\ttotal: 3m 20s\tremaining: 20.3s\n",
            "770:\tlearn: 46.8243028\ttotal: 3m 20s\tremaining: 20s\n",
            "771:\tlearn: 46.8126985\ttotal: 3m 20s\tremaining: 19.8s\n",
            "772:\tlearn: 46.8040272\ttotal: 3m 21s\tremaining: 19.5s\n",
            "773:\tlearn: 46.7824172\ttotal: 3m 21s\tremaining: 19.3s\n",
            "774:\tlearn: 46.7539428\ttotal: 3m 21s\tremaining: 19s\n",
            "775:\tlearn: 46.7402605\ttotal: 3m 21s\tremaining: 18.7s\n",
            "776:\tlearn: 46.7237879\ttotal: 3m 22s\tremaining: 18.5s\n",
            "777:\tlearn: 46.6904021\ttotal: 3m 22s\tremaining: 18.2s\n",
            "778:\tlearn: 46.6343653\ttotal: 3m 22s\tremaining: 18s\n",
            "779:\tlearn: 46.5880761\ttotal: 3m 23s\tremaining: 17.7s\n",
            "780:\tlearn: 46.5874882\ttotal: 3m 23s\tremaining: 17.4s\n",
            "781:\tlearn: 46.5454945\ttotal: 3m 23s\tremaining: 17.2s\n",
            "782:\tlearn: 46.5337591\ttotal: 3m 23s\tremaining: 16.9s\n",
            "783:\tlearn: 46.4733775\ttotal: 3m 24s\tremaining: 16.7s\n",
            "784:\tlearn: 46.4347868\ttotal: 3m 24s\tremaining: 16.4s\n",
            "785:\tlearn: 46.3695758\ttotal: 3m 24s\tremaining: 16.1s\n",
            "786:\tlearn: 46.3092805\ttotal: 3m 24s\tremaining: 15.9s\n",
            "787:\tlearn: 46.2742654\ttotal: 3m 25s\tremaining: 15.6s\n",
            "788:\tlearn: 46.2389231\ttotal: 3m 25s\tremaining: 15.4s\n",
            "789:\tlearn: 46.1833842\ttotal: 3m 25s\tremaining: 15.1s\n",
            "790:\tlearn: 46.0739200\ttotal: 3m 25s\tremaining: 14.8s\n",
            "791:\tlearn: 46.0547816\ttotal: 3m 26s\tremaining: 14.6s\n",
            "792:\tlearn: 46.0090139\ttotal: 3m 26s\tremaining: 14.3s\n",
            "793:\tlearn: 45.9930789\ttotal: 3m 26s\tremaining: 14s\n",
            "794:\tlearn: 45.9628049\ttotal: 3m 26s\tremaining: 13.8s\n",
            "795:\tlearn: 45.9462952\ttotal: 3m 27s\tremaining: 13.5s\n",
            "796:\tlearn: 45.9390467\ttotal: 3m 27s\tremaining: 13.3s\n",
            "797:\tlearn: 45.9137245\ttotal: 3m 27s\tremaining: 13s\n",
            "798:\tlearn: 45.8801636\ttotal: 3m 27s\tremaining: 12.8s\n",
            "799:\tlearn: 45.8647529\ttotal: 3m 28s\tremaining: 12.5s\n",
            "800:\tlearn: 45.7923577\ttotal: 3m 28s\tremaining: 12.2s\n",
            "801:\tlearn: 45.7728686\ttotal: 3m 28s\tremaining: 12s\n",
            "802:\tlearn: 45.7438785\ttotal: 3m 28s\tremaining: 11.7s\n",
            "803:\tlearn: 45.7280633\ttotal: 3m 29s\tremaining: 11.4s\n",
            "804:\tlearn: 45.7269589\ttotal: 3m 29s\tremaining: 11.2s\n",
            "805:\tlearn: 45.7013745\ttotal: 3m 29s\tremaining: 10.9s\n",
            "806:\tlearn: 45.6285270\ttotal: 3m 29s\tremaining: 10.7s\n",
            "807:\tlearn: 45.6061501\ttotal: 3m 30s\tremaining: 10.4s\n",
            "808:\tlearn: 45.5884245\ttotal: 3m 30s\tremaining: 10.1s\n",
            "809:\tlearn: 45.5712787\ttotal: 3m 30s\tremaining: 9.89s\n",
            "810:\tlearn: 45.5449691\ttotal: 3m 31s\tremaining: 9.63s\n",
            "811:\tlearn: 45.5186107\ttotal: 3m 31s\tremaining: 9.37s\n",
            "812:\tlearn: 45.4864954\ttotal: 3m 31s\tremaining: 9.11s\n",
            "813:\tlearn: 45.4758756\ttotal: 3m 31s\tremaining: 8.85s\n",
            "814:\tlearn: 45.4208999\ttotal: 3m 32s\tremaining: 8.59s\n",
            "815:\tlearn: 45.4001229\ttotal: 3m 32s\tremaining: 8.33s\n",
            "816:\tlearn: 45.3487219\ttotal: 3m 32s\tremaining: 8.07s\n",
            "817:\tlearn: 45.3485534\ttotal: 3m 32s\tremaining: 7.81s\n",
            "818:\tlearn: 45.3194071\ttotal: 3m 33s\tremaining: 7.55s\n",
            "819:\tlearn: 45.2997254\ttotal: 3m 33s\tremaining: 7.29s\n",
            "820:\tlearn: 45.2809821\ttotal: 3m 33s\tremaining: 7.04s\n",
            "821:\tlearn: 45.2800485\ttotal: 3m 34s\tremaining: 6.78s\n",
            "822:\tlearn: 45.2729382\ttotal: 3m 34s\tremaining: 6.51s\n",
            "823:\tlearn: 45.2339725\ttotal: 3m 34s\tremaining: 6.25s\n",
            "824:\tlearn: 45.2096063\ttotal: 3m 35s\tremaining: 5.99s\n",
            "825:\tlearn: 45.1955533\ttotal: 3m 35s\tremaining: 5.73s\n",
            "826:\tlearn: 45.1388897\ttotal: 3m 35s\tremaining: 5.47s\n",
            "827:\tlearn: 45.1172029\ttotal: 3m 35s\tremaining: 5.21s\n",
            "828:\tlearn: 45.0901902\ttotal: 3m 36s\tremaining: 4.95s\n",
            "829:\tlearn: 45.0774268\ttotal: 3m 36s\tremaining: 4.69s\n",
            "830:\tlearn: 45.0356328\ttotal: 3m 36s\tremaining: 4.43s\n",
            "831:\tlearn: 45.0166803\ttotal: 3m 36s\tremaining: 4.17s\n",
            "832:\tlearn: 44.9779656\ttotal: 3m 37s\tremaining: 3.91s\n",
            "833:\tlearn: 44.9537686\ttotal: 3m 37s\tremaining: 3.65s\n",
            "834:\tlearn: 44.9376432\ttotal: 3m 37s\tremaining: 3.39s\n",
            "835:\tlearn: 44.9018128\ttotal: 3m 38s\tremaining: 3.13s\n",
            "836:\tlearn: 44.8725171\ttotal: 3m 38s\tremaining: 2.87s\n",
            "837:\tlearn: 44.8540458\ttotal: 3m 38s\tremaining: 2.61s\n",
            "838:\tlearn: 44.8392818\ttotal: 3m 38s\tremaining: 2.35s\n",
            "839:\tlearn: 44.7957418\ttotal: 3m 39s\tremaining: 2.09s\n",
            "840:\tlearn: 44.7618188\ttotal: 3m 39s\tremaining: 1.82s\n",
            "841:\tlearn: 44.6735546\ttotal: 3m 39s\tremaining: 1.56s\n",
            "842:\tlearn: 44.6467916\ttotal: 3m 39s\tremaining: 1.3s\n",
            "843:\tlearn: 44.5868838\ttotal: 3m 40s\tremaining: 1.04s\n",
            "844:\tlearn: 44.5623477\ttotal: 3m 40s\tremaining: 782ms\n",
            "845:\tlearn: 44.5440402\ttotal: 3m 40s\tremaining: 522ms\n",
            "846:\tlearn: 44.5082771\ttotal: 3m 40s\tremaining: 261ms\n",
            "847:\tlearn: 44.4911918\ttotal: 3m 41s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 20:47:59,336] A new study created in memory with name: no-name-bd5c5d2d-4fc9-4033-bfaf-60bd2d625eaf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 9...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 20:57:43,340] Trial 0 finished with value: 56.32623115817601 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 56.32623115817601.\n",
            "[I 2023-10-08 21:02:01,693] Trial 1 finished with value: 40.7784785700102 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 40.7784785700102.\n",
            "[I 2023-10-08 21:09:23,209] Trial 2 finished with value: 42.46687009486398 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 40.7784785700102.\n",
            "[I 2023-10-08 21:14:27,502] Trial 3 finished with value: 43.03196180566118 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 40.7784785700102.\n",
            "[I 2023-10-08 21:18:36,265] Trial 4 finished with value: 40.09891636716128 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 21:27:26,151] Trial 5 finished with value: 73.69166788470379 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 21:31:15,589] Trial 6 finished with value: 43.81511073258031 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 21:48:34,361] Trial 7 finished with value: 52.32142946332162 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 21:50:57,630] Trial 8 finished with value: 56.28749203355033 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 21:55:03,967] Trial 9 finished with value: 46.11354588439612 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 21:58:56,233] Trial 10 finished with value: 44.33091472483898 and parameters: {'iterations': 994, 'learning_rate': 0.8097283490059026, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 99.85162236802904, 'subsample': 0.977445433799178, 'random_strength': 38.86027028730574, 'od_wait': 10, 'leaf_estimation_iterations': 1, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.9937641531018804}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 22:02:40,216] Trial 11 finished with value: 42.20682481554645 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 21, 'reg_lambda': 52.01743365000489, 'subsample': 0.5721183515791496, 'random_strength': 10.15982146169808, 'od_wait': 145, 'leaf_estimation_iterations': 4, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.35067266469560193}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 22:07:00,395] Trial 12 finished with value: 51.191115440275716 and parameters: {'iterations': 758, 'learning_rate': 0.983343987565645, 'depth': 8, 'min_data_in_leaf': 3, 'reg_lambda': 46.91007356584483, 'subsample': 0.5140127429398726, 'random_strength': 29.182549248552736, 'od_wait': 108, 'leaf_estimation_iterations': 20, 'bagging_temperature': 24.356631192739087, 'colsample_bylevel': 0.3074183969440657}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 22:13:35,357] Trial 13 finished with value: 50.84679154181299 and parameters: {'iterations': 971, 'learning_rate': 0.7345739305255561, 'depth': 11, 'min_data_in_leaf': 10, 'reg_lambda': 77.21591430401493, 'subsample': 0.4710018087923153, 'random_strength': 67.17789626310656, 'od_wait': 145, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.7353464012961046}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 22:16:40,259] Trial 14 finished with value: 42.42768753885254 and parameters: {'iterations': 725, 'learning_rate': 0.8586986413649368, 'depth': 8, 'min_data_in_leaf': 21, 'reg_lambda': 58.113406993512264, 'subsample': 0.6338482090954157, 'random_strength': 45.00496191936034, 'od_wait': 126, 'leaf_estimation_iterations': 1, 'bagging_temperature': 14.031025575549046, 'colsample_bylevel': 0.5544022353458152}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 22:20:31,191] Trial 15 finished with value: 51.335972446672784 and parameters: {'iterations': 811, 'learning_rate': 0.9945210671429636, 'depth': 10, 'min_data_in_leaf': 6, 'reg_lambda': 63.595495838471635, 'subsample': 0.46082857874135125, 'random_strength': 74.06415650341042, 'od_wait': 98, 'leaf_estimation_iterations': 5, 'bagging_temperature': 43.040289080815256, 'colsample_bylevel': 0.7670727501880628}. Best is trial 4 with value: 40.09891636716128.\n",
            "[I 2023-10-08 22:24:58,925] Trial 16 finished with value: 39.53719776650689 and parameters: {'iterations': 932, 'learning_rate': 0.6707699152504077, 'depth': 7, 'min_data_in_leaf': 15, 'reg_lambda': 30.26423248378008, 'subsample': 0.630224100896121, 'random_strength': 36.46720207402407, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6193404395539011}. Best is trial 16 with value: 39.53719776650689.\n",
            "[I 2023-10-08 22:26:15,859] Trial 17 finished with value: 53.10970523259692 and parameters: {'iterations': 305, 'learning_rate': 0.8455325558544353, 'depth': 7, 'min_data_in_leaf': 1, 'reg_lambda': 40.07580701957684, 'subsample': 0.43840876886682856, 'random_strength': 41.804455906060724, 'od_wait': 81, 'leaf_estimation_iterations': 4, 'bagging_temperature': 95.38524978295371, 'colsample_bylevel': 0.6233897960353694}. Best is trial 16 with value: 39.53719776650689.\n",
            "[I 2023-10-08 22:32:30,327] Trial 18 finished with value: 52.070738221134995 and parameters: {'iterations': 946, 'learning_rate': 0.751729220847763, 'depth': 12, 'min_data_in_leaf': 14, 'reg_lambda': 75.86868408846398, 'subsample': 0.5708673273336423, 'random_strength': 55.82048901072418, 'od_wait': 69, 'leaf_estimation_iterations': 9, 'bagging_temperature': 17.224182126605285, 'colsample_bylevel': 0.800712550834435}. Best is trial 16 with value: 39.53719776650689.\n",
            "[I 2023-10-08 22:36:08,646] Trial 19 finished with value: 41.24684187589003 and parameters: {'iterations': 920, 'learning_rate': 0.6893487254496348, 'depth': 6, 'min_data_in_leaf': 18, 'reg_lambda': 46.99749551383755, 'subsample': 0.6374687462211199, 'random_strength': 34.217175208731206, 'od_wait': 13, 'leaf_estimation_iterations': 6, 'bagging_temperature': 48.60689561818979, 'colsample_bylevel': 0.6818744770955735}. Best is trial 16 with value: 39.53719776650689.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 9: 39.53719776650689\n",
            "Best hyperparameters for fold 9: {'iterations': 932, 'learning_rate': 0.6707699152504077, 'depth': 7, 'min_data_in_leaf': 15, 'reg_lambda': 30.26423248378008, 'subsample': 0.630224100896121, 'random_strength': 36.46720207402407, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6193404395539011}\n",
            "0:\tlearn: 206.4959717\ttotal: 223ms\tremaining: 3m 27s\n",
            "1:\tlearn: 204.2604221\ttotal: 430ms\tremaining: 3m 19s\n",
            "2:\tlearn: 202.0552895\ttotal: 689ms\tremaining: 3m 33s\n",
            "3:\tlearn: 201.0725972\ttotal: 981ms\tremaining: 3m 47s\n",
            "4:\tlearn: 200.9093155\ttotal: 1.11s\tremaining: 3m 26s\n",
            "5:\tlearn: 200.6073513\ttotal: 1.26s\tremaining: 3m 14s\n",
            "6:\tlearn: 200.2180500\ttotal: 1.51s\tremaining: 3m 19s\n",
            "7:\tlearn: 199.6208566\ttotal: 1.75s\tremaining: 3m 22s\n",
            "8:\tlearn: 198.7332833\ttotal: 1.88s\tremaining: 3m 12s\n",
            "9:\tlearn: 198.4334071\ttotal: 2.07s\tremaining: 3m 11s\n",
            "10:\tlearn: 197.8566946\ttotal: 2.29s\tremaining: 3m 11s\n",
            "11:\tlearn: 197.7499968\ttotal: 2.5s\tremaining: 3m 11s\n",
            "12:\tlearn: 197.4639672\ttotal: 2.75s\tremaining: 3m 14s\n",
            "13:\tlearn: 196.7954171\ttotal: 2.99s\tremaining: 3m 16s\n",
            "14:\tlearn: 196.6009946\ttotal: 3.26s\tremaining: 3m 19s\n",
            "15:\tlearn: 196.2211876\ttotal: 3.49s\tremaining: 3m 19s\n",
            "16:\tlearn: 195.9418280\ttotal: 3.6s\tremaining: 3m 13s\n",
            "17:\tlearn: 195.8990413\ttotal: 3.85s\tremaining: 3m 15s\n",
            "18:\tlearn: 195.6694495\ttotal: 4.1s\tremaining: 3m 17s\n",
            "19:\tlearn: 195.6694495\ttotal: 4.17s\tremaining: 3m 10s\n",
            "20:\tlearn: 192.4813581\ttotal: 4.37s\tremaining: 3m 9s\n",
            "21:\tlearn: 191.9031062\ttotal: 4.6s\tremaining: 3m 10s\n",
            "22:\tlearn: 191.7506575\ttotal: 4.83s\tremaining: 3m 10s\n",
            "23:\tlearn: 190.1437853\ttotal: 5.09s\tremaining: 3m 12s\n",
            "24:\tlearn: 188.7933684\ttotal: 5.35s\tremaining: 3m 14s\n",
            "25:\tlearn: 183.9252886\ttotal: 5.58s\tremaining: 3m 14s\n",
            "26:\tlearn: 183.0457752\ttotal: 5.78s\tremaining: 3m 13s\n",
            "27:\tlearn: 181.4976051\ttotal: 6s\tremaining: 3m 13s\n",
            "28:\tlearn: 179.2561375\ttotal: 6.22s\tremaining: 3m 13s\n",
            "29:\tlearn: 176.9560154\ttotal: 6.44s\tremaining: 3m 13s\n",
            "30:\tlearn: 176.3603685\ttotal: 6.67s\tremaining: 3m 13s\n",
            "31:\tlearn: 174.6462844\ttotal: 6.86s\tremaining: 3m 12s\n",
            "32:\tlearn: 173.0268595\ttotal: 7.07s\tremaining: 3m 12s\n",
            "33:\tlearn: 171.8380745\ttotal: 7.28s\tremaining: 3m 12s\n",
            "34:\tlearn: 171.4120058\ttotal: 7.5s\tremaining: 3m 12s\n",
            "35:\tlearn: 171.0815380\ttotal: 7.76s\tremaining: 3m 13s\n",
            "36:\tlearn: 169.4056014\ttotal: 7.96s\tremaining: 3m 12s\n",
            "37:\tlearn: 168.3792923\ttotal: 8.19s\tremaining: 3m 12s\n",
            "38:\tlearn: 167.9153182\ttotal: 8.43s\tremaining: 3m 12s\n",
            "39:\tlearn: 167.5244165\ttotal: 8.64s\tremaining: 3m 12s\n",
            "40:\tlearn: 167.3051833\ttotal: 8.88s\tremaining: 3m 12s\n",
            "41:\tlearn: 166.3731790\ttotal: 9.11s\tremaining: 3m 13s\n",
            "42:\tlearn: 166.1735128\ttotal: 9.35s\tremaining: 3m 13s\n",
            "43:\tlearn: 165.6072133\ttotal: 9.64s\tremaining: 3m 14s\n",
            "44:\tlearn: 164.3102803\ttotal: 9.87s\tremaining: 3m 14s\n",
            "45:\tlearn: 163.2464222\ttotal: 10.1s\tremaining: 3m 14s\n",
            "46:\tlearn: 161.0821645\ttotal: 10.3s\tremaining: 3m 14s\n",
            "47:\tlearn: 160.6279791\ttotal: 10.6s\tremaining: 3m 15s\n",
            "48:\tlearn: 158.6491039\ttotal: 10.8s\tremaining: 3m 15s\n",
            "49:\tlearn: 158.0768669\ttotal: 11.1s\tremaining: 3m 15s\n",
            "50:\tlearn: 156.6586971\ttotal: 11.4s\tremaining: 3m 16s\n",
            "51:\tlearn: 156.2382783\ttotal: 11.6s\tremaining: 3m 16s\n",
            "52:\tlearn: 154.6316923\ttotal: 11.8s\tremaining: 3m 16s\n",
            "53:\tlearn: 154.0635893\ttotal: 12.1s\tremaining: 3m 16s\n",
            "54:\tlearn: 153.0293025\ttotal: 12.3s\tremaining: 3m 16s\n",
            "55:\tlearn: 152.7391216\ttotal: 12.6s\tremaining: 3m 16s\n",
            "56:\tlearn: 151.8549509\ttotal: 12.8s\tremaining: 3m 16s\n",
            "57:\tlearn: 150.7385630\ttotal: 13s\tremaining: 3m 16s\n",
            "58:\tlearn: 149.3329574\ttotal: 13.3s\tremaining: 3m 16s\n",
            "59:\tlearn: 148.3061173\ttotal: 13.5s\tremaining: 3m 16s\n",
            "60:\tlearn: 146.7540403\ttotal: 13.8s\tremaining: 3m 16s\n",
            "61:\tlearn: 146.2394321\ttotal: 14s\tremaining: 3m 16s\n",
            "62:\tlearn: 145.3528641\ttotal: 14.2s\tremaining: 3m 16s\n",
            "63:\tlearn: 145.0161931\ttotal: 14.5s\tremaining: 3m 15s\n",
            "64:\tlearn: 144.7614788\ttotal: 14.7s\tremaining: 3m 16s\n",
            "65:\tlearn: 144.4942774\ttotal: 15s\tremaining: 3m 16s\n",
            "66:\tlearn: 144.1663063\ttotal: 15.2s\tremaining: 3m 16s\n",
            "67:\tlearn: 143.4437503\ttotal: 15.4s\tremaining: 3m 16s\n",
            "68:\tlearn: 143.2873125\ttotal: 15.7s\tremaining: 3m 15s\n",
            "69:\tlearn: 142.5293031\ttotal: 15.9s\tremaining: 3m 15s\n",
            "70:\tlearn: 142.3960413\ttotal: 16.1s\tremaining: 3m 15s\n",
            "71:\tlearn: 141.4338377\ttotal: 16.4s\tremaining: 3m 15s\n",
            "72:\tlearn: 140.0439140\ttotal: 16.6s\tremaining: 3m 15s\n",
            "73:\tlearn: 138.9616676\ttotal: 16.8s\tremaining: 3m 14s\n",
            "74:\tlearn: 138.4798248\ttotal: 17.1s\tremaining: 3m 14s\n",
            "75:\tlearn: 137.9951161\ttotal: 17.3s\tremaining: 3m 14s\n",
            "76:\tlearn: 137.7024819\ttotal: 17.5s\tremaining: 3m 14s\n",
            "77:\tlearn: 137.5728001\ttotal: 17.7s\tremaining: 3m 14s\n",
            "78:\tlearn: 137.4685270\ttotal: 18s\tremaining: 3m 14s\n",
            "79:\tlearn: 136.9104693\ttotal: 18.2s\tremaining: 3m 14s\n",
            "80:\tlearn: 136.8540417\ttotal: 18.5s\tremaining: 3m 14s\n",
            "81:\tlearn: 136.6669756\ttotal: 18.7s\tremaining: 3m 14s\n",
            "82:\tlearn: 136.0094982\ttotal: 18.9s\tremaining: 3m 13s\n",
            "83:\tlearn: 134.4899882\ttotal: 19.1s\tremaining: 3m 13s\n",
            "84:\tlearn: 132.7344976\ttotal: 19.4s\tremaining: 3m 12s\n",
            "85:\tlearn: 131.5063257\ttotal: 19.6s\tremaining: 3m 12s\n",
            "86:\tlearn: 131.2370047\ttotal: 19.8s\tremaining: 3m 12s\n",
            "87:\tlearn: 130.5768441\ttotal: 20s\tremaining: 3m 12s\n",
            "88:\tlearn: 130.4108051\ttotal: 20.2s\tremaining: 3m 11s\n",
            "89:\tlearn: 130.1551315\ttotal: 20.5s\tremaining: 3m 11s\n",
            "90:\tlearn: 129.4446898\ttotal: 20.7s\tremaining: 3m 11s\n",
            "91:\tlearn: 129.2959135\ttotal: 20.9s\tremaining: 3m 10s\n",
            "92:\tlearn: 129.2311003\ttotal: 21.2s\tremaining: 3m 10s\n",
            "93:\tlearn: 129.0704144\ttotal: 21.4s\tremaining: 3m 10s\n",
            "94:\tlearn: 128.4609516\ttotal: 21.6s\tremaining: 3m 10s\n",
            "95:\tlearn: 128.3431059\ttotal: 21.9s\tremaining: 3m 10s\n",
            "96:\tlearn: 128.1764948\ttotal: 22.1s\tremaining: 3m 10s\n",
            "97:\tlearn: 127.3141466\ttotal: 22.3s\tremaining: 3m 9s\n",
            "98:\tlearn: 127.2441361\ttotal: 22.5s\tremaining: 3m 9s\n",
            "99:\tlearn: 126.4326213\ttotal: 22.7s\tremaining: 3m 9s\n",
            "100:\tlearn: 126.2950708\ttotal: 23s\tremaining: 3m 8s\n",
            "101:\tlearn: 125.5762637\ttotal: 23.2s\tremaining: 3m 8s\n",
            "102:\tlearn: 125.0081714\ttotal: 23.4s\tremaining: 3m 8s\n",
            "103:\tlearn: 124.5791300\ttotal: 23.6s\tremaining: 3m 7s\n",
            "104:\tlearn: 124.3612320\ttotal: 23.8s\tremaining: 3m 7s\n",
            "105:\tlearn: 123.7665547\ttotal: 24s\tremaining: 3m 7s\n",
            "106:\tlearn: 123.6629822\ttotal: 24.3s\tremaining: 3m 7s\n",
            "107:\tlearn: 123.5042613\ttotal: 24.5s\tremaining: 3m 7s\n",
            "108:\tlearn: 122.2120653\ttotal: 24.8s\tremaining: 3m 6s\n",
            "109:\tlearn: 121.8510060\ttotal: 25s\tremaining: 3m 6s\n",
            "110:\tlearn: 121.7492687\ttotal: 25.3s\tremaining: 3m 6s\n",
            "111:\tlearn: 121.2686392\ttotal: 25.5s\tremaining: 3m 6s\n",
            "112:\tlearn: 121.1124547\ttotal: 25.7s\tremaining: 3m 6s\n",
            "113:\tlearn: 119.9256927\ttotal: 25.9s\tremaining: 3m 6s\n",
            "114:\tlearn: 119.7309541\ttotal: 26.2s\tremaining: 3m 6s\n",
            "115:\tlearn: 119.5216379\ttotal: 26.5s\tremaining: 3m 6s\n",
            "116:\tlearn: 119.4176967\ttotal: 26.7s\tremaining: 3m 6s\n",
            "117:\tlearn: 119.0200768\ttotal: 27s\tremaining: 3m 5s\n",
            "118:\tlearn: 118.3185036\ttotal: 27.2s\tremaining: 3m 5s\n",
            "119:\tlearn: 118.2398662\ttotal: 27.5s\tremaining: 3m 5s\n",
            "120:\tlearn: 117.7935524\ttotal: 27.7s\tremaining: 3m 5s\n",
            "121:\tlearn: 117.4187569\ttotal: 27.9s\tremaining: 3m 5s\n",
            "122:\tlearn: 116.9248282\ttotal: 28.2s\tremaining: 3m 5s\n",
            "123:\tlearn: 116.6449745\ttotal: 28.4s\tremaining: 3m 5s\n",
            "124:\tlearn: 116.5484642\ttotal: 28.7s\tremaining: 3m 5s\n",
            "125:\tlearn: 115.6366909\ttotal: 28.9s\tremaining: 3m 5s\n",
            "126:\tlearn: 115.4402770\ttotal: 29.2s\tremaining: 3m 5s\n",
            "127:\tlearn: 115.1994115\ttotal: 29.4s\tremaining: 3m 4s\n",
            "128:\tlearn: 115.1059275\ttotal: 29.6s\tremaining: 3m 4s\n",
            "129:\tlearn: 115.0040264\ttotal: 29.9s\tremaining: 3m 4s\n",
            "130:\tlearn: 114.3764225\ttotal: 30.1s\tremaining: 3m 4s\n",
            "131:\tlearn: 112.6118744\ttotal: 30.4s\tremaining: 3m 4s\n",
            "132:\tlearn: 112.4851273\ttotal: 30.6s\tremaining: 3m 4s\n",
            "133:\tlearn: 112.3507856\ttotal: 30.9s\tremaining: 3m 4s\n",
            "134:\tlearn: 111.9951367\ttotal: 31.1s\tremaining: 3m 3s\n",
            "135:\tlearn: 111.7231009\ttotal: 31.4s\tremaining: 3m 3s\n",
            "136:\tlearn: 111.5119040\ttotal: 31.6s\tremaining: 3m 3s\n",
            "137:\tlearn: 111.2920154\ttotal: 31.8s\tremaining: 3m 3s\n",
            "138:\tlearn: 111.1490716\ttotal: 32.1s\tremaining: 3m 2s\n",
            "139:\tlearn: 111.1025820\ttotal: 32.3s\tremaining: 3m 2s\n",
            "140:\tlearn: 110.9132549\ttotal: 32.5s\tremaining: 3m 2s\n",
            "141:\tlearn: 110.2054807\ttotal: 32.8s\tremaining: 3m 2s\n",
            "142:\tlearn: 109.8691481\ttotal: 33s\tremaining: 3m 2s\n",
            "143:\tlearn: 109.5759629\ttotal: 33.2s\tremaining: 3m 1s\n",
            "144:\tlearn: 109.2804488\ttotal: 33.4s\tremaining: 3m 1s\n",
            "145:\tlearn: 109.1476941\ttotal: 33.6s\tremaining: 3m 1s\n",
            "146:\tlearn: 108.8365972\ttotal: 33.9s\tremaining: 3m\n",
            "147:\tlearn: 108.3383640\ttotal: 34.1s\tremaining: 3m\n",
            "148:\tlearn: 108.2749057\ttotal: 34.3s\tremaining: 3m\n",
            "149:\tlearn: 107.9935513\ttotal: 34.6s\tremaining: 3m\n",
            "150:\tlearn: 107.6299322\ttotal: 34.8s\tremaining: 3m\n",
            "151:\tlearn: 107.4418493\ttotal: 35s\tremaining: 2m 59s\n",
            "152:\tlearn: 107.3464869\ttotal: 35.3s\tremaining: 2m 59s\n",
            "153:\tlearn: 107.1827266\ttotal: 35.5s\tremaining: 2m 59s\n",
            "154:\tlearn: 107.1030145\ttotal: 35.7s\tremaining: 2m 58s\n",
            "155:\tlearn: 106.6501102\ttotal: 35.9s\tremaining: 2m 58s\n",
            "156:\tlearn: 106.4142563\ttotal: 36.1s\tremaining: 2m 58s\n",
            "157:\tlearn: 106.1403999\ttotal: 36.3s\tremaining: 2m 57s\n",
            "158:\tlearn: 106.0515846\ttotal: 36.5s\tremaining: 2m 57s\n",
            "159:\tlearn: 105.9922597\ttotal: 36.8s\tremaining: 2m 57s\n",
            "160:\tlearn: 105.7368158\ttotal: 37s\tremaining: 2m 57s\n",
            "161:\tlearn: 105.3272409\ttotal: 37.2s\tremaining: 2m 56s\n",
            "162:\tlearn: 105.1664520\ttotal: 37.4s\tremaining: 2m 56s\n",
            "163:\tlearn: 105.0251554\ttotal: 37.6s\tremaining: 2m 56s\n",
            "164:\tlearn: 104.8739755\ttotal: 37.9s\tremaining: 2m 56s\n",
            "165:\tlearn: 104.3926940\ttotal: 38.1s\tremaining: 2m 55s\n",
            "166:\tlearn: 104.2526261\ttotal: 38.3s\tremaining: 2m 55s\n",
            "167:\tlearn: 103.8583597\ttotal: 38.5s\tremaining: 2m 55s\n",
            "168:\tlearn: 103.5603703\ttotal: 38.7s\tremaining: 2m 54s\n",
            "169:\tlearn: 103.3831644\ttotal: 39s\tremaining: 2m 54s\n",
            "170:\tlearn: 103.2780797\ttotal: 39.2s\tremaining: 2m 54s\n",
            "171:\tlearn: 102.8997016\ttotal: 39.4s\tremaining: 2m 54s\n",
            "172:\tlearn: 102.8990031\ttotal: 39.7s\tremaining: 2m 54s\n",
            "173:\tlearn: 102.7691281\ttotal: 39.9s\tremaining: 2m 53s\n",
            "174:\tlearn: 102.3646500\ttotal: 40.1s\tremaining: 2m 53s\n",
            "175:\tlearn: 101.7589358\ttotal: 40.4s\tremaining: 2m 53s\n",
            "176:\tlearn: 101.4061350\ttotal: 40.6s\tremaining: 2m 53s\n",
            "177:\tlearn: 101.0625866\ttotal: 40.8s\tremaining: 2m 52s\n",
            "178:\tlearn: 100.9028222\ttotal: 41s\tremaining: 2m 52s\n",
            "179:\tlearn: 100.7631064\ttotal: 41.2s\tremaining: 2m 52s\n",
            "180:\tlearn: 100.5984794\ttotal: 41.5s\tremaining: 2m 52s\n",
            "181:\tlearn: 100.5165530\ttotal: 41.7s\tremaining: 2m 51s\n",
            "182:\tlearn: 100.3811435\ttotal: 41.9s\tremaining: 2m 51s\n",
            "183:\tlearn: 100.3500196\ttotal: 42.1s\tremaining: 2m 51s\n",
            "184:\tlearn: 100.2813864\ttotal: 42.4s\tremaining: 2m 51s\n",
            "185:\tlearn: 100.1394411\ttotal: 42.6s\tremaining: 2m 50s\n",
            "186:\tlearn: 99.8466493\ttotal: 42.9s\tremaining: 2m 50s\n",
            "187:\tlearn: 99.7828213\ttotal: 43.2s\tremaining: 2m 50s\n",
            "188:\tlearn: 99.7640732\ttotal: 43.5s\tremaining: 2m 51s\n",
            "189:\tlearn: 99.7265180\ttotal: 43.8s\tremaining: 2m 51s\n",
            "190:\tlearn: 99.6382845\ttotal: 44s\tremaining: 2m 50s\n",
            "191:\tlearn: 99.5893891\ttotal: 44.3s\tremaining: 2m 50s\n",
            "192:\tlearn: 99.2072971\ttotal: 44.6s\tremaining: 2m 50s\n",
            "193:\tlearn: 98.8944926\ttotal: 44.8s\tremaining: 2m 50s\n",
            "194:\tlearn: 98.7476987\ttotal: 45.1s\tremaining: 2m 50s\n",
            "195:\tlearn: 98.4827137\ttotal: 45.3s\tremaining: 2m 50s\n",
            "196:\tlearn: 98.1741669\ttotal: 45.5s\tremaining: 2m 49s\n",
            "197:\tlearn: 97.4228609\ttotal: 45.8s\tremaining: 2m 49s\n",
            "198:\tlearn: 97.3493612\ttotal: 46.1s\tremaining: 2m 49s\n",
            "199:\tlearn: 97.2559838\ttotal: 46.4s\tremaining: 2m 49s\n",
            "200:\tlearn: 97.1249106\ttotal: 46.7s\tremaining: 2m 49s\n",
            "201:\tlearn: 97.1014791\ttotal: 46.9s\tremaining: 2m 49s\n",
            "202:\tlearn: 97.0227179\ttotal: 47.1s\tremaining: 2m 49s\n",
            "203:\tlearn: 96.8545601\ttotal: 47.4s\tremaining: 2m 49s\n",
            "204:\tlearn: 96.6642972\ttotal: 47.6s\tremaining: 2m 48s\n",
            "205:\tlearn: 96.1890526\ttotal: 47.8s\tremaining: 2m 48s\n",
            "206:\tlearn: 96.1497759\ttotal: 48s\tremaining: 2m 48s\n",
            "207:\tlearn: 95.8303821\ttotal: 48.3s\tremaining: 2m 48s\n",
            "208:\tlearn: 95.5635342\ttotal: 48.5s\tremaining: 2m 47s\n",
            "209:\tlearn: 95.5498468\ttotal: 48.8s\tremaining: 2m 47s\n",
            "210:\tlearn: 95.0495926\ttotal: 49s\tremaining: 2m 47s\n",
            "211:\tlearn: 94.8796417\ttotal: 49.2s\tremaining: 2m 47s\n",
            "212:\tlearn: 94.6780753\ttotal: 49.4s\tremaining: 2m 46s\n",
            "213:\tlearn: 94.4876544\ttotal: 49.7s\tremaining: 2m 46s\n",
            "214:\tlearn: 94.4116460\ttotal: 49.9s\tremaining: 2m 46s\n",
            "215:\tlearn: 94.3222796\ttotal: 50.2s\tremaining: 2m 46s\n",
            "216:\tlearn: 94.0411866\ttotal: 50.4s\tremaining: 2m 46s\n",
            "217:\tlearn: 93.7743679\ttotal: 50.7s\tremaining: 2m 45s\n",
            "218:\tlearn: 93.3529364\ttotal: 50.9s\tremaining: 2m 45s\n",
            "219:\tlearn: 93.1516926\ttotal: 51.1s\tremaining: 2m 45s\n",
            "220:\tlearn: 93.0691574\ttotal: 51.4s\tremaining: 2m 45s\n",
            "221:\tlearn: 92.9001815\ttotal: 51.6s\tremaining: 2m 45s\n",
            "222:\tlearn: 92.7386594\ttotal: 51.9s\tremaining: 2m 44s\n",
            "223:\tlearn: 92.5805167\ttotal: 52.1s\tremaining: 2m 44s\n",
            "224:\tlearn: 92.5387303\ttotal: 52.3s\tremaining: 2m 44s\n",
            "225:\tlearn: 92.2755177\ttotal: 52.6s\tremaining: 2m 44s\n",
            "226:\tlearn: 92.0732909\ttotal: 52.8s\tremaining: 2m 43s\n",
            "227:\tlearn: 91.6478594\ttotal: 53s\tremaining: 2m 43s\n",
            "228:\tlearn: 91.5630028\ttotal: 53.3s\tremaining: 2m 43s\n",
            "229:\tlearn: 91.4115054\ttotal: 53.5s\tremaining: 2m 43s\n",
            "230:\tlearn: 91.2779412\ttotal: 53.7s\tremaining: 2m 42s\n",
            "231:\tlearn: 91.0202145\ttotal: 53.9s\tremaining: 2m 42s\n",
            "232:\tlearn: 90.9329865\ttotal: 54.2s\tremaining: 2m 42s\n",
            "233:\tlearn: 90.8464983\ttotal: 54.4s\tremaining: 2m 42s\n",
            "234:\tlearn: 90.5063117\ttotal: 54.6s\tremaining: 2m 42s\n",
            "235:\tlearn: 90.4242511\ttotal: 54.9s\tremaining: 2m 41s\n",
            "236:\tlearn: 90.4089822\ttotal: 55.2s\tremaining: 2m 41s\n",
            "237:\tlearn: 90.4002032\ttotal: 55.4s\tremaining: 2m 41s\n",
            "238:\tlearn: 90.1736498\ttotal: 55.6s\tremaining: 2m 41s\n",
            "239:\tlearn: 89.9564598\ttotal: 55.8s\tremaining: 2m 40s\n",
            "240:\tlearn: 89.6120999\ttotal: 56s\tremaining: 2m 40s\n",
            "241:\tlearn: 89.3637212\ttotal: 56.3s\tremaining: 2m 40s\n",
            "242:\tlearn: 89.2540514\ttotal: 56.5s\tremaining: 2m 40s\n",
            "243:\tlearn: 89.1575259\ttotal: 56.7s\tremaining: 2m 39s\n",
            "244:\tlearn: 88.9682699\ttotal: 56.9s\tremaining: 2m 39s\n",
            "245:\tlearn: 88.9022563\ttotal: 57.1s\tremaining: 2m 39s\n",
            "246:\tlearn: 88.8270623\ttotal: 57.5s\tremaining: 2m 39s\n",
            "247:\tlearn: 88.7290441\ttotal: 57.7s\tremaining: 2m 39s\n",
            "248:\tlearn: 88.4691075\ttotal: 57.9s\tremaining: 2m 38s\n",
            "249:\tlearn: 88.4349501\ttotal: 58.2s\tremaining: 2m 38s\n",
            "250:\tlearn: 88.3023658\ttotal: 58.4s\tremaining: 2m 38s\n",
            "251:\tlearn: 88.1461609\ttotal: 58.6s\tremaining: 2m 38s\n",
            "252:\tlearn: 88.0446721\ttotal: 58.9s\tremaining: 2m 38s\n",
            "253:\tlearn: 87.9596983\ttotal: 59.1s\tremaining: 2m 37s\n",
            "254:\tlearn: 87.7772192\ttotal: 59.4s\tremaining: 2m 37s\n",
            "255:\tlearn: 87.7100486\ttotal: 59.6s\tremaining: 2m 37s\n",
            "256:\tlearn: 87.4011920\ttotal: 59.8s\tremaining: 2m 37s\n",
            "257:\tlearn: 87.1053724\ttotal: 1m\tremaining: 2m 36s\n",
            "258:\tlearn: 86.9271925\ttotal: 1m\tremaining: 2m 36s\n",
            "259:\tlearn: 86.8364164\ttotal: 1m\tremaining: 2m 36s\n",
            "260:\tlearn: 86.6134006\ttotal: 1m\tremaining: 2m 36s\n",
            "261:\tlearn: 86.5746608\ttotal: 1m 1s\tremaining: 2m 36s\n",
            "262:\tlearn: 86.4965171\ttotal: 1m 1s\tremaining: 2m 35s\n",
            "263:\tlearn: 86.4424137\ttotal: 1m 1s\tremaining: 2m 35s\n",
            "264:\tlearn: 86.3830608\ttotal: 1m 1s\tremaining: 2m 35s\n",
            "265:\tlearn: 86.1874921\ttotal: 1m 2s\tremaining: 2m 35s\n",
            "266:\tlearn: 86.1225147\ttotal: 1m 2s\tremaining: 2m 35s\n",
            "267:\tlearn: 85.9409163\ttotal: 1m 2s\tremaining: 2m 34s\n",
            "268:\tlearn: 85.8506959\ttotal: 1m 2s\tremaining: 2m 34s\n",
            "269:\tlearn: 85.4782045\ttotal: 1m 2s\tremaining: 2m 34s\n",
            "270:\tlearn: 85.4163144\ttotal: 1m 3s\tremaining: 2m 34s\n",
            "271:\tlearn: 85.3283900\ttotal: 1m 3s\tremaining: 2m 33s\n",
            "272:\tlearn: 85.2333351\ttotal: 1m 3s\tremaining: 2m 33s\n",
            "273:\tlearn: 85.0411564\ttotal: 1m 3s\tremaining: 2m 33s\n",
            "274:\tlearn: 84.9637124\ttotal: 1m 4s\tremaining: 2m 33s\n",
            "275:\tlearn: 84.9599593\ttotal: 1m 4s\tremaining: 2m 32s\n",
            "276:\tlearn: 84.9586030\ttotal: 1m 4s\tremaining: 2m 32s\n",
            "277:\tlearn: 84.8966831\ttotal: 1m 4s\tremaining: 2m 32s\n",
            "278:\tlearn: 84.7572695\ttotal: 1m 5s\tremaining: 2m 32s\n",
            "279:\tlearn: 84.6134118\ttotal: 1m 5s\tremaining: 2m 31s\n",
            "280:\tlearn: 84.4857393\ttotal: 1m 5s\tremaining: 2m 31s\n",
            "281:\tlearn: 84.2940340\ttotal: 1m 5s\tremaining: 2m 31s\n",
            "282:\tlearn: 84.2365727\ttotal: 1m 5s\tremaining: 2m 31s\n",
            "283:\tlearn: 84.0715942\ttotal: 1m 6s\tremaining: 2m 30s\n",
            "284:\tlearn: 83.9350465\ttotal: 1m 6s\tremaining: 2m 30s\n",
            "285:\tlearn: 83.6934678\ttotal: 1m 6s\tremaining: 2m 30s\n",
            "286:\tlearn: 83.6378480\ttotal: 1m 6s\tremaining: 2m 30s\n",
            "287:\tlearn: 83.5623619\ttotal: 1m 7s\tremaining: 2m 30s\n",
            "288:\tlearn: 83.4817353\ttotal: 1m 7s\tremaining: 2m 29s\n",
            "289:\tlearn: 83.4376820\ttotal: 1m 7s\tremaining: 2m 29s\n",
            "290:\tlearn: 83.3811699\ttotal: 1m 7s\tremaining: 2m 29s\n",
            "291:\tlearn: 83.2858466\ttotal: 1m 8s\tremaining: 2m 29s\n",
            "292:\tlearn: 83.2411443\ttotal: 1m 8s\tremaining: 2m 28s\n",
            "293:\tlearn: 83.2405441\ttotal: 1m 8s\tremaining: 2m 28s\n",
            "294:\tlearn: 82.9287093\ttotal: 1m 8s\tremaining: 2m 28s\n",
            "295:\tlearn: 82.6848217\ttotal: 1m 8s\tremaining: 2m 28s\n",
            "296:\tlearn: 82.5772343\ttotal: 1m 9s\tremaining: 2m 27s\n",
            "297:\tlearn: 82.4700177\ttotal: 1m 9s\tremaining: 2m 27s\n",
            "298:\tlearn: 82.4404133\ttotal: 1m 9s\tremaining: 2m 27s\n",
            "299:\tlearn: 82.3077910\ttotal: 1m 9s\tremaining: 2m 27s\n",
            "300:\tlearn: 82.2706821\ttotal: 1m 10s\tremaining: 2m 26s\n",
            "301:\tlearn: 82.1634162\ttotal: 1m 10s\tremaining: 2m 26s\n",
            "302:\tlearn: 82.1165258\ttotal: 1m 10s\tremaining: 2m 26s\n",
            "303:\tlearn: 82.0586019\ttotal: 1m 10s\tremaining: 2m 26s\n",
            "304:\tlearn: 81.8878792\ttotal: 1m 10s\tremaining: 2m 25s\n",
            "305:\tlearn: 81.8616185\ttotal: 1m 11s\tremaining: 2m 25s\n",
            "306:\tlearn: 81.7496183\ttotal: 1m 11s\tremaining: 2m 25s\n",
            "307:\tlearn: 81.6365833\ttotal: 1m 11s\tremaining: 2m 25s\n",
            "308:\tlearn: 81.3487351\ttotal: 1m 11s\tremaining: 2m 24s\n",
            "309:\tlearn: 81.1184495\ttotal: 1m 12s\tremaining: 2m 24s\n",
            "310:\tlearn: 81.0380683\ttotal: 1m 12s\tremaining: 2m 24s\n",
            "311:\tlearn: 81.0379175\ttotal: 1m 12s\tremaining: 2m 24s\n",
            "312:\tlearn: 80.9151103\ttotal: 1m 12s\tremaining: 2m 23s\n",
            "313:\tlearn: 80.7743359\ttotal: 1m 13s\tremaining: 2m 23s\n",
            "314:\tlearn: 80.7598474\ttotal: 1m 13s\tremaining: 2m 23s\n",
            "315:\tlearn: 80.7275654\ttotal: 1m 13s\tremaining: 2m 23s\n",
            "316:\tlearn: 80.5978637\ttotal: 1m 13s\tremaining: 2m 23s\n",
            "317:\tlearn: 80.4787734\ttotal: 1m 14s\tremaining: 2m 22s\n",
            "318:\tlearn: 80.4205681\ttotal: 1m 14s\tremaining: 2m 22s\n",
            "319:\tlearn: 80.3103680\ttotal: 1m 14s\tremaining: 2m 22s\n",
            "320:\tlearn: 80.1094415\ttotal: 1m 14s\tremaining: 2m 22s\n",
            "321:\tlearn: 80.0411670\ttotal: 1m 15s\tremaining: 2m 22s\n",
            "322:\tlearn: 79.9063083\ttotal: 1m 15s\tremaining: 2m 21s\n",
            "323:\tlearn: 79.8677325\ttotal: 1m 15s\tremaining: 2m 21s\n",
            "324:\tlearn: 79.8500920\ttotal: 1m 15s\tremaining: 2m 21s\n",
            "325:\tlearn: 79.8351704\ttotal: 1m 16s\tremaining: 2m 21s\n",
            "326:\tlearn: 79.6619268\ttotal: 1m 16s\tremaining: 2m 21s\n",
            "327:\tlearn: 79.5682099\ttotal: 1m 16s\tremaining: 2m 20s\n",
            "328:\tlearn: 79.5127384\ttotal: 1m 16s\tremaining: 2m 20s\n",
            "329:\tlearn: 79.3913527\ttotal: 1m 16s\tremaining: 2m 20s\n",
            "330:\tlearn: 79.3228704\ttotal: 1m 17s\tremaining: 2m 20s\n",
            "331:\tlearn: 79.2710292\ttotal: 1m 17s\tremaining: 2m 20s\n",
            "332:\tlearn: 79.1605920\ttotal: 1m 17s\tremaining: 2m 19s\n",
            "333:\tlearn: 78.9959762\ttotal: 1m 18s\tremaining: 2m 19s\n",
            "334:\tlearn: 78.7447313\ttotal: 1m 18s\tremaining: 2m 19s\n",
            "335:\tlearn: 78.6504119\ttotal: 1m 18s\tremaining: 2m 19s\n",
            "336:\tlearn: 78.5631083\ttotal: 1m 18s\tremaining: 2m 19s\n",
            "337:\tlearn: 78.5349412\ttotal: 1m 19s\tremaining: 2m 18s\n",
            "338:\tlearn: 78.4419926\ttotal: 1m 19s\tremaining: 2m 18s\n",
            "339:\tlearn: 78.3952487\ttotal: 1m 19s\tremaining: 2m 18s\n",
            "340:\tlearn: 78.3560509\ttotal: 1m 19s\tremaining: 2m 18s\n",
            "341:\tlearn: 78.3103433\ttotal: 1m 20s\tremaining: 2m 18s\n",
            "342:\tlearn: 78.1244161\ttotal: 1m 20s\tremaining: 2m 17s\n",
            "343:\tlearn: 78.1238123\ttotal: 1m 20s\tremaining: 2m 17s\n",
            "344:\tlearn: 78.0070353\ttotal: 1m 20s\tremaining: 2m 17s\n",
            "345:\tlearn: 77.9584290\ttotal: 1m 20s\tremaining: 2m 17s\n",
            "346:\tlearn: 77.8760390\ttotal: 1m 21s\tremaining: 2m 16s\n",
            "347:\tlearn: 77.8559590\ttotal: 1m 21s\tremaining: 2m 16s\n",
            "348:\tlearn: 77.8376932\ttotal: 1m 21s\tremaining: 2m 16s\n",
            "349:\tlearn: 77.6791982\ttotal: 1m 21s\tremaining: 2m 16s\n",
            "350:\tlearn: 77.6172635\ttotal: 1m 22s\tremaining: 2m 15s\n",
            "351:\tlearn: 77.5746913\ttotal: 1m 22s\tremaining: 2m 15s\n",
            "352:\tlearn: 77.5106053\ttotal: 1m 22s\tremaining: 2m 15s\n",
            "353:\tlearn: 77.4820418\ttotal: 1m 22s\tremaining: 2m 15s\n",
            "354:\tlearn: 77.3710681\ttotal: 1m 23s\tremaining: 2m 15s\n",
            "355:\tlearn: 77.2224095\ttotal: 1m 23s\tremaining: 2m 14s\n",
            "356:\tlearn: 76.9866367\ttotal: 1m 23s\tremaining: 2m 14s\n",
            "357:\tlearn: 76.8938031\ttotal: 1m 23s\tremaining: 2m 14s\n",
            "358:\tlearn: 76.8571315\ttotal: 1m 24s\tremaining: 2m 14s\n",
            "359:\tlearn: 76.8116495\ttotal: 1m 24s\tremaining: 2m 14s\n",
            "360:\tlearn: 76.7447186\ttotal: 1m 24s\tremaining: 2m 13s\n",
            "361:\tlearn: 76.5760473\ttotal: 1m 24s\tremaining: 2m 13s\n",
            "362:\tlearn: 76.5151251\ttotal: 1m 25s\tremaining: 2m 13s\n",
            "363:\tlearn: 76.4592881\ttotal: 1m 25s\tremaining: 2m 13s\n",
            "364:\tlearn: 76.3923303\ttotal: 1m 25s\tremaining: 2m 12s\n",
            "365:\tlearn: 76.3580343\ttotal: 1m 25s\tremaining: 2m 12s\n",
            "366:\tlearn: 76.3124464\ttotal: 1m 25s\tremaining: 2m 12s\n",
            "367:\tlearn: 76.2774932\ttotal: 1m 26s\tremaining: 2m 12s\n",
            "368:\tlearn: 76.2128918\ttotal: 1m 26s\tremaining: 2m 11s\n",
            "369:\tlearn: 76.1801080\ttotal: 1m 26s\tremaining: 2m 11s\n",
            "370:\tlearn: 76.1362578\ttotal: 1m 26s\tremaining: 2m 11s\n",
            "371:\tlearn: 76.1126929\ttotal: 1m 27s\tremaining: 2m 11s\n",
            "372:\tlearn: 76.0130372\ttotal: 1m 27s\tremaining: 2m 11s\n",
            "373:\tlearn: 75.9197197\ttotal: 1m 27s\tremaining: 2m 10s\n",
            "374:\tlearn: 75.9170581\ttotal: 1m 27s\tremaining: 2m 10s\n",
            "375:\tlearn: 75.8693659\ttotal: 1m 28s\tremaining: 2m 10s\n",
            "376:\tlearn: 75.6510441\ttotal: 1m 28s\tremaining: 2m 10s\n",
            "377:\tlearn: 75.4308359\ttotal: 1m 28s\tremaining: 2m 9s\n",
            "378:\tlearn: 75.4018771\ttotal: 1m 28s\tremaining: 2m 9s\n",
            "379:\tlearn: 75.3757626\ttotal: 1m 29s\tremaining: 2m 9s\n",
            "380:\tlearn: 75.2635599\ttotal: 1m 29s\tremaining: 2m 9s\n",
            "381:\tlearn: 75.1156833\ttotal: 1m 29s\tremaining: 2m 8s\n",
            "382:\tlearn: 75.0609821\ttotal: 1m 29s\tremaining: 2m 8s\n",
            "383:\tlearn: 74.8750381\ttotal: 1m 30s\tremaining: 2m 8s\n",
            "384:\tlearn: 74.6835621\ttotal: 1m 30s\tremaining: 2m 8s\n",
            "385:\tlearn: 74.6018261\ttotal: 1m 30s\tremaining: 2m 7s\n",
            "386:\tlearn: 74.4450009\ttotal: 1m 30s\tremaining: 2m 7s\n",
            "387:\tlearn: 74.4068071\ttotal: 1m 31s\tremaining: 2m 7s\n",
            "388:\tlearn: 74.2942544\ttotal: 1m 31s\tremaining: 2m 7s\n",
            "389:\tlearn: 74.1747128\ttotal: 1m 31s\tremaining: 2m 7s\n",
            "390:\tlearn: 74.1375456\ttotal: 1m 31s\tremaining: 2m 6s\n",
            "391:\tlearn: 74.0967916\ttotal: 1m 32s\tremaining: 2m 6s\n",
            "392:\tlearn: 73.9500470\ttotal: 1m 32s\tremaining: 2m 6s\n",
            "393:\tlearn: 73.8742411\ttotal: 1m 32s\tremaining: 2m 6s\n",
            "394:\tlearn: 73.7940600\ttotal: 1m 32s\tremaining: 2m 6s\n",
            "395:\tlearn: 73.7919734\ttotal: 1m 33s\tremaining: 2m 6s\n",
            "396:\tlearn: 73.6365114\ttotal: 1m 33s\tremaining: 2m 5s\n",
            "397:\tlearn: 73.6112664\ttotal: 1m 33s\tremaining: 2m 5s\n",
            "398:\tlearn: 73.5531521\ttotal: 1m 33s\tremaining: 2m 5s\n",
            "399:\tlearn: 73.4088711\ttotal: 1m 34s\tremaining: 2m 5s\n",
            "400:\tlearn: 73.2826312\ttotal: 1m 34s\tremaining: 2m 4s\n",
            "401:\tlearn: 73.1791465\ttotal: 1m 34s\tremaining: 2m 4s\n",
            "402:\tlearn: 72.7220698\ttotal: 1m 34s\tremaining: 2m 4s\n",
            "403:\tlearn: 72.6369079\ttotal: 1m 35s\tremaining: 2m 4s\n",
            "404:\tlearn: 72.5690186\ttotal: 1m 35s\tremaining: 2m 4s\n",
            "405:\tlearn: 72.5320103\ttotal: 1m 35s\tremaining: 2m 3s\n",
            "406:\tlearn: 72.3849931\ttotal: 1m 35s\tremaining: 2m 3s\n",
            "407:\tlearn: 72.3114808\ttotal: 1m 35s\tremaining: 2m 3s\n",
            "408:\tlearn: 72.1724261\ttotal: 1m 36s\tremaining: 2m 2s\n",
            "409:\tlearn: 72.0566383\ttotal: 1m 36s\tremaining: 2m 2s\n",
            "410:\tlearn: 72.0007729\ttotal: 1m 36s\tremaining: 2m 2s\n",
            "411:\tlearn: 71.9478996\ttotal: 1m 36s\tremaining: 2m 2s\n",
            "412:\tlearn: 71.7718725\ttotal: 1m 37s\tremaining: 2m 1s\n",
            "413:\tlearn: 71.7274472\ttotal: 1m 37s\tremaining: 2m 1s\n",
            "414:\tlearn: 71.6510207\ttotal: 1m 37s\tremaining: 2m 1s\n",
            "415:\tlearn: 71.4898159\ttotal: 1m 37s\tremaining: 2m 1s\n",
            "416:\tlearn: 71.4142254\ttotal: 1m 37s\tremaining: 2m\n",
            "417:\tlearn: 71.3485674\ttotal: 1m 38s\tremaining: 2m\n",
            "418:\tlearn: 71.3347107\ttotal: 1m 38s\tremaining: 2m\n",
            "419:\tlearn: 71.3020639\ttotal: 1m 38s\tremaining: 2m\n",
            "420:\tlearn: 71.2290400\ttotal: 1m 38s\tremaining: 2m\n",
            "421:\tlearn: 71.1634121\ttotal: 1m 39s\tremaining: 1m 59s\n",
            "422:\tlearn: 71.1394769\ttotal: 1m 39s\tremaining: 1m 59s\n",
            "423:\tlearn: 71.0806276\ttotal: 1m 39s\tremaining: 1m 59s\n",
            "424:\tlearn: 71.0453995\ttotal: 1m 39s\tremaining: 1m 59s\n",
            "425:\tlearn: 70.9498645\ttotal: 1m 40s\tremaining: 1m 58s\n",
            "426:\tlearn: 70.8199789\ttotal: 1m 40s\tremaining: 1m 58s\n",
            "427:\tlearn: 70.7633286\ttotal: 1m 40s\tremaining: 1m 58s\n",
            "428:\tlearn: 70.6518021\ttotal: 1m 40s\tremaining: 1m 58s\n",
            "429:\tlearn: 70.4949919\ttotal: 1m 40s\tremaining: 1m 57s\n",
            "430:\tlearn: 70.3695584\ttotal: 1m 41s\tremaining: 1m 57s\n",
            "431:\tlearn: 70.2708422\ttotal: 1m 41s\tremaining: 1m 57s\n",
            "432:\tlearn: 70.2487306\ttotal: 1m 41s\tremaining: 1m 57s\n",
            "433:\tlearn: 70.1707120\ttotal: 1m 41s\tremaining: 1m 56s\n",
            "434:\tlearn: 70.0771300\ttotal: 1m 42s\tremaining: 1m 56s\n",
            "435:\tlearn: 69.9783598\ttotal: 1m 42s\tremaining: 1m 56s\n",
            "436:\tlearn: 69.9279247\ttotal: 1m 42s\tremaining: 1m 56s\n",
            "437:\tlearn: 69.8868890\ttotal: 1m 42s\tremaining: 1m 56s\n",
            "438:\tlearn: 69.7419684\ttotal: 1m 43s\tremaining: 1m 55s\n",
            "439:\tlearn: 69.6377322\ttotal: 1m 43s\tremaining: 1m 55s\n",
            "440:\tlearn: 69.5941932\ttotal: 1m 43s\tremaining: 1m 55s\n",
            "441:\tlearn: 69.5402144\ttotal: 1m 43s\tremaining: 1m 55s\n",
            "442:\tlearn: 69.3569558\ttotal: 1m 43s\tremaining: 1m 54s\n",
            "443:\tlearn: 69.3104187\ttotal: 1m 44s\tremaining: 1m 54s\n",
            "444:\tlearn: 69.1894338\ttotal: 1m 44s\tremaining: 1m 54s\n",
            "445:\tlearn: 69.0188730\ttotal: 1m 44s\tremaining: 1m 54s\n",
            "446:\tlearn: 68.9294032\ttotal: 1m 44s\tremaining: 1m 53s\n",
            "447:\tlearn: 68.8802816\ttotal: 1m 45s\tremaining: 1m 53s\n",
            "448:\tlearn: 68.8427734\ttotal: 1m 45s\tremaining: 1m 53s\n",
            "449:\tlearn: 68.7762679\ttotal: 1m 45s\tremaining: 1m 53s\n",
            "450:\tlearn: 68.6280507\ttotal: 1m 45s\tremaining: 1m 52s\n",
            "451:\tlearn: 68.5747221\ttotal: 1m 45s\tremaining: 1m 52s\n",
            "452:\tlearn: 68.5464210\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "453:\tlearn: 68.5000586\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "454:\tlearn: 68.4247251\ttotal: 1m 46s\tremaining: 1m 51s\n",
            "455:\tlearn: 68.3639172\ttotal: 1m 46s\tremaining: 1m 51s\n",
            "456:\tlearn: 68.3464189\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "457:\tlearn: 68.1478553\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "458:\tlearn: 68.0942807\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "459:\tlearn: 68.0894755\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "460:\tlearn: 68.0811014\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "461:\tlearn: 68.0267921\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "462:\tlearn: 67.9193596\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "463:\tlearn: 67.7347507\ttotal: 1m 49s\tremaining: 1m 50s\n",
            "464:\tlearn: 67.6602081\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "465:\tlearn: 67.6368429\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "466:\tlearn: 67.5664237\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "467:\tlearn: 67.5650520\ttotal: 1m 50s\tremaining: 1m 49s\n",
            "468:\tlearn: 67.5326706\ttotal: 1m 50s\tremaining: 1m 49s\n",
            "469:\tlearn: 67.5145389\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "470:\tlearn: 67.3982935\ttotal: 1m 51s\tremaining: 1m 48s\n",
            "471:\tlearn: 67.3571369\ttotal: 1m 51s\tremaining: 1m 48s\n",
            "472:\tlearn: 67.3388188\ttotal: 1m 51s\tremaining: 1m 48s\n",
            "473:\tlearn: 67.2988935\ttotal: 1m 51s\tremaining: 1m 48s\n",
            "474:\tlearn: 67.2855711\ttotal: 1m 52s\tremaining: 1m 47s\n",
            "475:\tlearn: 67.2661375\ttotal: 1m 52s\tremaining: 1m 47s\n",
            "476:\tlearn: 67.2147275\ttotal: 1m 52s\tremaining: 1m 47s\n",
            "477:\tlearn: 67.1164950\ttotal: 1m 52s\tremaining: 1m 47s\n",
            "478:\tlearn: 67.0765390\ttotal: 1m 52s\tremaining: 1m 46s\n",
            "479:\tlearn: 67.0620889\ttotal: 1m 53s\tremaining: 1m 46s\n",
            "480:\tlearn: 66.9440779\ttotal: 1m 53s\tremaining: 1m 46s\n",
            "481:\tlearn: 66.9022296\ttotal: 1m 53s\tremaining: 1m 46s\n",
            "482:\tlearn: 66.8786200\ttotal: 1m 53s\tremaining: 1m 45s\n",
            "483:\tlearn: 66.7179745\ttotal: 1m 54s\tremaining: 1m 45s\n",
            "484:\tlearn: 66.7138405\ttotal: 1m 54s\tremaining: 1m 45s\n",
            "485:\tlearn: 66.6365945\ttotal: 1m 54s\tremaining: 1m 45s\n",
            "486:\tlearn: 66.5560165\ttotal: 1m 54s\tremaining: 1m 44s\n",
            "487:\tlearn: 66.3946842\ttotal: 1m 55s\tremaining: 1m 44s\n",
            "488:\tlearn: 66.2962553\ttotal: 1m 55s\tremaining: 1m 44s\n",
            "489:\tlearn: 66.2526995\ttotal: 1m 55s\tremaining: 1m 44s\n",
            "490:\tlearn: 66.2128548\ttotal: 1m 55s\tremaining: 1m 43s\n",
            "491:\tlearn: 66.1558437\ttotal: 1m 55s\tremaining: 1m 43s\n",
            "492:\tlearn: 66.0238876\ttotal: 1m 56s\tremaining: 1m 43s\n",
            "493:\tlearn: 65.8687335\ttotal: 1m 56s\tremaining: 1m 43s\n",
            "494:\tlearn: 65.8003438\ttotal: 1m 56s\tremaining: 1m 42s\n",
            "495:\tlearn: 65.7570784\ttotal: 1m 56s\tremaining: 1m 42s\n",
            "496:\tlearn: 65.6725396\ttotal: 1m 57s\tremaining: 1m 42s\n",
            "497:\tlearn: 65.5951372\ttotal: 1m 57s\tremaining: 1m 42s\n",
            "498:\tlearn: 65.5558933\ttotal: 1m 57s\tremaining: 1m 42s\n",
            "499:\tlearn: 65.5276664\ttotal: 1m 57s\tremaining: 1m 41s\n",
            "500:\tlearn: 65.3166909\ttotal: 1m 58s\tremaining: 1m 41s\n",
            "501:\tlearn: 65.2411086\ttotal: 1m 58s\tremaining: 1m 41s\n",
            "502:\tlearn: 65.2130457\ttotal: 1m 58s\tremaining: 1m 41s\n",
            "503:\tlearn: 65.2118163\ttotal: 1m 58s\tremaining: 1m 40s\n",
            "504:\tlearn: 65.1321123\ttotal: 1m 59s\tremaining: 1m 40s\n",
            "505:\tlearn: 65.0216014\ttotal: 1m 59s\tremaining: 1m 40s\n",
            "506:\tlearn: 64.8663926\ttotal: 1m 59s\tremaining: 1m 40s\n",
            "507:\tlearn: 64.8418008\ttotal: 1m 59s\tremaining: 1m 39s\n",
            "508:\tlearn: 64.8166845\ttotal: 2m\tremaining: 1m 39s\n",
            "509:\tlearn: 64.6957957\ttotal: 2m\tremaining: 1m 39s\n",
            "510:\tlearn: 64.6602088\ttotal: 2m\tremaining: 1m 39s\n",
            "511:\tlearn: 64.5754237\ttotal: 2m\tremaining: 1m 38s\n",
            "512:\tlearn: 64.5402606\ttotal: 2m\tremaining: 1m 38s\n",
            "513:\tlearn: 64.4516867\ttotal: 2m 1s\tremaining: 1m 38s\n",
            "514:\tlearn: 64.4441452\ttotal: 2m 1s\tremaining: 1m 38s\n",
            "515:\tlearn: 64.4064216\ttotal: 2m 1s\tremaining: 1m 38s\n",
            "516:\tlearn: 64.2457884\ttotal: 2m 1s\tremaining: 1m 37s\n",
            "517:\tlearn: 64.1841144\ttotal: 2m 2s\tremaining: 1m 37s\n",
            "518:\tlearn: 64.1027532\ttotal: 2m 2s\tremaining: 1m 37s\n",
            "519:\tlearn: 64.0429730\ttotal: 2m 2s\tremaining: 1m 37s\n",
            "520:\tlearn: 63.9848156\ttotal: 2m 2s\tremaining: 1m 36s\n",
            "521:\tlearn: 63.9404146\ttotal: 2m 3s\tremaining: 1m 36s\n",
            "522:\tlearn: 63.8139255\ttotal: 2m 3s\tremaining: 1m 36s\n",
            "523:\tlearn: 63.7843087\ttotal: 2m 3s\tremaining: 1m 36s\n",
            "524:\tlearn: 63.7529696\ttotal: 2m 3s\tremaining: 1m 35s\n",
            "525:\tlearn: 63.7225091\ttotal: 2m 4s\tremaining: 1m 35s\n",
            "526:\tlearn: 63.6878651\ttotal: 2m 4s\tremaining: 1m 35s\n",
            "527:\tlearn: 63.6286968\ttotal: 2m 4s\tremaining: 1m 35s\n",
            "528:\tlearn: 63.6246120\ttotal: 2m 4s\tremaining: 1m 35s\n",
            "529:\tlearn: 63.5940498\ttotal: 2m 5s\tremaining: 1m 34s\n",
            "530:\tlearn: 63.4662903\ttotal: 2m 5s\tremaining: 1m 34s\n",
            "531:\tlearn: 63.4110026\ttotal: 2m 5s\tremaining: 1m 34s\n",
            "532:\tlearn: 63.3728143\ttotal: 2m 5s\tremaining: 1m 34s\n",
            "533:\tlearn: 63.3628060\ttotal: 2m 6s\tremaining: 1m 33s\n",
            "534:\tlearn: 63.3088338\ttotal: 2m 6s\tremaining: 1m 33s\n",
            "535:\tlearn: 63.1364483\ttotal: 2m 6s\tremaining: 1m 33s\n",
            "536:\tlearn: 63.0403918\ttotal: 2m 6s\tremaining: 1m 33s\n",
            "537:\tlearn: 62.9866307\ttotal: 2m 7s\tremaining: 1m 33s\n",
            "538:\tlearn: 62.9756554\ttotal: 2m 7s\tremaining: 1m 32s\n",
            "539:\tlearn: 62.9652924\ttotal: 2m 7s\tremaining: 1m 32s\n",
            "540:\tlearn: 62.9357161\ttotal: 2m 7s\tremaining: 1m 32s\n",
            "541:\tlearn: 62.9043308\ttotal: 2m 8s\tremaining: 1m 32s\n",
            "542:\tlearn: 62.8489578\ttotal: 2m 8s\tremaining: 1m 31s\n",
            "543:\tlearn: 62.8316497\ttotal: 2m 8s\tremaining: 1m 31s\n",
            "544:\tlearn: 62.7996158\ttotal: 2m 8s\tremaining: 1m 31s\n",
            "545:\tlearn: 62.7799096\ttotal: 2m 8s\tremaining: 1m 31s\n",
            "546:\tlearn: 62.6697814\ttotal: 2m 9s\tremaining: 1m 30s\n",
            "547:\tlearn: 62.6199862\ttotal: 2m 9s\tremaining: 1m 30s\n",
            "548:\tlearn: 62.6013780\ttotal: 2m 9s\tremaining: 1m 30s\n",
            "549:\tlearn: 62.5023014\ttotal: 2m 9s\tremaining: 1m 30s\n",
            "550:\tlearn: 62.4739802\ttotal: 2m 10s\tremaining: 1m 29s\n",
            "551:\tlearn: 62.4510615\ttotal: 2m 10s\tremaining: 1m 29s\n",
            "552:\tlearn: 62.4191108\ttotal: 2m 10s\tremaining: 1m 29s\n",
            "553:\tlearn: 62.4055172\ttotal: 2m 10s\tremaining: 1m 29s\n",
            "554:\tlearn: 62.3866216\ttotal: 2m 11s\tremaining: 1m 29s\n",
            "555:\tlearn: 62.3641176\ttotal: 2m 11s\tremaining: 1m 28s\n",
            "556:\tlearn: 62.3199402\ttotal: 2m 11s\tremaining: 1m 28s\n",
            "557:\tlearn: 62.2765141\ttotal: 2m 11s\tremaining: 1m 28s\n",
            "558:\tlearn: 62.2328959\ttotal: 2m 12s\tremaining: 1m 28s\n",
            "559:\tlearn: 62.1927997\ttotal: 2m 12s\tremaining: 1m 27s\n",
            "560:\tlearn: 62.1868608\ttotal: 2m 12s\tremaining: 1m 27s\n",
            "561:\tlearn: 62.1262467\ttotal: 2m 12s\tremaining: 1m 27s\n",
            "562:\tlearn: 62.0438670\ttotal: 2m 13s\tremaining: 1m 27s\n",
            "563:\tlearn: 62.0299977\ttotal: 2m 13s\tremaining: 1m 26s\n",
            "564:\tlearn: 61.9640759\ttotal: 2m 13s\tremaining: 1m 26s\n",
            "565:\tlearn: 61.9535434\ttotal: 2m 13s\tremaining: 1m 26s\n",
            "566:\tlearn: 61.9002774\ttotal: 2m 13s\tremaining: 1m 26s\n",
            "567:\tlearn: 61.8454501\ttotal: 2m 14s\tremaining: 1m 25s\n",
            "568:\tlearn: 61.8234591\ttotal: 2m 14s\tremaining: 1m 25s\n",
            "569:\tlearn: 61.8058518\ttotal: 2m 14s\tremaining: 1m 25s\n",
            "570:\tlearn: 61.7718055\ttotal: 2m 14s\tremaining: 1m 25s\n",
            "571:\tlearn: 61.6509262\ttotal: 2m 15s\tremaining: 1m 25s\n",
            "572:\tlearn: 61.5358103\ttotal: 2m 15s\tremaining: 1m 24s\n",
            "573:\tlearn: 61.4543657\ttotal: 2m 15s\tremaining: 1m 24s\n",
            "574:\tlearn: 61.4302374\ttotal: 2m 15s\tremaining: 1m 24s\n",
            "575:\tlearn: 61.3966866\ttotal: 2m 16s\tremaining: 1m 24s\n",
            "576:\tlearn: 61.3539863\ttotal: 2m 16s\tremaining: 1m 23s\n",
            "577:\tlearn: 61.3297608\ttotal: 2m 16s\tremaining: 1m 23s\n",
            "578:\tlearn: 61.3198451\ttotal: 2m 16s\tremaining: 1m 23s\n",
            "579:\tlearn: 61.2287313\ttotal: 2m 17s\tremaining: 1m 23s\n",
            "580:\tlearn: 61.2046587\ttotal: 2m 17s\tremaining: 1m 22s\n",
            "581:\tlearn: 61.1951018\ttotal: 2m 17s\tremaining: 1m 22s\n",
            "582:\tlearn: 61.1731374\ttotal: 2m 17s\tremaining: 1m 22s\n",
            "583:\tlearn: 61.1133586\ttotal: 2m 17s\tremaining: 1m 22s\n",
            "584:\tlearn: 61.0964932\ttotal: 2m 18s\tremaining: 1m 21s\n",
            "585:\tlearn: 61.0702953\ttotal: 2m 18s\tremaining: 1m 21s\n",
            "586:\tlearn: 61.0423706\ttotal: 2m 18s\tremaining: 1m 21s\n",
            "587:\tlearn: 60.9496353\ttotal: 2m 18s\tremaining: 1m 21s\n",
            "588:\tlearn: 60.8776062\ttotal: 2m 19s\tremaining: 1m 21s\n",
            "589:\tlearn: 60.8388098\ttotal: 2m 19s\tremaining: 1m 20s\n",
            "590:\tlearn: 60.8319767\ttotal: 2m 19s\tremaining: 1m 20s\n",
            "591:\tlearn: 60.8275099\ttotal: 2m 20s\tremaining: 1m 20s\n",
            "592:\tlearn: 60.7886308\ttotal: 2m 20s\tremaining: 1m 20s\n",
            "593:\tlearn: 60.6874444\ttotal: 2m 20s\tremaining: 1m 19s\n",
            "594:\tlearn: 60.4359697\ttotal: 2m 20s\tremaining: 1m 19s\n",
            "595:\tlearn: 60.3669531\ttotal: 2m 21s\tremaining: 1m 19s\n",
            "596:\tlearn: 60.3328796\ttotal: 2m 21s\tremaining: 1m 19s\n",
            "597:\tlearn: 60.2863058\ttotal: 2m 21s\tremaining: 1m 19s\n",
            "598:\tlearn: 60.2460390\ttotal: 2m 21s\tremaining: 1m 18s\n",
            "599:\tlearn: 60.2373174\ttotal: 2m 22s\tremaining: 1m 18s\n",
            "600:\tlearn: 60.2315138\ttotal: 2m 22s\tremaining: 1m 18s\n",
            "601:\tlearn: 60.1957290\ttotal: 2m 22s\tremaining: 1m 18s\n",
            "602:\tlearn: 60.1445059\ttotal: 2m 22s\tremaining: 1m 17s\n",
            "603:\tlearn: 60.0687912\ttotal: 2m 23s\tremaining: 1m 17s\n",
            "604:\tlearn: 60.0598976\ttotal: 2m 23s\tremaining: 1m 17s\n",
            "605:\tlearn: 60.0247335\ttotal: 2m 23s\tremaining: 1m 17s\n",
            "606:\tlearn: 59.9740622\ttotal: 2m 23s\tremaining: 1m 16s\n",
            "607:\tlearn: 59.9445395\ttotal: 2m 23s\tremaining: 1m 16s\n",
            "608:\tlearn: 59.9041948\ttotal: 2m 24s\tremaining: 1m 16s\n",
            "609:\tlearn: 59.8803544\ttotal: 2m 24s\tremaining: 1m 16s\n",
            "610:\tlearn: 59.8369397\ttotal: 2m 24s\tremaining: 1m 16s\n",
            "611:\tlearn: 59.8237327\ttotal: 2m 24s\tremaining: 1m 15s\n",
            "612:\tlearn: 59.7630660\ttotal: 2m 25s\tremaining: 1m 15s\n",
            "613:\tlearn: 59.7330353\ttotal: 2m 25s\tremaining: 1m 15s\n",
            "614:\tlearn: 59.6564249\ttotal: 2m 25s\tremaining: 1m 15s\n",
            "615:\tlearn: 59.6275203\ttotal: 2m 25s\tremaining: 1m 14s\n",
            "616:\tlearn: 59.6027334\ttotal: 2m 26s\tremaining: 1m 14s\n",
            "617:\tlearn: 59.5889647\ttotal: 2m 26s\tremaining: 1m 14s\n",
            "618:\tlearn: 59.5652851\ttotal: 2m 26s\tremaining: 1m 14s\n",
            "619:\tlearn: 59.5130320\ttotal: 2m 26s\tremaining: 1m 13s\n",
            "620:\tlearn: 59.4917968\ttotal: 2m 27s\tremaining: 1m 13s\n",
            "621:\tlearn: 59.4713453\ttotal: 2m 27s\tremaining: 1m 13s\n",
            "622:\tlearn: 59.4470546\ttotal: 2m 27s\tremaining: 1m 13s\n",
            "623:\tlearn: 59.4277776\ttotal: 2m 27s\tremaining: 1m 12s\n",
            "624:\tlearn: 59.4013780\ttotal: 2m 28s\tremaining: 1m 12s\n",
            "625:\tlearn: 59.3711933\ttotal: 2m 28s\tremaining: 1m 12s\n",
            "626:\tlearn: 59.2818885\ttotal: 2m 28s\tremaining: 1m 12s\n",
            "627:\tlearn: 59.2744115\ttotal: 2m 28s\tremaining: 1m 12s\n",
            "628:\tlearn: 59.2200808\ttotal: 2m 29s\tremaining: 1m 11s\n",
            "629:\tlearn: 59.1975673\ttotal: 2m 29s\tremaining: 1m 11s\n",
            "630:\tlearn: 59.1763691\ttotal: 2m 29s\tremaining: 1m 11s\n",
            "631:\tlearn: 59.1298006\ttotal: 2m 29s\tremaining: 1m 11s\n",
            "632:\tlearn: 59.1039556\ttotal: 2m 30s\tremaining: 1m 10s\n",
            "633:\tlearn: 59.0386796\ttotal: 2m 30s\tremaining: 1m 10s\n",
            "634:\tlearn: 58.8995572\ttotal: 2m 30s\tremaining: 1m 10s\n",
            "635:\tlearn: 58.8489046\ttotal: 2m 30s\tremaining: 1m 10s\n",
            "636:\tlearn: 58.8305853\ttotal: 2m 30s\tremaining: 1m 9s\n",
            "637:\tlearn: 58.8106341\ttotal: 2m 31s\tremaining: 1m 9s\n",
            "638:\tlearn: 58.7592591\ttotal: 2m 31s\tremaining: 1m 9s\n",
            "639:\tlearn: 58.7127399\ttotal: 2m 31s\tremaining: 1m 9s\n",
            "640:\tlearn: 58.6887077\ttotal: 2m 31s\tremaining: 1m 8s\n",
            "641:\tlearn: 58.6488609\ttotal: 2m 32s\tremaining: 1m 8s\n",
            "642:\tlearn: 58.5857546\ttotal: 2m 32s\tremaining: 1m 8s\n",
            "643:\tlearn: 58.4729426\ttotal: 2m 32s\tremaining: 1m 8s\n",
            "644:\tlearn: 58.4395187\ttotal: 2m 32s\tremaining: 1m 7s\n",
            "645:\tlearn: 58.3383325\ttotal: 2m 33s\tremaining: 1m 7s\n",
            "646:\tlearn: 58.2672503\ttotal: 2m 33s\tremaining: 1m 7s\n",
            "647:\tlearn: 58.1811806\ttotal: 2m 33s\tremaining: 1m 7s\n",
            "648:\tlearn: 58.1424531\ttotal: 2m 33s\tremaining: 1m 7s\n",
            "649:\tlearn: 58.1175744\ttotal: 2m 33s\tremaining: 1m 6s\n",
            "650:\tlearn: 58.0989633\ttotal: 2m 34s\tremaining: 1m 6s\n",
            "651:\tlearn: 58.0300039\ttotal: 2m 34s\tremaining: 1m 6s\n",
            "652:\tlearn: 58.0089884\ttotal: 2m 34s\tremaining: 1m 6s\n",
            "653:\tlearn: 57.9858372\ttotal: 2m 34s\tremaining: 1m 5s\n",
            "654:\tlearn: 57.9589102\ttotal: 2m 35s\tremaining: 1m 5s\n",
            "655:\tlearn: 57.9060199\ttotal: 2m 35s\tremaining: 1m 5s\n",
            "656:\tlearn: 57.8666322\ttotal: 2m 35s\tremaining: 1m 5s\n",
            "657:\tlearn: 57.8431554\ttotal: 2m 35s\tremaining: 1m 4s\n",
            "658:\tlearn: 57.8224502\ttotal: 2m 36s\tremaining: 1m 4s\n",
            "659:\tlearn: 57.7862281\ttotal: 2m 36s\tremaining: 1m 4s\n",
            "660:\tlearn: 57.7713499\ttotal: 2m 36s\tremaining: 1m 4s\n",
            "661:\tlearn: 57.7190457\ttotal: 2m 36s\tremaining: 1m 4s\n",
            "662:\tlearn: 57.7131395\ttotal: 2m 37s\tremaining: 1m 3s\n",
            "663:\tlearn: 57.6850815\ttotal: 2m 37s\tremaining: 1m 3s\n",
            "664:\tlearn: 57.5928088\ttotal: 2m 37s\tremaining: 1m 3s\n",
            "665:\tlearn: 57.5787503\ttotal: 2m 38s\tremaining: 1m 3s\n",
            "666:\tlearn: 57.5619590\ttotal: 2m 38s\tremaining: 1m 2s\n",
            "667:\tlearn: 57.4870505\ttotal: 2m 38s\tremaining: 1m 2s\n",
            "668:\tlearn: 57.3897818\ttotal: 2m 38s\tremaining: 1m 2s\n",
            "669:\tlearn: 57.3629686\ttotal: 2m 38s\tremaining: 1m 2s\n",
            "670:\tlearn: 57.2652288\ttotal: 2m 39s\tremaining: 1m 1s\n",
            "671:\tlearn: 57.2003130\ttotal: 2m 39s\tremaining: 1m 1s\n",
            "672:\tlearn: 57.1814162\ttotal: 2m 39s\tremaining: 1m 1s\n",
            "673:\tlearn: 57.1605912\ttotal: 2m 39s\tremaining: 1m 1s\n",
            "674:\tlearn: 57.1327870\ttotal: 2m 40s\tremaining: 1m\n",
            "675:\tlearn: 57.1151802\ttotal: 2m 40s\tremaining: 1m\n",
            "676:\tlearn: 57.0684918\ttotal: 2m 40s\tremaining: 1m\n",
            "677:\tlearn: 57.0612599\ttotal: 2m 40s\tremaining: 1m\n",
            "678:\tlearn: 57.0221391\ttotal: 2m 41s\tremaining: 1m\n",
            "679:\tlearn: 56.9874649\ttotal: 2m 41s\tremaining: 59.8s\n",
            "680:\tlearn: 56.9183420\ttotal: 2m 41s\tremaining: 59.5s\n",
            "681:\tlearn: 56.8773126\ttotal: 2m 41s\tremaining: 59.3s\n",
            "682:\tlearn: 56.8489939\ttotal: 2m 41s\tremaining: 59.1s\n",
            "683:\tlearn: 56.8142929\ttotal: 2m 42s\tremaining: 58.8s\n",
            "684:\tlearn: 56.7972434\ttotal: 2m 42s\tremaining: 58.6s\n",
            "685:\tlearn: 56.7929208\ttotal: 2m 42s\tremaining: 58.3s\n",
            "686:\tlearn: 56.7634988\ttotal: 2m 43s\tremaining: 58.1s\n",
            "687:\tlearn: 56.7588607\ttotal: 2m 43s\tremaining: 57.9s\n",
            "688:\tlearn: 56.7105062\ttotal: 2m 43s\tremaining: 57.7s\n",
            "689:\tlearn: 56.6664311\ttotal: 2m 43s\tremaining: 57.4s\n",
            "690:\tlearn: 56.6254513\ttotal: 2m 43s\tremaining: 57.2s\n",
            "691:\tlearn: 56.5957836\ttotal: 2m 44s\tremaining: 57s\n",
            "692:\tlearn: 56.5623611\ttotal: 2m 44s\tremaining: 56.7s\n",
            "693:\tlearn: 56.4862707\ttotal: 2m 44s\tremaining: 56.5s\n",
            "694:\tlearn: 56.4825782\ttotal: 2m 44s\tremaining: 56.3s\n",
            "695:\tlearn: 56.4532856\ttotal: 2m 45s\tremaining: 56s\n",
            "696:\tlearn: 56.4359398\ttotal: 2m 45s\tremaining: 55.8s\n",
            "697:\tlearn: 56.3426816\ttotal: 2m 45s\tremaining: 55.6s\n",
            "698:\tlearn: 56.2648655\ttotal: 2m 45s\tremaining: 55.3s\n",
            "699:\tlearn: 56.2085377\ttotal: 2m 46s\tremaining: 55.1s\n",
            "700:\tlearn: 56.1828424\ttotal: 2m 46s\tremaining: 54.8s\n",
            "701:\tlearn: 56.1542936\ttotal: 2m 46s\tremaining: 54.6s\n",
            "702:\tlearn: 56.0970193\ttotal: 2m 46s\tremaining: 54.4s\n",
            "703:\tlearn: 56.0864974\ttotal: 2m 47s\tremaining: 54.2s\n",
            "704:\tlearn: 56.0254563\ttotal: 2m 47s\tremaining: 53.9s\n",
            "705:\tlearn: 55.9861955\ttotal: 2m 47s\tremaining: 53.7s\n",
            "706:\tlearn: 55.9571580\ttotal: 2m 47s\tremaining: 53.4s\n",
            "707:\tlearn: 55.8817215\ttotal: 2m 48s\tremaining: 53.2s\n",
            "708:\tlearn: 55.8640625\ttotal: 2m 48s\tremaining: 53s\n",
            "709:\tlearn: 55.8216929\ttotal: 2m 48s\tremaining: 52.7s\n",
            "710:\tlearn: 55.8121131\ttotal: 2m 48s\tremaining: 52.5s\n",
            "711:\tlearn: 55.7629030\ttotal: 2m 49s\tremaining: 52.2s\n",
            "712:\tlearn: 55.7314480\ttotal: 2m 49s\tremaining: 52s\n",
            "713:\tlearn: 55.6707289\ttotal: 2m 49s\tremaining: 51.7s\n",
            "714:\tlearn: 55.6091066\ttotal: 2m 49s\tremaining: 51.5s\n",
            "715:\tlearn: 55.5764209\ttotal: 2m 50s\tremaining: 51.3s\n",
            "716:\tlearn: 55.5626682\ttotal: 2m 50s\tremaining: 51.1s\n",
            "717:\tlearn: 55.5520417\ttotal: 2m 50s\tremaining: 50.9s\n",
            "718:\tlearn: 55.5261294\ttotal: 2m 50s\tremaining: 50.6s\n",
            "719:\tlearn: 55.4298519\ttotal: 2m 51s\tremaining: 50.4s\n",
            "720:\tlearn: 55.3571924\ttotal: 2m 51s\tremaining: 50.2s\n",
            "721:\tlearn: 55.2749869\ttotal: 2m 51s\tremaining: 49.9s\n",
            "722:\tlearn: 55.2414260\ttotal: 2m 51s\tremaining: 49.7s\n",
            "723:\tlearn: 55.2061196\ttotal: 2m 52s\tremaining: 49.5s\n",
            "724:\tlearn: 55.1794515\ttotal: 2m 52s\tremaining: 49.2s\n",
            "725:\tlearn: 55.1482853\ttotal: 2m 52s\tremaining: 49s\n",
            "726:\tlearn: 55.1235103\ttotal: 2m 52s\tremaining: 48.8s\n",
            "727:\tlearn: 55.0767763\ttotal: 2m 53s\tremaining: 48.5s\n",
            "728:\tlearn: 55.0537950\ttotal: 2m 53s\tremaining: 48.3s\n",
            "729:\tlearn: 55.0357746\ttotal: 2m 53s\tremaining: 48.1s\n",
            "730:\tlearn: 55.0110665\ttotal: 2m 54s\tremaining: 47.8s\n",
            "731:\tlearn: 54.9627669\ttotal: 2m 54s\tremaining: 47.6s\n",
            "732:\tlearn: 54.9104494\ttotal: 2m 54s\tremaining: 47.4s\n",
            "733:\tlearn: 54.8946557\ttotal: 2m 54s\tremaining: 47.1s\n",
            "734:\tlearn: 54.7840981\ttotal: 2m 55s\tremaining: 46.9s\n",
            "735:\tlearn: 54.7375916\ttotal: 2m 55s\tremaining: 46.7s\n",
            "736:\tlearn: 54.7254701\ttotal: 2m 55s\tremaining: 46.4s\n",
            "737:\tlearn: 54.7002960\ttotal: 2m 55s\tremaining: 46.2s\n",
            "738:\tlearn: 54.6906609\ttotal: 2m 56s\tremaining: 46s\n",
            "739:\tlearn: 54.6706874\ttotal: 2m 56s\tremaining: 45.8s\n",
            "740:\tlearn: 54.6245428\ttotal: 2m 56s\tremaining: 45.5s\n",
            "741:\tlearn: 54.5893711\ttotal: 2m 56s\tremaining: 45.3s\n",
            "742:\tlearn: 54.5414405\ttotal: 2m 57s\tremaining: 45s\n",
            "743:\tlearn: 54.4727095\ttotal: 2m 57s\tremaining: 44.8s\n",
            "744:\tlearn: 54.4612528\ttotal: 2m 57s\tremaining: 44.5s\n",
            "745:\tlearn: 54.4077086\ttotal: 2m 57s\tremaining: 44.3s\n",
            "746:\tlearn: 54.3873169\ttotal: 2m 57s\tremaining: 44.1s\n",
            "747:\tlearn: 54.3594803\ttotal: 2m 58s\tremaining: 43.8s\n",
            "748:\tlearn: 54.2712998\ttotal: 2m 58s\tremaining: 43.6s\n",
            "749:\tlearn: 54.2368423\ttotal: 2m 58s\tremaining: 43.3s\n",
            "750:\tlearn: 54.2109114\ttotal: 2m 58s\tremaining: 43.1s\n",
            "751:\tlearn: 54.1596959\ttotal: 2m 59s\tremaining: 42.9s\n",
            "752:\tlearn: 54.1445341\ttotal: 2m 59s\tremaining: 42.6s\n",
            "753:\tlearn: 54.1318724\ttotal: 2m 59s\tremaining: 42.4s\n",
            "754:\tlearn: 54.0328011\ttotal: 2m 59s\tremaining: 42.2s\n",
            "755:\tlearn: 54.0242062\ttotal: 3m\tremaining: 41.9s\n",
            "756:\tlearn: 54.0054974\ttotal: 3m\tremaining: 41.7s\n",
            "757:\tlearn: 53.9799804\ttotal: 3m\tremaining: 41.5s\n",
            "758:\tlearn: 53.9759605\ttotal: 3m\tremaining: 41.2s\n",
            "759:\tlearn: 53.9684836\ttotal: 3m 1s\tremaining: 41s\n",
            "760:\tlearn: 53.9667856\ttotal: 3m 1s\tremaining: 40.8s\n",
            "761:\tlearn: 53.9288011\ttotal: 3m 1s\tremaining: 40.5s\n",
            "762:\tlearn: 53.9248037\ttotal: 3m 1s\tremaining: 40.3s\n",
            "763:\tlearn: 53.8738874\ttotal: 3m 2s\tremaining: 40s\n",
            "764:\tlearn: 53.8720610\ttotal: 3m 2s\tremaining: 39.8s\n",
            "765:\tlearn: 53.8581759\ttotal: 3m 2s\tremaining: 39.6s\n",
            "766:\tlearn: 53.7659550\ttotal: 3m 2s\tremaining: 39.3s\n",
            "767:\tlearn: 53.7321991\ttotal: 3m 2s\tremaining: 39.1s\n",
            "768:\tlearn: 53.6715917\ttotal: 3m 3s\tremaining: 38.8s\n",
            "769:\tlearn: 53.6499068\ttotal: 3m 3s\tremaining: 38.6s\n",
            "770:\tlearn: 53.6287302\ttotal: 3m 3s\tremaining: 38.4s\n",
            "771:\tlearn: 53.6020121\ttotal: 3m 3s\tremaining: 38.1s\n",
            "772:\tlearn: 53.5921539\ttotal: 3m 4s\tremaining: 37.9s\n",
            "773:\tlearn: 53.5637378\ttotal: 3m 4s\tremaining: 37.6s\n",
            "774:\tlearn: 53.4980640\ttotal: 3m 4s\tremaining: 37.4s\n",
            "775:\tlearn: 53.4860553\ttotal: 3m 4s\tremaining: 37.2s\n",
            "776:\tlearn: 53.4118813\ttotal: 3m 5s\tremaining: 36.9s\n",
            "777:\tlearn: 53.3012181\ttotal: 3m 5s\tremaining: 36.7s\n",
            "778:\tlearn: 53.2728621\ttotal: 3m 5s\tremaining: 36.4s\n",
            "779:\tlearn: 53.2410943\ttotal: 3m 5s\tremaining: 36.2s\n",
            "780:\tlearn: 53.1798669\ttotal: 3m 6s\tremaining: 36s\n",
            "781:\tlearn: 53.1687809\ttotal: 3m 6s\tremaining: 35.7s\n",
            "782:\tlearn: 53.1437678\ttotal: 3m 6s\tremaining: 35.5s\n",
            "783:\tlearn: 53.1017202\ttotal: 3m 6s\tremaining: 35.2s\n",
            "784:\tlearn: 53.0094028\ttotal: 3m 6s\tremaining: 35s\n",
            "785:\tlearn: 52.9651821\ttotal: 3m 7s\tremaining: 34.8s\n",
            "786:\tlearn: 52.9265396\ttotal: 3m 7s\tremaining: 34.5s\n",
            "787:\tlearn: 52.8865977\ttotal: 3m 7s\tremaining: 34.3s\n",
            "788:\tlearn: 52.8173486\ttotal: 3m 7s\tremaining: 34.1s\n",
            "789:\tlearn: 52.7741304\ttotal: 3m 8s\tremaining: 33.8s\n",
            "790:\tlearn: 52.7397842\ttotal: 3m 8s\tremaining: 33.6s\n",
            "791:\tlearn: 52.7244693\ttotal: 3m 8s\tremaining: 33.4s\n",
            "792:\tlearn: 52.6968485\ttotal: 3m 8s\tremaining: 33.1s\n",
            "793:\tlearn: 52.6770768\ttotal: 3m 9s\tremaining: 32.9s\n",
            "794:\tlearn: 52.6644551\ttotal: 3m 9s\tremaining: 32.6s\n",
            "795:\tlearn: 52.6475700\ttotal: 3m 9s\tremaining: 32.4s\n",
            "796:\tlearn: 52.6388344\ttotal: 3m 9s\tremaining: 32.2s\n",
            "797:\tlearn: 52.5979686\ttotal: 3m 10s\tremaining: 31.9s\n",
            "798:\tlearn: 52.5974408\ttotal: 3m 10s\tremaining: 31.7s\n",
            "799:\tlearn: 52.5803496\ttotal: 3m 10s\tremaining: 31.5s\n",
            "800:\tlearn: 52.5626513\ttotal: 3m 10s\tremaining: 31.2s\n",
            "801:\tlearn: 52.5305408\ttotal: 3m 11s\tremaining: 31s\n",
            "802:\tlearn: 52.5048724\ttotal: 3m 11s\tremaining: 30.7s\n",
            "803:\tlearn: 52.4931209\ttotal: 3m 11s\tremaining: 30.5s\n",
            "804:\tlearn: 52.4756977\ttotal: 3m 11s\tremaining: 30.3s\n",
            "805:\tlearn: 52.4605394\ttotal: 3m 12s\tremaining: 30s\n",
            "806:\tlearn: 52.4265299\ttotal: 3m 12s\tremaining: 29.8s\n",
            "807:\tlearn: 52.3848156\ttotal: 3m 12s\tremaining: 29.6s\n",
            "808:\tlearn: 52.3822889\ttotal: 3m 12s\tremaining: 29.3s\n",
            "809:\tlearn: 52.3810514\ttotal: 3m 13s\tremaining: 29.1s\n",
            "810:\tlearn: 52.3377898\ttotal: 3m 13s\tremaining: 28.8s\n",
            "811:\tlearn: 52.3257957\ttotal: 3m 13s\tremaining: 28.6s\n",
            "812:\tlearn: 52.2979182\ttotal: 3m 13s\tremaining: 28.4s\n",
            "813:\tlearn: 52.2842468\ttotal: 3m 14s\tremaining: 28.1s\n",
            "814:\tlearn: 52.2665855\ttotal: 3m 14s\tremaining: 27.9s\n",
            "815:\tlearn: 52.2252267\ttotal: 3m 14s\tremaining: 27.7s\n",
            "816:\tlearn: 52.1925687\ttotal: 3m 14s\tremaining: 27.4s\n",
            "817:\tlearn: 52.1440381\ttotal: 3m 15s\tremaining: 27.2s\n",
            "818:\tlearn: 52.1184313\ttotal: 3m 15s\tremaining: 26.9s\n",
            "819:\tlearn: 52.1173980\ttotal: 3m 15s\tremaining: 26.7s\n",
            "820:\tlearn: 52.1000766\ttotal: 3m 15s\tremaining: 26.5s\n",
            "821:\tlearn: 52.0047365\ttotal: 3m 15s\tremaining: 26.2s\n",
            "822:\tlearn: 51.8953617\ttotal: 3m 16s\tremaining: 26s\n",
            "823:\tlearn: 51.8705984\ttotal: 3m 16s\tremaining: 25.8s\n",
            "824:\tlearn: 51.8271985\ttotal: 3m 16s\tremaining: 25.5s\n",
            "825:\tlearn: 51.8222939\ttotal: 3m 16s\tremaining: 25.3s\n",
            "826:\tlearn: 51.7612784\ttotal: 3m 17s\tremaining: 25s\n",
            "827:\tlearn: 51.7523039\ttotal: 3m 17s\tremaining: 24.8s\n",
            "828:\tlearn: 51.7425595\ttotal: 3m 17s\tremaining: 24.6s\n",
            "829:\tlearn: 51.7237109\ttotal: 3m 17s\tremaining: 24.3s\n",
            "830:\tlearn: 51.7001464\ttotal: 3m 18s\tremaining: 24.1s\n",
            "831:\tlearn: 51.6755654\ttotal: 3m 18s\tremaining: 23.8s\n",
            "832:\tlearn: 51.6676508\ttotal: 3m 18s\tremaining: 23.6s\n",
            "833:\tlearn: 51.6097985\ttotal: 3m 18s\tremaining: 23.4s\n",
            "834:\tlearn: 51.5668183\ttotal: 3m 18s\tremaining: 23.1s\n",
            "835:\tlearn: 51.5523407\ttotal: 3m 19s\tremaining: 22.9s\n",
            "836:\tlearn: 51.5428237\ttotal: 3m 19s\tremaining: 22.6s\n",
            "837:\tlearn: 51.5165900\ttotal: 3m 19s\tremaining: 22.4s\n",
            "838:\tlearn: 51.5061733\ttotal: 3m 19s\tremaining: 22.2s\n",
            "839:\tlearn: 51.4978144\ttotal: 3m 20s\tremaining: 21.9s\n",
            "840:\tlearn: 51.4734491\ttotal: 3m 20s\tremaining: 21.7s\n",
            "841:\tlearn: 51.4602483\ttotal: 3m 20s\tremaining: 21.5s\n",
            "842:\tlearn: 51.4183928\ttotal: 3m 20s\tremaining: 21.2s\n",
            "843:\tlearn: 51.4032511\ttotal: 3m 21s\tremaining: 21s\n",
            "844:\tlearn: 51.3554053\ttotal: 3m 21s\tremaining: 20.7s\n",
            "845:\tlearn: 51.3393606\ttotal: 3m 21s\tremaining: 20.5s\n",
            "846:\tlearn: 51.3100935\ttotal: 3m 21s\tremaining: 20.3s\n",
            "847:\tlearn: 51.2993645\ttotal: 3m 22s\tremaining: 20s\n",
            "848:\tlearn: 51.2798565\ttotal: 3m 22s\tremaining: 19.8s\n",
            "849:\tlearn: 51.2171147\ttotal: 3m 22s\tremaining: 19.6s\n",
            "850:\tlearn: 51.2077525\ttotal: 3m 22s\tremaining: 19.3s\n",
            "851:\tlearn: 51.1840043\ttotal: 3m 23s\tremaining: 19.1s\n",
            "852:\tlearn: 51.1374782\ttotal: 3m 23s\tremaining: 18.8s\n",
            "853:\tlearn: 51.1009498\ttotal: 3m 23s\tremaining: 18.6s\n",
            "854:\tlearn: 51.0429961\ttotal: 3m 23s\tremaining: 18.4s\n",
            "855:\tlearn: 51.0018681\ttotal: 3m 24s\tremaining: 18.1s\n",
            "856:\tlearn: 50.9669021\ttotal: 3m 24s\tremaining: 17.9s\n",
            "857:\tlearn: 50.9211121\ttotal: 3m 24s\tremaining: 17.6s\n",
            "858:\tlearn: 50.8815435\ttotal: 3m 24s\tremaining: 17.4s\n",
            "859:\tlearn: 50.8592633\ttotal: 3m 25s\tremaining: 17.2s\n",
            "860:\tlearn: 50.8075663\ttotal: 3m 25s\tremaining: 16.9s\n",
            "861:\tlearn: 50.7695636\ttotal: 3m 25s\tremaining: 16.7s\n",
            "862:\tlearn: 50.7368842\ttotal: 3m 25s\tremaining: 16.5s\n",
            "863:\tlearn: 50.7139948\ttotal: 3m 26s\tremaining: 16.2s\n",
            "864:\tlearn: 50.6954789\ttotal: 3m 26s\tremaining: 16s\n",
            "865:\tlearn: 50.6767975\ttotal: 3m 26s\tremaining: 15.8s\n",
            "866:\tlearn: 50.6600088\ttotal: 3m 27s\tremaining: 15.5s\n",
            "867:\tlearn: 50.6077908\ttotal: 3m 27s\tremaining: 15.3s\n",
            "868:\tlearn: 50.6070034\ttotal: 3m 27s\tremaining: 15s\n",
            "869:\tlearn: 50.5901024\ttotal: 3m 27s\tremaining: 14.8s\n",
            "870:\tlearn: 50.5747980\ttotal: 3m 28s\tremaining: 14.6s\n",
            "871:\tlearn: 50.5535613\ttotal: 3m 28s\tremaining: 14.3s\n",
            "872:\tlearn: 50.5216594\ttotal: 3m 28s\tremaining: 14.1s\n",
            "873:\tlearn: 50.4797238\ttotal: 3m 28s\tremaining: 13.9s\n",
            "874:\tlearn: 50.4711668\ttotal: 3m 29s\tremaining: 13.6s\n",
            "875:\tlearn: 50.4628351\ttotal: 3m 29s\tremaining: 13.4s\n",
            "876:\tlearn: 50.4510200\ttotal: 3m 29s\tremaining: 13.1s\n",
            "877:\tlearn: 50.4271095\ttotal: 3m 29s\tremaining: 12.9s\n",
            "878:\tlearn: 50.4108951\ttotal: 3m 30s\tremaining: 12.7s\n",
            "879:\tlearn: 50.3965065\ttotal: 3m 30s\tremaining: 12.4s\n",
            "880:\tlearn: 50.3858498\ttotal: 3m 30s\tremaining: 12.2s\n",
            "881:\tlearn: 50.3789454\ttotal: 3m 30s\tremaining: 12s\n",
            "882:\tlearn: 50.3701425\ttotal: 3m 31s\tremaining: 11.7s\n",
            "883:\tlearn: 50.3547796\ttotal: 3m 31s\tremaining: 11.5s\n",
            "884:\tlearn: 50.3539028\ttotal: 3m 31s\tremaining: 11.2s\n",
            "885:\tlearn: 50.3482576\ttotal: 3m 31s\tremaining: 11s\n",
            "886:\tlearn: 50.2255194\ttotal: 3m 31s\tremaining: 10.8s\n",
            "887:\tlearn: 50.1925787\ttotal: 3m 32s\tremaining: 10.5s\n",
            "888:\tlearn: 50.1559752\ttotal: 3m 32s\tremaining: 10.3s\n",
            "889:\tlearn: 50.0939765\ttotal: 3m 32s\tremaining: 10s\n",
            "890:\tlearn: 50.0536370\ttotal: 3m 32s\tremaining: 9.8s\n",
            "891:\tlearn: 50.0406503\ttotal: 3m 33s\tremaining: 9.56s\n",
            "892:\tlearn: 50.0335852\ttotal: 3m 33s\tremaining: 9.32s\n",
            "893:\tlearn: 49.9542572\ttotal: 3m 33s\tremaining: 9.08s\n",
            "894:\tlearn: 49.9370342\ttotal: 3m 33s\tremaining: 8.84s\n",
            "895:\tlearn: 49.9272781\ttotal: 3m 34s\tremaining: 8.6s\n",
            "896:\tlearn: 49.8265407\ttotal: 3m 34s\tremaining: 8.36s\n",
            "897:\tlearn: 49.8009323\ttotal: 3m 34s\tremaining: 8.12s\n",
            "898:\tlearn: 49.7941300\ttotal: 3m 34s\tremaining: 7.88s\n",
            "899:\tlearn: 49.7754476\ttotal: 3m 35s\tremaining: 7.64s\n",
            "900:\tlearn: 49.7282494\ttotal: 3m 35s\tremaining: 7.41s\n",
            "901:\tlearn: 49.7252668\ttotal: 3m 35s\tremaining: 7.17s\n",
            "902:\tlearn: 49.7096504\ttotal: 3m 35s\tremaining: 6.93s\n",
            "903:\tlearn: 49.6793976\ttotal: 3m 35s\tremaining: 6.69s\n",
            "904:\tlearn: 49.6311897\ttotal: 3m 36s\tremaining: 6.45s\n",
            "905:\tlearn: 49.5824371\ttotal: 3m 36s\tremaining: 6.21s\n",
            "906:\tlearn: 49.5175368\ttotal: 3m 36s\tremaining: 5.97s\n",
            "907:\tlearn: 49.4811660\ttotal: 3m 36s\tremaining: 5.73s\n",
            "908:\tlearn: 49.4426395\ttotal: 3m 37s\tremaining: 5.49s\n",
            "909:\tlearn: 49.4354137\ttotal: 3m 37s\tremaining: 5.25s\n",
            "910:\tlearn: 49.3681709\ttotal: 3m 37s\tremaining: 5.01s\n",
            "911:\tlearn: 49.3560657\ttotal: 3m 37s\tremaining: 4.78s\n",
            "912:\tlearn: 49.3439271\ttotal: 3m 37s\tremaining: 4.54s\n",
            "913:\tlearn: 49.3204522\ttotal: 3m 38s\tremaining: 4.3s\n",
            "914:\tlearn: 49.2432063\ttotal: 3m 38s\tremaining: 4.06s\n",
            "915:\tlearn: 49.2156989\ttotal: 3m 38s\tremaining: 3.82s\n",
            "916:\tlearn: 49.1558579\ttotal: 3m 38s\tremaining: 3.58s\n",
            "917:\tlearn: 49.1344225\ttotal: 3m 39s\tremaining: 3.34s\n",
            "918:\tlearn: 49.1119309\ttotal: 3m 39s\tremaining: 3.1s\n",
            "919:\tlearn: 49.0640031\ttotal: 3m 39s\tremaining: 2.86s\n",
            "920:\tlearn: 49.0593527\ttotal: 3m 39s\tremaining: 2.63s\n",
            "921:\tlearn: 49.0498273\ttotal: 3m 40s\tremaining: 2.39s\n",
            "922:\tlearn: 49.0382403\ttotal: 3m 40s\tremaining: 2.15s\n",
            "923:\tlearn: 49.0198884\ttotal: 3m 40s\tremaining: 1.91s\n",
            "924:\tlearn: 48.9829117\ttotal: 3m 40s\tremaining: 1.67s\n",
            "925:\tlearn: 48.9164044\ttotal: 3m 41s\tremaining: 1.43s\n",
            "926:\tlearn: 48.8968269\ttotal: 3m 41s\tremaining: 1.19s\n",
            "927:\tlearn: 48.8802807\ttotal: 3m 41s\tremaining: 955ms\n",
            "928:\tlearn: 48.8749008\ttotal: 3m 41s\tremaining: 717ms\n",
            "929:\tlearn: 48.8729132\ttotal: 3m 42s\tremaining: 478ms\n",
            "930:\tlearn: 48.8583696\ttotal: 3m 42s\tremaining: 239ms\n",
            "931:\tlearn: 48.8576522\ttotal: 3m 42s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 22:39:54,298] A new study created in memory with name: no-name-4c439aa4-b6b8-4d87-9e36-84bd9ac7bafe\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 10...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-08 22:51:31,774] Trial 0 finished with value: 56.59134201174269 and parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306}. Best is trial 0 with value: 56.59134201174269.\n",
            "[I 2023-10-08 22:55:49,995] Trial 1 finished with value: 40.88310623035718 and parameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:03:13,757] Trial 2 finished with value: 43.9227177645403 and parameters: {'iterations': 505, 'learning_rate': 0.2618606543169106, 'depth': 12, 'min_data_in_leaf': 8, 'reg_lambda': 30.014748821264817, 'subsample': 0.8475285344319161, 'random_strength': 57.30790465944011, 'od_wait': 67, 'leaf_estimation_iterations': 13, 'bagging_temperature': 7.9900177724845936, 'colsample_bylevel': 0.6844477897147302}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:08:12,304] Trial 3 finished with value: 44.23136453562379 and parameters: {'iterations': 898, 'learning_rate': 0.9130683253133137, 'depth': 7, 'min_data_in_leaf': 27, 'reg_lambda': 39.82957240471868, 'subsample': 0.5510460609398131, 'random_strength': 49.91198988150496, 'od_wait': 79, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.361144973643148, 'colsample_bylevel': 0.9479496331713275}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:12:21,995] Trial 4 finished with value: 41.66940363809906 and parameters: {'iterations': 852, 'learning_rate': 0.9885202610774658, 'depth': 8, 'min_data_in_leaf': 7, 'reg_lambda': 87.92944009390116, 'subsample': 0.4956872562389312, 'random_strength': 57.322884125565835, 'od_wait': 53, 'leaf_estimation_iterations': 1, 'bagging_temperature': 84.18529003962686, 'colsample_bylevel': 0.7921362230743637}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:21:12,099] Trial 5 finished with value: 73.37625522134731 and parameters: {'iterations': 677, 'learning_rate': 0.2636942736182947, 'depth': 14, 'min_data_in_leaf': 4, 'reg_lambda': 84.52650337490007, 'subsample': 0.34809811654518935, 'random_strength': 95.03774795532136, 'od_wait': 115, 'leaf_estimation_iterations': 12, 'bagging_temperature': 7.601405123866443, 'colsample_bylevel': 0.03686762318418402}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:24:54,767] Trial 6 finished with value: 45.02886005628249 and parameters: {'iterations': 862, 'learning_rate': 0.5736146775857691, 'depth': 6, 'min_data_in_leaf': 12, 'reg_lambda': 58.399002762659634, 'subsample': 0.31536466740376373, 'random_strength': 10.467345338648709, 'od_wait': 32, 'leaf_estimation_iterations': 13, 'bagging_temperature': 46.52202836590652, 'colsample_bylevel': 0.6242955985642463}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:44:25,562] Trial 7 finished with value: 54.2140104386608 and parameters: {'iterations': 581, 'learning_rate': 0.48266290079512386, 'depth': 14, 'min_data_in_leaf': 27, 'reg_lambda': 67.0626719001935, 'subsample': 0.7585391533726871, 'random_strength': 75.65848410890035, 'od_wait': 44, 'leaf_estimation_iterations': 17, 'bagging_temperature': 27.258440783520065, 'colsample_bylevel': 0.8465314206184689}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:46:41,158] Trial 8 finished with value: 55.94197131995532 and parameters: {'iterations': 643, 'learning_rate': 0.44591888731787543, 'depth': 5, 'min_data_in_leaf': 26, 'reg_lambda': 86.86955595629883, 'subsample': 0.34810155291325084, 'random_strength': 28.2789279370624, 'od_wait': 54, 'leaf_estimation_iterations': 13, 'bagging_temperature': 73.24501780279176, 'colsample_bylevel': 0.5041819471931002}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:50:36,344] Trial 9 finished with value: 46.426996103039016 and parameters: {'iterations': 859, 'learning_rate': 0.956967487234481, 'depth': 5, 'min_data_in_leaf': 28, 'reg_lambda': 71.75671675286505, 'subsample': 0.8272172352379608, 'random_strength': 58.84776166305671, 'od_wait': 100, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.70552302525834, 'colsample_bylevel': 0.8725675486341326}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-08 23:56:15,944] Trial 10 finished with value: 49.19699372122947 and parameters: {'iterations': 994, 'learning_rate': 0.7499226326046825, 'depth': 10, 'min_data_in_leaf': 19, 'reg_lambda': 48.14865529228105, 'subsample': 0.992897000692206, 'random_strength': 11.134614510989433, 'od_wait': 146, 'leaf_estimation_iterations': 20, 'bagging_temperature': 1.4023623168947625, 'colsample_bylevel': 0.2686120287821341}. Best is trial 1 with value: 40.88310623035718.\n",
            "[I 2023-10-09 00:00:26,933] Trial 11 finished with value: 40.476288019824096 and parameters: {'iterations': 767, 'learning_rate': 0.7401373266385217, 'depth': 9, 'min_data_in_leaf': 14, 'reg_lambda': 53.44796714019611, 'subsample': 0.5390364116216142, 'random_strength': 36.87469170000216, 'od_wait': 12, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.39113334695546, 'colsample_bylevel': 0.7224207983820307}. Best is trial 11 with value: 40.476288019824096.\n",
            "[I 2023-10-09 00:03:57,517] Trial 12 finished with value: 42.19819591875611 and parameters: {'iterations': 743, 'learning_rate': 0.7356346127088652, 'depth': 9, 'min_data_in_leaf': 17, 'reg_lambda': 49.16423147603391, 'subsample': 0.5854290622440648, 'random_strength': 31.588734753749215, 'od_wait': 122, 'leaf_estimation_iterations': 4, 'bagging_temperature': 23.993242751388063, 'colsample_bylevel': 0.3642319611435846}. Best is trial 11 with value: 40.476288019824096.\n",
            "[I 2023-10-09 00:06:29,683] Trial 13 finished with value: 53.13396028436657 and parameters: {'iterations': 757, 'learning_rate': 0.7501800009545696, 'depth': 11, 'min_data_in_leaf': 13, 'reg_lambda': 30.221360663173336, 'subsample': 0.4774950955421056, 'random_strength': 30.328584321736876, 'od_wait': 18, 'leaf_estimation_iterations': 7, 'bagging_temperature': 26.88595476478946, 'colsample_bylevel': 0.6071397112449649}. Best is trial 11 with value: 40.476288019824096.\n",
            "[I 2023-10-09 00:11:00,651] Trial 14 finished with value: 40.36773045770164 and parameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}. Best is trial 14 with value: 40.36773045770164.\n",
            "[I 2023-10-09 00:20:03,308] Trial 15 finished with value: 41.78276558883964 and parameters: {'iterations': 951, 'learning_rate': 0.8511066461626123, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 57.12168849740681, 'subsample': 0.6332455121753336, 'random_strength': 42.985904488419436, 'od_wait': 150, 'leaf_estimation_iterations': 1, 'bagging_temperature': 92.32936110772192, 'colsample_bylevel': 0.9920572964452485}. Best is trial 14 with value: 40.36773045770164.\n",
            "[I 2023-10-09 00:21:51,820] Trial 16 finished with value: 49.90959698040207 and parameters: {'iterations': 772, 'learning_rate': 0.8503707442954018, 'depth': 9, 'min_data_in_leaf': 22, 'reg_lambda': 45.56988826790886, 'subsample': 0.43361830890771325, 'random_strength': 40.85285355325164, 'od_wait': 10, 'leaf_estimation_iterations': 4, 'bagging_temperature': 48.52697376311865, 'colsample_bylevel': 0.6952854200311266}. Best is trial 14 with value: 40.36773045770164.\n",
            "[I 2023-10-09 00:23:17,538] Trial 17 finished with value: 53.89779445972489 and parameters: {'iterations': 305, 'learning_rate': 0.6940981700259699, 'depth': 7, 'min_data_in_leaf': 13, 'reg_lambda': 59.169649275320815, 'subsample': 0.5588258864286549, 'random_strength': 65.67001229048435, 'od_wait': 81, 'leaf_estimation_iterations': 4, 'bagging_temperature': 94.38598838290035, 'colsample_bylevel': 0.7425321676611494}. Best is trial 14 with value: 40.36773045770164.\n",
            "[I 2023-10-09 00:35:58,786] Trial 18 finished with value: 44.07977225292132 and parameters: {'iterations': 942, 'learning_rate': 0.4590762268369928, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 71.62634725545307, 'subsample': 0.44325233558749344, 'random_strength': 42.23473947881512, 'od_wait': 133, 'leaf_estimation_iterations': 6, 'bagging_temperature': 49.18023756004181, 'colsample_bylevel': 0.8075324887137805}. Best is trial 14 with value: 40.36773045770164.\n",
            "[I 2023-10-09 00:39:34,415] Trial 19 finished with value: 42.27758279099377 and parameters: {'iterations': 671, 'learning_rate': 0.8288728088600124, 'depth': 9, 'min_data_in_leaf': 22, 'reg_lambda': 55.697461616307315, 'subsample': 0.6156215600174659, 'random_strength': 21.01912084832779, 'od_wait': 94, 'leaf_estimation_iterations': 2, 'bagging_temperature': 59.80021622617315, 'colsample_bylevel': 0.5705483795639386}. Best is trial 14 with value: 40.36773045770164.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 10: 40.36773045770164\n",
            "Best hyperparameters for fold 10: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}\n",
            "0:\tlearn: 206.6364584\ttotal: 256ms\tremaining: 4m 4s\n",
            "1:\tlearn: 204.1231940\ttotal: 542ms\tremaining: 4m 18s\n",
            "2:\tlearn: 202.2868470\ttotal: 795ms\tremaining: 4m 12s\n",
            "3:\tlearn: 202.1752991\ttotal: 1.06s\tremaining: 4m 12s\n",
            "4:\tlearn: 201.6863019\ttotal: 1.32s\tremaining: 4m 10s\n",
            "5:\tlearn: 201.6850300\ttotal: 1.39s\tremaining: 3m 40s\n",
            "6:\tlearn: 201.4278804\ttotal: 1.66s\tremaining: 3m 44s\n",
            "7:\tlearn: 200.5960486\ttotal: 1.92s\tremaining: 3m 47s\n",
            "8:\tlearn: 200.5954997\ttotal: 1.97s\tremaining: 3m 27s\n",
            "9:\tlearn: 200.5952012\ttotal: 2.02s\tremaining: 3m 10s\n",
            "10:\tlearn: 200.3324715\ttotal: 2.21s\tremaining: 3m 10s\n",
            "11:\tlearn: 200.3020730\ttotal: 2.34s\tremaining: 3m 4s\n",
            "12:\tlearn: 199.1061162\ttotal: 2.56s\tremaining: 3m 5s\n",
            "13:\tlearn: 198.5741321\ttotal: 2.81s\tremaining: 3m 8s\n",
            "14:\tlearn: 198.5731125\ttotal: 2.85s\tremaining: 2m 59s\n",
            "15:\tlearn: 198.3493388\ttotal: 3.11s\tremaining: 3m 2s\n",
            "16:\tlearn: 198.1232596\ttotal: 3.38s\tremaining: 3m 6s\n",
            "17:\tlearn: 197.9978665\ttotal: 3.6s\tremaining: 3m 7s\n",
            "18:\tlearn: 197.9959456\ttotal: 3.64s\tremaining: 2m 59s\n",
            "19:\tlearn: 197.1667568\ttotal: 3.88s\tremaining: 3m 1s\n",
            "20:\tlearn: 196.4389251\ttotal: 4.13s\tremaining: 3m 4s\n",
            "21:\tlearn: 194.0785944\ttotal: 4.36s\tremaining: 3m 5s\n",
            "22:\tlearn: 193.6623830\ttotal: 4.59s\tremaining: 3m 6s\n",
            "23:\tlearn: 192.9099741\ttotal: 4.79s\tremaining: 3m 6s\n",
            "24:\tlearn: 189.2809052\ttotal: 5.01s\tremaining: 3m 6s\n",
            "25:\tlearn: 187.9273913\ttotal: 5.25s\tremaining: 3m 7s\n",
            "26:\tlearn: 186.3619378\ttotal: 5.49s\tremaining: 3m 9s\n",
            "27:\tlearn: 185.7916993\ttotal: 5.75s\tremaining: 3m 10s\n",
            "28:\tlearn: 184.2255682\ttotal: 5.96s\tremaining: 3m 10s\n",
            "29:\tlearn: 183.5152752\ttotal: 6.19s\tremaining: 3m 11s\n",
            "30:\tlearn: 182.5519816\ttotal: 6.42s\tremaining: 3m 11s\n",
            "31:\tlearn: 181.9692908\ttotal: 6.61s\tremaining: 3m 10s\n",
            "32:\tlearn: 181.7625287\ttotal: 6.87s\tremaining: 3m 12s\n",
            "33:\tlearn: 181.2951366\ttotal: 7.1s\tremaining: 3m 12s\n",
            "34:\tlearn: 180.9044851\ttotal: 7.38s\tremaining: 3m 14s\n",
            "35:\tlearn: 180.4768118\ttotal: 7.63s\tremaining: 3m 14s\n",
            "36:\tlearn: 177.4225913\ttotal: 7.84s\tremaining: 3m 14s\n",
            "37:\tlearn: 175.5258251\ttotal: 8.05s\tremaining: 3m 14s\n",
            "38:\tlearn: 175.1958051\ttotal: 8.28s\tremaining: 3m 14s\n",
            "39:\tlearn: 173.5873210\ttotal: 8.48s\tremaining: 3m 14s\n",
            "40:\tlearn: 172.8670490\ttotal: 8.7s\tremaining: 3m 14s\n",
            "41:\tlearn: 171.0261443\ttotal: 8.9s\tremaining: 3m 13s\n",
            "42:\tlearn: 169.7574878\ttotal: 9.11s\tremaining: 3m 13s\n",
            "43:\tlearn: 169.6374592\ttotal: 9.34s\tremaining: 3m 13s\n",
            "44:\tlearn: 168.8875575\ttotal: 9.52s\tremaining: 3m 12s\n",
            "45:\tlearn: 168.7664574\ttotal: 9.76s\tremaining: 3m 13s\n",
            "46:\tlearn: 167.9473706\ttotal: 9.95s\tremaining: 3m 12s\n",
            "47:\tlearn: 167.5069007\ttotal: 10.2s\tremaining: 3m 12s\n",
            "48:\tlearn: 166.7549506\ttotal: 10.4s\tremaining: 3m 12s\n",
            "49:\tlearn: 164.6091831\ttotal: 10.6s\tremaining: 3m 11s\n",
            "50:\tlearn: 164.0963473\ttotal: 10.8s\tremaining: 3m 11s\n",
            "51:\tlearn: 163.8388450\ttotal: 11s\tremaining: 3m 11s\n",
            "52:\tlearn: 162.8759209\ttotal: 11.2s\tremaining: 3m 11s\n",
            "53:\tlearn: 161.6409204\ttotal: 11.5s\tremaining: 3m 11s\n",
            "54:\tlearn: 159.7906061\ttotal: 11.7s\tremaining: 3m 11s\n",
            "55:\tlearn: 158.9107989\ttotal: 11.9s\tremaining: 3m 10s\n",
            "56:\tlearn: 157.2502878\ttotal: 12.1s\tremaining: 3m 10s\n",
            "57:\tlearn: 155.6470011\ttotal: 12.3s\tremaining: 3m 10s\n",
            "58:\tlearn: 155.4113239\ttotal: 12.5s\tremaining: 3m 10s\n",
            "59:\tlearn: 154.7714023\ttotal: 12.8s\tremaining: 3m 10s\n",
            "60:\tlearn: 152.4525336\ttotal: 13s\tremaining: 3m 10s\n",
            "61:\tlearn: 151.6260523\ttotal: 13.2s\tremaining: 3m 10s\n",
            "62:\tlearn: 150.7686184\ttotal: 13.5s\tremaining: 3m 10s\n",
            "63:\tlearn: 150.5865008\ttotal: 13.7s\tremaining: 3m 11s\n",
            "64:\tlearn: 150.2197539\ttotal: 13.9s\tremaining: 3m 11s\n",
            "65:\tlearn: 149.4907227\ttotal: 14.2s\tremaining: 3m 11s\n",
            "66:\tlearn: 148.6203997\ttotal: 14.4s\tremaining: 3m 11s\n",
            "67:\tlearn: 148.1054412\ttotal: 14.6s\tremaining: 3m 10s\n",
            "68:\tlearn: 147.8277694\ttotal: 14.9s\tremaining: 3m 10s\n",
            "69:\tlearn: 147.7066613\ttotal: 15.1s\tremaining: 3m 10s\n",
            "70:\tlearn: 147.1989711\ttotal: 15.3s\tremaining: 3m 10s\n",
            "71:\tlearn: 146.7824471\ttotal: 15.5s\tremaining: 3m 10s\n",
            "72:\tlearn: 146.2833935\ttotal: 15.8s\tremaining: 3m 10s\n",
            "73:\tlearn: 145.2539354\ttotal: 16s\tremaining: 3m 10s\n",
            "74:\tlearn: 144.3261152\ttotal: 16.2s\tremaining: 3m 10s\n",
            "75:\tlearn: 144.0154720\ttotal: 16.5s\tremaining: 3m 11s\n",
            "76:\tlearn: 143.8447866\ttotal: 16.8s\tremaining: 3m 11s\n",
            "77:\tlearn: 143.6346683\ttotal: 17s\tremaining: 3m 11s\n",
            "78:\tlearn: 142.4263703\ttotal: 17.2s\tremaining: 3m 11s\n",
            "79:\tlearn: 141.8918282\ttotal: 17.4s\tremaining: 3m 10s\n",
            "80:\tlearn: 141.2559917\ttotal: 17.6s\tremaining: 3m 10s\n",
            "81:\tlearn: 140.0952901\ttotal: 17.8s\tremaining: 3m 10s\n",
            "82:\tlearn: 139.8812268\ttotal: 18s\tremaining: 3m 9s\n",
            "83:\tlearn: 139.6561139\ttotal: 18.3s\tremaining: 3m 9s\n",
            "84:\tlearn: 139.6560474\ttotal: 18.5s\tremaining: 3m 9s\n",
            "85:\tlearn: 139.6530097\ttotal: 18.7s\tremaining: 3m 9s\n",
            "86:\tlearn: 139.6519151\ttotal: 19s\tremaining: 3m 9s\n",
            "87:\tlearn: 139.6513739\ttotal: 19.2s\tremaining: 3m 9s\n",
            "88:\tlearn: 139.6510747\ttotal: 19.4s\tremaining: 3m 9s\n",
            "89:\tlearn: 138.4229578\ttotal: 19.6s\tremaining: 3m 9s\n",
            "90:\tlearn: 137.5614028\ttotal: 19.9s\tremaining: 3m 9s\n",
            "91:\tlearn: 137.0800546\ttotal: 20.1s\tremaining: 3m 9s\n",
            "92:\tlearn: 136.7605993\ttotal: 20.3s\tremaining: 3m 8s\n",
            "93:\tlearn: 136.0931570\ttotal: 20.5s\tremaining: 3m 8s\n",
            "94:\tlearn: 135.6223509\ttotal: 20.8s\tremaining: 3m 8s\n",
            "95:\tlearn: 135.1675283\ttotal: 21s\tremaining: 3m 7s\n",
            "96:\tlearn: 134.3236911\ttotal: 21.2s\tremaining: 3m 7s\n",
            "97:\tlearn: 133.9396946\ttotal: 21.4s\tremaining: 3m 7s\n",
            "98:\tlearn: 133.7584195\ttotal: 21.7s\tremaining: 3m 7s\n",
            "99:\tlearn: 133.4851265\ttotal: 21.9s\tremaining: 3m 7s\n",
            "100:\tlearn: 133.3275272\ttotal: 22.1s\tremaining: 3m 7s\n",
            "101:\tlearn: 133.2326867\ttotal: 22.3s\tremaining: 3m 6s\n",
            "102:\tlearn: 132.5435842\ttotal: 22.6s\tremaining: 3m 6s\n",
            "103:\tlearn: 132.4531140\ttotal: 22.8s\tremaining: 3m 6s\n",
            "104:\tlearn: 132.2092308\ttotal: 23s\tremaining: 3m 6s\n",
            "105:\tlearn: 132.2090936\ttotal: 23.2s\tremaining: 3m 6s\n",
            "106:\tlearn: 132.2086772\ttotal: 23.4s\tremaining: 3m 5s\n",
            "107:\tlearn: 131.4773744\ttotal: 23.6s\tremaining: 3m 5s\n",
            "108:\tlearn: 130.9393986\ttotal: 23.8s\tremaining: 3m 5s\n",
            "109:\tlearn: 130.8902881\ttotal: 24.1s\tremaining: 3m 5s\n",
            "110:\tlearn: 129.9286145\ttotal: 24.3s\tremaining: 3m 4s\n",
            "111:\tlearn: 128.9907282\ttotal: 24.5s\tremaining: 3m 4s\n",
            "112:\tlearn: 128.6735539\ttotal: 24.7s\tremaining: 3m 4s\n",
            "113:\tlearn: 127.8931071\ttotal: 24.9s\tremaining: 3m 4s\n",
            "114:\tlearn: 127.2098680\ttotal: 25.2s\tremaining: 3m 4s\n",
            "115:\tlearn: 126.8117070\ttotal: 25.4s\tremaining: 3m 3s\n",
            "116:\tlearn: 126.6696973\ttotal: 25.7s\tremaining: 3m 4s\n",
            "117:\tlearn: 126.4262397\ttotal: 25.9s\tremaining: 3m 3s\n",
            "118:\tlearn: 126.4256997\ttotal: 26.1s\tremaining: 3m 3s\n",
            "119:\tlearn: 126.2447038\ttotal: 26.3s\tremaining: 3m 3s\n",
            "120:\tlearn: 125.9587122\ttotal: 26.6s\tremaining: 3m 3s\n",
            "121:\tlearn: 125.7514482\ttotal: 26.8s\tremaining: 3m 2s\n",
            "122:\tlearn: 125.5957769\ttotal: 27s\tremaining: 3m 2s\n",
            "123:\tlearn: 125.3541699\ttotal: 27.2s\tremaining: 3m 2s\n",
            "124:\tlearn: 124.5904385\ttotal: 27.4s\tremaining: 3m 2s\n",
            "125:\tlearn: 124.3722165\ttotal: 27.6s\tremaining: 3m 1s\n",
            "126:\tlearn: 124.1534953\ttotal: 27.8s\tremaining: 3m 1s\n",
            "127:\tlearn: 123.8408926\ttotal: 28s\tremaining: 3m 1s\n",
            "128:\tlearn: 123.8376772\ttotal: 28.2s\tremaining: 3m\n",
            "129:\tlearn: 123.6472529\ttotal: 28.5s\tremaining: 3m 1s\n",
            "130:\tlearn: 123.3948498\ttotal: 28.7s\tremaining: 3m\n",
            "131:\tlearn: 122.1602670\ttotal: 29s\tremaining: 3m\n",
            "132:\tlearn: 121.6600842\ttotal: 29.2s\tremaining: 3m\n",
            "133:\tlearn: 121.6174858\ttotal: 29.4s\tremaining: 3m\n",
            "134:\tlearn: 121.1678294\ttotal: 29.7s\tremaining: 3m\n",
            "135:\tlearn: 121.0206224\ttotal: 29.9s\tremaining: 3m\n",
            "136:\tlearn: 120.8085956\ttotal: 30.1s\tremaining: 2m 59s\n",
            "137:\tlearn: 120.6616583\ttotal: 30.3s\tremaining: 2m 59s\n",
            "138:\tlearn: 120.3599135\ttotal: 30.5s\tremaining: 2m 59s\n",
            "139:\tlearn: 120.1167323\ttotal: 30.8s\tremaining: 2m 59s\n",
            "140:\tlearn: 119.9340899\ttotal: 31s\tremaining: 2m 59s\n",
            "141:\tlearn: 119.5969271\ttotal: 31.2s\tremaining: 2m 58s\n",
            "142:\tlearn: 119.1238654\ttotal: 31.5s\tremaining: 2m 58s\n",
            "143:\tlearn: 119.0001774\ttotal: 31.7s\tremaining: 2m 58s\n",
            "144:\tlearn: 118.8721981\ttotal: 31.9s\tremaining: 2m 58s\n",
            "145:\tlearn: 118.5313001\ttotal: 32.2s\tremaining: 2m 58s\n",
            "146:\tlearn: 118.2855891\ttotal: 32.4s\tremaining: 2m 58s\n",
            "147:\tlearn: 118.0333287\ttotal: 32.6s\tremaining: 2m 58s\n",
            "148:\tlearn: 117.8315575\ttotal: 32.9s\tremaining: 2m 58s\n",
            "149:\tlearn: 117.7030398\ttotal: 33.1s\tremaining: 2m 57s\n",
            "150:\tlearn: 117.4313356\ttotal: 33.3s\tremaining: 2m 57s\n",
            "151:\tlearn: 117.3128301\ttotal: 33.5s\tremaining: 2m 57s\n",
            "152:\tlearn: 117.3085495\ttotal: 33.8s\tremaining: 2m 57s\n",
            "153:\tlearn: 117.2405639\ttotal: 34s\tremaining: 2m 57s\n",
            "154:\tlearn: 117.2382583\ttotal: 34.3s\tremaining: 2m 57s\n",
            "155:\tlearn: 116.9824156\ttotal: 34.5s\tremaining: 2m 56s\n",
            "156:\tlearn: 116.7207177\ttotal: 34.7s\tremaining: 2m 56s\n",
            "157:\tlearn: 116.7169641\ttotal: 34.9s\tremaining: 2m 56s\n",
            "158:\tlearn: 116.6392405\ttotal: 35.2s\tremaining: 2m 56s\n",
            "159:\tlearn: 116.5223937\ttotal: 35.4s\tremaining: 2m 56s\n",
            "160:\tlearn: 116.4855654\ttotal: 35.7s\tremaining: 2m 56s\n",
            "161:\tlearn: 116.2432967\ttotal: 35.9s\tremaining: 2m 55s\n",
            "162:\tlearn: 116.1546380\ttotal: 36.2s\tremaining: 2m 55s\n",
            "163:\tlearn: 115.7250518\ttotal: 36.4s\tremaining: 2m 55s\n",
            "164:\tlearn: 115.5499416\ttotal: 36.6s\tremaining: 2m 55s\n",
            "165:\tlearn: 115.4259916\ttotal: 36.8s\tremaining: 2m 55s\n",
            "166:\tlearn: 115.3216017\ttotal: 37s\tremaining: 2m 54s\n",
            "167:\tlearn: 114.9427424\ttotal: 37.2s\tremaining: 2m 54s\n",
            "168:\tlearn: 114.8553964\ttotal: 37.5s\tremaining: 2m 54s\n",
            "169:\tlearn: 114.8503242\ttotal: 37.7s\tremaining: 2m 54s\n",
            "170:\tlearn: 114.8494864\ttotal: 37.9s\tremaining: 2m 53s\n",
            "171:\tlearn: 114.5462415\ttotal: 38.1s\tremaining: 2m 53s\n",
            "172:\tlearn: 114.2835770\ttotal: 38.3s\tremaining: 2m 53s\n",
            "173:\tlearn: 114.2100794\ttotal: 38.5s\tremaining: 2m 53s\n",
            "174:\tlearn: 114.1650169\ttotal: 38.7s\tremaining: 2m 52s\n",
            "175:\tlearn: 114.0483980\ttotal: 38.9s\tremaining: 2m 52s\n",
            "176:\tlearn: 113.7947522\ttotal: 39.1s\tremaining: 2m 52s\n",
            "177:\tlearn: 113.5880376\ttotal: 39.4s\tremaining: 2m 52s\n",
            "178:\tlearn: 113.4658822\ttotal: 39.6s\tremaining: 2m 51s\n",
            "179:\tlearn: 113.0937181\ttotal: 39.8s\tremaining: 2m 51s\n",
            "180:\tlearn: 112.7630782\ttotal: 40s\tremaining: 2m 51s\n",
            "181:\tlearn: 112.5465185\ttotal: 40.2s\tremaining: 2m 50s\n",
            "182:\tlearn: 112.4174059\ttotal: 40.4s\tremaining: 2m 50s\n",
            "183:\tlearn: 112.2318220\ttotal: 40.6s\tremaining: 2m 50s\n",
            "184:\tlearn: 112.1763820\ttotal: 40.8s\tremaining: 2m 50s\n",
            "185:\tlearn: 112.1733437\ttotal: 41s\tremaining: 2m 49s\n",
            "186:\tlearn: 112.0547134\ttotal: 41.2s\tremaining: 2m 49s\n",
            "187:\tlearn: 111.5349111\ttotal: 41.4s\tremaining: 2m 49s\n",
            "188:\tlearn: 111.2902302\ttotal: 41.6s\tremaining: 2m 48s\n",
            "189:\tlearn: 110.8433702\ttotal: 41.8s\tremaining: 2m 48s\n",
            "190:\tlearn: 110.6558502\ttotal: 42.1s\tremaining: 2m 48s\n",
            "191:\tlearn: 110.4834883\ttotal: 42.3s\tremaining: 2m 48s\n",
            "192:\tlearn: 110.2787878\ttotal: 42.5s\tremaining: 2m 47s\n",
            "193:\tlearn: 110.1432986\ttotal: 42.7s\tremaining: 2m 47s\n",
            "194:\tlearn: 109.6941359\ttotal: 42.9s\tremaining: 2m 47s\n",
            "195:\tlearn: 109.5807786\ttotal: 43.1s\tremaining: 2m 47s\n",
            "196:\tlearn: 109.3852300\ttotal: 43.3s\tremaining: 2m 46s\n",
            "197:\tlearn: 108.8844325\ttotal: 43.5s\tremaining: 2m 46s\n",
            "198:\tlearn: 108.4196863\ttotal: 43.7s\tremaining: 2m 46s\n",
            "199:\tlearn: 108.1298103\ttotal: 43.9s\tremaining: 2m 46s\n",
            "200:\tlearn: 107.8986532\ttotal: 44.1s\tremaining: 2m 45s\n",
            "201:\tlearn: 107.7859785\ttotal: 44.4s\tremaining: 2m 45s\n",
            "202:\tlearn: 107.6828088\ttotal: 44.7s\tremaining: 2m 45s\n",
            "203:\tlearn: 107.5237145\ttotal: 44.9s\tremaining: 2m 45s\n",
            "204:\tlearn: 107.2656526\ttotal: 45.1s\tremaining: 2m 45s\n",
            "205:\tlearn: 107.0190092\ttotal: 45.3s\tremaining: 2m 45s\n",
            "206:\tlearn: 106.7990448\ttotal: 45.5s\tremaining: 2m 44s\n",
            "207:\tlearn: 106.4826393\ttotal: 45.8s\tremaining: 2m 44s\n",
            "208:\tlearn: 106.2753135\ttotal: 46s\tremaining: 2m 44s\n",
            "209:\tlearn: 106.1626321\ttotal: 46.2s\tremaining: 2m 44s\n",
            "210:\tlearn: 105.8527351\ttotal: 46.5s\tremaining: 2m 44s\n",
            "211:\tlearn: 105.6322384\ttotal: 46.7s\tremaining: 2m 43s\n",
            "212:\tlearn: 105.5358552\ttotal: 46.9s\tremaining: 2m 43s\n",
            "213:\tlearn: 105.5352617\ttotal: 47.2s\tremaining: 2m 43s\n",
            "214:\tlearn: 105.4713101\ttotal: 47.4s\tremaining: 2m 43s\n",
            "215:\tlearn: 105.3209874\ttotal: 47.6s\tremaining: 2m 43s\n",
            "216:\tlearn: 105.2254459\ttotal: 47.8s\tremaining: 2m 42s\n",
            "217:\tlearn: 104.9487799\ttotal: 48.1s\tremaining: 2m 42s\n",
            "218:\tlearn: 104.8482493\ttotal: 48.3s\tremaining: 2m 42s\n",
            "219:\tlearn: 104.8112498\ttotal: 48.6s\tremaining: 2m 42s\n",
            "220:\tlearn: 104.8107394\ttotal: 48.8s\tremaining: 2m 42s\n",
            "221:\tlearn: 104.7353510\ttotal: 49.1s\tremaining: 2m 42s\n",
            "222:\tlearn: 104.6084856\ttotal: 49.3s\tremaining: 2m 42s\n",
            "223:\tlearn: 104.6059845\ttotal: 49.5s\tremaining: 2m 41s\n",
            "224:\tlearn: 104.4760864\ttotal: 49.7s\tremaining: 2m 41s\n",
            "225:\tlearn: 104.0621062\ttotal: 50s\tremaining: 2m 41s\n",
            "226:\tlearn: 103.9766531\ttotal: 50.2s\tremaining: 2m 41s\n",
            "227:\tlearn: 103.8020724\ttotal: 50.4s\tremaining: 2m 40s\n",
            "228:\tlearn: 103.8012768\ttotal: 50.6s\tremaining: 2m 40s\n",
            "229:\tlearn: 103.6890420\ttotal: 50.8s\tremaining: 2m 40s\n",
            "230:\tlearn: 103.4401380\ttotal: 51s\tremaining: 2m 40s\n",
            "231:\tlearn: 103.2076533\ttotal: 51.2s\tremaining: 2m 39s\n",
            "232:\tlearn: 103.0955128\ttotal: 51.4s\tremaining: 2m 39s\n",
            "233:\tlearn: 103.0948731\ttotal: 51.6s\tremaining: 2m 39s\n",
            "234:\tlearn: 102.7944574\ttotal: 51.8s\tremaining: 2m 38s\n",
            "235:\tlearn: 102.6523264\ttotal: 52s\tremaining: 2m 38s\n",
            "236:\tlearn: 102.5625032\ttotal: 52.3s\tremaining: 2m 38s\n",
            "237:\tlearn: 102.5211904\ttotal: 52.5s\tremaining: 2m 38s\n",
            "238:\tlearn: 102.5207912\ttotal: 52.7s\tremaining: 2m 38s\n",
            "239:\tlearn: 102.5180235\ttotal: 53s\tremaining: 2m 38s\n",
            "240:\tlearn: 102.4238940\ttotal: 53.3s\tremaining: 2m 38s\n",
            "241:\tlearn: 102.4202806\ttotal: 53.5s\tremaining: 2m 37s\n",
            "242:\tlearn: 102.4178012\ttotal: 53.7s\tremaining: 2m 37s\n",
            "243:\tlearn: 102.4157251\ttotal: 53.9s\tremaining: 2m 37s\n",
            "244:\tlearn: 102.3037884\ttotal: 54.1s\tremaining: 2m 37s\n",
            "245:\tlearn: 102.1133350\ttotal: 54.4s\tremaining: 2m 36s\n",
            "246:\tlearn: 102.0616863\ttotal: 54.6s\tremaining: 2m 36s\n",
            "247:\tlearn: 101.9679303\ttotal: 54.8s\tremaining: 2m 36s\n",
            "248:\tlearn: 101.9569851\ttotal: 55s\tremaining: 2m 36s\n",
            "249:\tlearn: 101.8865007\ttotal: 55.2s\tremaining: 2m 35s\n",
            "250:\tlearn: 101.7244994\ttotal: 55.4s\tremaining: 2m 35s\n",
            "251:\tlearn: 101.5924723\ttotal: 55.7s\tremaining: 2m 35s\n",
            "252:\tlearn: 101.5918859\ttotal: 55.9s\tremaining: 2m 35s\n",
            "253:\tlearn: 101.3076973\ttotal: 56.1s\tremaining: 2m 35s\n",
            "254:\tlearn: 101.0829889\ttotal: 56.3s\tremaining: 2m 34s\n",
            "255:\tlearn: 100.9716540\ttotal: 56.5s\tremaining: 2m 34s\n",
            "256:\tlearn: 100.6876664\ttotal: 56.7s\tremaining: 2m 34s\n",
            "257:\tlearn: 100.6099682\ttotal: 56.9s\tremaining: 2m 34s\n",
            "258:\tlearn: 100.3193846\ttotal: 57.2s\tremaining: 2m 33s\n",
            "259:\tlearn: 100.1902857\ttotal: 57.4s\tremaining: 2m 33s\n",
            "260:\tlearn: 100.0519249\ttotal: 57.6s\tremaining: 2m 33s\n",
            "261:\tlearn: 100.0454972\ttotal: 57.8s\tremaining: 2m 33s\n",
            "262:\tlearn: 99.8825205\ttotal: 58s\tremaining: 2m 32s\n",
            "263:\tlearn: 99.7789654\ttotal: 58.2s\tremaining: 2m 32s\n",
            "264:\tlearn: 99.5527827\ttotal: 58.4s\tremaining: 2m 32s\n",
            "265:\tlearn: 99.3720648\ttotal: 58.6s\tremaining: 2m 32s\n",
            "266:\tlearn: 99.3550609\ttotal: 58.8s\tremaining: 2m 31s\n",
            "267:\tlearn: 99.2602173\ttotal: 59.1s\tremaining: 2m 31s\n",
            "268:\tlearn: 99.1958967\ttotal: 59.3s\tremaining: 2m 31s\n",
            "269:\tlearn: 99.1484648\ttotal: 59.6s\tremaining: 2m 31s\n",
            "270:\tlearn: 99.0502372\ttotal: 59.8s\tremaining: 2m 31s\n",
            "271:\tlearn: 98.9622410\ttotal: 1m\tremaining: 2m 31s\n",
            "272:\tlearn: 98.8016407\ttotal: 1m\tremaining: 2m 30s\n",
            "273:\tlearn: 98.7987309\ttotal: 1m\tremaining: 2m 30s\n",
            "274:\tlearn: 98.5182787\ttotal: 1m\tremaining: 2m 30s\n",
            "275:\tlearn: 98.3696590\ttotal: 1m\tremaining: 2m 30s\n",
            "276:\tlearn: 98.2435936\ttotal: 1m 1s\tremaining: 2m 29s\n",
            "277:\tlearn: 98.1668220\ttotal: 1m 1s\tremaining: 2m 29s\n",
            "278:\tlearn: 97.6215651\ttotal: 1m 1s\tremaining: 2m 29s\n",
            "279:\tlearn: 97.3723019\ttotal: 1m 1s\tremaining: 2m 29s\n",
            "280:\tlearn: 97.2999110\ttotal: 1m 2s\tremaining: 2m 29s\n",
            "281:\tlearn: 97.2485755\ttotal: 1m 2s\tremaining: 2m 28s\n",
            "282:\tlearn: 97.2024568\ttotal: 1m 2s\tremaining: 2m 28s\n",
            "283:\tlearn: 97.2004245\ttotal: 1m 2s\tremaining: 2m 28s\n",
            "284:\tlearn: 97.0922215\ttotal: 1m 3s\tremaining: 2m 28s\n",
            "285:\tlearn: 96.7839557\ttotal: 1m 3s\tremaining: 2m 28s\n",
            "286:\tlearn: 96.6040881\ttotal: 1m 3s\tremaining: 2m 28s\n",
            "287:\tlearn: 96.5058277\ttotal: 1m 3s\tremaining: 2m 27s\n",
            "288:\tlearn: 96.4379361\ttotal: 1m 4s\tremaining: 2m 27s\n",
            "289:\tlearn: 96.3489823\ttotal: 1m 4s\tremaining: 2m 27s\n",
            "290:\tlearn: 96.1126395\ttotal: 1m 4s\tremaining: 2m 27s\n",
            "291:\tlearn: 95.9222498\ttotal: 1m 4s\tremaining: 2m 27s\n",
            "292:\tlearn: 95.5957416\ttotal: 1m 4s\tremaining: 2m 27s\n",
            "293:\tlearn: 95.5537938\ttotal: 1m 5s\tremaining: 2m 26s\n",
            "294:\tlearn: 95.2871689\ttotal: 1m 5s\tremaining: 2m 26s\n",
            "295:\tlearn: 95.1325342\ttotal: 1m 5s\tremaining: 2m 26s\n",
            "296:\tlearn: 94.9961584\ttotal: 1m 5s\tremaining: 2m 26s\n",
            "297:\tlearn: 94.9122851\ttotal: 1m 6s\tremaining: 2m 26s\n",
            "298:\tlearn: 94.6455344\ttotal: 1m 6s\tremaining: 2m 25s\n",
            "299:\tlearn: 94.5275244\ttotal: 1m 6s\tremaining: 2m 25s\n",
            "300:\tlearn: 94.4919148\ttotal: 1m 6s\tremaining: 2m 25s\n",
            "301:\tlearn: 94.4358849\ttotal: 1m 7s\tremaining: 2m 25s\n",
            "302:\tlearn: 94.3588295\ttotal: 1m 7s\tremaining: 2m 25s\n",
            "303:\tlearn: 94.2349214\ttotal: 1m 7s\tremaining: 2m 24s\n",
            "304:\tlearn: 94.2335216\ttotal: 1m 7s\tremaining: 2m 24s\n",
            "305:\tlearn: 93.9457907\ttotal: 1m 7s\tremaining: 2m 24s\n",
            "306:\tlearn: 93.9096620\ttotal: 1m 8s\tremaining: 2m 24s\n",
            "307:\tlearn: 93.8436863\ttotal: 1m 8s\tremaining: 2m 23s\n",
            "308:\tlearn: 93.8142882\ttotal: 1m 8s\tremaining: 2m 23s\n",
            "309:\tlearn: 93.6359884\ttotal: 1m 8s\tremaining: 2m 23s\n",
            "310:\tlearn: 93.5482641\ttotal: 1m 9s\tremaining: 2m 23s\n",
            "311:\tlearn: 93.4830902\ttotal: 1m 9s\tremaining: 2m 22s\n",
            "312:\tlearn: 93.4333973\ttotal: 1m 9s\tremaining: 2m 22s\n",
            "313:\tlearn: 93.3451819\ttotal: 1m 9s\tremaining: 2m 22s\n",
            "314:\tlearn: 93.1702114\ttotal: 1m 9s\tremaining: 2m 22s\n",
            "315:\tlearn: 92.9364825\ttotal: 1m 10s\tremaining: 2m 21s\n",
            "316:\tlearn: 92.8225067\ttotal: 1m 10s\tremaining: 2m 21s\n",
            "317:\tlearn: 92.7414436\ttotal: 1m 10s\tremaining: 2m 21s\n",
            "318:\tlearn: 92.7381999\ttotal: 1m 10s\tremaining: 2m 21s\n",
            "319:\tlearn: 92.6863804\ttotal: 1m 10s\tremaining: 2m 21s\n",
            "320:\tlearn: 92.5157379\ttotal: 1m 11s\tremaining: 2m 20s\n",
            "321:\tlearn: 92.3344720\ttotal: 1m 11s\tremaining: 2m 20s\n",
            "322:\tlearn: 92.2870141\ttotal: 1m 11s\tremaining: 2m 20s\n",
            "323:\tlearn: 92.1505691\ttotal: 1m 11s\tremaining: 2m 20s\n",
            "324:\tlearn: 91.9558300\ttotal: 1m 12s\tremaining: 2m 19s\n",
            "325:\tlearn: 91.7824941\ttotal: 1m 12s\tremaining: 2m 19s\n",
            "326:\tlearn: 91.5133099\ttotal: 1m 12s\tremaining: 2m 19s\n",
            "327:\tlearn: 91.3073666\ttotal: 1m 12s\tremaining: 2m 19s\n",
            "328:\tlearn: 91.2567084\ttotal: 1m 12s\tremaining: 2m 18s\n",
            "329:\tlearn: 91.0165129\ttotal: 1m 13s\tremaining: 2m 18s\n",
            "330:\tlearn: 90.8038834\ttotal: 1m 13s\tremaining: 2m 18s\n",
            "331:\tlearn: 90.7487153\ttotal: 1m 13s\tremaining: 2m 18s\n",
            "332:\tlearn: 90.7479513\ttotal: 1m 13s\tremaining: 2m 17s\n",
            "333:\tlearn: 90.6812460\ttotal: 1m 14s\tremaining: 2m 17s\n",
            "334:\tlearn: 90.5416181\ttotal: 1m 14s\tremaining: 2m 17s\n",
            "335:\tlearn: 90.5108121\ttotal: 1m 14s\tremaining: 2m 17s\n",
            "336:\tlearn: 90.4582478\ttotal: 1m 14s\tremaining: 2m 17s\n",
            "337:\tlearn: 90.3729497\ttotal: 1m 14s\tremaining: 2m 16s\n",
            "338:\tlearn: 90.2309876\ttotal: 1m 15s\tremaining: 2m 16s\n",
            "339:\tlearn: 90.0454122\ttotal: 1m 15s\tremaining: 2m 16s\n",
            "340:\tlearn: 89.9173442\ttotal: 1m 15s\tremaining: 2m 16s\n",
            "341:\tlearn: 89.7168404\ttotal: 1m 15s\tremaining: 2m 16s\n",
            "342:\tlearn: 89.5563066\ttotal: 1m 15s\tremaining: 2m 15s\n",
            "343:\tlearn: 89.5328062\ttotal: 1m 16s\tremaining: 2m 15s\n",
            "344:\tlearn: 89.4490134\ttotal: 1m 16s\tremaining: 2m 15s\n",
            "345:\tlearn: 89.3693506\ttotal: 1m 16s\tremaining: 2m 15s\n",
            "346:\tlearn: 89.2631185\ttotal: 1m 16s\tremaining: 2m 14s\n",
            "347:\tlearn: 89.1544560\ttotal: 1m 17s\tremaining: 2m 14s\n",
            "348:\tlearn: 89.0005338\ttotal: 1m 17s\tremaining: 2m 14s\n",
            "349:\tlearn: 88.8827717\ttotal: 1m 17s\tremaining: 2m 14s\n",
            "350:\tlearn: 88.8290073\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "351:\tlearn: 88.6014166\ttotal: 1m 17s\tremaining: 2m 13s\n",
            "352:\tlearn: 88.4582564\ttotal: 1m 18s\tremaining: 2m 13s\n",
            "353:\tlearn: 88.3763030\ttotal: 1m 18s\tremaining: 2m 13s\n",
            "354:\tlearn: 88.0439836\ttotal: 1m 18s\tremaining: 2m 13s\n",
            "355:\tlearn: 87.9718910\ttotal: 1m 18s\tremaining: 2m 12s\n",
            "356:\tlearn: 87.8644882\ttotal: 1m 19s\tremaining: 2m 12s\n",
            "357:\tlearn: 87.7126127\ttotal: 1m 19s\tremaining: 2m 12s\n",
            "358:\tlearn: 87.5675242\ttotal: 1m 19s\tremaining: 2m 12s\n",
            "359:\tlearn: 87.2895748\ttotal: 1m 19s\tremaining: 2m 12s\n",
            "360:\tlearn: 87.2283424\ttotal: 1m 20s\tremaining: 2m 12s\n",
            "361:\tlearn: 87.0825446\ttotal: 1m 20s\tremaining: 2m 11s\n",
            "362:\tlearn: 86.8964543\ttotal: 1m 20s\tremaining: 2m 11s\n",
            "363:\tlearn: 86.8939100\ttotal: 1m 20s\tremaining: 2m 11s\n",
            "364:\tlearn: 86.8219253\ttotal: 1m 21s\tremaining: 2m 11s\n",
            "365:\tlearn: 86.5495342\ttotal: 1m 21s\tremaining: 2m 11s\n",
            "366:\tlearn: 86.4615675\ttotal: 1m 21s\tremaining: 2m 10s\n",
            "367:\tlearn: 86.4083839\ttotal: 1m 21s\tremaining: 2m 10s\n",
            "368:\tlearn: 86.3533437\ttotal: 1m 22s\tremaining: 2m 10s\n",
            "369:\tlearn: 86.3138701\ttotal: 1m 22s\tremaining: 2m 10s\n",
            "370:\tlearn: 86.3098225\ttotal: 1m 22s\tremaining: 2m 10s\n",
            "371:\tlearn: 86.3079973\ttotal: 1m 22s\tremaining: 2m 10s\n",
            "372:\tlearn: 86.2791510\ttotal: 1m 23s\tremaining: 2m 9s\n",
            "373:\tlearn: 86.2788271\ttotal: 1m 23s\tremaining: 2m 9s\n",
            "374:\tlearn: 86.2637452\ttotal: 1m 23s\tremaining: 2m 9s\n",
            "375:\tlearn: 86.2282088\ttotal: 1m 23s\tremaining: 2m 9s\n",
            "376:\tlearn: 86.2029243\ttotal: 1m 23s\tremaining: 2m 8s\n",
            "377:\tlearn: 86.2014898\ttotal: 1m 24s\tremaining: 2m 8s\n",
            "378:\tlearn: 86.1651312\ttotal: 1m 24s\tremaining: 2m 8s\n",
            "379:\tlearn: 86.1625488\ttotal: 1m 24s\tremaining: 2m 8s\n",
            "380:\tlearn: 86.1219946\ttotal: 1m 24s\tremaining: 2m 8s\n",
            "381:\tlearn: 86.0350036\ttotal: 1m 25s\tremaining: 2m 8s\n",
            "382:\tlearn: 85.9946194\ttotal: 1m 25s\tremaining: 2m 7s\n",
            "383:\tlearn: 85.9445624\ttotal: 1m 25s\tremaining: 2m 7s\n",
            "384:\tlearn: 85.8286685\ttotal: 1m 25s\tremaining: 2m 7s\n",
            "385:\tlearn: 85.7139560\ttotal: 1m 26s\tremaining: 2m 7s\n",
            "386:\tlearn: 85.6332109\ttotal: 1m 26s\tremaining: 2m 6s\n",
            "387:\tlearn: 85.3496128\ttotal: 1m 26s\tremaining: 2m 6s\n",
            "388:\tlearn: 85.2084773\ttotal: 1m 26s\tremaining: 2m 6s\n",
            "389:\tlearn: 85.0862110\ttotal: 1m 26s\tremaining: 2m 6s\n",
            "390:\tlearn: 85.0472765\ttotal: 1m 27s\tremaining: 2m 5s\n",
            "391:\tlearn: 84.9580806\ttotal: 1m 27s\tremaining: 2m 5s\n",
            "392:\tlearn: 84.9478462\ttotal: 1m 27s\tremaining: 2m 5s\n",
            "393:\tlearn: 84.8794730\ttotal: 1m 27s\tremaining: 2m 5s\n",
            "394:\tlearn: 84.7862558\ttotal: 1m 28s\tremaining: 2m 5s\n",
            "395:\tlearn: 84.7094431\ttotal: 1m 28s\tremaining: 2m 4s\n",
            "396:\tlearn: 84.5847293\ttotal: 1m 28s\tremaining: 2m 4s\n",
            "397:\tlearn: 84.5446529\ttotal: 1m 28s\tremaining: 2m 4s\n",
            "398:\tlearn: 84.3802003\ttotal: 1m 28s\tremaining: 2m 4s\n",
            "399:\tlearn: 84.1222486\ttotal: 1m 29s\tremaining: 2m 3s\n",
            "400:\tlearn: 84.0334808\ttotal: 1m 29s\tremaining: 2m 3s\n",
            "401:\tlearn: 83.9074105\ttotal: 1m 29s\tremaining: 2m 3s\n",
            "402:\tlearn: 83.8884666\ttotal: 1m 29s\tremaining: 2m 3s\n",
            "403:\tlearn: 83.6906246\ttotal: 1m 30s\tremaining: 2m 2s\n",
            "404:\tlearn: 83.5559019\ttotal: 1m 30s\tremaining: 2m 2s\n",
            "405:\tlearn: 83.4223484\ttotal: 1m 30s\tremaining: 2m 2s\n",
            "406:\tlearn: 83.3970118\ttotal: 1m 30s\tremaining: 2m 2s\n",
            "407:\tlearn: 83.3582711\ttotal: 1m 30s\tremaining: 2m 2s\n",
            "408:\tlearn: 83.2912486\ttotal: 1m 31s\tremaining: 2m 1s\n",
            "409:\tlearn: 83.1641080\ttotal: 1m 31s\tremaining: 2m 1s\n",
            "410:\tlearn: 83.0947550\ttotal: 1m 31s\tremaining: 2m 1s\n",
            "411:\tlearn: 83.0470670\ttotal: 1m 31s\tremaining: 2m 1s\n",
            "412:\tlearn: 82.9644178\ttotal: 1m 31s\tremaining: 2m\n",
            "413:\tlearn: 82.8313537\ttotal: 1m 32s\tremaining: 2m\n",
            "414:\tlearn: 82.7488298\ttotal: 1m 32s\tremaining: 2m\n",
            "415:\tlearn: 82.6904149\ttotal: 1m 32s\tremaining: 2m\n",
            "416:\tlearn: 82.6894781\ttotal: 1m 32s\tremaining: 1m 59s\n",
            "417:\tlearn: 82.6025754\ttotal: 1m 33s\tremaining: 1m 59s\n",
            "418:\tlearn: 82.3099900\ttotal: 1m 33s\tremaining: 1m 59s\n",
            "419:\tlearn: 82.2676919\ttotal: 1m 33s\tremaining: 1m 59s\n",
            "420:\tlearn: 82.1366425\ttotal: 1m 33s\tremaining: 1m 59s\n",
            "421:\tlearn: 82.0389100\ttotal: 1m 33s\tremaining: 1m 58s\n",
            "422:\tlearn: 81.9777885\ttotal: 1m 34s\tremaining: 1m 58s\n",
            "423:\tlearn: 81.9423570\ttotal: 1m 34s\tremaining: 1m 58s\n",
            "424:\tlearn: 81.8848340\ttotal: 1m 34s\tremaining: 1m 58s\n",
            "425:\tlearn: 81.8170636\ttotal: 1m 34s\tremaining: 1m 57s\n",
            "426:\tlearn: 81.7854354\ttotal: 1m 35s\tremaining: 1m 57s\n",
            "427:\tlearn: 81.7849524\ttotal: 1m 35s\tremaining: 1m 57s\n",
            "428:\tlearn: 81.7725457\ttotal: 1m 35s\tremaining: 1m 57s\n",
            "429:\tlearn: 81.7099335\ttotal: 1m 35s\tremaining: 1m 57s\n",
            "430:\tlearn: 81.6620430\ttotal: 1m 35s\tremaining: 1m 56s\n",
            "431:\tlearn: 81.6312661\ttotal: 1m 36s\tremaining: 1m 56s\n",
            "432:\tlearn: 81.5709482\ttotal: 1m 36s\tremaining: 1m 56s\n",
            "433:\tlearn: 81.5706878\ttotal: 1m 36s\tremaining: 1m 56s\n",
            "434:\tlearn: 81.5332129\ttotal: 1m 36s\tremaining: 1m 56s\n",
            "435:\tlearn: 81.5059849\ttotal: 1m 37s\tremaining: 1m 55s\n",
            "436:\tlearn: 81.4808339\ttotal: 1m 37s\tremaining: 1m 55s\n",
            "437:\tlearn: 81.4596206\ttotal: 1m 37s\tremaining: 1m 55s\n",
            "438:\tlearn: 81.4239082\ttotal: 1m 37s\tremaining: 1m 55s\n",
            "439:\tlearn: 81.3964815\ttotal: 1m 38s\tremaining: 1m 55s\n",
            "440:\tlearn: 81.3958327\ttotal: 1m 38s\tremaining: 1m 54s\n",
            "441:\tlearn: 81.3926374\ttotal: 1m 38s\tremaining: 1m 54s\n",
            "442:\tlearn: 81.3403608\ttotal: 1m 38s\tremaining: 1m 54s\n",
            "443:\tlearn: 81.3073930\ttotal: 1m 39s\tremaining: 1m 54s\n",
            "444:\tlearn: 81.3026443\ttotal: 1m 39s\tremaining: 1m 54s\n",
            "445:\tlearn: 81.2075840\ttotal: 1m 39s\tremaining: 1m 53s\n",
            "446:\tlearn: 81.2060485\ttotal: 1m 39s\tremaining: 1m 53s\n",
            "447:\tlearn: 81.0226792\ttotal: 1m 40s\tremaining: 1m 53s\n",
            "448:\tlearn: 80.8852590\ttotal: 1m 40s\tremaining: 1m 53s\n",
            "449:\tlearn: 80.7625772\ttotal: 1m 40s\tremaining: 1m 52s\n",
            "450:\tlearn: 80.6901336\ttotal: 1m 40s\tremaining: 1m 52s\n",
            "451:\tlearn: 80.5276248\ttotal: 1m 40s\tremaining: 1m 52s\n",
            "452:\tlearn: 80.4339790\ttotal: 1m 41s\tremaining: 1m 52s\n",
            "453:\tlearn: 80.2661715\ttotal: 1m 41s\tremaining: 1m 51s\n",
            "454:\tlearn: 80.2375432\ttotal: 1m 41s\tremaining: 1m 51s\n",
            "455:\tlearn: 80.2023945\ttotal: 1m 41s\tremaining: 1m 51s\n",
            "456:\tlearn: 80.1958167\ttotal: 1m 42s\tremaining: 1m 51s\n",
            "457:\tlearn: 80.1920670\ttotal: 1m 42s\tremaining: 1m 51s\n",
            "458:\tlearn: 80.1681980\ttotal: 1m 42s\tremaining: 1m 51s\n",
            "459:\tlearn: 80.1666361\ttotal: 1m 42s\tremaining: 1m 50s\n",
            "460:\tlearn: 80.1264351\ttotal: 1m 42s\tremaining: 1m 50s\n",
            "461:\tlearn: 80.0876189\ttotal: 1m 43s\tremaining: 1m 50s\n",
            "462:\tlearn: 80.0305900\ttotal: 1m 43s\tremaining: 1m 50s\n",
            "463:\tlearn: 79.8559353\ttotal: 1m 43s\tremaining: 1m 49s\n",
            "464:\tlearn: 79.8379580\ttotal: 1m 43s\tremaining: 1m 49s\n",
            "465:\tlearn: 79.5372864\ttotal: 1m 44s\tremaining: 1m 49s\n",
            "466:\tlearn: 79.4857068\ttotal: 1m 44s\tremaining: 1m 49s\n",
            "467:\tlearn: 79.4596869\ttotal: 1m 44s\tremaining: 1m 49s\n",
            "468:\tlearn: 79.4352071\ttotal: 1m 44s\tremaining: 1m 48s\n",
            "469:\tlearn: 79.3887694\ttotal: 1m 45s\tremaining: 1m 48s\n",
            "470:\tlearn: 79.3473078\ttotal: 1m 45s\tremaining: 1m 48s\n",
            "471:\tlearn: 79.2638717\ttotal: 1m 45s\tremaining: 1m 48s\n",
            "472:\tlearn: 79.2205285\ttotal: 1m 45s\tremaining: 1m 47s\n",
            "473:\tlearn: 79.1517231\ttotal: 1m 45s\tremaining: 1m 47s\n",
            "474:\tlearn: 78.9722251\ttotal: 1m 46s\tremaining: 1m 47s\n",
            "475:\tlearn: 78.9258962\ttotal: 1m 46s\tremaining: 1m 47s\n",
            "476:\tlearn: 78.9220368\ttotal: 1m 46s\tremaining: 1m 47s\n",
            "477:\tlearn: 78.8565770\ttotal: 1m 46s\tremaining: 1m 46s\n",
            "478:\tlearn: 78.7142887\ttotal: 1m 47s\tremaining: 1m 46s\n",
            "479:\tlearn: 78.6518756\ttotal: 1m 47s\tremaining: 1m 46s\n",
            "480:\tlearn: 78.6147968\ttotal: 1m 47s\tremaining: 1m 46s\n",
            "481:\tlearn: 78.5526124\ttotal: 1m 47s\tremaining: 1m 45s\n",
            "482:\tlearn: 78.5210474\ttotal: 1m 47s\tremaining: 1m 45s\n",
            "483:\tlearn: 78.4305323\ttotal: 1m 48s\tremaining: 1m 45s\n",
            "484:\tlearn: 78.3761483\ttotal: 1m 48s\tremaining: 1m 45s\n",
            "485:\tlearn: 78.2562468\ttotal: 1m 48s\tremaining: 1m 45s\n",
            "486:\tlearn: 78.1536382\ttotal: 1m 48s\tremaining: 1m 44s\n",
            "487:\tlearn: 78.0791436\ttotal: 1m 49s\tremaining: 1m 44s\n",
            "488:\tlearn: 78.0226042\ttotal: 1m 49s\tremaining: 1m 44s\n",
            "489:\tlearn: 77.8494023\ttotal: 1m 49s\tremaining: 1m 44s\n",
            "490:\tlearn: 77.8270972\ttotal: 1m 49s\tremaining: 1m 44s\n",
            "491:\tlearn: 77.7863488\ttotal: 1m 50s\tremaining: 1m 43s\n",
            "492:\tlearn: 77.6743827\ttotal: 1m 50s\tremaining: 1m 43s\n",
            "493:\tlearn: 77.6203595\ttotal: 1m 50s\tremaining: 1m 43s\n",
            "494:\tlearn: 77.5697053\ttotal: 1m 50s\tremaining: 1m 43s\n",
            "495:\tlearn: 77.4953683\ttotal: 1m 51s\tremaining: 1m 43s\n",
            "496:\tlearn: 77.3930025\ttotal: 1m 51s\tremaining: 1m 42s\n",
            "497:\tlearn: 77.3504888\ttotal: 1m 51s\tremaining: 1m 42s\n",
            "498:\tlearn: 77.3050820\ttotal: 1m 51s\tremaining: 1m 42s\n",
            "499:\tlearn: 77.2528726\ttotal: 1m 51s\tremaining: 1m 42s\n",
            "500:\tlearn: 77.2325281\ttotal: 1m 52s\tremaining: 1m 41s\n",
            "501:\tlearn: 77.2315851\ttotal: 1m 52s\tremaining: 1m 41s\n",
            "502:\tlearn: 77.0865835\ttotal: 1m 52s\tremaining: 1m 41s\n",
            "503:\tlearn: 77.0650222\ttotal: 1m 52s\tremaining: 1m 41s\n",
            "504:\tlearn: 77.0170878\ttotal: 1m 53s\tremaining: 1m 41s\n",
            "505:\tlearn: 76.9502397\ttotal: 1m 53s\tremaining: 1m 40s\n",
            "506:\tlearn: 76.8972053\ttotal: 1m 53s\tremaining: 1m 40s\n",
            "507:\tlearn: 76.7643071\ttotal: 1m 53s\tremaining: 1m 40s\n",
            "508:\tlearn: 76.6577506\ttotal: 1m 54s\tremaining: 1m 40s\n",
            "509:\tlearn: 76.5658922\ttotal: 1m 54s\tremaining: 1m 39s\n",
            "510:\tlearn: 76.4724736\ttotal: 1m 54s\tremaining: 1m 39s\n",
            "511:\tlearn: 76.4350323\ttotal: 1m 54s\tremaining: 1m 39s\n",
            "512:\tlearn: 76.3785168\ttotal: 1m 54s\tremaining: 1m 39s\n",
            "513:\tlearn: 76.3316131\ttotal: 1m 55s\tremaining: 1m 38s\n",
            "514:\tlearn: 76.2940917\ttotal: 1m 55s\tremaining: 1m 38s\n",
            "515:\tlearn: 76.2613130\ttotal: 1m 55s\tremaining: 1m 38s\n",
            "516:\tlearn: 76.2050904\ttotal: 1m 55s\tremaining: 1m 38s\n",
            "517:\tlearn: 76.1756912\ttotal: 1m 56s\tremaining: 1m 38s\n",
            "518:\tlearn: 76.1641998\ttotal: 1m 56s\tremaining: 1m 37s\n",
            "519:\tlearn: 76.1235056\ttotal: 1m 56s\tremaining: 1m 37s\n",
            "520:\tlearn: 76.0355078\ttotal: 1m 56s\tremaining: 1m 37s\n",
            "521:\tlearn: 75.9785067\ttotal: 1m 56s\tremaining: 1m 37s\n",
            "522:\tlearn: 75.9671909\ttotal: 1m 57s\tremaining: 1m 37s\n",
            "523:\tlearn: 75.6614651\ttotal: 1m 57s\tremaining: 1m 36s\n",
            "524:\tlearn: 75.5329808\ttotal: 1m 57s\tremaining: 1m 36s\n",
            "525:\tlearn: 75.4605248\ttotal: 1m 57s\tremaining: 1m 36s\n",
            "526:\tlearn: 75.3849165\ttotal: 1m 58s\tremaining: 1m 36s\n",
            "527:\tlearn: 75.3104659\ttotal: 1m 58s\tremaining: 1m 35s\n",
            "528:\tlearn: 75.1188755\ttotal: 1m 58s\tremaining: 1m 35s\n",
            "529:\tlearn: 75.0696572\ttotal: 1m 58s\tremaining: 1m 35s\n",
            "530:\tlearn: 75.0280726\ttotal: 1m 58s\tremaining: 1m 35s\n",
            "531:\tlearn: 74.9545153\ttotal: 1m 59s\tremaining: 1m 34s\n",
            "532:\tlearn: 74.9198696\ttotal: 1m 59s\tremaining: 1m 34s\n",
            "533:\tlearn: 74.7950791\ttotal: 1m 59s\tremaining: 1m 34s\n",
            "534:\tlearn: 74.6962079\ttotal: 1m 59s\tremaining: 1m 34s\n",
            "535:\tlearn: 74.6791185\ttotal: 1m 59s\tremaining: 1m 34s\n",
            "536:\tlearn: 74.6734957\ttotal: 2m\tremaining: 1m 33s\n",
            "537:\tlearn: 74.6572776\ttotal: 2m\tremaining: 1m 33s\n",
            "538:\tlearn: 74.6126007\ttotal: 2m\tremaining: 1m 33s\n",
            "539:\tlearn: 74.5752102\ttotal: 2m\tremaining: 1m 33s\n",
            "540:\tlearn: 74.5564079\ttotal: 2m 1s\tremaining: 1m 32s\n",
            "541:\tlearn: 74.4444535\ttotal: 2m 1s\tremaining: 1m 32s\n",
            "542:\tlearn: 74.4314972\ttotal: 2m 1s\tremaining: 1m 32s\n",
            "543:\tlearn: 74.3275624\ttotal: 2m 1s\tremaining: 1m 32s\n",
            "544:\tlearn: 74.2775342\ttotal: 2m 1s\tremaining: 1m 31s\n",
            "545:\tlearn: 74.1820913\ttotal: 2m 2s\tremaining: 1m 31s\n",
            "546:\tlearn: 74.1256050\ttotal: 2m 2s\tremaining: 1m 31s\n",
            "547:\tlearn: 74.1102236\ttotal: 2m 2s\tremaining: 1m 31s\n",
            "548:\tlearn: 74.0394735\ttotal: 2m 2s\tremaining: 1m 30s\n",
            "549:\tlearn: 74.0060566\ttotal: 2m 2s\tremaining: 1m 30s\n",
            "550:\tlearn: 73.9684909\ttotal: 2m 3s\tremaining: 1m 30s\n",
            "551:\tlearn: 73.9677226\ttotal: 2m 3s\tremaining: 1m 30s\n",
            "552:\tlearn: 73.9275474\ttotal: 2m 3s\tremaining: 1m 30s\n",
            "553:\tlearn: 73.9269240\ttotal: 2m 3s\tremaining: 1m 29s\n",
            "554:\tlearn: 73.9102079\ttotal: 2m 3s\tremaining: 1m 29s\n",
            "555:\tlearn: 73.8734666\ttotal: 2m 4s\tremaining: 1m 29s\n",
            "556:\tlearn: 73.8727912\ttotal: 2m 4s\tremaining: 1m 29s\n",
            "557:\tlearn: 73.8143524\ttotal: 2m 4s\tremaining: 1m 28s\n",
            "558:\tlearn: 73.8125792\ttotal: 2m 4s\tremaining: 1m 28s\n",
            "559:\tlearn: 73.7481312\ttotal: 2m 5s\tremaining: 1m 28s\n",
            "560:\tlearn: 73.6936846\ttotal: 2m 5s\tremaining: 1m 28s\n",
            "561:\tlearn: 73.6684104\ttotal: 2m 5s\tremaining: 1m 28s\n",
            "562:\tlearn: 73.6337875\ttotal: 2m 5s\tremaining: 1m 27s\n",
            "563:\tlearn: 73.6033447\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "564:\tlearn: 73.5864221\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "565:\tlearn: 73.5332915\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "566:\tlearn: 73.5151211\ttotal: 2m 6s\tremaining: 1m 26s\n",
            "567:\tlearn: 73.5100378\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "568:\tlearn: 73.4431659\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "569:\tlearn: 73.4338144\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "570:\tlearn: 73.4324285\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "571:\tlearn: 73.3817172\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "572:\tlearn: 73.3331732\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "573:\tlearn: 73.2531971\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "574:\tlearn: 73.1559323\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "575:\tlearn: 73.0907012\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "576:\tlearn: 72.9542514\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "577:\tlearn: 72.9111916\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "578:\tlearn: 72.9003898\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "579:\tlearn: 72.8068324\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "580:\tlearn: 72.7784847\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "581:\tlearn: 72.7221182\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "582:\tlearn: 72.6233268\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "583:\tlearn: 72.5816791\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "584:\tlearn: 72.5349371\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "585:\tlearn: 72.5289089\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "586:\tlearn: 72.5287370\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "587:\tlearn: 72.4899319\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "588:\tlearn: 72.4725702\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "589:\tlearn: 72.4495186\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "590:\tlearn: 72.4314203\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "591:\tlearn: 72.4308264\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "592:\tlearn: 72.4306910\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "593:\tlearn: 72.3305557\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "594:\tlearn: 72.3055924\ttotal: 2m 13s\tremaining: 1m 20s\n",
            "595:\tlearn: 72.2851426\ttotal: 2m 13s\tremaining: 1m 20s\n",
            "596:\tlearn: 72.2538011\ttotal: 2m 13s\tremaining: 1m 20s\n",
            "597:\tlearn: 72.2060075\ttotal: 2m 13s\tremaining: 1m 20s\n",
            "598:\tlearn: 72.0989436\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "599:\tlearn: 71.9548806\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "600:\tlearn: 71.8685050\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "601:\tlearn: 71.7975048\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "602:\tlearn: 71.7008129\ttotal: 2m 14s\tremaining: 1m 18s\n",
            "603:\tlearn: 71.6599867\ttotal: 2m 15s\tremaining: 1m 18s\n",
            "604:\tlearn: 71.6274318\ttotal: 2m 15s\tremaining: 1m 18s\n",
            "605:\tlearn: 71.5741972\ttotal: 2m 15s\tremaining: 1m 18s\n",
            "606:\tlearn: 71.4692982\ttotal: 2m 15s\tremaining: 1m 18s\n",
            "607:\tlearn: 71.4309104\ttotal: 2m 16s\tremaining: 1m 17s\n",
            "608:\tlearn: 71.4075644\ttotal: 2m 16s\tremaining: 1m 17s\n",
            "609:\tlearn: 71.2589768\ttotal: 2m 16s\tremaining: 1m 17s\n",
            "610:\tlearn: 71.1845734\ttotal: 2m 16s\tremaining: 1m 17s\n",
            "611:\tlearn: 71.1116632\ttotal: 2m 16s\tremaining: 1m 16s\n",
            "612:\tlearn: 71.0587035\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "613:\tlearn: 71.0328197\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "614:\tlearn: 71.0169135\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "615:\tlearn: 70.9531738\ttotal: 2m 17s\tremaining: 1m 16s\n",
            "616:\tlearn: 70.8821509\ttotal: 2m 17s\tremaining: 1m 15s\n",
            "617:\tlearn: 70.7440124\ttotal: 2m 18s\tremaining: 1m 15s\n",
            "618:\tlearn: 70.7083906\ttotal: 2m 18s\tremaining: 1m 15s\n",
            "619:\tlearn: 70.6611136\ttotal: 2m 18s\tremaining: 1m 15s\n",
            "620:\tlearn: 70.6399677\ttotal: 2m 18s\tremaining: 1m 14s\n",
            "621:\tlearn: 70.5443663\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "622:\tlearn: 70.5436564\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "623:\tlearn: 70.4187701\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "624:\tlearn: 70.4092602\ttotal: 2m 19s\tremaining: 1m 14s\n",
            "625:\tlearn: 70.3888056\ttotal: 2m 20s\tremaining: 1m 13s\n",
            "626:\tlearn: 70.3828787\ttotal: 2m 20s\tremaining: 1m 13s\n",
            "627:\tlearn: 70.3820902\ttotal: 2m 20s\tremaining: 1m 13s\n",
            "628:\tlearn: 70.3199592\ttotal: 2m 20s\tremaining: 1m 13s\n",
            "629:\tlearn: 70.2328976\ttotal: 2m 21s\tremaining: 1m 12s\n",
            "630:\tlearn: 70.2261883\ttotal: 2m 21s\tremaining: 1m 12s\n",
            "631:\tlearn: 70.2107594\ttotal: 2m 21s\tremaining: 1m 12s\n",
            "632:\tlearn: 70.2014873\ttotal: 2m 21s\tremaining: 1m 12s\n",
            "633:\tlearn: 70.0892885\ttotal: 2m 22s\tremaining: 1m 12s\n",
            "634:\tlearn: 70.0768869\ttotal: 2m 22s\tremaining: 1m 11s\n",
            "635:\tlearn: 70.0232896\ttotal: 2m 22s\tremaining: 1m 11s\n",
            "636:\tlearn: 70.0045829\ttotal: 2m 22s\tremaining: 1m 11s\n",
            "637:\tlearn: 69.9049545\ttotal: 2m 23s\tremaining: 1m 11s\n",
            "638:\tlearn: 69.8562806\ttotal: 2m 23s\tremaining: 1m 11s\n",
            "639:\tlearn: 69.7991462\ttotal: 2m 23s\tremaining: 1m 10s\n",
            "640:\tlearn: 69.7205069\ttotal: 2m 23s\tremaining: 1m 10s\n",
            "641:\tlearn: 69.6461500\ttotal: 2m 23s\tremaining: 1m 10s\n",
            "642:\tlearn: 69.5950114\ttotal: 2m 24s\tremaining: 1m 10s\n",
            "643:\tlearn: 69.5476036\ttotal: 2m 24s\tremaining: 1m 9s\n",
            "644:\tlearn: 69.5096513\ttotal: 2m 24s\tremaining: 1m 9s\n",
            "645:\tlearn: 69.5028761\ttotal: 2m 24s\tremaining: 1m 9s\n",
            "646:\tlearn: 69.4878504\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "647:\tlearn: 69.4289095\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "648:\tlearn: 69.4151087\ttotal: 2m 25s\tremaining: 1m 8s\n",
            "649:\tlearn: 69.3823928\ttotal: 2m 25s\tremaining: 1m 8s\n",
            "650:\tlearn: 69.3663122\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "651:\tlearn: 69.3352298\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "652:\tlearn: 69.2782949\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "653:\tlearn: 69.1718520\ttotal: 2m 26s\tremaining: 1m 7s\n",
            "654:\tlearn: 69.1470111\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "655:\tlearn: 69.0836575\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "656:\tlearn: 69.0545276\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "657:\tlearn: 69.0539807\ttotal: 2m 27s\tremaining: 1m 6s\n",
            "658:\tlearn: 69.0166615\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "659:\tlearn: 68.9272406\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "660:\tlearn: 68.9030157\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "661:\tlearn: 68.8899719\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "662:\tlearn: 68.8641376\ttotal: 2m 28s\tremaining: 1m 5s\n",
            "663:\tlearn: 68.8028339\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "664:\tlearn: 68.7876444\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "665:\tlearn: 68.7865529\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "666:\tlearn: 68.7196271\ttotal: 2m 29s\tremaining: 1m 4s\n",
            "667:\tlearn: 68.6819033\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "668:\tlearn: 68.6167928\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "669:\tlearn: 68.6081747\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "670:\tlearn: 68.5565406\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "671:\tlearn: 68.4773160\ttotal: 2m 30s\tremaining: 1m 3s\n",
            "672:\tlearn: 68.4476804\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "673:\tlearn: 68.4219115\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "674:\tlearn: 68.3955425\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "675:\tlearn: 68.3229456\ttotal: 2m 31s\tremaining: 1m 2s\n",
            "676:\tlearn: 68.3036641\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "677:\tlearn: 68.2697180\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "678:\tlearn: 68.2695993\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "679:\tlearn: 68.1729762\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "680:\tlearn: 68.1287923\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "681:\tlearn: 68.0525500\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "682:\tlearn: 68.0161176\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "683:\tlearn: 67.9858733\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "684:\tlearn: 67.8714283\ttotal: 2m 33s\tremaining: 1m\n",
            "685:\tlearn: 67.7923177\ttotal: 2m 34s\tremaining: 1m\n",
            "686:\tlearn: 67.7591335\ttotal: 2m 34s\tremaining: 1m\n",
            "687:\tlearn: 67.7377267\ttotal: 2m 34s\tremaining: 1m\n",
            "688:\tlearn: 67.7130586\ttotal: 2m 34s\tremaining: 60s\n",
            "689:\tlearn: 67.6778385\ttotal: 2m 34s\tremaining: 59.7s\n",
            "690:\tlearn: 67.6228034\ttotal: 2m 35s\tremaining: 59.5s\n",
            "691:\tlearn: 67.5934934\ttotal: 2m 35s\tremaining: 59.3s\n",
            "692:\tlearn: 67.5376471\ttotal: 2m 35s\tremaining: 59.1s\n",
            "693:\tlearn: 67.5168996\ttotal: 2m 35s\tremaining: 58.9s\n",
            "694:\tlearn: 67.5017759\ttotal: 2m 36s\tremaining: 58.6s\n",
            "695:\tlearn: 67.3577887\ttotal: 2m 36s\tremaining: 58.4s\n",
            "696:\tlearn: 67.3524536\ttotal: 2m 36s\tremaining: 58.2s\n",
            "697:\tlearn: 67.3302007\ttotal: 2m 36s\tremaining: 57.9s\n",
            "698:\tlearn: 67.3041799\ttotal: 2m 37s\tremaining: 57.7s\n",
            "699:\tlearn: 67.2881646\ttotal: 2m 37s\tremaining: 57.5s\n",
            "700:\tlearn: 67.2177385\ttotal: 2m 37s\tremaining: 57.3s\n",
            "701:\tlearn: 67.1520672\ttotal: 2m 37s\tremaining: 57.1s\n",
            "702:\tlearn: 67.0496941\ttotal: 2m 37s\tremaining: 56.8s\n",
            "703:\tlearn: 66.9588383\ttotal: 2m 38s\tremaining: 56.6s\n",
            "704:\tlearn: 66.9160171\ttotal: 2m 38s\tremaining: 56.4s\n",
            "705:\tlearn: 66.8554591\ttotal: 2m 38s\tremaining: 56.2s\n",
            "706:\tlearn: 66.7756128\ttotal: 2m 38s\tremaining: 56s\n",
            "707:\tlearn: 66.7069291\ttotal: 2m 39s\tremaining: 55.7s\n",
            "708:\tlearn: 66.6582951\ttotal: 2m 39s\tremaining: 55.5s\n",
            "709:\tlearn: 66.6284992\ttotal: 2m 39s\tremaining: 55.3s\n",
            "710:\tlearn: 66.6041785\ttotal: 2m 39s\tremaining: 55.1s\n",
            "711:\tlearn: 66.5441240\ttotal: 2m 40s\tremaining: 54.9s\n",
            "712:\tlearn: 66.5293061\ttotal: 2m 40s\tremaining: 54.6s\n",
            "713:\tlearn: 66.4832060\ttotal: 2m 40s\tremaining: 54.4s\n",
            "714:\tlearn: 66.4543949\ttotal: 2m 40s\tremaining: 54.2s\n",
            "715:\tlearn: 66.4206793\ttotal: 2m 40s\tremaining: 53.9s\n",
            "716:\tlearn: 66.4189136\ttotal: 2m 41s\tremaining: 53.7s\n",
            "717:\tlearn: 66.3327434\ttotal: 2m 41s\tremaining: 53.5s\n",
            "718:\tlearn: 66.3114600\ttotal: 2m 41s\tremaining: 53.3s\n",
            "719:\tlearn: 66.2774701\ttotal: 2m 41s\tremaining: 53.1s\n",
            "720:\tlearn: 66.2416113\ttotal: 2m 42s\tremaining: 52.8s\n",
            "721:\tlearn: 66.1946413\ttotal: 2m 42s\tremaining: 52.6s\n",
            "722:\tlearn: 66.1723139\ttotal: 2m 42s\tremaining: 52.4s\n",
            "723:\tlearn: 66.1391681\ttotal: 2m 42s\tremaining: 52.2s\n",
            "724:\tlearn: 66.0936965\ttotal: 2m 43s\tremaining: 51.9s\n",
            "725:\tlearn: 66.0777137\ttotal: 2m 43s\tremaining: 51.7s\n",
            "726:\tlearn: 66.0761928\ttotal: 2m 43s\tremaining: 51.5s\n",
            "727:\tlearn: 66.0517862\ttotal: 2m 43s\tremaining: 51.3s\n",
            "728:\tlearn: 66.0514460\ttotal: 2m 44s\tremaining: 51.1s\n",
            "729:\tlearn: 66.0133585\ttotal: 2m 44s\tremaining: 50.8s\n",
            "730:\tlearn: 65.9989366\ttotal: 2m 44s\tremaining: 50.6s\n",
            "731:\tlearn: 65.9688923\ttotal: 2m 44s\tremaining: 50.4s\n",
            "732:\tlearn: 65.9195571\ttotal: 2m 44s\tremaining: 50.2s\n",
            "733:\tlearn: 65.8922381\ttotal: 2m 45s\tremaining: 49.9s\n",
            "734:\tlearn: 65.8023736\ttotal: 2m 45s\tremaining: 49.7s\n",
            "735:\tlearn: 65.7791608\ttotal: 2m 45s\tremaining: 49.5s\n",
            "736:\tlearn: 65.7593930\ttotal: 2m 45s\tremaining: 49.3s\n",
            "737:\tlearn: 65.7535192\ttotal: 2m 46s\tremaining: 49s\n",
            "738:\tlearn: 65.7050439\ttotal: 2m 46s\tremaining: 48.8s\n",
            "739:\tlearn: 65.7007512\ttotal: 2m 46s\tremaining: 48.6s\n",
            "740:\tlearn: 65.6807015\ttotal: 2m 46s\tremaining: 48.4s\n",
            "741:\tlearn: 65.6031141\ttotal: 2m 46s\tremaining: 48.2s\n",
            "742:\tlearn: 65.5680509\ttotal: 2m 47s\tremaining: 47.9s\n",
            "743:\tlearn: 65.5225417\ttotal: 2m 47s\tremaining: 47.7s\n",
            "744:\tlearn: 65.5219125\ttotal: 2m 47s\tremaining: 47.5s\n",
            "745:\tlearn: 65.4865500\ttotal: 2m 47s\tremaining: 47.3s\n",
            "746:\tlearn: 65.4452405\ttotal: 2m 48s\tremaining: 47s\n",
            "747:\tlearn: 65.4119698\ttotal: 2m 48s\tremaining: 46.8s\n",
            "748:\tlearn: 65.3777601\ttotal: 2m 48s\tremaining: 46.6s\n",
            "749:\tlearn: 65.3714117\ttotal: 2m 48s\tremaining: 46.4s\n",
            "750:\tlearn: 65.3694632\ttotal: 2m 49s\tremaining: 46.2s\n",
            "751:\tlearn: 65.3448132\ttotal: 2m 49s\tremaining: 45.9s\n",
            "752:\tlearn: 65.2725558\ttotal: 2m 49s\tremaining: 45.7s\n",
            "753:\tlearn: 65.2388347\ttotal: 2m 49s\tremaining: 45.5s\n",
            "754:\tlearn: 65.1459809\ttotal: 2m 49s\tremaining: 45.2s\n",
            "755:\tlearn: 65.0049839\ttotal: 2m 50s\tremaining: 45s\n",
            "756:\tlearn: 64.9549164\ttotal: 2m 50s\tremaining: 44.8s\n",
            "757:\tlearn: 64.8712764\ttotal: 2m 50s\tremaining: 44.5s\n",
            "758:\tlearn: 64.8352261\ttotal: 2m 50s\tremaining: 44.3s\n",
            "759:\tlearn: 64.7748381\ttotal: 2m 51s\tremaining: 44.1s\n",
            "760:\tlearn: 64.7151836\ttotal: 2m 51s\tremaining: 43.9s\n",
            "761:\tlearn: 64.6730810\ttotal: 2m 51s\tremaining: 43.6s\n",
            "762:\tlearn: 64.6157474\ttotal: 2m 51s\tremaining: 43.4s\n",
            "763:\tlearn: 64.5530996\ttotal: 2m 51s\tremaining: 43.2s\n",
            "764:\tlearn: 64.5119721\ttotal: 2m 52s\tremaining: 43s\n",
            "765:\tlearn: 64.4296777\ttotal: 2m 52s\tremaining: 42.7s\n",
            "766:\tlearn: 64.3901914\ttotal: 2m 52s\tremaining: 42.5s\n",
            "767:\tlearn: 64.3452704\ttotal: 2m 52s\tremaining: 42.3s\n",
            "768:\tlearn: 64.3117143\ttotal: 2m 52s\tremaining: 42.1s\n",
            "769:\tlearn: 64.2582921\ttotal: 2m 53s\tremaining: 41.8s\n",
            "770:\tlearn: 64.2104562\ttotal: 2m 53s\tremaining: 41.6s\n",
            "771:\tlearn: 64.2092993\ttotal: 2m 53s\tremaining: 41.4s\n",
            "772:\tlearn: 64.1795670\ttotal: 2m 53s\tremaining: 41.2s\n",
            "773:\tlearn: 64.1673709\ttotal: 2m 54s\tremaining: 41s\n",
            "774:\tlearn: 64.1607319\ttotal: 2m 54s\tremaining: 40.8s\n",
            "775:\tlearn: 64.1346200\ttotal: 2m 54s\tremaining: 40.5s\n",
            "776:\tlearn: 64.1329599\ttotal: 2m 55s\tremaining: 40.3s\n",
            "777:\tlearn: 64.1309503\ttotal: 2m 55s\tremaining: 40.1s\n",
            "778:\tlearn: 64.0982707\ttotal: 2m 55s\tremaining: 39.9s\n",
            "779:\tlearn: 64.0383777\ttotal: 2m 55s\tremaining: 39.7s\n",
            "780:\tlearn: 64.0073139\ttotal: 2m 55s\tremaining: 39.4s\n",
            "781:\tlearn: 63.9536779\ttotal: 2m 56s\tremaining: 39.2s\n",
            "782:\tlearn: 63.9344744\ttotal: 2m 56s\tremaining: 39s\n",
            "783:\tlearn: 63.8432541\ttotal: 2m 56s\tremaining: 38.8s\n",
            "784:\tlearn: 63.8242169\ttotal: 2m 56s\tremaining: 38.5s\n",
            "785:\tlearn: 63.8135443\ttotal: 2m 57s\tremaining: 38.3s\n",
            "786:\tlearn: 63.7869548\ttotal: 2m 57s\tremaining: 38.1s\n",
            "787:\tlearn: 63.7864060\ttotal: 2m 57s\tremaining: 37.9s\n",
            "788:\tlearn: 63.7492763\ttotal: 2m 57s\tremaining: 37.6s\n",
            "789:\tlearn: 63.7078680\ttotal: 2m 58s\tremaining: 37.4s\n",
            "790:\tlearn: 63.6227044\ttotal: 2m 58s\tremaining: 37.2s\n",
            "791:\tlearn: 63.6103037\ttotal: 2m 58s\tremaining: 37s\n",
            "792:\tlearn: 63.5458572\ttotal: 2m 58s\tremaining: 36.7s\n",
            "793:\tlearn: 63.5186134\ttotal: 2m 58s\tremaining: 36.5s\n",
            "794:\tlearn: 63.4877697\ttotal: 2m 59s\tremaining: 36.3s\n",
            "795:\tlearn: 63.4675888\ttotal: 2m 59s\tremaining: 36s\n",
            "796:\tlearn: 63.4244383\ttotal: 2m 59s\tremaining: 35.8s\n",
            "797:\tlearn: 63.3806815\ttotal: 2m 59s\tremaining: 35.6s\n",
            "798:\tlearn: 63.3361799\ttotal: 3m\tremaining: 35.4s\n",
            "799:\tlearn: 63.3090751\ttotal: 3m\tremaining: 35.1s\n",
            "800:\tlearn: 63.3030334\ttotal: 3m\tremaining: 34.9s\n",
            "801:\tlearn: 63.2838241\ttotal: 3m\tremaining: 34.7s\n",
            "802:\tlearn: 63.2338331\ttotal: 3m\tremaining: 34.5s\n",
            "803:\tlearn: 63.1769457\ttotal: 3m 1s\tremaining: 34.2s\n",
            "804:\tlearn: 63.1478597\ttotal: 3m 1s\tremaining: 34s\n",
            "805:\tlearn: 63.0720448\ttotal: 3m 1s\tremaining: 33.8s\n",
            "806:\tlearn: 63.0444330\ttotal: 3m 1s\tremaining: 33.5s\n",
            "807:\tlearn: 63.0020671\ttotal: 3m 1s\tremaining: 33.3s\n",
            "808:\tlearn: 62.9304642\ttotal: 3m 2s\tremaining: 33.1s\n",
            "809:\tlearn: 62.9093958\ttotal: 3m 2s\tremaining: 32.9s\n",
            "810:\tlearn: 62.8806562\ttotal: 3m 2s\tremaining: 32.6s\n",
            "811:\tlearn: 62.8536686\ttotal: 3m 2s\tremaining: 32.4s\n",
            "812:\tlearn: 62.8530080\ttotal: 3m 2s\tremaining: 32.2s\n",
            "813:\tlearn: 62.8083717\ttotal: 3m 3s\tremaining: 32s\n",
            "814:\tlearn: 62.7669222\ttotal: 3m 3s\tremaining: 31.7s\n",
            "815:\tlearn: 62.6972134\ttotal: 3m 3s\tremaining: 31.5s\n",
            "816:\tlearn: 62.6174757\ttotal: 3m 3s\tremaining: 31.3s\n",
            "817:\tlearn: 62.6005260\ttotal: 3m 4s\tremaining: 31s\n",
            "818:\tlearn: 62.5881705\ttotal: 3m 4s\tremaining: 30.8s\n",
            "819:\tlearn: 62.5452715\ttotal: 3m 4s\tremaining: 30.6s\n",
            "820:\tlearn: 62.5274986\ttotal: 3m 4s\tremaining: 30.4s\n",
            "821:\tlearn: 62.4744389\ttotal: 3m 4s\tremaining: 30.1s\n",
            "822:\tlearn: 62.4249772\ttotal: 3m 5s\tremaining: 29.9s\n",
            "823:\tlearn: 62.4192750\ttotal: 3m 5s\tremaining: 29.7s\n",
            "824:\tlearn: 62.4063371\ttotal: 3m 5s\tremaining: 29.4s\n",
            "825:\tlearn: 62.3798737\ttotal: 3m 5s\tremaining: 29.2s\n",
            "826:\tlearn: 62.3441257\ttotal: 3m 5s\tremaining: 29s\n",
            "827:\tlearn: 62.3188065\ttotal: 3m 6s\tremaining: 28.8s\n",
            "828:\tlearn: 62.2845107\ttotal: 3m 6s\tremaining: 28.6s\n",
            "829:\tlearn: 62.1974920\ttotal: 3m 6s\tremaining: 28.3s\n",
            "830:\tlearn: 62.1545427\ttotal: 3m 6s\tremaining: 28.1s\n",
            "831:\tlearn: 62.1216691\ttotal: 3m 7s\tremaining: 27.9s\n",
            "832:\tlearn: 62.0638378\ttotal: 3m 7s\tremaining: 27.7s\n",
            "833:\tlearn: 62.0418705\ttotal: 3m 7s\tremaining: 27.4s\n",
            "834:\tlearn: 62.0192959\ttotal: 3m 7s\tremaining: 27.2s\n",
            "835:\tlearn: 61.9923802\ttotal: 3m 8s\tremaining: 27s\n",
            "836:\tlearn: 61.9919619\ttotal: 3m 8s\tremaining: 26.8s\n",
            "837:\tlearn: 61.9534429\ttotal: 3m 8s\tremaining: 26.5s\n",
            "838:\tlearn: 61.9296544\ttotal: 3m 8s\tremaining: 26.3s\n",
            "839:\tlearn: 61.9027910\ttotal: 3m 8s\tremaining: 26.1s\n",
            "840:\tlearn: 61.8908580\ttotal: 3m 9s\tremaining: 25.9s\n",
            "841:\tlearn: 61.8763219\ttotal: 3m 9s\tremaining: 25.6s\n",
            "842:\tlearn: 61.8515542\ttotal: 3m 9s\tremaining: 25.4s\n",
            "843:\tlearn: 61.8011709\ttotal: 3m 9s\tremaining: 25.2s\n",
            "844:\tlearn: 61.7992181\ttotal: 3m 10s\tremaining: 25s\n",
            "845:\tlearn: 61.7878821\ttotal: 3m 10s\tremaining: 24.8s\n",
            "846:\tlearn: 61.7684497\ttotal: 3m 10s\tremaining: 24.5s\n",
            "847:\tlearn: 61.6828112\ttotal: 3m 10s\tremaining: 24.3s\n",
            "848:\tlearn: 61.6339513\ttotal: 3m 11s\tremaining: 24.1s\n",
            "849:\tlearn: 61.6149967\ttotal: 3m 11s\tremaining: 23.9s\n",
            "850:\tlearn: 61.6135751\ttotal: 3m 11s\tremaining: 23.6s\n",
            "851:\tlearn: 61.5987504\ttotal: 3m 11s\tremaining: 23.4s\n",
            "852:\tlearn: 61.5597514\ttotal: 3m 12s\tremaining: 23.2s\n",
            "853:\tlearn: 61.5120042\ttotal: 3m 12s\tremaining: 23s\n",
            "854:\tlearn: 61.4860942\ttotal: 3m 12s\tremaining: 22.8s\n",
            "855:\tlearn: 61.4599707\ttotal: 3m 12s\tremaining: 22.5s\n",
            "856:\tlearn: 61.4321135\ttotal: 3m 13s\tremaining: 22.3s\n",
            "857:\tlearn: 61.4006607\ttotal: 3m 13s\tremaining: 22.1s\n",
            "858:\tlearn: 61.3754317\ttotal: 3m 13s\tremaining: 21.9s\n",
            "859:\tlearn: 61.3386752\ttotal: 3m 13s\tremaining: 21.6s\n",
            "860:\tlearn: 61.2516855\ttotal: 3m 14s\tremaining: 21.4s\n",
            "861:\tlearn: 61.1749182\ttotal: 3m 14s\tremaining: 21.2s\n",
            "862:\tlearn: 61.0937953\ttotal: 3m 14s\tremaining: 21s\n",
            "863:\tlearn: 61.0839501\ttotal: 3m 14s\tremaining: 20.7s\n",
            "864:\tlearn: 61.0477772\ttotal: 3m 14s\tremaining: 20.5s\n",
            "865:\tlearn: 60.9831206\ttotal: 3m 15s\tremaining: 20.3s\n",
            "866:\tlearn: 60.9262483\ttotal: 3m 15s\tremaining: 20.1s\n",
            "867:\tlearn: 60.8856765\ttotal: 3m 15s\tremaining: 19.8s\n",
            "868:\tlearn: 60.8824794\ttotal: 3m 15s\tremaining: 19.6s\n",
            "869:\tlearn: 60.8798344\ttotal: 3m 16s\tremaining: 19.4s\n",
            "870:\tlearn: 60.8506577\ttotal: 3m 16s\tremaining: 19.2s\n",
            "871:\tlearn: 60.8023635\ttotal: 3m 16s\tremaining: 18.9s\n",
            "872:\tlearn: 60.7943296\ttotal: 3m 16s\tremaining: 18.7s\n",
            "873:\tlearn: 60.7714719\ttotal: 3m 17s\tremaining: 18.5s\n",
            "874:\tlearn: 60.7499544\ttotal: 3m 17s\tremaining: 18.3s\n",
            "875:\tlearn: 60.6741660\ttotal: 3m 17s\tremaining: 18s\n",
            "876:\tlearn: 60.6457306\ttotal: 3m 17s\tremaining: 17.8s\n",
            "877:\tlearn: 60.6151658\ttotal: 3m 17s\tremaining: 17.6s\n",
            "878:\tlearn: 60.6145459\ttotal: 3m 18s\tremaining: 17.4s\n",
            "879:\tlearn: 60.6041418\ttotal: 3m 18s\tremaining: 17.1s\n",
            "880:\tlearn: 60.5726092\ttotal: 3m 18s\tremaining: 16.9s\n",
            "881:\tlearn: 60.5421517\ttotal: 3m 18s\tremaining: 16.7s\n",
            "882:\tlearn: 60.5037776\ttotal: 3m 19s\tremaining: 16.5s\n",
            "883:\tlearn: 60.4637640\ttotal: 3m 19s\tremaining: 16.2s\n",
            "884:\tlearn: 60.3995186\ttotal: 3m 19s\tremaining: 16s\n",
            "885:\tlearn: 60.3858942\ttotal: 3m 19s\tremaining: 15.8s\n",
            "886:\tlearn: 60.3180583\ttotal: 3m 19s\tremaining: 15.6s\n",
            "887:\tlearn: 60.2768814\ttotal: 3m 20s\tremaining: 15.3s\n",
            "888:\tlearn: 60.2374101\ttotal: 3m 20s\tremaining: 15.1s\n",
            "889:\tlearn: 60.2366342\ttotal: 3m 20s\tremaining: 14.9s\n",
            "890:\tlearn: 60.2315024\ttotal: 3m 20s\tremaining: 14.7s\n",
            "891:\tlearn: 60.1883594\ttotal: 3m 21s\tremaining: 14.4s\n",
            "892:\tlearn: 60.1709090\ttotal: 3m 21s\tremaining: 14.2s\n",
            "893:\tlearn: 60.1690168\ttotal: 3m 21s\tremaining: 14s\n",
            "894:\tlearn: 60.1488272\ttotal: 3m 21s\tremaining: 13.8s\n",
            "895:\tlearn: 60.0980604\ttotal: 3m 22s\tremaining: 13.5s\n",
            "896:\tlearn: 60.0852064\ttotal: 3m 22s\tremaining: 13.3s\n",
            "897:\tlearn: 60.0552682\ttotal: 3m 22s\tremaining: 13.1s\n",
            "898:\tlearn: 59.9747788\ttotal: 3m 22s\tremaining: 12.9s\n",
            "899:\tlearn: 59.9579426\ttotal: 3m 22s\tremaining: 12.6s\n",
            "900:\tlearn: 59.9568014\ttotal: 3m 23s\tremaining: 12.4s\n",
            "901:\tlearn: 59.9538686\ttotal: 3m 23s\tremaining: 12.2s\n",
            "902:\tlearn: 59.9378024\ttotal: 3m 23s\tremaining: 12s\n",
            "903:\tlearn: 59.8510901\ttotal: 3m 23s\tremaining: 11.7s\n",
            "904:\tlearn: 59.8141016\ttotal: 3m 24s\tremaining: 11.5s\n",
            "905:\tlearn: 59.8072705\ttotal: 3m 24s\tremaining: 11.3s\n",
            "906:\tlearn: 59.7755264\ttotal: 3m 24s\tremaining: 11s\n",
            "907:\tlearn: 59.7489334\ttotal: 3m 24s\tremaining: 10.8s\n",
            "908:\tlearn: 59.7468646\ttotal: 3m 25s\tremaining: 10.6s\n",
            "909:\tlearn: 59.6776041\ttotal: 3m 25s\tremaining: 10.4s\n",
            "910:\tlearn: 59.6006073\ttotal: 3m 25s\tremaining: 10.2s\n",
            "911:\tlearn: 59.5637167\ttotal: 3m 25s\tremaining: 9.93s\n",
            "912:\tlearn: 59.5130204\ttotal: 3m 26s\tremaining: 9.7s\n",
            "913:\tlearn: 59.4570190\ttotal: 3m 26s\tremaining: 9.48s\n",
            "914:\tlearn: 59.4333228\ttotal: 3m 26s\tremaining: 9.25s\n",
            "915:\tlearn: 59.4038868\ttotal: 3m 26s\tremaining: 9.03s\n",
            "916:\tlearn: 59.3915177\ttotal: 3m 26s\tremaining: 8.8s\n",
            "917:\tlearn: 59.3749322\ttotal: 3m 27s\tremaining: 8.58s\n",
            "918:\tlearn: 59.3630497\ttotal: 3m 27s\tremaining: 8.35s\n",
            "919:\tlearn: 59.3457618\ttotal: 3m 27s\tremaining: 8.13s\n",
            "920:\tlearn: 59.3445624\ttotal: 3m 27s\tremaining: 7.9s\n",
            "921:\tlearn: 59.2678919\ttotal: 3m 28s\tremaining: 7.68s\n",
            "922:\tlearn: 59.2360451\ttotal: 3m 28s\tremaining: 7.45s\n",
            "923:\tlearn: 59.2132560\ttotal: 3m 28s\tremaining: 7.23s\n",
            "924:\tlearn: 59.1881584\ttotal: 3m 28s\tremaining: 7s\n",
            "925:\tlearn: 59.1715438\ttotal: 3m 29s\tremaining: 6.78s\n",
            "926:\tlearn: 59.1193894\ttotal: 3m 29s\tremaining: 6.55s\n",
            "927:\tlearn: 59.1140234\ttotal: 3m 29s\tremaining: 6.33s\n",
            "928:\tlearn: 58.9856706\ttotal: 3m 29s\tremaining: 6.1s\n",
            "929:\tlearn: 58.9286325\ttotal: 3m 30s\tremaining: 5.87s\n",
            "930:\tlearn: 58.8916974\ttotal: 3m 30s\tremaining: 5.65s\n",
            "931:\tlearn: 58.8448513\ttotal: 3m 30s\tremaining: 5.42s\n",
            "932:\tlearn: 58.7957334\ttotal: 3m 30s\tremaining: 5.19s\n",
            "933:\tlearn: 58.7907282\ttotal: 3m 30s\tremaining: 4.97s\n",
            "934:\tlearn: 58.7676851\ttotal: 3m 31s\tremaining: 4.74s\n",
            "935:\tlearn: 58.7402198\ttotal: 3m 31s\tremaining: 4.52s\n",
            "936:\tlearn: 58.7039109\ttotal: 3m 31s\tremaining: 4.29s\n",
            "937:\tlearn: 58.6552140\ttotal: 3m 31s\tremaining: 4.06s\n",
            "938:\tlearn: 58.6459912\ttotal: 3m 32s\tremaining: 3.84s\n",
            "939:\tlearn: 58.5778658\ttotal: 3m 32s\tremaining: 3.61s\n",
            "940:\tlearn: 58.5274241\ttotal: 3m 32s\tremaining: 3.38s\n",
            "941:\tlearn: 58.5150664\ttotal: 3m 32s\tremaining: 3.16s\n",
            "942:\tlearn: 58.5085427\ttotal: 3m 32s\tremaining: 2.93s\n",
            "943:\tlearn: 58.4740117\ttotal: 3m 33s\tremaining: 2.71s\n",
            "944:\tlearn: 58.4433947\ttotal: 3m 33s\tremaining: 2.48s\n",
            "945:\tlearn: 58.4097270\ttotal: 3m 33s\tremaining: 2.26s\n",
            "946:\tlearn: 58.3360164\ttotal: 3m 33s\tremaining: 2.03s\n",
            "947:\tlearn: 58.3196688\ttotal: 3m 33s\tremaining: 1.8s\n",
            "948:\tlearn: 58.3129011\ttotal: 3m 34s\tremaining: 1.58s\n",
            "949:\tlearn: 58.2810750\ttotal: 3m 34s\tremaining: 1.35s\n",
            "950:\tlearn: 58.2534270\ttotal: 3m 34s\tremaining: 1.13s\n",
            "951:\tlearn: 58.1869521\ttotal: 3m 34s\tremaining: 903ms\n",
            "952:\tlearn: 58.1462576\ttotal: 3m 35s\tremaining: 677ms\n",
            "953:\tlearn: 58.1027942\ttotal: 3m 35s\tremaining: 451ms\n",
            "954:\tlearn: 58.0848287\ttotal: 3m 35s\tremaining: 226ms\n",
            "955:\tlearn: 58.0772619\ttotal: 3m 35s\tremaining: 0us\n",
            "Fold 1, Best MAE: 38.956845627351825, Best hyperparameters: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}\n",
            "Fold 2, Best MAE: 39.25723835375426, Best hyperparameters: {'iterations': 932, 'learning_rate': 0.6707699152504077, 'depth': 7, 'min_data_in_leaf': 15, 'reg_lambda': 30.26423248378008, 'subsample': 0.630224100896121, 'random_strength': 36.46720207402407, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6193404395539011}\n",
            "Fold 3, Best MAE: 39.88297935806743, Best hyperparameters: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}\n",
            "Fold 4, Best MAE: 39.96326864844413, Best hyperparameters: {'iterations': 959, 'learning_rate': 0.15246377938647954, 'depth': 11, 'min_data_in_leaf': 22, 'reg_lambda': 60.70860649497886, 'subsample': 0.630224100896121, 'random_strength': 41.49227012066627, 'od_wait': 129, 'leaf_estimation_iterations': 9, 'bagging_temperature': 40.6193387802687, 'colsample_bylevel': 0.9641530671864936}\n",
            "Fold 5, Best MAE: 39.48495224853988, Best hyperparameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}\n",
            "Fold 6, Best MAE: 40.698052925472865, Best hyperparameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}\n",
            "Fold 7, Best MAE: 38.5779982043375, Best hyperparameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}\n",
            "Fold 8, Best MAE: 41.15381890867699, Best hyperparameters: {'iterations': 848, 'learning_rate': 0.6328973842342651, 'depth': 8, 'min_data_in_leaf': 26, 'reg_lambda': 34.58816504797888, 'subsample': 0.6702438670285498, 'random_strength': 15.418176184174204, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 8.240046236200172, 'colsample_bylevel': 0.3846889167241476}\n",
            "Fold 9, Best MAE: 39.53719776650689, Best hyperparameters: {'iterations': 932, 'learning_rate': 0.6707699152504077, 'depth': 7, 'min_data_in_leaf': 15, 'reg_lambda': 30.26423248378008, 'subsample': 0.630224100896121, 'random_strength': 36.46720207402407, 'od_wait': 80, 'leaf_estimation_iterations': 9, 'bagging_temperature': 15.96269248640496, 'colsample_bylevel': 0.6193404395539011}\n",
            "Fold 10, Best MAE: 40.36773045770164, Best hyperparameters: {'iterations': 956, 'learning_rate': 0.6650173404473864, 'depth': 8, 'min_data_in_leaf': 22, 'reg_lambda': 55.792506057232174, 'subsample': 0.6338482090954157, 'random_strength': 40.03515015533865, 'od_wait': 149, 'leaf_estimation_iterations': 1, 'bagging_temperature': 99.63801371137798, 'colsample_bylevel': 0.7185802277906428}\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=1008)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "# 각 Fold에 대해서 Optuna로 하이퍼파라미터 튜닝\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index, :], train_y.iloc[val_index, :]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=313))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best RMSE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = CatBoostRegressor(**best_params)\n",
        "    model.fit(X_train, y_train, cat_features=cat_features)\n",
        "    test_pred = model.predict(test)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "# 평균 앙상블\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "# 결과 출력\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")\n",
        "\n",
        "# 최종 예측 저장\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['MLM'])\n",
        "submission['CI_HOUR'] = final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>97.257813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>336.809458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>68.130348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>156.044942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>417.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1051.383338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "0       TEST_000000    97.257813\n",
              "1       TEST_000001   336.809458\n",
              "2       TEST_000002     0.000000\n",
              "3       TEST_000003     0.000000\n",
              "4       TEST_000004    68.130348\n",
              "...             ...          ...\n",
              "244984  TEST_244984   156.044942\n",
              "244985  TEST_244985   417.542500\n",
              "244986  TEST_244986     0.000000\n",
              "244987  TEST_244987     0.000000\n",
              "244988  TEST_244988  1051.383338\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('CAT_v2_1008.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bHexh3orVzUq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AlogP</th>\n",
              "      <th>Molecular_Weight</th>\n",
              "      <th>Num_H_Acceptors</th>\n",
              "      <th>Num_H_Donors</th>\n",
              "      <th>Num_RotatableBonds</th>\n",
              "      <th>LogD</th>\n",
              "      <th>Molecular_PolarSurfaceArea</th>\n",
              "      <th>nAcid</th>\n",
              "      <th>nBase</th>\n",
              "      <th>SpAbs_A</th>\n",
              "      <th>...</th>\n",
              "      <th>MACCS_key_158</th>\n",
              "      <th>MACCS_key_159</th>\n",
              "      <th>MACCS_key_160</th>\n",
              "      <th>MACCS_key_161</th>\n",
              "      <th>MACCS_key_162</th>\n",
              "      <th>MACCS_key_163</th>\n",
              "      <th>MACCS_key_164</th>\n",
              "      <th>MACCS_key_165</th>\n",
              "      <th>MLM</th>\n",
              "      <th>HLM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.259</td>\n",
              "      <td>400.495</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3.259</td>\n",
              "      <td>117.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.689316</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.010</td>\n",
              "      <td>50.680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.169</td>\n",
              "      <td>301.407</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.172</td>\n",
              "      <td>73.47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.575899</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>29.270</td>\n",
              "      <td>50.590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.593</td>\n",
              "      <td>297.358</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.585</td>\n",
              "      <td>62.45</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>29.802128</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.586</td>\n",
              "      <td>80.892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.771</td>\n",
              "      <td>494.652</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3.475</td>\n",
              "      <td>92.60</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>45.884166</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.710</td>\n",
              "      <td>2.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.335</td>\n",
              "      <td>268.310</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.337</td>\n",
              "      <td>42.43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.308663</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>93.270</td>\n",
              "      <td>99.990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3.409</td>\n",
              "      <td>396.195</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3.409</td>\n",
              "      <td>64.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.902711</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.556</td>\n",
              "      <td>3.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>1.912</td>\n",
              "      <td>359.381</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.844</td>\n",
              "      <td>77.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.887372</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.560</td>\n",
              "      <td>47.630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3468</th>\n",
              "      <td>1.941</td>\n",
              "      <td>261.320</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2.124</td>\n",
              "      <td>70.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.546531</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>56.150</td>\n",
              "      <td>1.790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3469</th>\n",
              "      <td>0.989</td>\n",
              "      <td>284.696</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.989</td>\n",
              "      <td>91.51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.936088</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030</td>\n",
              "      <td>2.770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3470</th>\n",
              "      <td>4.321</td>\n",
              "      <td>295.399</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4.321</td>\n",
              "      <td>50.36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.713581</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.450</td>\n",
              "      <td>2.650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3471 rows × 5313 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
              "0     3.259           400.495                5             2   \n",
              "1     2.169           301.407                2             1   \n",
              "2     1.593           297.358                5             0   \n",
              "3     4.771           494.652                6             0   \n",
              "4     2.335           268.310                3             0   \n",
              "...     ...               ...              ...           ...   \n",
              "3466  3.409           396.195                3             1   \n",
              "3467  1.912           359.381                4             1   \n",
              "3468  1.941           261.320                3             1   \n",
              "3469  0.989           284.696                5             1   \n",
              "3470  4.321           295.399                2             0   \n",
              "\n",
              "      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  nAcid  nBase  \\\n",
              "0                      8  3.259                      117.37      0      0   \n",
              "1                      2  2.172                       73.47      0      0   \n",
              "2                      3  1.585                       62.45      2      1   \n",
              "3                      5  3.475                       92.60      0      1   \n",
              "4                      1  2.337                       42.43      0      0   \n",
              "...                  ...    ...                         ...    ...    ...   \n",
              "3466                   5  3.409                       64.74      0      0   \n",
              "3467                   3  1.844                       77.37      0      0   \n",
              "3468                   6  2.124                       70.14      0      0   \n",
              "3469                   5  0.989                       91.51      0      0   \n",
              "3470                   4  4.321                       50.36      0      0   \n",
              "\n",
              "        SpAbs_A  ...  MACCS_key_158  MACCS_key_159  MACCS_key_160  \\\n",
              "0     35.689316  ...              1              1              1   \n",
              "1     26.575899  ...              1              0              1   \n",
              "2     29.802128  ...              1              0              1   \n",
              "3     45.884166  ...              1              1              1   \n",
              "4     26.308663  ...              1              1              1   \n",
              "...         ...  ...            ...            ...            ...   \n",
              "3466  30.902711  ...              1              0              1   \n",
              "3467  35.887372  ...              1              1              1   \n",
              "3468  23.546531  ...              1              1              1   \n",
              "3469  23.936088  ...              1              1              0   \n",
              "3470  27.713581  ...              0              0              1   \n",
              "\n",
              "      MACCS_key_161  MACCS_key_162  MACCS_key_163  MACCS_key_164  \\\n",
              "0                 1              1              1              1   \n",
              "1                 1              1              1              1   \n",
              "2                 1              1              1              0   \n",
              "3                 1              1              1              1   \n",
              "4                 1              1              1              1   \n",
              "...             ...            ...            ...            ...   \n",
              "3466              1              1              0              1   \n",
              "3467              1              1              1              1   \n",
              "3468              1              1              1              1   \n",
              "3469              1              1              1              1   \n",
              "3470              1              1              1              1   \n",
              "\n",
              "      MACCS_key_165     MLM     HLM  \n",
              "0                 1  26.010  50.680  \n",
              "1                 1  29.270  50.590  \n",
              "2                 1   5.586  80.892  \n",
              "3                 1   5.710   2.000  \n",
              "4                 1  93.270  99.990  \n",
              "...             ...     ...     ...  \n",
              "3466              1   1.556   3.079  \n",
              "3467              1  35.560  47.630  \n",
              "3468              1  56.150   1.790  \n",
              "3469              1   0.030   2.770  \n",
              "3470              1   0.450   2.650  \n",
              "\n",
              "[3471 rows x 5313 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
