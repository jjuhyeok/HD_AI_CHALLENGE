{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kvY2HmDexFj3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets, ensemble\n",
        "from catboost import CatBoostRegressor\n",
        "from tqdm import tqdm\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "import itertools\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>3.77</td>\n",
              "      <td>15.9</td>\n",
              "      <td>2.730798</td>\n",
              "      <td>12</td>\n",
              "      <td>42.01</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>3.048333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.72</td>\n",
              "      <td>24.5</td>\n",
              "      <td>4.289058</td>\n",
              "      <td>10</td>\n",
              "      <td>67.53</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>17.138611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>65.30</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>98.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06</td>\n",
              "      <td>B726632</td>\n",
              "      <td>10.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1490</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.31</td>\n",
              "      <td>22.1</td>\n",
              "      <td>4.693735</td>\n",
              "      <td>7</td>\n",
              "      <td>43.02</td>\n",
              "      <td>43.15</td>\n",
              "      <td>41.11</td>\n",
              "      <td>1169.853455</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>2.31</td>\n",
              "      <td>22.8</td>\n",
              "      <td>2.345875</td>\n",
              "      <td>14</td>\n",
              "      <td>90.45</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>96.030556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367436</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>61.25</td>\n",
              "      <td>62.21</td>\n",
              "      <td>55.70</td>\n",
              "      <td>1333.609109</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>65.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367437</th>\n",
              "      <td>TRAIN_367437</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 2:58</td>\n",
              "      <td>D847216</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>1280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.028558</td>\n",
              "      <td>11</td>\n",
              "      <td>105.37</td>\n",
              "      <td>109.34</td>\n",
              "      <td>104.69</td>\n",
              "      <td>1955.103846</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367438</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>3.36</td>\n",
              "      <td>31.7</td>\n",
              "      <td>2.557156</td>\n",
              "      <td>15</td>\n",
              "      <td>97.73</td>\n",
              "      <td>99.10</td>\n",
              "      <td>95.78</td>\n",
              "      <td>1601.291086</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.997500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367439</th>\n",
              "      <td>TRAIN_367439</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07</td>\n",
              "      <td>N211282</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2400</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.44</td>\n",
              "      <td>10.8</td>\n",
              "      <td>3.055715</td>\n",
              "      <td>19</td>\n",
              "      <td>49.75</td>\n",
              "      <td>50.08</td>\n",
              "      <td>47.02</td>\n",
              "      <td>1191.353331</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367440</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>19.8</td>\n",
              "      <td>3.177475</td>\n",
              "      <td>22</td>\n",
              "      <td>70.10</td>\n",
              "      <td>71.89</td>\n",
              "      <td>69.62</td>\n",
              "      <td>2115.046707</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>8.464167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>367441 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000003     JP   TMR7              Cargo   0.000000   \n",
              "4       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "367436  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "367437  TRAIN_367437     JP   QYY1             Tanker   0.000000   \n",
              "367438  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "367439  TRAIN_367439     JP   TMR7              Cargo   0.000000   \n",
              "367440  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  V_WIND  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...    3.77   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...   -6.72   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...    0.00   \n",
              "3       2020-09-18 22:06  B726632     10.0     33        1490  ...   -7.31   \n",
              "4       2022-08-13 12:57  D215135     30.0     10       27600  ...    2.31   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "367436  2017-11-11 22:23  J661243     40.0     13       93200  ...     NaN   \n",
              "367437   2022-04-29 2:58  D847216     10.0      9        1280  ...    0.87   \n",
              "367438   2022-07-14 7:58  Q635545     30.0      6       25000  ...    3.36   \n",
              "367439  2020-12-22 10:07  N211282     10.0      8        2400  ...   -2.44   \n",
              "367440  2021-06-04 14:54  V628821     40.0     10       87200  ...   -0.84   \n",
              "\n",
              "        AIR_TEMPERATURE        BN  ATA_LT   DUBAI   BRENT     WTI  \\\n",
              "0                  15.9  2.730798      12   42.01   43.16   40.96   \n",
              "1                  24.5  4.289058      10   67.53   64.55   59.34   \n",
              "2                   9.4  0.000000      14   65.30   66.39   56.94   \n",
              "3                  22.1  4.693735       7   43.02   43.15   41.11   \n",
              "4                  22.8  2.345875      14   90.45   93.65   88.11   \n",
              "...                 ...       ...     ...     ...     ...     ...   \n",
              "367436              NaN       NaN       6   61.25   62.21   55.70   \n",
              "367437             17.1  1.028558      11  105.37  109.34  104.69   \n",
              "367438             31.7  2.557156      15   97.73   99.10   95.78   \n",
              "367439             10.8  3.055715      19   49.75   50.08   47.02   \n",
              "367440             19.8  3.177475      22   70.10   71.89   69.62   \n",
              "\n",
              "            BDI_ADJ  PORT_SIZE    CI_HOUR  \n",
              "0       1407.668330   0.001660   3.048333  \n",
              "1       2089.046774   0.001614  17.138611  \n",
              "2        603.193047   0.001743  98.827500  \n",
              "3       1169.853455   0.000069   0.000000  \n",
              "4       1107.944894   0.000197  96.030556  \n",
              "...             ...        ...        ...  \n",
              "367436  1333.609109   0.000360  65.850000  \n",
              "367437  1955.103846   0.000552   0.000000  \n",
              "367438  1601.291086   0.002615   0.997500  \n",
              "367439  1191.353331   0.000069   0.000000  \n",
              "367440  2115.046707   0.001660   8.464167  \n",
              "\n",
              "[367441 rows x 27 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>종가</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>15.9</td>\n",
              "      <td>2.730798</td>\n",
              "      <td>12</td>\n",
              "      <td>42.01</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>3.048333</td>\n",
              "      <td>1145.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>24.5</td>\n",
              "      <td>4.289058</td>\n",
              "      <td>10</td>\n",
              "      <td>67.53</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>17.138611</td>\n",
              "      <td>1186.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>65.30</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>98.827500</td>\n",
              "      <td>1122.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06</td>\n",
              "      <td>B726632</td>\n",
              "      <td>10.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1490</td>\n",
              "      <td>...</td>\n",
              "      <td>22.1</td>\n",
              "      <td>4.693735</td>\n",
              "      <td>7</td>\n",
              "      <td>43.02</td>\n",
              "      <td>43.15</td>\n",
              "      <td>41.11</td>\n",
              "      <td>1169.853455</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1164.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>22.8</td>\n",
              "      <td>2.345875</td>\n",
              "      <td>14</td>\n",
              "      <td>90.45</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>96.030556</td>\n",
              "      <td>1301.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367436</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>61.25</td>\n",
              "      <td>62.21</td>\n",
              "      <td>55.70</td>\n",
              "      <td>1333.609109</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>65.850000</td>\n",
              "      <td>1120.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367437</th>\n",
              "      <td>TRAIN_367437</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 2:58</td>\n",
              "      <td>D847216</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>1280</td>\n",
              "      <td>...</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.028558</td>\n",
              "      <td>11</td>\n",
              "      <td>105.37</td>\n",
              "      <td>109.34</td>\n",
              "      <td>104.69</td>\n",
              "      <td>1955.103846</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1263.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367438</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>31.7</td>\n",
              "      <td>2.557156</td>\n",
              "      <td>15</td>\n",
              "      <td>97.73</td>\n",
              "      <td>99.10</td>\n",
              "      <td>95.78</td>\n",
              "      <td>1601.291086</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.997500</td>\n",
              "      <td>1316.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367439</th>\n",
              "      <td>TRAIN_367439</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07</td>\n",
              "      <td>N211282</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2400</td>\n",
              "      <td>...</td>\n",
              "      <td>10.8</td>\n",
              "      <td>3.055715</td>\n",
              "      <td>19</td>\n",
              "      <td>49.75</td>\n",
              "      <td>50.08</td>\n",
              "      <td>47.02</td>\n",
              "      <td>1191.353331</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1109.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367440</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>19.8</td>\n",
              "      <td>3.177475</td>\n",
              "      <td>22</td>\n",
              "      <td>70.10</td>\n",
              "      <td>71.89</td>\n",
              "      <td>69.62</td>\n",
              "      <td>2115.046707</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>8.464167</td>\n",
              "      <td>1110.52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>367441 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000003     JP   TMR7              Cargo   0.000000   \n",
              "4       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "367436  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "367437  TRAIN_367437     JP   QYY1             Tanker   0.000000   \n",
              "367438  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "367439  TRAIN_367439     JP   TMR7              Cargo   0.000000   \n",
              "367440  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...   \n",
              "3       2020-09-18 22:06  B726632     10.0     33        1490  ...   \n",
              "4       2022-08-13 12:57  D215135     30.0     10       27600  ...   \n",
              "...                  ...      ...      ...    ...         ...  ...   \n",
              "367436  2017-11-11 22:23  J661243     40.0     13       93200  ...   \n",
              "367437   2022-04-29 2:58  D847216     10.0      9        1280  ...   \n",
              "367438   2022-07-14 7:58  Q635545     30.0      6       25000  ...   \n",
              "367439  2020-12-22 10:07  N211282     10.0      8        2400  ...   \n",
              "367440  2021-06-04 14:54  V628821     40.0     10       87200  ...   \n",
              "\n",
              "        AIR_TEMPERATURE        BN  ATA_LT   DUBAI   BRENT     WTI  \\\n",
              "0                  15.9  2.730798      12   42.01   43.16   40.96   \n",
              "1                  24.5  4.289058      10   67.53   64.55   59.34   \n",
              "2                   9.4  0.000000      14   65.30   66.39   56.94   \n",
              "3                  22.1  4.693735       7   43.02   43.15   41.11   \n",
              "4                  22.8  2.345875      14   90.45   93.65   88.11   \n",
              "...                 ...       ...     ...     ...     ...     ...   \n",
              "367436              NaN       NaN       6   61.25   62.21   55.70   \n",
              "367437             17.1  1.028558      11  105.37  109.34  104.69   \n",
              "367438             31.7  2.557156      15   97.73   99.10   95.78   \n",
              "367439             10.8  3.055715      19   49.75   50.08   47.02   \n",
              "367440             19.8  3.177475      22   70.10   71.89   69.62   \n",
              "\n",
              "            BDI_ADJ  PORT_SIZE    CI_HOUR       종가  \n",
              "0       1407.668330   0.001660   3.048333  1145.14  \n",
              "1       2089.046774   0.001614  17.138611  1186.62  \n",
              "2        603.193047   0.001743  98.827500  1122.99  \n",
              "3       1169.853455   0.000069   0.000000  1164.81  \n",
              "4       1107.944894   0.000197  96.030556  1301.31  \n",
              "...             ...        ...        ...      ...  \n",
              "367436  1333.609109   0.000360  65.850000  1120.96  \n",
              "367437  1955.103846   0.000552   0.000000  1263.03  \n",
              "367438  1601.291086   0.002615   0.997500  1316.41  \n",
              "367439  1191.353331   0.000069   0.000000  1109.72  \n",
              "367440  2115.046707   0.001660   8.464167  1110.52  \n",
              "\n",
              "[367441 rows x 28 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Load the USD data\n",
        "usd_data = pd.read_csv(\"USD.csv\")\n",
        "\n",
        "# 2. Convert the '날짜' column in USD data to datetime format\n",
        "#usd_data['날짜'] = pd.to_datetime(usd_data['날짜'], format='%Y- %m- %d').dt.date\n",
        "usd_data['날짜'] = pd.to_datetime(usd_data['날짜'], format='%Y-%m-%d').dt.date\n",
        "\n",
        "# Convert the 'ATA' column in train and test data to only the date format\n",
        "train['ATA_date'] = pd.to_datetime(train['ATA']).dt.date\n",
        "test['ATA_date'] = pd.to_datetime(test['ATA']).dt.date\n",
        "\n",
        "# 3. Merge the USD data with train and test data on the date columns\n",
        "train_merged = pd.merge(train, usd_data, left_on='ATA_date', right_on='날짜', how='left')\n",
        "test_merged = pd.merge(test, usd_data, left_on='ATA_date', right_on='날짜', how='left')\n",
        "\n",
        "# Drop the '날짜' and 'ATA_date' columns from merged datasets as they are redundant\n",
        "train_merged.drop(['날짜', 'ATA_date'], axis=1, inplace=True)\n",
        "test_merged.drop(['날짜', 'ATA_date'], axis=1, inplace=True)\n",
        "train_merged.drop(['거래량', '변동 %'],axis=1,inplace=True)\n",
        "test_merged.drop(['거래량', '변동 %'],axis=1,inplace=True)\n",
        "train_merged.drop(['시가', '고가', '저가'],axis=1,inplace=True)\n",
        "test_merged.drop(['시가', '고가', '저가'],axis=1,inplace=True)\n",
        "train_merged['종가'] = train_merged['종가'].str.replace(',', '').astype(float)\n",
        "test_merged['종가'] = test_merged['종가'].str.replace(',', '').astype(float)\n",
        "train = train_merged.copy()\n",
        "test = test_merged.copy()\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0포함 o x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>종가</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>15.9</td>\n",
              "      <td>2.730798</td>\n",
              "      <td>12</td>\n",
              "      <td>42.01</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>3.048333</td>\n",
              "      <td>1145.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>24.5</td>\n",
              "      <td>4.289058</td>\n",
              "      <td>10</td>\n",
              "      <td>67.53</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>17.138611</td>\n",
              "      <td>1186.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>65.30</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>98.827500</td>\n",
              "      <td>1122.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>22.8</td>\n",
              "      <td>2.345875</td>\n",
              "      <td>14</td>\n",
              "      <td>90.45</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>96.030556</td>\n",
              "      <td>1301.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000005</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>81.435335</td>\n",
              "      <td>2015-09-08 14:24</td>\n",
              "      <td>Z156413</td>\n",
              "      <td>30.0</td>\n",
              "      <td>22</td>\n",
              "      <td>18100</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22</td>\n",
              "      <td>45.75</td>\n",
              "      <td>48.89</td>\n",
              "      <td>45.92</td>\n",
              "      <td>820.288044</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>42.078056</td>\n",
              "      <td>1194.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220050</th>\n",
              "      <td>TRAIN_367433</td>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.199074</td>\n",
              "      <td>2022-03-23 8:35</td>\n",
              "      <td>Y242521</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2</td>\n",
              "      <td>63500</td>\n",
              "      <td>...</td>\n",
              "      <td>36.5</td>\n",
              "      <td>4.306719</td>\n",
              "      <td>14</td>\n",
              "      <td>111.93</td>\n",
              "      <td>120.65</td>\n",
              "      <td>113.90</td>\n",
              "      <td>2077.159292</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>53.400833</td>\n",
              "      <td>1218.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220051</th>\n",
              "      <td>TRAIN_367434</td>\n",
              "      <td>CN</td>\n",
              "      <td>QQW1</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>55.408765</td>\n",
              "      <td>2022-06-16 14:27</td>\n",
              "      <td>D236761</td>\n",
              "      <td>30.0</td>\n",
              "      <td>16</td>\n",
              "      <td>26500</td>\n",
              "      <td>...</td>\n",
              "      <td>28.2</td>\n",
              "      <td>2.651752</td>\n",
              "      <td>22</td>\n",
              "      <td>108.43</td>\n",
              "      <td>114.13</td>\n",
              "      <td>109.56</td>\n",
              "      <td>2067.433444</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>83.960833</td>\n",
              "      <td>1288.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220052</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>61.25</td>\n",
              "      <td>62.21</td>\n",
              "      <td>55.70</td>\n",
              "      <td>1333.609109</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>65.850000</td>\n",
              "      <td>1120.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220053</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>31.7</td>\n",
              "      <td>2.557156</td>\n",
              "      <td>15</td>\n",
              "      <td>97.73</td>\n",
              "      <td>99.10</td>\n",
              "      <td>95.78</td>\n",
              "      <td>1601.291086</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.997500</td>\n",
              "      <td>1316.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220054</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>19.8</td>\n",
              "      <td>3.177475</td>\n",
              "      <td>22</td>\n",
              "      <td>70.10</td>\n",
              "      <td>71.89</td>\n",
              "      <td>69.62</td>\n",
              "      <td>2115.046707</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>8.464167</td>\n",
              "      <td>1110.52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220055 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "4       TRAIN_000005     CN   NGG6          Container  81.435335   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "220050  TRAIN_367433     IN   UJM2               Bulk  30.199074   \n",
              "220051  TRAIN_367434     CN   QQW1               Bulk  55.408765   \n",
              "220052  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "220053  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "220054  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...   \n",
              "3       2022-08-13 12:57  D215135     30.0     10       27600  ...   \n",
              "4       2015-09-08 14:24  Z156413     30.0     22       18100  ...   \n",
              "...                  ...      ...      ...    ...         ...  ...   \n",
              "220050   2022-03-23 8:35  Y242521     30.0      2       63500  ...   \n",
              "220051  2022-06-16 14:27  D236761     30.0     16       26500  ...   \n",
              "220052  2017-11-11 22:23  J661243     40.0     13       93200  ...   \n",
              "220053   2022-07-14 7:58  Q635545     30.0      6       25000  ...   \n",
              "220054  2021-06-04 14:54  V628821     40.0     10       87200  ...   \n",
              "\n",
              "        AIR_TEMPERATURE        BN  ATA_LT   DUBAI   BRENT     WTI  \\\n",
              "0                  15.9  2.730798      12   42.01   43.16   40.96   \n",
              "1                  24.5  4.289058      10   67.53   64.55   59.34   \n",
              "2                   9.4  0.000000      14   65.30   66.39   56.94   \n",
              "3                  22.8  2.345875      14   90.45   93.65   88.11   \n",
              "4                   NaN       NaN      22   45.75   48.89   45.92   \n",
              "...                 ...       ...     ...     ...     ...     ...   \n",
              "220050             36.5  4.306719      14  111.93  120.65  113.90   \n",
              "220051             28.2  2.651752      22  108.43  114.13  109.56   \n",
              "220052              NaN       NaN       6   61.25   62.21   55.70   \n",
              "220053             31.7  2.557156      15   97.73   99.10   95.78   \n",
              "220054             19.8  3.177475      22   70.10   71.89   69.62   \n",
              "\n",
              "            BDI_ADJ  PORT_SIZE    CI_HOUR       종가  \n",
              "0       1407.668330   0.001660   3.048333  1145.14  \n",
              "1       2089.046774   0.001614  17.138611  1186.62  \n",
              "2        603.193047   0.001743  98.827500  1122.99  \n",
              "3       1107.944894   0.000197  96.030556  1301.31  \n",
              "4        820.288044   0.001743  42.078056  1194.96  \n",
              "...             ...        ...        ...      ...  \n",
              "220050  2077.159292   0.000217  53.400833  1218.62  \n",
              "220051  2067.433444   0.000595  83.960833  1288.23  \n",
              "220052  1333.609109   0.000360  65.850000  1120.96  \n",
              "220053  1601.291086   0.002615   0.997500  1316.41  \n",
              "220054  2115.046707   0.001660   8.464167  1110.52  \n",
              "\n",
              "[220055 rows x 28 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = train[train['DIST'] != 0]\n",
        "train = train.reset_index(drop = True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['SAMPLE_ID'],axis=1,inplace=True)\n",
        "test.drop(['SAMPLE_ID'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>rounded_hour</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022</td>\n",
              "      <td>8</td>\n",
              "      <td>27</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>6</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month  day  weekday  rounded_hour\n",
              "0  2022      8   27        5             8\n",
              "1  2022      3   27        6            21\n",
              "2  2023      1   18        2             2\n",
              "3  2016      8    2        1             1\n",
              "4  2023      1   24        1             0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['ATA'] = pd.to_datetime(train['ATA'])\n",
        "test['ATA'] = pd.to_datetime(test['ATA'])\n",
        "train['year'] = train['ATA'].dt.year\n",
        "train['month'] = train['ATA'].dt.month\n",
        "train['day'] = train['ATA'].dt.day\n",
        "train['weekday'] = train['ATA'].dt.dayofweek\n",
        "train['rounded_hour'] = (train['ATA'].dt.hour + (train['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "test['year'] = test['ATA'].dt.year\n",
        "test['month'] = test['ATA'].dt.month\n",
        "test['day'] = test['ATA'].dt.day\n",
        "test['weekday'] = test['ATA'].dt.dayofweek\n",
        "test['rounded_hour'] = (test['ATA'].dt.hour + (test['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "\n",
        "# sin, cos 변환 함수 정의\n",
        "def encode_cyclic_feature(data, column, max_val):\n",
        "    data[column + '_sin'] = np.sin(2 * np.pi * data[column] / max_val)\n",
        "    data[column + '_cos'] = np.cos(2 * np.pi * data[column] / max_val)\n",
        "    return data\n",
        "\n",
        "# 각 피처에 대해 sin, cos 변환 수행\n",
        "train = encode_cyclic_feature(train, 'month', 12)\n",
        "train = encode_cyclic_feature(train, 'day', 31)\n",
        "train = encode_cyclic_feature(train, 'weekday', 7)\n",
        "train = encode_cyclic_feature(train, 'rounded_hour', 24)\n",
        "test = encode_cyclic_feature(test, 'month', 12)\n",
        "test = encode_cyclic_feature(test, 'day', 31)\n",
        "test = encode_cyclic_feature(test, 'weekday', 7)\n",
        "test = encode_cyclic_feature(test, 'rounded_hour', 24)\n",
        "\n",
        "train.drop(['ATA'],axis=1,inplace=True)\n",
        "test.drop(['ATA'],axis=1,inplace=True)\n",
        "\n",
        "test[['year', 'month', 'day', 'weekday', 'rounded_hour']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['DEADWEIGHT', 'DEPTH','ARI_CO','ARI_PO','SHIP_TYPE_CATEGORY','DIST','ID','BREADTH','BUILT','DRAUGHT','GT','LENGTH','SHIPMANAGER','FLAG','U_WIND','V_WIND','AIR_TEMPERATURE','BN','ATA_LT','year','month','day','weekday','rounded_hour','PORT_SIZE'],axis=1,inplace=True)\n",
        "test.drop(['DEADWEIGHT', 'DEPTH', 'ARI_CO','ARI_PO','SHIP_TYPE_CATEGORY','DIST','ID','BREADTH','BUILT','DRAUGHT','GT','LENGTH','SHIPMANAGER','FLAG','U_WIND','V_WIND','AIR_TEMPERATURE','BN','ATA_LT','year','month','day','weekday','rounded_hour','PORT_SIZE'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['rounded_hour_sin','rounded_hour_cos','weekday_sin','weekday_cos'],axis=1,inplace=True)\n",
        "test.drop(['rounded_hour_sin','rounded_hour_cos','weekday_sin','weekday_cos'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 여기 위까지 11   /  0같이 학습 시 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Select the features for PCA\n",
        "features = ['WTI', 'BRENT', 'DUBAI']\n",
        "train_subset = train[features]\n",
        "test_subset = test[features]\n",
        "\n",
        "# 3. Apply PCA and get 2 principal components\n",
        "pca = PCA(n_components=1)\n",
        "train_pca = pca.fit_transform(train_subset)\n",
        "test_pca = pca.transform(test_subset)\n",
        "\n",
        "# 4. Add the principal components to the train and test data\n",
        "train['PCA1'] = train_pca[:, 0]\n",
        "test['PCA1'] = test_pca[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Select the features for PCA\n",
        "features = ['WTI', '종가']\n",
        "train_subset = train[features]\n",
        "test_subset = test[features]\n",
        "\n",
        "# 3. Apply PCA and get 2 principal components\n",
        "pca = PCA(n_components=1)\n",
        "train_pca = pca.fit_transform(train_subset)\n",
        "test_pca = pca.transform(test_subset)\n",
        "\n",
        "# 4. Add the principal components to the train and test data\n",
        "train['PCA2'] = train_pca[:, 0]\n",
        "test['PCA2'] = test_pca[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Polynomial features\n",
        "train['DUBAI_squared'] = train['DUBAI'] ** 2\n",
        "train['BRENT_squared'] = train['BRENT'] ** 2\n",
        "train['WTI_squared'] = train['WTI'] ** 2\n",
        "\n",
        "# 2. Ratios\n",
        "train['DUBAI_WTI_ratio'] = train['DUBAI'] / train['WTI']\n",
        "train['BRENT_WTI_ratio'] = train['BRENT'] / train['WTI']\n",
        "\n",
        "# 1. Polynomial features\n",
        "test['DUBAI_squared'] = test['DUBAI'] ** 2\n",
        "test['BRENT_squared'] = test['BRENT'] ** 2\n",
        "test['WTI_squared'] = test['WTI'] ** 2\n",
        "\n",
        "# 2. Ratios\n",
        "test['DUBAI_WTI_ratio'] = test['DUBAI'] / test['WTI']\n",
        "test['BRENT_WTI_ratio'] = test['BRENT'] / test['WTI']\n",
        "\n",
        "train['oil_price_mean'] = train[['DUBAI', 'BRENT', 'WTI']].mean(axis=1)\n",
        "test['oil_price_mean'] = test[['DUBAI', 'BRENT', 'WTI']].mean(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>종가</th>\n",
              "      <th>month_sin</th>\n",
              "      <th>month_cos</th>\n",
              "      <th>day_sin</th>\n",
              "      <th>day_cos</th>\n",
              "      <th>...</th>\n",
              "      <th>PCA2</th>\n",
              "      <th>DUBAI_squared</th>\n",
              "      <th>BRENT_squared</th>\n",
              "      <th>WTI_squared</th>\n",
              "      <th>DUBAI_WTI_ratio</th>\n",
              "      <th>BRENT_WTI_ratio</th>\n",
              "      <th>oil_price_mean</th>\n",
              "      <th>BDI_ADJ_DUBAI_ratio</th>\n",
              "      <th>BDI_ADJ_BRENT_ratio</th>\n",
              "      <th>BDI_ADJ_WTI_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42.01</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>3.048333</td>\n",
              "      <td>1145.14</td>\n",
              "      <td>-0.866025</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>0.101168</td>\n",
              "      <td>-0.994869</td>\n",
              "      <td>...</td>\n",
              "      <td>-29.171610</td>\n",
              "      <td>1764.8401</td>\n",
              "      <td>1862.7856</td>\n",
              "      <td>1677.7216</td>\n",
              "      <td>1.025635</td>\n",
              "      <td>1.053711</td>\n",
              "      <td>42.043333</td>\n",
              "      <td>33.507935</td>\n",
              "      <td>32.615114</td>\n",
              "      <td>34.366903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>67.53</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>17.138611</td>\n",
              "      <td>1186.62</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.836970e-16</td>\n",
              "      <td>-0.299363</td>\n",
              "      <td>-0.954139</td>\n",
              "      <td>...</td>\n",
              "      <td>14.511907</td>\n",
              "      <td>4560.3009</td>\n",
              "      <td>4166.7025</td>\n",
              "      <td>3521.2356</td>\n",
              "      <td>1.138018</td>\n",
              "      <td>1.087799</td>\n",
              "      <td>63.806667</td>\n",
              "      <td>30.935092</td>\n",
              "      <td>32.363234</td>\n",
              "      <td>35.204698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>65.30</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>98.827500</td>\n",
              "      <td>1122.99</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>-0.998717</td>\n",
              "      <td>-0.050649</td>\n",
              "      <td>...</td>\n",
              "      <td>-48.806535</td>\n",
              "      <td>4264.0900</td>\n",
              "      <td>4407.6321</td>\n",
              "      <td>3242.1636</td>\n",
              "      <td>1.146821</td>\n",
              "      <td>1.165964</td>\n",
              "      <td>62.876667</td>\n",
              "      <td>9.237260</td>\n",
              "      <td>9.085601</td>\n",
              "      <td>10.593485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43.02</td>\n",
              "      <td>43.15</td>\n",
              "      <td>41.11</td>\n",
              "      <td>1169.853455</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1164.81</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.836970e-16</td>\n",
              "      <td>-0.485302</td>\n",
              "      <td>-0.874347</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.682637</td>\n",
              "      <td>1850.7204</td>\n",
              "      <td>1861.9225</td>\n",
              "      <td>1690.0321</td>\n",
              "      <td>1.046461</td>\n",
              "      <td>1.049623</td>\n",
              "      <td>42.426667</td>\n",
              "      <td>27.193246</td>\n",
              "      <td>27.111320</td>\n",
              "      <td>28.456664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90.45</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>96.030556</td>\n",
              "      <td>1301.31</td>\n",
              "      <td>-0.866025</td>\n",
              "      <td>-5.000000e-01</td>\n",
              "      <td>0.485302</td>\n",
              "      <td>-0.874347</td>\n",
              "      <td>...</td>\n",
              "      <td>132.138770</td>\n",
              "      <td>8181.2025</td>\n",
              "      <td>8770.3225</td>\n",
              "      <td>7763.3721</td>\n",
              "      <td>1.026558</td>\n",
              "      <td>1.062876</td>\n",
              "      <td>90.736667</td>\n",
              "      <td>12.249253</td>\n",
              "      <td>11.830698</td>\n",
              "      <td>12.574565</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   DUBAI  BRENT    WTI      BDI_ADJ    CI_HOUR       종가  month_sin  \\\n",
              "0  42.01  43.16  40.96  1407.668330   3.048333  1145.14  -0.866025   \n",
              "1  67.53  64.55  59.34  2089.046774  17.138611  1186.62  -1.000000   \n",
              "2  65.30  66.39  56.94   603.193047  98.827500  1122.99   0.866025   \n",
              "3  43.02  43.15  41.11  1169.853455   0.000000  1164.81  -1.000000   \n",
              "4  90.45  93.65  88.11  1107.944894  96.030556  1301.31  -0.866025   \n",
              "\n",
              "      month_cos   day_sin   day_cos  ...        PCA2  DUBAI_squared  \\\n",
              "0  5.000000e-01  0.101168 -0.994869  ...  -29.171610      1764.8401   \n",
              "1 -1.836970e-16 -0.299363 -0.954139  ...   14.511907      4560.3009   \n",
              "2  5.000000e-01 -0.998717 -0.050649  ...  -48.806535      4264.0900   \n",
              "3 -1.836970e-16 -0.485302 -0.874347  ...   -9.682637      1850.7204   \n",
              "4 -5.000000e-01  0.485302 -0.874347  ...  132.138770      8181.2025   \n",
              "\n",
              "   BRENT_squared  WTI_squared  DUBAI_WTI_ratio  BRENT_WTI_ratio  \\\n",
              "0      1862.7856    1677.7216         1.025635         1.053711   \n",
              "1      4166.7025    3521.2356         1.138018         1.087799   \n",
              "2      4407.6321    3242.1636         1.146821         1.165964   \n",
              "3      1861.9225    1690.0321         1.046461         1.049623   \n",
              "4      8770.3225    7763.3721         1.026558         1.062876   \n",
              "\n",
              "   oil_price_mean  BDI_ADJ_DUBAI_ratio  BDI_ADJ_BRENT_ratio  BDI_ADJ_WTI_ratio  \n",
              "0       42.043333            33.507935            32.615114          34.366903  \n",
              "1       63.806667            30.935092            32.363234          35.204698  \n",
              "2       62.876667             9.237260             9.085601          10.593485  \n",
              "3       42.426667            27.193246            27.111320          28.456664  \n",
              "4       90.736667            12.249253            11.830698          12.574565  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Ratios with oil prices\n",
        "train['BDI_ADJ_DUBAI_ratio'] = train['BDI_ADJ'] / train['DUBAI']\n",
        "train['BDI_ADJ_BRENT_ratio'] = train['BDI_ADJ'] / train['BRENT']\n",
        "train['BDI_ADJ_WTI_ratio'] = train['BDI_ADJ'] / train['WTI']\n",
        "\n",
        "test['BDI_ADJ_DUBAI_ratio'] = test['BDI_ADJ'] / test['DUBAI']\n",
        "test['BDI_ADJ_BRENT_ratio'] = test['BDI_ADJ'] / test['BRENT']\n",
        "test['BDI_ADJ_WTI_ratio'] = test['BDI_ADJ'] / test['WTI']\n",
        "\n",
        "# Display the first few rows of the transformed training dataset after adding new interaction features\n",
        "train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 여까지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231013_010835\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (220055 samples, 21.13 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231013_010835\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.6\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.19041\n",
            "Disk Space Avail:   75.63 GB / 498.62 GB (15.2%)\n",
            "Train Data Rows:    220055\n",
            "Train Data Columns: 11\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.002777778, 103.30825, 210.47157)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    6204.9 MB\n",
            "\tTrain Data (Original)  Memory Usage: 19.36 MB (0.3% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DUBAI', 'BRENT', 'WTI', 'BDI_ADJ', '종가', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DUBAI', 'BRENT', 'WTI', 'BDI_ADJ', '종가', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t11 features in original data used to generate 11 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 19.36 MB (0.3% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.3s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.011360796164595215, Train Rows: 217555, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-31.6104\t = Validation score   (-mean_absolute_error)\n",
            "\t0.75s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-23.2849\t = Validation score   (-mean_absolute_error)\n",
            "\t0.82s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_set's l1: 63.0481\n",
            "[2000]\tvalid_set's l1: 51.3807\n",
            "[3000]\tvalid_set's l1: 44.6376\n",
            "[4000]\tvalid_set's l1: 40.1058\n",
            "[5000]\tvalid_set's l1: 36.8774\n",
            "[6000]\tvalid_set's l1: 34.589\n",
            "[7000]\tvalid_set's l1: 32.659\n",
            "[8000]\tvalid_set's l1: 31.1117\n",
            "[9000]\tvalid_set's l1: 29.7672\n",
            "[10000]\tvalid_set's l1: 28.5996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t-28.5975\t = Validation score   (-mean_absolute_error)\n",
            "\t23.47s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_set's l1: 47.483\n",
            "[2000]\tvalid_set's l1: 33.1863\n",
            "[3000]\tvalid_set's l1: 26.1733\n",
            "[4000]\tvalid_set's l1: 22.4433\n",
            "[5000]\tvalid_set's l1: 20.0248\n",
            "[6000]\tvalid_set's l1: 18.3516\n",
            "[7000]\tvalid_set's l1: 17.1177\n",
            "[8000]\tvalid_set's l1: 16.3392\n",
            "[9000]\tvalid_set's l1: 15.6508\n",
            "[10000]\tvalid_set's l1: 15.1717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t-15.1715\t = Validation score   (-mean_absolute_error)\n",
            "\t17.71s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-13.4291\t = Validation score   (-mean_absolute_error)\n",
            "\t41.88s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t-25.3636\t = Validation score   (-mean_absolute_error)\n",
            "\t229.98s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-13.1244\t = Validation score   (-mean_absolute_error)\n",
            "\t11.36s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-75.0309\t = Validation score   (-mean_absolute_error)\n",
            "\t89.78s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-78.4901\t = Validation score   (-mean_absolute_error)\n",
            "\t0.29s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-37.6122\t = Validation score   (-mean_absolute_error)\n",
            "\t981.57s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_set's l1: 28.9797\n",
            "[2000]\tvalid_set's l1: 18.0876\n",
            "[3000]\tvalid_set's l1: 14.5018\n",
            "[4000]\tvalid_set's l1: 12.9483\n",
            "[5000]\tvalid_set's l1: 12.1523\n",
            "[6000]\tvalid_set's l1: 11.668\n",
            "[7000]\tvalid_set's l1: 11.4397\n",
            "[8000]\tvalid_set's l1: 11.2641\n",
            "[9000]\tvalid_set's l1: 11.1501\n",
            "[10000]\tvalid_set's l1: 11.0836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t-11.0826\t = Validation score   (-mean_absolute_error)\n",
            "\t30.82s\t = Training   runtime\n",
            "\t0.32s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-11.014\t = Validation score   (-mean_absolute_error)\n",
            "\t0.15s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 1433.3s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231013_010835\\\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_test</th>\n",
              "      <th>score_val</th>\n",
              "      <th>pred_time_test</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_test_marginal</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNeighborsDist</td>\n",
              "      <td>-6.591493</td>\n",
              "      <td>-23.284874</td>\n",
              "      <td>0.444403</td>\n",
              "      <td>0.013012</td>\n",
              "      <td>0.815742</td>\n",
              "      <td>0.444403</td>\n",
              "      <td>0.013012</td>\n",
              "      <td>0.815742</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LightGBMLarge</td>\n",
              "      <td>-7.245406</td>\n",
              "      <td>-11.082593</td>\n",
              "      <td>30.634839</td>\n",
              "      <td>0.322293</td>\n",
              "      <td>30.816002</td>\n",
              "      <td>30.634839</td>\n",
              "      <td>0.322293</td>\n",
              "      <td>30.816002</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>-7.447660</td>\n",
              "      <td>-11.014002</td>\n",
              "      <td>32.170233</td>\n",
              "      <td>0.360328</td>\n",
              "      <td>42.318454</td>\n",
              "      <td>0.011010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.146132</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RandomForestMSE</td>\n",
              "      <td>-10.134063</td>\n",
              "      <td>-13.429098</td>\n",
              "      <td>1.647497</td>\n",
              "      <td>0.039035</td>\n",
              "      <td>41.877053</td>\n",
              "      <td>1.647497</td>\n",
              "      <td>0.039035</td>\n",
              "      <td>41.877053</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ExtraTreesMSE</td>\n",
              "      <td>-10.553143</td>\n",
              "      <td>-13.124426</td>\n",
              "      <td>1.524385</td>\n",
              "      <td>0.038035</td>\n",
              "      <td>11.356319</td>\n",
              "      <td>1.524385</td>\n",
              "      <td>0.038035</td>\n",
              "      <td>11.356319</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>-13.061882</td>\n",
              "      <td>-15.171476</td>\n",
              "      <td>17.376790</td>\n",
              "      <td>0.202184</td>\n",
              "      <td>17.708091</td>\n",
              "      <td>17.376790</td>\n",
              "      <td>0.202184</td>\n",
              "      <td>17.708091</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>-25.245778</td>\n",
              "      <td>-25.363623</td>\n",
              "      <td>0.235214</td>\n",
              "      <td>0.005004</td>\n",
              "      <td>229.979964</td>\n",
              "      <td>0.235214</td>\n",
              "      <td>0.005004</td>\n",
              "      <td>229.979964</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LightGBMXT</td>\n",
              "      <td>-29.551170</td>\n",
              "      <td>-28.597470</td>\n",
              "      <td>24.349126</td>\n",
              "      <td>0.286260</td>\n",
              "      <td>23.473330</td>\n",
              "      <td>24.349126</td>\n",
              "      <td>0.286260</td>\n",
              "      <td>23.473330</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>KNeighborsUnif</td>\n",
              "      <td>-29.909579</td>\n",
              "      <td>-31.610439</td>\n",
              "      <td>0.454413</td>\n",
              "      <td>0.015013</td>\n",
              "      <td>0.747680</td>\n",
              "      <td>0.454413</td>\n",
              "      <td>0.015013</td>\n",
              "      <td>0.747680</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NeuralNetTorch</td>\n",
              "      <td>-40.861173</td>\n",
              "      <td>-37.612200</td>\n",
              "      <td>0.574522</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>981.571129</td>\n",
              "      <td>0.574522</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>981.571129</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NeuralNetFastAI</td>\n",
              "      <td>-82.445953</td>\n",
              "      <td>-75.030927</td>\n",
              "      <td>0.899818</td>\n",
              "      <td>0.012011</td>\n",
              "      <td>89.779907</td>\n",
              "      <td>0.899818</td>\n",
              "      <td>0.012011</td>\n",
              "      <td>89.779907</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>-87.570283</td>\n",
              "      <td>-78.490135</td>\n",
              "      <td>0.115104</td>\n",
              "      <td>0.004004</td>\n",
              "      <td>0.286260</td>\n",
              "      <td>0.115104</td>\n",
              "      <td>0.004004</td>\n",
              "      <td>0.286260</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n",
              "0        KNeighborsDist   -6.591493 -23.284874        0.444403       0.013012   \n",
              "1         LightGBMLarge   -7.245406 -11.082593       30.634839       0.322293   \n",
              "2   WeightedEnsemble_L2   -7.447660 -11.014002       32.170233       0.360328   \n",
              "3       RandomForestMSE  -10.134063 -13.429098        1.647497       0.039035   \n",
              "4         ExtraTreesMSE  -10.553143 -13.124426        1.524385       0.038035   \n",
              "5              LightGBM  -13.061882 -15.171476       17.376790       0.202184   \n",
              "6              CatBoost  -25.245778 -25.363623        0.235214       0.005004   \n",
              "7            LightGBMXT  -29.551170 -28.597470       24.349126       0.286260   \n",
              "8        KNeighborsUnif  -29.909579 -31.610439        0.454413       0.015013   \n",
              "9        NeuralNetTorch  -40.861173 -37.612200        0.574522       0.010009   \n",
              "10      NeuralNetFastAI  -82.445953 -75.030927        0.899818       0.012011   \n",
              "11              XGBoost  -87.570283 -78.490135        0.115104       0.004004   \n",
              "\n",
              "      fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
              "0     0.815742                 0.444403                0.013012   \n",
              "1    30.816002                30.634839                0.322293   \n",
              "2    42.318454                 0.011010                0.000000   \n",
              "3    41.877053                 1.647497                0.039035   \n",
              "4    11.356319                 1.524385                0.038035   \n",
              "5    17.708091                17.376790                0.202184   \n",
              "6   229.979964                 0.235214                0.005004   \n",
              "7    23.473330                24.349126                0.286260   \n",
              "8     0.747680                 0.454413                0.015013   \n",
              "9   981.571129                 0.574522                0.010009   \n",
              "10   89.779907                 0.899818                0.012011   \n",
              "11    0.286260                 0.115104                0.004004   \n",
              "\n",
              "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
              "0            0.815742            1       True          2  \n",
              "1           30.816002            1       True         11  \n",
              "2            0.146132            2       True         12  \n",
              "3           41.877053            1       True          5  \n",
              "4           11.356319            1       True          7  \n",
              "5           17.708091            1       True          4  \n",
              "6          229.979964            1       True          6  \n",
              "7           23.473330            1       True          3  \n",
              "8            0.747680            1       True          1  \n",
              "9          981.571129            1       True         10  \n",
              "10          89.779907            1       True          8  \n",
              "11           0.286260            1       True          9  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(train)\n",
        "test_data = TabularDataset(test)\n",
        "\n",
        "\n",
        "#predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['RF','GBM','XGB'])\n",
        "predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0})\n",
        "\n",
        "predictor.leaderboard(train, silent=True)\n",
        "\n",
        "\n",
        "y_pred = predictor.predict(test_data)\n",
        "y_pred = pd.DataFrame(y_pred, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred['CI_HOUR']\n",
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231014_125112\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (220055 samples, 36.97 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231014_125112\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.6\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.19041\n",
            "Disk Space Avail:   92.29 GB / 498.62 GB (18.5%)\n",
            "Train Data Rows:    220055\n",
            "Train Data Columns: 20\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.002777778, 103.30825, 210.47157)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    6321.69 MB\n",
            "\tTrain Data (Original)  Memory Usage: 35.21 MB (0.6% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 20 | ['DUBAI', 'BRENT', 'WTI', 'BDI_ADJ', '종가', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 20 | ['DUBAI', 'BRENT', 'WTI', 'BDI_ADJ', '종가', ...]\n",
            "\t0.4s = Fit runtime\n",
            "\t20 features in original data used to generate 20 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 35.21 MB (0.6% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.45s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "\t-39.596\t = Validation score   (-mean_absolute_error)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t863.29s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "\t-29.836\t = Validation score   (-mean_absolute_error)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t848.77s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "\t-15.1696\t = Validation score   (-mean_absolute_error)\n",
            "\t84.72s\t = Training   runtime\n",
            "\t6.21s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "\t-14.761\t = Validation score   (-mean_absolute_error)\n",
            "\t21.29s\t = Training   runtime\n",
            "\t5.84s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-14.6717\t = Validation score   (-mean_absolute_error)\n",
            "\t1.2s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "\t-11.177\t = Validation score   (-mean_absolute_error)\n",
            "\t123.34s\t = Training   runtime\n",
            "\t6.03s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "\t-11.7762\t = Validation score   (-mean_absolute_error)\n",
            "\t23.55s\t = Training   runtime\n",
            "\t5.52s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-11.177\t = Validation score   (-mean_absolute_error)\n",
            "\t0.87s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 1995.67s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231014_125112\\\")\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\XGB_v3.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v3.ipynb#X53sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred_best \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(y_pred_best, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mCI_HOUR\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v3.ipynb#X53sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m submission[\u001b[39m'\u001b[39m\u001b[39mCI_HOUR\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m y_pred_best[\u001b[39m'\u001b[39m\u001b[39mCI_HOUR\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v3.ipynb#X53sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m predictor\u001b[39m.\u001b[39mleaderboard(train, silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1926\u001b[0m, in \u001b[0;36mTabularPredictor.leaderboard\u001b[1;34m(self, data, extra_info, extra_metrics, decision_threshold, only_pareto_frontier, skip_score, silent)\u001b[0m\n\u001b[0;32m   1924\u001b[0m \u001b[39mif\u001b[39;00m decision_threshold \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1925\u001b[0m     decision_threshold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_threshold\n\u001b[1;32m-> 1926\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mleaderboard(\n\u001b[0;32m   1927\u001b[0m     X\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m   1928\u001b[0m     extra_info\u001b[39m=\u001b[39;49mextra_info,\n\u001b[0;32m   1929\u001b[0m     extra_metrics\u001b[39m=\u001b[39;49mextra_metrics,\n\u001b[0;32m   1930\u001b[0m     decision_threshold\u001b[39m=\u001b[39;49mdecision_threshold,\n\u001b[0;32m   1931\u001b[0m     only_pareto_frontier\u001b[39m=\u001b[39;49monly_pareto_frontier,\n\u001b[0;32m   1932\u001b[0m     skip_score\u001b[39m=\u001b[39;49mskip_score,\n\u001b[0;32m   1933\u001b[0m     silent\u001b[39m=\u001b[39;49msilent,\n\u001b[0;32m   1934\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:822\u001b[0m, in \u001b[0;36mAbstractTabularLearner.leaderboard\u001b[1;34m(self, X, y, extra_info, extra_metrics, decision_threshold, only_pareto_frontier, skip_score, silent)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mleaderboard\u001b[39m(\n\u001b[0;32m    819\u001b[0m     \u001b[39mself\u001b[39m, X\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extra_info\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, extra_metrics\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, decision_threshold\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, only_pareto_frontier\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, skip_score\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    820\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m    821\u001b[0m     \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m         leaderboard \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore_debug(\n\u001b[0;32m    823\u001b[0m             X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, extra_info\u001b[39m=\u001b[39;49mextra_info, extra_metrics\u001b[39m=\u001b[39;49mextra_metrics, decision_threshold\u001b[39m=\u001b[39;49mdecision_threshold, skip_score\u001b[39m=\u001b[39;49mskip_score, silent\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m         )\n\u001b[0;32m    825\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    826\u001b[0m         \u001b[39mif\u001b[39;00m extra_metrics:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:512\u001b[0m, in \u001b[0;36mAbstractTabularLearner.score_debug\u001b[1;34m(self, X, y, extra_info, compute_oracle, extra_metrics, decision_threshold, skip_score, silent)\u001b[0m\n\u001b[0;32m    510\u001b[0m all_trained_models_can_infer \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mget_model_names(can_infer\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    511\u001b[0m all_trained_models_original \u001b[39m=\u001b[39m all_trained_models\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 512\u001b[0m model_pred_proba_dict, pred_time_test_marginal \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mget_model_pred_proba_dict(X\u001b[39m=\u001b[39;49mX, models\u001b[39m=\u001b[39;49mall_trained_models_can_infer, record_pred_time\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m compute_oracle:\n\u001b[0;32m    515\u001b[0m     pred_probas \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(model_pred_proba_dict\u001b[39m.\u001b[39mvalues())\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1018\u001b[0m, in \u001b[0;36mAbstractTrainer.get_model_pred_proba_dict\u001b[1;34m(self, X, models, model_pred_proba_dict, model_pred_time_dict, record_pred_time, use_val_cache, cascade, cascade_threshold)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1017\u001b[0m         preprocess_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(infer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, model_pred_proba_dict\u001b[39m=\u001b[39mmodel_pred_proba_dict)\n\u001b[1;32m-> 1018\u001b[0m     model_pred_proba_dict[model_name] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpreprocess_kwargs)\n\u001b[0;32m   1019\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1020\u001b[0m     model_pred_proba_dict[model_name] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:346\u001b[0m, in \u001b[0;36mBaggedEnsembleModel.predict_proba\u001b[1;34m(self, X, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_child(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels[\u001b[39m0\u001b[39m])\n\u001b[0;32m    345\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(X, model\u001b[39m=\u001b[39mmodel, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 346\u001b[0m pred_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(X\u001b[39m=\u001b[39;49mX, preprocess_nonadaptive\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, normalize\u001b[39m=\u001b[39;49mnormalize)\n\u001b[0;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m    348\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_child(model)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:931\u001b[0m, in \u001b[0;36mAbstractModel.predict_proba\u001b[1;34m(self, X, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[39mif\u001b[39;00m normalize \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     normalize \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_pred_probas\n\u001b[1;32m--> 931\u001b[0m y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_proba(X\u001b[39m=\u001b[39;49mX, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    932\u001b[0m \u001b[39mif\u001b[39;00m normalize:\n\u001b[0;32m    933\u001b[0m     y_pred_proba \u001b[39m=\u001b[39m normalize_pred_probas(y_pred_proba, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:946\u001b[0m, in \u001b[0;36mAbstractModel._predict_proba\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    945\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type \u001b[39min\u001b[39;00m [REGRESSION, QUANTILE]:\n\u001b[1;32m--> 946\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    947\u001b[0m     \u001b[39mreturn\u001b[39;00m y_pred\n\u001b[0;32m    949\u001b[0m y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict_proba(X)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neighbors\\_regression.py:229\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39m\"\"\"Predict the target for the provided data.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[0;32m    215\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39m    Target values.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    227\u001b[0m     \u001b[39m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[39m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    230\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neighbors\\_base.py:796\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         kwds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_params_\n\u001b[1;32m--> 796\u001b[0m     chunked_results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m    797\u001b[0m         pairwise_distances_chunked(\n\u001b[0;32m    798\u001b[0m             X,\n\u001b[0;32m    799\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[0;32m    800\u001b[0m             reduce_func\u001b[39m=\u001b[39;49mreduce_func,\n\u001b[0;32m    801\u001b[0m             metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[0;32m    802\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    803\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[0;32m    804\u001b[0m         )\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    807\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mball_tree\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkd_tree\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    808\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1859\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m reduce_func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1858\u001b[0m     chunk_size \u001b[39m=\u001b[39m D_chunk\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1859\u001b[0m     D_chunk \u001b[39m=\u001b[39m reduce_func(D_chunk, sl\u001b[39m.\u001b[39;49mstart)\n\u001b[0;32m   1860\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[0;32m   1861\u001b[0m \u001b[39myield\u001b[39;00m D_chunk\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neighbors\\_base.py:657\u001b[0m, in \u001b[0;36mKNeighborsMixin._kneighbors_reduce_func\u001b[1;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[39mCallback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[39m    The neighbors indices.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    656\u001b[0m sample_range \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(dist\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])[:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m--> 657\u001b[0m neigh_ind \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margpartition(dist, n_neighbors \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    658\u001b[0m neigh_ind \u001b[39m=\u001b[39m neigh_ind[:, :n_neighbors]\n\u001b[0;32m    659\u001b[0m \u001b[39m# argpartition doesn't guarantee sorted order, so we sort again\u001b[39;00m\n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:845\u001b[0m, in \u001b[0;36margpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[0;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margpartition\u001b[39m(a, kth, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mintroselect\u001b[39m\u001b[39m'\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    768\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[39m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[39m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    843\u001b[0m \n\u001b[0;32m    844\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margpartition\u001b[39;49m\u001b[39m'\u001b[39;49m, kth, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(train)\n",
        "test_data = TabularDataset(test)\n",
        "\n",
        "\n",
        "#predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['RF','GBM','XGB'])\n",
        "predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='best_quality',  ag_args_fit={'num_gpus': 0})\n",
        "\n",
        "y_pred_best = predictor.predict(test_data)\n",
        "y_pred_best = pd.DataFrame(y_pred_best, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred_best['CI_HOUR']\n",
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>TEST_000065</td>\n",
              "      <td>922.186829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>TEST_000067</td>\n",
              "      <td>667.122498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>TEST_000076</td>\n",
              "      <td>617.021118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>TEST_000082</td>\n",
              "      <td>590.948669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>TEST_000188</td>\n",
              "      <td>1215.226562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244838</th>\n",
              "      <td>TEST_244838</td>\n",
              "      <td>1361.329224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244897</th>\n",
              "      <td>TEST_244897</td>\n",
              "      <td>550.927917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244910</th>\n",
              "      <td>TEST_244910</td>\n",
              "      <td>1247.631348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244945</th>\n",
              "      <td>TEST_244945</td>\n",
              "      <td>885.531555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1156.092896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5830 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "65      TEST_000065   922.186829\n",
              "67      TEST_000067   667.122498\n",
              "76      TEST_000076   617.021118\n",
              "82      TEST_000082   590.948669\n",
              "188     TEST_000188  1215.226562\n",
              "...             ...          ...\n",
              "244838  TEST_244838  1361.329224\n",
              "244897  TEST_244897   550.927917\n",
              "244910  TEST_244910  1247.631348\n",
              "244945  TEST_244945   885.531555\n",
              "244988  TEST_244988  1156.092896\n",
              "\n",
              "[5830 rows x 2 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission[submission['CI_HOUR'] > 500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Get indices of samples in submission where CI_HOUR is greater than 500\n",
        "indices_to_refit = submission[submission['CI_HOUR'] >= 500].index\n",
        "\n",
        "# 2. Train a model on the entire training data using MSE as the evaluation metric\n",
        "predictor_mse = TabularPredictor(label='CI_HOUR', eval_metric='root_mean_squared_error').fit(train, presets='best_quality', ag_args_fit={'num_gpus': 0})\n",
        "\n",
        "# 3. Predict only for the selected samples using the model trained with MSE\n",
        "test_samples_to_refit = test_data.iloc[indices_to_refit]\n",
        "y_pred_refit = predictor_mse.predict(test_samples_to_refit)\n",
        "\n",
        "# 4. Replace the values in the original submission with the new predictions\n",
        "submission.loc[indices_to_refit, 'CI_HOUR'] = y_pred_refit.values\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_test</th>\n",
              "      <th>score_val</th>\n",
              "      <th>pred_time_test</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_test_marginal</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNeighborsDist_BAG_L1</td>\n",
              "      <td>-4.705637</td>\n",
              "      <td>-18.651591</td>\n",
              "      <td>1.369853</td>\n",
              "      <td>1.322202</td>\n",
              "      <td>2.369352</td>\n",
              "      <td>1.369853</td>\n",
              "      <td>1.322202</td>\n",
              "      <td>2.369352</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestMSE_BAG_L2</td>\n",
              "      <td>-6.087152</td>\n",
              "      <td>-7.843315</td>\n",
              "      <td>10.826452</td>\n",
              "      <td>31.640868</td>\n",
              "      <td>368.139901</td>\n",
              "      <td>2.218017</td>\n",
              "      <td>8.556647</td>\n",
              "      <td>161.189589</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WeightedEnsemble_L3</td>\n",
              "      <td>-6.087152</td>\n",
              "      <td>-7.843315</td>\n",
              "      <td>10.831456</td>\n",
              "      <td>31.645873</td>\n",
              "      <td>369.781198</td>\n",
              "      <td>0.005005</td>\n",
              "      <td>0.005005</td>\n",
              "      <td>1.641297</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ExtraTreesMSE_BAG_L2</td>\n",
              "      <td>-6.194661</td>\n",
              "      <td>-8.111462</td>\n",
              "      <td>10.418909</td>\n",
              "      <td>30.463945</td>\n",
              "      <td>252.829004</td>\n",
              "      <td>1.810474</td>\n",
              "      <td>7.379725</td>\n",
              "      <td>45.878691</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RandomForestMSE_BAG_L1</td>\n",
              "      <td>-7.428190</td>\n",
              "      <td>-10.608984</td>\n",
              "      <td>2.966654</td>\n",
              "      <td>10.619943</td>\n",
              "      <td>152.603943</td>\n",
              "      <td>2.966654</td>\n",
              "      <td>10.619943</td>\n",
              "      <td>152.603943</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>-7.441861</td>\n",
              "      <td>-10.475369</td>\n",
              "      <td>5.740220</td>\n",
              "      <td>20.401750</td>\n",
              "      <td>204.205781</td>\n",
              "      <td>0.007007</td>\n",
              "      <td>0.004004</td>\n",
              "      <td>2.019999</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1</td>\n",
              "      <td>-7.640721</td>\n",
              "      <td>-10.693210</td>\n",
              "      <td>2.766559</td>\n",
              "      <td>9.777804</td>\n",
              "      <td>49.581840</td>\n",
              "      <td>2.766559</td>\n",
              "      <td>9.777804</td>\n",
              "      <td>49.581840</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>KNeighborsUnif_BAG_L1</td>\n",
              "      <td>-19.081965</td>\n",
              "      <td>-24.261792</td>\n",
              "      <td>1.505368</td>\n",
              "      <td>1.364272</td>\n",
              "      <td>2.395178</td>\n",
              "      <td>1.505368</td>\n",
              "      <td>1.364272</td>\n",
              "      <td>2.395178</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    model  score_test  score_val  pred_time_test  \\\n",
              "0   KNeighborsDist_BAG_L1   -4.705637 -18.651591        1.369853   \n",
              "1  RandomForestMSE_BAG_L2   -6.087152  -7.843315       10.826452   \n",
              "2     WeightedEnsemble_L3   -6.087152  -7.843315       10.831456   \n",
              "3    ExtraTreesMSE_BAG_L2   -6.194661  -8.111462       10.418909   \n",
              "4  RandomForestMSE_BAG_L1   -7.428190 -10.608984        2.966654   \n",
              "5     WeightedEnsemble_L2   -7.441861 -10.475369        5.740220   \n",
              "6    ExtraTreesMSE_BAG_L1   -7.640721 -10.693210        2.766559   \n",
              "7   KNeighborsUnif_BAG_L1  -19.081965 -24.261792        1.505368   \n",
              "\n",
              "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
              "0       1.322202    2.369352                 1.369853                1.322202   \n",
              "1      31.640868  368.139901                 2.218017                8.556647   \n",
              "2      31.645873  369.781198                 0.005005                0.005005   \n",
              "3      30.463945  252.829004                 1.810474                7.379725   \n",
              "4      10.619943  152.603943                 2.966654               10.619943   \n",
              "5      20.401750  204.205781                 0.007007                0.004004   \n",
              "6       9.777804   49.581840                 2.766559                9.777804   \n",
              "7       1.364272    2.395178                 1.505368                1.364272   \n",
              "\n",
              "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
              "0           2.369352            1       True          2  \n",
              "1         161.189589            2       True          6  \n",
              "2           1.641297            3       True          8  \n",
              "3          45.878691            2       True          7  \n",
              "4         152.603943            1       True          3  \n",
              "5           2.019999            2       True          5  \n",
              "6          49.581840            1       True          4  \n",
              "7           2.395178            1       True          1  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing feature importance via permutation shuffling for 15 features using 5000 rows with 5 shuffle sets...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t121.11s\t= Expected runtime (24.22s per shuffle set)\n",
            "\t23.42s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>importance</th>\n",
              "      <th>stddev</th>\n",
              "      <th>p_value</th>\n",
              "      <th>n</th>\n",
              "      <th>p99_high</th>\n",
              "      <th>p99_low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <td>700.538737</td>\n",
              "      <td>4.610450</td>\n",
              "      <td>2.251132e-10</td>\n",
              "      <td>5</td>\n",
              "      <td>710.031718</td>\n",
              "      <td>691.045755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_sin</th>\n",
              "      <td>611.403242</td>\n",
              "      <td>9.600074</td>\n",
              "      <td>7.291651e-09</td>\n",
              "      <td>5</td>\n",
              "      <td>631.169927</td>\n",
              "      <td>591.636557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_cos</th>\n",
              "      <td>606.572601</td>\n",
              "      <td>3.499666</td>\n",
              "      <td>1.329649e-10</td>\n",
              "      <td>5</td>\n",
              "      <td>613.778461</td>\n",
              "      <td>599.366741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>종가</th>\n",
              "      <td>594.202767</td>\n",
              "      <td>4.975863</td>\n",
              "      <td>5.900319e-10</td>\n",
              "      <td>5</td>\n",
              "      <td>604.448137</td>\n",
              "      <td>583.957397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WTI</th>\n",
              "      <td>434.767073</td>\n",
              "      <td>4.513935</td>\n",
              "      <td>1.394163e-09</td>\n",
              "      <td>5</td>\n",
              "      <td>444.061328</td>\n",
              "      <td>425.472818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BRENT_WTI_ratio</th>\n",
              "      <td>423.840708</td>\n",
              "      <td>7.220452</td>\n",
              "      <td>1.010323e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>438.707720</td>\n",
              "      <td>408.973697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WTI_squared</th>\n",
              "      <td>393.544277</td>\n",
              "      <td>3.415519</td>\n",
              "      <td>6.807537e-10</td>\n",
              "      <td>5</td>\n",
              "      <td>400.576878</td>\n",
              "      <td>386.511676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DUBAI_squared</th>\n",
              "      <td>386.989083</td>\n",
              "      <td>4.064721</td>\n",
              "      <td>1.460311e-09</td>\n",
              "      <td>5</td>\n",
              "      <td>395.358399</td>\n",
              "      <td>378.619767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DUBAI</th>\n",
              "      <td>374.958851</td>\n",
              "      <td>2.891071</td>\n",
              "      <td>4.240795e-10</td>\n",
              "      <td>5</td>\n",
              "      <td>380.911606</td>\n",
              "      <td>369.006096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BRENT_squared</th>\n",
              "      <td>324.761469</td>\n",
              "      <td>4.213908</td>\n",
              "      <td>3.400682e-09</td>\n",
              "      <td>5</td>\n",
              "      <td>333.437965</td>\n",
              "      <td>316.084974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oil_price_mean</th>\n",
              "      <td>319.517429</td>\n",
              "      <td>3.726026</td>\n",
              "      <td>2.218746e-09</td>\n",
              "      <td>5</td>\n",
              "      <td>327.189368</td>\n",
              "      <td>311.845491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BRENT</th>\n",
              "      <td>289.995072</td>\n",
              "      <td>3.017169</td>\n",
              "      <td>1.405901e-09</td>\n",
              "      <td>5</td>\n",
              "      <td>296.207465</td>\n",
              "      <td>283.782678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DUBAI_WTI_ratio</th>\n",
              "      <td>182.538313</td>\n",
              "      <td>4.337203</td>\n",
              "      <td>3.821882e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>191.468674</td>\n",
              "      <td>173.607952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_sin</th>\n",
              "      <td>122.589680</td>\n",
              "      <td>2.819270</td>\n",
              "      <td>3.354331e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>128.394595</td>\n",
              "      <td>116.784765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_cos</th>\n",
              "      <td>85.241144</td>\n",
              "      <td>1.995500</td>\n",
              "      <td>3.601419e-08</td>\n",
              "      <td>5</td>\n",
              "      <td>89.349905</td>\n",
              "      <td>81.132382</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 importance    stddev       p_value  n    p99_high     p99_low\n",
              "BDI_ADJ          700.538737  4.610450  2.251132e-10  5  710.031718  691.045755\n",
              "month_sin        611.403242  9.600074  7.291651e-09  5  631.169927  591.636557\n",
              "month_cos        606.572601  3.499666  1.329649e-10  5  613.778461  599.366741\n",
              "종가               594.202767  4.975863  5.900319e-10  5  604.448137  583.957397\n",
              "WTI              434.767073  4.513935  1.394163e-09  5  444.061328  425.472818\n",
              "BRENT_WTI_ratio  423.840708  7.220452  1.010323e-08  5  438.707720  408.973697\n",
              "WTI_squared      393.544277  3.415519  6.807537e-10  5  400.576878  386.511676\n",
              "DUBAI_squared    386.989083  4.064721  1.460311e-09  5  395.358399  378.619767\n",
              "DUBAI            374.958851  2.891071  4.240795e-10  5  380.911606  369.006096\n",
              "BRENT_squared    324.761469  4.213908  3.400682e-09  5  333.437965  316.084974\n",
              "oil_price_mean   319.517429  3.726026  2.218746e-09  5  327.189368  311.845491\n",
              "BRENT            289.995072  3.017169  1.405901e-09  5  296.207465  283.782678\n",
              "DUBAI_WTI_ratio  182.538313  4.337203  3.821882e-08  5  191.468674  173.607952\n",
              "day_sin          122.589680  2.819270  3.354331e-08  5  128.394595  116.784765\n",
              "day_cos           85.241144  1.995500  3.601419e-08  5   89.349905   81.132382"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.feature_importance(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>93.535583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>389.344513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>2.585824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>46.205944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>489.423889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1145.928589</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "0       TEST_000000    93.535583\n",
              "1       TEST_000001   389.344513\n",
              "2       TEST_000002     0.000000\n",
              "3       TEST_000003     0.000000\n",
              "4       TEST_000004     2.585824\n",
              "...             ...          ...\n",
              "244984  TEST_244984    46.205944\n",
              "244985  TEST_244985   489.423889\n",
              "244986  TEST_244986     0.000000\n",
              "244987  TEST_244987     0.000000\n",
              "244988  TEST_244988  1145.928589\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "#all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "all.loc[all['DIST'] == 0, 'CI_HOUR'] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('best_include_0.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "#import xgboost as xgb\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def objective(trial: Trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "            'iterations':trial.suggest_int(\"iterations\", 300, 1000),\n",
        "            'learning_rate' : trial.suggest_uniform('learning_rate',0.1, 1),\n",
        "            'depth': trial.suggest_int('depth',5, 16),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
        "            'reg_lambda': trial.suggest_uniform('reg_lambda',30,100),\n",
        "            'subsample': trial.suggest_uniform('subsample',0.3,1),\n",
        "            'random_strength': trial.suggest_uniform('random_strength',10,100),\n",
        "            'od_wait':trial.suggest_int('od_wait', 10, 150),\n",
        "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,20),\n",
        "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 1, 100),\n",
        "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0., 1.0),\n",
        "            'random_state' : 724,\n",
        "            'verbose' : 0,\n",
        "        }\n",
        "    #'task_type' : 'GPU',\n",
        "    #\"eval_metric\":'RMSE',\n",
        "    cat = CatBoostRegressor(**params)\n",
        "    cat.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val,y_val)],cat_features=cat_features,\n",
        "              verbose=False)\n",
        "    cat_pred = cat.predict(X_val)\n",
        "    score = mean_absolute_error(y_val, cat_pred)\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import bisect\n",
        "'''\n",
        "# Categorical 컬럼 인코딩\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "encoders = {}\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    le = LabelEncoder()\n",
        "    train[feature] = le.fit_transform(train[feature].astype(str))\n",
        "    le_classes_set = set(le.classes_)\n",
        "    test[feature] = test[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
        "    le_classes = le.classes_.tolist()\n",
        "    bisect.insort_left(le_classes, '-1')\n",
        "    le.classes_ = np.array(le_classes)\n",
        "    test[feature] = le.transform(test[feature].astype(str))\n",
        "    encoders[feature] = le\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "\n",
        "# 결측치 처리\n",
        "train_x.fillna(train_x.mean(), inplace=True)\n",
        "test.fillna(train_x.mean(), inplace=True)\n",
        "'''\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG', 'country_cluster', 'PORT_SIZE_Zone', 'BREADTH', 'DEPTH', 'DRAUGHT']\n",
        "\n",
        "# 수치형 변수만 대상으로 결측치 대체\n",
        "numeric_cols = train_x.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 훈련 데이터의 평균 계산\n",
        "mean_values = train_x[numeric_cols].mean()\n",
        "\n",
        "# 결측치 대체\n",
        "train_x[numeric_cols] = train_x[numeric_cols].fillna(mean_values)\n",
        "test[numeric_cols] = test[numeric_cols].fillna(mean_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 220055 entries, 0 to 220054\n",
            "Data columns (total 57 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   ARI_CO                 220055 non-null  object \n",
            " 1   ARI_PO                 220055 non-null  object \n",
            " 2   SHIP_TYPE_CATEGORY     220055 non-null  object \n",
            " 3   DIST                   220055 non-null  float64\n",
            " 4   ID                     220055 non-null  object \n",
            " 5   BREADTH                220055 non-null  object \n",
            " 6   BUILT                  220055 non-null  int64  \n",
            " 7   DEADWEIGHT             220055 non-null  int64  \n",
            " 8   DEPTH                  220055 non-null  object \n",
            " 9   DRAUGHT                220055 non-null  object \n",
            " 10  GT                     220055 non-null  int64  \n",
            " 11  LENGTH                 220055 non-null  float64\n",
            " 12  SHIPMANAGER            220055 non-null  object \n",
            " 13  FLAG                   220055 non-null  object \n",
            " 14  U_WIND                 220055 non-null  float64\n",
            " 15  V_WIND                 220055 non-null  float64\n",
            " 16  AIR_TEMPERATURE        220055 non-null  float64\n",
            " 17  BN                     220055 non-null  float64\n",
            " 18  ATA_LT                 220055 non-null  int64  \n",
            " 19  DUBAI                  220055 non-null  float64\n",
            " 20  BRENT                  220055 non-null  float64\n",
            " 21  WTI                    220055 non-null  float64\n",
            " 22  BDI_ADJ                220055 non-null  float64\n",
            " 23  PORT_SIZE              220055 non-null  float64\n",
            " 24  year                   220055 non-null  int64  \n",
            " 25  month                  220055 non-null  int64  \n",
            " 26  day                    220055 non-null  int64  \n",
            " 27  weekday                220055 non-null  int64  \n",
            " 28  rounded_hour           220055 non-null  int64  \n",
            " 29  month_sin              220055 non-null  float64\n",
            " 30  month_cos              220055 non-null  float64\n",
            " 31  day_sin                220055 non-null  float64\n",
            " 32  day_cos                220055 non-null  float64\n",
            " 33  weekday_sin            220055 non-null  float64\n",
            " 34  weekday_cos            220055 non-null  float64\n",
            " 35  rounded_hour_sin       220055 non-null  float64\n",
            " 36  rounded_hour_cos       220055 non-null  float64\n",
            " 37  mean_enc_ARI_CO        220055 non-null  float64\n",
            " 38  std_enc_ARI_CO         220055 non-null  float64\n",
            " 39  mean_enc_ARI_PO        220055 non-null  float64\n",
            " 40  std_enc_ARI_PO         220055 non-null  float64\n",
            " 41  mean_enc_year          220055 non-null  float64\n",
            " 42  std_enc_year           220055 non-null  float64\n",
            " 43  mean_enc_month         220055 non-null  float64\n",
            " 44  std_enc_month          220055 non-null  float64\n",
            " 45  mean_enc_day           220055 non-null  float64\n",
            " 46  std_enc_day            220055 non-null  float64\n",
            " 47  mean_enc_weekday       220055 non-null  float64\n",
            " 48  std_enc_weekday        220055 non-null  float64\n",
            " 49  mean_enc_rounded_hour  220055 non-null  float64\n",
            " 50  std_enc_rounded_hour   220055 non-null  float64\n",
            " 51  DUBAI_BRENT            220055 non-null  float64\n",
            " 52  DUBAI_WTI              220055 non-null  float64\n",
            " 53  BRENT_WTI              220055 non-null  float64\n",
            " 54  DUBAI_BRENT_WTI        220055 non-null  float64\n",
            " 55  country_cluster        220055 non-null  object \n",
            " 56  PORT_SIZE_Zone         220055 non-null  object \n",
            "dtypes: float64(37), int64(9), object(11)\n",
            "memory usage: 95.7+ MB\n"
          ]
        }
      ],
      "source": [
        "train_x.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모든 카테고리형 변수를 문자열로 변환\n",
        "categorical_cols = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "train_x[categorical_cols] = train_x[categorical_cols].astype(str)\n",
        "test[categorical_cols] = test[categorical_cols].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-05 23:13:14,949]\u001b[0m A new study created in memory with name: no-name-d5bdd458-3bc8-4f77-9659-7cde3598a38d\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-05 23:15:17,832]\u001b[0m Trial 0 finished with value: 55.43213467773245 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 55.43213467773245.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:17:27,695]\u001b[0m Trial 1 finished with value: 46.73698461577896 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 46.73698461577896.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:19:36,021]\u001b[0m Trial 2 finished with value: 46.62981528541205 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:34:52,448]\u001b[0m Trial 3 finished with value: 74.28485108171094 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:36:54,216]\u001b[0m Trial 4 finished with value: 54.26968333125182 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:52:36,066]\u001b[0m Trial 5 finished with value: 63.15186508619714 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:55:38,924]\u001b[0m Trial 6 finished with value: 44.95889748083088 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:58:44,801]\u001b[0m Trial 7 finished with value: 49.657188254768535 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:00:20,943]\u001b[0m Trial 8 finished with value: 66.80661066382237 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:20:16,993]\u001b[0m Trial 9 finished with value: 56.57158302952333 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:21:19,395]\u001b[0m Trial 10 finished with value: 82.3248366699982 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:22:46,639]\u001b[0m Trial 11 finished with value: 57.13620617491669 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:26:08,909]\u001b[0m Trial 12 finished with value: 49.14633388193534 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:28:25,938]\u001b[0m Trial 13 finished with value: 45.26200780404295 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:30:19,639]\u001b[0m Trial 14 finished with value: 63.67955570219674 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:31:27,115]\u001b[0m Trial 15 finished with value: 64.94279214643767 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:32:41,363]\u001b[0m Trial 16 finished with value: 66.59544160127574 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:35:02,315]\u001b[0m Trial 17 finished with value: 42.25324087037929 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 42.25324087037929.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:42:54,049]\u001b[0m Trial 18 finished with value: 51.32044274263317 and parameters: {'iterations': 838, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 87.60982739927447, 'od_wait': 86, 'leaf_estimation_iterations': 15, 'bagging_temperature': 55.57442205671075, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 42.25324087037929.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:45:56,620]\u001b[0m Trial 19 finished with value: 45.429234667463305 and parameters: {'iterations': 996, 'learning_rate': 0.18484933160405892, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 45.629127031175706, 'od_wait': 63, 'leaf_estimation_iterations': 17, 'bagging_temperature': 50.428094719702216, 'colsample_bylevel': 0.46751740730087066}. Best is trial 17 with value: 42.25324087037929.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 1: 42.25324087037929\n",
            "Best hyperparameters for fold 1: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 210.0952407\ttotal: 96.6ms\tremaining: 1m 16s\n",
            "1:\tlearn: 210.0940552\ttotal: 147ms\tremaining: 58.1s\n",
            "2:\tlearn: 210.0938004\ttotal: 208ms\tremaining: 54.6s\n",
            "3:\tlearn: 209.1358980\ttotal: 362ms\tremaining: 1m 11s\n",
            "4:\tlearn: 207.6132118\ttotal: 492ms\tremaining: 1m 17s\n",
            "5:\tlearn: 206.2839777\ttotal: 653ms\tremaining: 1m 25s\n",
            "6:\tlearn: 206.0279903\ttotal: 791ms\tremaining: 1m 28s\n",
            "7:\tlearn: 204.0681873\ttotal: 966ms\tremaining: 1m 34s\n",
            "8:\tlearn: 204.0638360\ttotal: 1.01s\tremaining: 1m 28s\n",
            "9:\tlearn: 203.4280451\ttotal: 1.22s\tremaining: 1m 35s\n",
            "10:\tlearn: 203.1901894\ttotal: 1.39s\tremaining: 1m 38s\n",
            "11:\tlearn: 203.1871571\ttotal: 1.44s\tremaining: 1m 33s\n",
            "12:\tlearn: 203.1871571\ttotal: 1.48s\tremaining: 1m 28s\n",
            "13:\tlearn: 202.7864161\ttotal: 1.64s\tremaining: 1m 31s\n",
            "14:\tlearn: 202.7498112\ttotal: 1.79s\tremaining: 1m 32s\n",
            "15:\tlearn: 202.7169463\ttotal: 1.88s\tremaining: 1m 31s\n",
            "16:\tlearn: 202.7116879\ttotal: 1.94s\tremaining: 1m 28s\n",
            "17:\tlearn: 202.6794050\ttotal: 2s\tremaining: 1m 25s\n",
            "18:\tlearn: 202.5796489\ttotal: 2.15s\tremaining: 1m 27s\n",
            "19:\tlearn: 201.9102861\ttotal: 2.31s\tremaining: 1m 29s\n",
            "20:\tlearn: 201.7871639\ttotal: 2.46s\tremaining: 1m 30s\n",
            "21:\tlearn: 201.1280271\ttotal: 2.62s\tremaining: 1m 31s\n",
            "22:\tlearn: 201.0473370\ttotal: 2.76s\tremaining: 1m 32s\n",
            "23:\tlearn: 201.0349141\ttotal: 2.85s\tremaining: 1m 31s\n",
            "24:\tlearn: 201.0337697\ttotal: 2.92s\tremaining: 1m 29s\n",
            "25:\tlearn: 200.9380558\ttotal: 3.07s\tremaining: 1m 30s\n",
            "26:\tlearn: 199.9271637\ttotal: 3.21s\tremaining: 1m 31s\n",
            "27:\tlearn: 199.8806071\ttotal: 3.35s\tremaining: 1m 31s\n",
            "28:\tlearn: 199.8806071\ttotal: 3.39s\tremaining: 1m 29s\n",
            "29:\tlearn: 199.8760280\ttotal: 3.49s\tremaining: 1m 28s\n",
            "30:\tlearn: 199.8224237\ttotal: 3.65s\tremaining: 1m 29s\n",
            "31:\tlearn: 199.7900642\ttotal: 3.76s\tremaining: 1m 29s\n",
            "32:\tlearn: 199.1561864\ttotal: 3.84s\tremaining: 1m 28s\n",
            "33:\tlearn: 199.1077818\ttotal: 3.95s\tremaining: 1m 27s\n",
            "34:\tlearn: 198.0753486\ttotal: 4.08s\tremaining: 1m 28s\n",
            "35:\tlearn: 197.3901252\ttotal: 4.23s\tremaining: 1m 28s\n",
            "36:\tlearn: 196.5337674\ttotal: 4.37s\tremaining: 1m 29s\n",
            "37:\tlearn: 195.9275310\ttotal: 4.45s\tremaining: 1m 28s\n",
            "38:\tlearn: 194.2686065\ttotal: 4.6s\tremaining: 1m 28s\n",
            "39:\tlearn: 193.1647405\ttotal: 4.77s\tremaining: 1m 29s\n",
            "40:\tlearn: 191.9579967\ttotal: 4.92s\tremaining: 1m 30s\n",
            "41:\tlearn: 191.0653347\ttotal: 5.06s\tremaining: 1m 30s\n",
            "42:\tlearn: 190.0334194\ttotal: 5.22s\tremaining: 1m 30s\n",
            "43:\tlearn: 186.4136590\ttotal: 5.36s\tremaining: 1m 31s\n",
            "44:\tlearn: 182.6431525\ttotal: 5.49s\tremaining: 1m 31s\n",
            "45:\tlearn: 181.7990732\ttotal: 5.63s\tremaining: 1m 31s\n",
            "46:\tlearn: 181.4313116\ttotal: 5.78s\tremaining: 1m 31s\n",
            "47:\tlearn: 181.1349264\ttotal: 5.94s\tremaining: 1m 32s\n",
            "48:\tlearn: 179.8998331\ttotal: 6.06s\tremaining: 1m 31s\n",
            "49:\tlearn: 179.5230399\ttotal: 6.2s\tremaining: 1m 32s\n",
            "50:\tlearn: 178.9687448\ttotal: 6.33s\tremaining: 1m 31s\n",
            "51:\tlearn: 177.2375298\ttotal: 6.45s\tremaining: 1m 31s\n",
            "52:\tlearn: 175.8033348\ttotal: 6.58s\tremaining: 1m 31s\n",
            "53:\tlearn: 172.4107905\ttotal: 6.71s\tremaining: 1m 31s\n",
            "54:\tlearn: 170.5903894\ttotal: 6.85s\tremaining: 1m 31s\n",
            "55:\tlearn: 168.6452129\ttotal: 7s\tremaining: 1m 31s\n",
            "56:\tlearn: 168.2381520\ttotal: 7.15s\tremaining: 1m 32s\n",
            "57:\tlearn: 167.6856517\ttotal: 7.28s\tremaining: 1m 32s\n",
            "58:\tlearn: 167.0560245\ttotal: 7.42s\tremaining: 1m 32s\n",
            "59:\tlearn: 165.1588766\ttotal: 7.56s\tremaining: 1m 32s\n",
            "60:\tlearn: 164.8136277\ttotal: 7.69s\tremaining: 1m 32s\n",
            "61:\tlearn: 164.4797273\ttotal: 7.86s\tremaining: 1m 32s\n",
            "62:\tlearn: 163.4114617\ttotal: 7.99s\tremaining: 1m 32s\n",
            "63:\tlearn: 162.8691691\ttotal: 8.12s\tremaining: 1m 32s\n",
            "64:\tlearn: 162.4686740\ttotal: 8.26s\tremaining: 1m 32s\n",
            "65:\tlearn: 161.7235236\ttotal: 8.41s\tremaining: 1m 32s\n",
            "66:\tlearn: 161.7221740\ttotal: 8.56s\tremaining: 1m 32s\n",
            "67:\tlearn: 160.6943269\ttotal: 8.7s\tremaining: 1m 32s\n",
            "68:\tlearn: 160.4930070\ttotal: 8.85s\tremaining: 1m 32s\n",
            "69:\tlearn: 159.6979241\ttotal: 8.98s\tremaining: 1m 32s\n",
            "70:\tlearn: 159.3741101\ttotal: 9.14s\tremaining: 1m 32s\n",
            "71:\tlearn: 158.1264455\ttotal: 9.28s\tremaining: 1m 32s\n",
            "72:\tlearn: 156.4218738\ttotal: 9.42s\tremaining: 1m 32s\n",
            "73:\tlearn: 155.7613544\ttotal: 9.55s\tremaining: 1m 32s\n",
            "74:\tlearn: 154.8884126\ttotal: 9.69s\tremaining: 1m 32s\n",
            "75:\tlearn: 154.6643223\ttotal: 9.84s\tremaining: 1m 32s\n",
            "76:\tlearn: 154.3643960\ttotal: 10s\tremaining: 1m 32s\n",
            "77:\tlearn: 153.4344879\ttotal: 10.1s\tremaining: 1m 32s\n",
            "78:\tlearn: 153.1934894\ttotal: 10.3s\tremaining: 1m 32s\n",
            "79:\tlearn: 151.1676158\ttotal: 10.4s\tremaining: 1m 32s\n",
            "80:\tlearn: 149.8963672\ttotal: 10.5s\tremaining: 1m 32s\n",
            "81:\tlearn: 148.7130206\ttotal: 10.7s\tremaining: 1m 32s\n",
            "82:\tlearn: 148.4868546\ttotal: 10.8s\tremaining: 1m 32s\n",
            "83:\tlearn: 148.0747566\ttotal: 11s\tremaining: 1m 32s\n",
            "84:\tlearn: 147.7053038\ttotal: 11.1s\tremaining: 1m 32s\n",
            "85:\tlearn: 146.8548651\ttotal: 11.2s\tremaining: 1m 32s\n",
            "86:\tlearn: 146.3716506\ttotal: 11.4s\tremaining: 1m 32s\n",
            "87:\tlearn: 146.1204575\ttotal: 11.5s\tremaining: 1m 32s\n",
            "88:\tlearn: 145.9078541\ttotal: 11.7s\tremaining: 1m 32s\n",
            "89:\tlearn: 145.2668398\ttotal: 11.8s\tremaining: 1m 32s\n",
            "90:\tlearn: 142.9933332\ttotal: 11.9s\tremaining: 1m 32s\n",
            "91:\tlearn: 142.3837054\ttotal: 12.1s\tremaining: 1m 31s\n",
            "92:\tlearn: 140.7833630\ttotal: 12.2s\tremaining: 1m 31s\n",
            "93:\tlearn: 139.8799827\ttotal: 12.4s\tremaining: 1m 31s\n",
            "94:\tlearn: 139.4488272\ttotal: 12.5s\tremaining: 1m 31s\n",
            "95:\tlearn: 139.2982959\ttotal: 12.6s\tremaining: 1m 31s\n",
            "96:\tlearn: 139.0891601\ttotal: 12.8s\tremaining: 1m 31s\n",
            "97:\tlearn: 138.6579933\ttotal: 12.9s\tremaining: 1m 31s\n",
            "98:\tlearn: 138.1032388\ttotal: 13.1s\tremaining: 1m 31s\n",
            "99:\tlearn: 137.3941482\ttotal: 13.2s\tremaining: 1m 31s\n",
            "100:\tlearn: 136.9778564\ttotal: 13.3s\tremaining: 1m 31s\n",
            "101:\tlearn: 136.7626121\ttotal: 13.5s\tremaining: 1m 31s\n",
            "102:\tlearn: 136.7606318\ttotal: 13.6s\tremaining: 1m 31s\n",
            "103:\tlearn: 136.6103311\ttotal: 13.8s\tremaining: 1m 31s\n",
            "104:\tlearn: 136.3113936\ttotal: 13.9s\tremaining: 1m 30s\n",
            "105:\tlearn: 136.2095836\ttotal: 14.1s\tremaining: 1m 31s\n",
            "106:\tlearn: 135.9230702\ttotal: 14.2s\tremaining: 1m 31s\n",
            "107:\tlearn: 135.8137929\ttotal: 14.4s\tremaining: 1m 30s\n",
            "108:\tlearn: 135.5717783\ttotal: 14.5s\tremaining: 1m 30s\n",
            "109:\tlearn: 134.9473610\ttotal: 14.6s\tremaining: 1m 30s\n",
            "110:\tlearn: 133.8762430\ttotal: 14.8s\tremaining: 1m 30s\n",
            "111:\tlearn: 133.3428223\ttotal: 14.9s\tremaining: 1m 30s\n",
            "112:\tlearn: 133.1797235\ttotal: 15.1s\tremaining: 1m 30s\n",
            "113:\tlearn: 132.6931764\ttotal: 15.2s\tremaining: 1m 30s\n",
            "114:\tlearn: 131.9367748\ttotal: 15.3s\tremaining: 1m 30s\n",
            "115:\tlearn: 131.1855789\ttotal: 15.5s\tremaining: 1m 30s\n",
            "116:\tlearn: 130.8149515\ttotal: 15.6s\tremaining: 1m 29s\n",
            "117:\tlearn: 130.2619289\ttotal: 15.7s\tremaining: 1m 29s\n",
            "118:\tlearn: 129.9223056\ttotal: 15.9s\tremaining: 1m 29s\n",
            "119:\tlearn: 129.8130120\ttotal: 16s\tremaining: 1m 29s\n",
            "120:\tlearn: 129.6323397\ttotal: 16.1s\tremaining: 1m 29s\n",
            "121:\tlearn: 129.4790607\ttotal: 16.3s\tremaining: 1m 29s\n",
            "122:\tlearn: 128.8986469\ttotal: 16.4s\tremaining: 1m 29s\n",
            "123:\tlearn: 128.5807345\ttotal: 16.5s\tremaining: 1m 29s\n",
            "124:\tlearn: 127.8073098\ttotal: 16.7s\tremaining: 1m 28s\n",
            "125:\tlearn: 126.5196171\ttotal: 16.8s\tremaining: 1m 28s\n",
            "126:\tlearn: 126.3781441\ttotal: 16.9s\tremaining: 1m 28s\n",
            "127:\tlearn: 126.0887387\ttotal: 17.1s\tremaining: 1m 28s\n",
            "128:\tlearn: 125.9257902\ttotal: 17.2s\tremaining: 1m 28s\n",
            "129:\tlearn: 125.7848247\ttotal: 17.4s\tremaining: 1m 28s\n",
            "130:\tlearn: 125.3476768\ttotal: 17.6s\tremaining: 1m 28s\n",
            "131:\tlearn: 123.6859347\ttotal: 17.7s\tremaining: 1m 28s\n",
            "132:\tlearn: 123.3636746\ttotal: 17.8s\tremaining: 1m 28s\n",
            "133:\tlearn: 123.2226466\ttotal: 18s\tremaining: 1m 28s\n",
            "134:\tlearn: 122.8461255\ttotal: 18.1s\tremaining: 1m 28s\n",
            "135:\tlearn: 122.3126261\ttotal: 18.2s\tremaining: 1m 27s\n",
            "136:\tlearn: 121.3855679\ttotal: 18.4s\tremaining: 1m 27s\n",
            "137:\tlearn: 121.0656076\ttotal: 18.5s\tremaining: 1m 27s\n",
            "138:\tlearn: 120.9348360\ttotal: 18.6s\tremaining: 1m 27s\n",
            "139:\tlearn: 120.7479529\ttotal: 18.8s\tremaining: 1m 27s\n",
            "140:\tlearn: 120.6465056\ttotal: 18.9s\tremaining: 1m 27s\n",
            "141:\tlearn: 120.5414389\ttotal: 19.1s\tremaining: 1m 27s\n",
            "142:\tlearn: 119.7243098\ttotal: 19.2s\tremaining: 1m 27s\n",
            "143:\tlearn: 119.5769180\ttotal: 19.4s\tremaining: 1m 27s\n",
            "144:\tlearn: 119.3580356\ttotal: 19.5s\tremaining: 1m 27s\n",
            "145:\tlearn: 119.2272936\ttotal: 19.7s\tremaining: 1m 26s\n",
            "146:\tlearn: 119.0441174\ttotal: 19.8s\tremaining: 1m 26s\n",
            "147:\tlearn: 117.9700071\ttotal: 19.9s\tremaining: 1m 26s\n",
            "148:\tlearn: 117.8059741\ttotal: 20.1s\tremaining: 1m 26s\n",
            "149:\tlearn: 117.6200735\ttotal: 20.2s\tremaining: 1m 26s\n",
            "150:\tlearn: 117.5087627\ttotal: 20.4s\tremaining: 1m 26s\n",
            "151:\tlearn: 117.3282076\ttotal: 20.5s\tremaining: 1m 26s\n",
            "152:\tlearn: 116.8791058\ttotal: 20.7s\tremaining: 1m 26s\n",
            "153:\tlearn: 116.5544420\ttotal: 20.8s\tremaining: 1m 26s\n",
            "154:\tlearn: 115.6938623\ttotal: 20.9s\tremaining: 1m 25s\n",
            "155:\tlearn: 115.5739669\ttotal: 21.1s\tremaining: 1m 25s\n",
            "156:\tlearn: 115.3302841\ttotal: 21.2s\tremaining: 1m 25s\n",
            "157:\tlearn: 114.6562768\ttotal: 21.3s\tremaining: 1m 25s\n",
            "158:\tlearn: 114.5403863\ttotal: 21.5s\tremaining: 1m 25s\n",
            "159:\tlearn: 114.0321405\ttotal: 21.6s\tremaining: 1m 25s\n",
            "160:\tlearn: 113.4554827\ttotal: 21.8s\tremaining: 1m 25s\n",
            "161:\tlearn: 113.2252116\ttotal: 21.9s\tremaining: 1m 25s\n",
            "162:\tlearn: 112.6154248\ttotal: 22.1s\tremaining: 1m 25s\n",
            "163:\tlearn: 112.3924245\ttotal: 22.2s\tremaining: 1m 25s\n",
            "164:\tlearn: 112.1799044\ttotal: 22.3s\tremaining: 1m 24s\n",
            "165:\tlearn: 111.8524878\ttotal: 22.5s\tremaining: 1m 24s\n",
            "166:\tlearn: 111.6615266\ttotal: 22.6s\tremaining: 1m 24s\n",
            "167:\tlearn: 111.3793556\ttotal: 22.7s\tremaining: 1m 24s\n",
            "168:\tlearn: 111.1843352\ttotal: 22.9s\tremaining: 1m 24s\n",
            "169:\tlearn: 110.6985066\ttotal: 23s\tremaining: 1m 24s\n",
            "170:\tlearn: 110.6192167\ttotal: 23.2s\tremaining: 1m 24s\n",
            "171:\tlearn: 109.9474991\ttotal: 23.3s\tremaining: 1m 24s\n",
            "172:\tlearn: 109.8585393\ttotal: 23.5s\tremaining: 1m 24s\n",
            "173:\tlearn: 109.7345032\ttotal: 23.6s\tremaining: 1m 23s\n",
            "174:\tlearn: 109.6423525\ttotal: 23.8s\tremaining: 1m 23s\n",
            "175:\tlearn: 109.5816780\ttotal: 23.9s\tremaining: 1m 23s\n",
            "176:\tlearn: 108.9044012\ttotal: 24s\tremaining: 1m 23s\n",
            "177:\tlearn: 108.4430840\ttotal: 24.2s\tremaining: 1m 23s\n",
            "178:\tlearn: 108.0803470\ttotal: 24.3s\tremaining: 1m 23s\n",
            "179:\tlearn: 107.8248946\ttotal: 24.4s\tremaining: 1m 23s\n",
            "180:\tlearn: 107.6708706\ttotal: 24.6s\tremaining: 1m 22s\n",
            "181:\tlearn: 107.5961077\ttotal: 24.8s\tremaining: 1m 22s\n",
            "182:\tlearn: 107.0231279\ttotal: 24.9s\tremaining: 1m 22s\n",
            "183:\tlearn: 106.8187897\ttotal: 25s\tremaining: 1m 22s\n",
            "184:\tlearn: 106.5582515\ttotal: 25.1s\tremaining: 1m 22s\n",
            "185:\tlearn: 106.3967160\ttotal: 25.3s\tremaining: 1m 22s\n",
            "186:\tlearn: 105.9589557\ttotal: 25.4s\tremaining: 1m 22s\n",
            "187:\tlearn: 105.7116903\ttotal: 25.6s\tremaining: 1m 22s\n",
            "188:\tlearn: 105.4703524\ttotal: 25.7s\tremaining: 1m 21s\n",
            "189:\tlearn: 105.3597666\ttotal: 25.9s\tremaining: 1m 21s\n",
            "190:\tlearn: 105.1278874\ttotal: 26s\tremaining: 1m 21s\n",
            "191:\tlearn: 104.7975760\ttotal: 26.1s\tremaining: 1m 21s\n",
            "192:\tlearn: 104.6585242\ttotal: 26.3s\tremaining: 1m 21s\n",
            "193:\tlearn: 104.5676483\ttotal: 26.4s\tremaining: 1m 21s\n",
            "194:\tlearn: 104.3572228\ttotal: 26.6s\tremaining: 1m 21s\n",
            "195:\tlearn: 104.1865305\ttotal: 26.7s\tremaining: 1m 21s\n",
            "196:\tlearn: 104.0987645\ttotal: 26.9s\tremaining: 1m 21s\n",
            "197:\tlearn: 104.0952030\ttotal: 27s\tremaining: 1m 21s\n",
            "198:\tlearn: 104.0177617\ttotal: 27.2s\tremaining: 1m 21s\n",
            "199:\tlearn: 103.9413795\ttotal: 27.3s\tremaining: 1m 20s\n",
            "200:\tlearn: 103.5236940\ttotal: 27.5s\tremaining: 1m 20s\n",
            "201:\tlearn: 103.4409015\ttotal: 27.6s\tremaining: 1m 20s\n",
            "202:\tlearn: 103.0300789\ttotal: 27.8s\tremaining: 1m 20s\n",
            "203:\tlearn: 102.9039120\ttotal: 27.9s\tremaining: 1m 20s\n",
            "204:\tlearn: 102.7611080\ttotal: 28s\tremaining: 1m 20s\n",
            "205:\tlearn: 102.5969076\ttotal: 28.2s\tremaining: 1m 20s\n",
            "206:\tlearn: 102.5292034\ttotal: 28.4s\tremaining: 1m 20s\n",
            "207:\tlearn: 101.9416208\ttotal: 28.5s\tremaining: 1m 19s\n",
            "208:\tlearn: 101.6522649\ttotal: 28.6s\tremaining: 1m 19s\n",
            "209:\tlearn: 101.5840252\ttotal: 28.7s\tremaining: 1m 19s\n",
            "210:\tlearn: 101.5025119\ttotal: 28.9s\tremaining: 1m 19s\n",
            "211:\tlearn: 101.1531987\ttotal: 29s\tremaining: 1m 19s\n",
            "212:\tlearn: 100.9116228\ttotal: 29.1s\tremaining: 1m 19s\n",
            "213:\tlearn: 100.8049936\ttotal: 29.3s\tremaining: 1m 19s\n",
            "214:\tlearn: 100.4064304\ttotal: 29.5s\tremaining: 1m 19s\n",
            "215:\tlearn: 100.3139164\ttotal: 29.6s\tremaining: 1m 18s\n",
            "216:\tlearn: 100.2646989\ttotal: 29.8s\tremaining: 1m 18s\n",
            "217:\tlearn: 100.1654750\ttotal: 29.9s\tremaining: 1m 18s\n",
            "218:\tlearn: 99.8221844\ttotal: 30s\tremaining: 1m 18s\n",
            "219:\tlearn: 99.5320405\ttotal: 30.2s\tremaining: 1m 18s\n",
            "220:\tlearn: 99.3838228\ttotal: 30.3s\tremaining: 1m 18s\n",
            "221:\tlearn: 99.2297651\ttotal: 30.5s\tremaining: 1m 18s\n",
            "222:\tlearn: 99.0951114\ttotal: 30.6s\tremaining: 1m 18s\n",
            "223:\tlearn: 98.8019940\ttotal: 30.7s\tremaining: 1m 17s\n",
            "224:\tlearn: 98.7163208\ttotal: 30.9s\tremaining: 1m 17s\n",
            "225:\tlearn: 98.6012922\ttotal: 31s\tremaining: 1m 17s\n",
            "226:\tlearn: 98.4886869\ttotal: 31.2s\tremaining: 1m 17s\n",
            "227:\tlearn: 98.4228923\ttotal: 31.3s\tremaining: 1m 17s\n",
            "228:\tlearn: 98.2588328\ttotal: 31.4s\tremaining: 1m 17s\n",
            "229:\tlearn: 98.2555381\ttotal: 31.6s\tremaining: 1m 17s\n",
            "230:\tlearn: 98.1583016\ttotal: 31.7s\tremaining: 1m 17s\n",
            "231:\tlearn: 97.9899179\ttotal: 31.9s\tremaining: 1m 16s\n",
            "232:\tlearn: 97.9228007\ttotal: 32.1s\tremaining: 1m 16s\n",
            "233:\tlearn: 97.8867925\ttotal: 32.2s\tremaining: 1m 16s\n",
            "234:\tlearn: 97.4058282\ttotal: 32.3s\tremaining: 1m 16s\n",
            "235:\tlearn: 97.1630261\ttotal: 32.5s\tremaining: 1m 16s\n",
            "236:\tlearn: 96.9987648\ttotal: 32.6s\tremaining: 1m 16s\n",
            "237:\tlearn: 96.8220212\ttotal: 32.8s\tremaining: 1m 16s\n",
            "238:\tlearn: 96.7310125\ttotal: 32.9s\tremaining: 1m 16s\n",
            "239:\tlearn: 96.4690182\ttotal: 33s\tremaining: 1m 15s\n",
            "240:\tlearn: 96.3302006\ttotal: 33.2s\tremaining: 1m 15s\n",
            "241:\tlearn: 96.1690217\ttotal: 33.3s\tremaining: 1m 15s\n",
            "242:\tlearn: 96.0563863\ttotal: 33.4s\tremaining: 1m 15s\n",
            "243:\tlearn: 95.6922593\ttotal: 33.6s\tremaining: 1m 15s\n",
            "244:\tlearn: 95.1598318\ttotal: 33.7s\tremaining: 1m 15s\n",
            "245:\tlearn: 95.0527915\ttotal: 33.8s\tremaining: 1m 15s\n",
            "246:\tlearn: 94.9894984\ttotal: 34s\tremaining: 1m 15s\n",
            "247:\tlearn: 94.4988418\ttotal: 34.1s\tremaining: 1m 14s\n",
            "248:\tlearn: 94.4143138\ttotal: 34.3s\tremaining: 1m 14s\n",
            "249:\tlearn: 94.3383201\ttotal: 34.5s\tremaining: 1m 14s\n",
            "250:\tlearn: 94.2190453\ttotal: 34.6s\tremaining: 1m 14s\n",
            "251:\tlearn: 94.0627239\ttotal: 34.7s\tremaining: 1m 14s\n",
            "252:\tlearn: 93.9443145\ttotal: 34.9s\tremaining: 1m 14s\n",
            "253:\tlearn: 93.7600114\ttotal: 35s\tremaining: 1m 14s\n",
            "254:\tlearn: 93.7068131\ttotal: 35.2s\tremaining: 1m 14s\n",
            "255:\tlearn: 93.6615921\ttotal: 35.3s\tremaining: 1m 13s\n",
            "256:\tlearn: 93.5533111\ttotal: 35.5s\tremaining: 1m 13s\n",
            "257:\tlearn: 93.4294652\ttotal: 35.6s\tremaining: 1m 13s\n",
            "258:\tlearn: 93.4118247\ttotal: 35.8s\tremaining: 1m 13s\n",
            "259:\tlearn: 93.1414778\ttotal: 35.9s\tremaining: 1m 13s\n",
            "260:\tlearn: 93.0664370\ttotal: 36s\tremaining: 1m 13s\n",
            "261:\tlearn: 92.9614661\ttotal: 36.2s\tremaining: 1m 13s\n",
            "262:\tlearn: 92.6407340\ttotal: 36.3s\tremaining: 1m 13s\n",
            "263:\tlearn: 92.5719822\ttotal: 36.5s\tremaining: 1m 12s\n",
            "264:\tlearn: 92.5262793\ttotal: 36.6s\tremaining: 1m 12s\n",
            "265:\tlearn: 92.2736954\ttotal: 36.7s\tremaining: 1m 12s\n",
            "266:\tlearn: 91.9038996\ttotal: 36.9s\tremaining: 1m 12s\n",
            "267:\tlearn: 91.8206471\ttotal: 37s\tremaining: 1m 12s\n",
            "268:\tlearn: 91.5246603\ttotal: 37.2s\tremaining: 1m 12s\n",
            "269:\tlearn: 91.3301693\ttotal: 37.3s\tremaining: 1m 12s\n",
            "270:\tlearn: 91.2557751\ttotal: 37.5s\tremaining: 1m 12s\n",
            "271:\tlearn: 91.0788696\ttotal: 37.6s\tremaining: 1m 11s\n",
            "272:\tlearn: 91.0491470\ttotal: 37.7s\tremaining: 1m 11s\n",
            "273:\tlearn: 90.9145443\ttotal: 37.9s\tremaining: 1m 11s\n",
            "274:\tlearn: 90.8515932\ttotal: 38.1s\tremaining: 1m 11s\n",
            "275:\tlearn: 90.6601013\ttotal: 38.2s\tremaining: 1m 11s\n",
            "276:\tlearn: 90.5290841\ttotal: 38.3s\tremaining: 1m 11s\n",
            "277:\tlearn: 90.4225213\ttotal: 38.5s\tremaining: 1m 11s\n",
            "278:\tlearn: 90.3225507\ttotal: 38.6s\tremaining: 1m 10s\n",
            "279:\tlearn: 90.0679889\ttotal: 38.7s\tremaining: 1m 10s\n",
            "280:\tlearn: 89.8516685\ttotal: 38.9s\tremaining: 1m 10s\n",
            "281:\tlearn: 89.6412157\ttotal: 39s\tremaining: 1m 10s\n",
            "282:\tlearn: 89.2795076\ttotal: 39.1s\tremaining: 1m 10s\n",
            "283:\tlearn: 89.1586855\ttotal: 39.2s\tremaining: 1m 10s\n",
            "284:\tlearn: 89.0657634\ttotal: 39.4s\tremaining: 1m 10s\n",
            "285:\tlearn: 88.9136256\ttotal: 39.5s\tremaining: 1m 9s\n",
            "286:\tlearn: 88.8432720\ttotal: 39.6s\tremaining: 1m 9s\n",
            "287:\tlearn: 88.5716723\ttotal: 39.8s\tremaining: 1m 9s\n",
            "288:\tlearn: 88.5214619\ttotal: 39.9s\tremaining: 1m 9s\n",
            "289:\tlearn: 88.4195664\ttotal: 40.1s\tremaining: 1m 9s\n",
            "290:\tlearn: 88.3795960\ttotal: 40.2s\tremaining: 1m 9s\n",
            "291:\tlearn: 88.3791199\ttotal: 40.3s\tremaining: 1m 9s\n",
            "292:\tlearn: 88.3284695\ttotal: 40.5s\tremaining: 1m 8s\n",
            "293:\tlearn: 88.2566886\ttotal: 40.7s\tremaining: 1m 8s\n",
            "294:\tlearn: 88.1147694\ttotal: 40.8s\tremaining: 1m 8s\n",
            "295:\tlearn: 88.0260199\ttotal: 40.9s\tremaining: 1m 8s\n",
            "296:\tlearn: 87.9016193\ttotal: 41.1s\tremaining: 1m 8s\n",
            "297:\tlearn: 87.4024339\ttotal: 41.2s\tremaining: 1m 8s\n",
            "298:\tlearn: 87.3544105\ttotal: 41.3s\tremaining: 1m 8s\n",
            "299:\tlearn: 87.2884400\ttotal: 41.5s\tremaining: 1m 8s\n",
            "300:\tlearn: 87.2270228\ttotal: 41.6s\tremaining: 1m 7s\n",
            "301:\tlearn: 87.1800454\ttotal: 41.8s\tremaining: 1m 7s\n",
            "302:\tlearn: 87.1450682\ttotal: 41.9s\tremaining: 1m 7s\n",
            "303:\tlearn: 87.0687204\ttotal: 42s\tremaining: 1m 7s\n",
            "304:\tlearn: 86.9836885\ttotal: 42.2s\tremaining: 1m 7s\n",
            "305:\tlearn: 86.5528032\ttotal: 42.3s\tremaining: 1m 7s\n",
            "306:\tlearn: 86.4940688\ttotal: 42.5s\tremaining: 1m 7s\n",
            "307:\tlearn: 86.3028859\ttotal: 42.6s\tremaining: 1m 6s\n",
            "308:\tlearn: 86.0787085\ttotal: 42.7s\tremaining: 1m 6s\n",
            "309:\tlearn: 85.9067495\ttotal: 42.9s\tremaining: 1m 6s\n",
            "310:\tlearn: 85.8391400\ttotal: 43s\tremaining: 1m 6s\n",
            "311:\tlearn: 85.7512263\ttotal: 43.2s\tremaining: 1m 6s\n",
            "312:\tlearn: 85.7467144\ttotal: 43.3s\tremaining: 1m 6s\n",
            "313:\tlearn: 85.4857352\ttotal: 43.5s\tremaining: 1m 6s\n",
            "314:\tlearn: 85.3779462\ttotal: 43.6s\tremaining: 1m 6s\n",
            "315:\tlearn: 85.3299013\ttotal: 43.8s\tremaining: 1m 5s\n",
            "316:\tlearn: 85.1600544\ttotal: 43.9s\tremaining: 1m 5s\n",
            "317:\tlearn: 85.1534932\ttotal: 44.1s\tremaining: 1m 5s\n",
            "318:\tlearn: 85.0258301\ttotal: 44.2s\tremaining: 1m 5s\n",
            "319:\tlearn: 84.7393158\ttotal: 44.3s\tremaining: 1m 5s\n",
            "320:\tlearn: 84.4781467\ttotal: 44.5s\tremaining: 1m 5s\n",
            "321:\tlearn: 84.1134675\ttotal: 44.6s\tremaining: 1m 5s\n",
            "322:\tlearn: 84.0821525\ttotal: 44.8s\tremaining: 1m 5s\n",
            "323:\tlearn: 83.9962692\ttotal: 44.9s\tremaining: 1m 4s\n",
            "324:\tlearn: 83.9391172\ttotal: 45.1s\tremaining: 1m 4s\n",
            "325:\tlearn: 83.8363487\ttotal: 45.2s\tremaining: 1m 4s\n",
            "326:\tlearn: 83.6814025\ttotal: 45.3s\tremaining: 1m 4s\n",
            "327:\tlearn: 83.6389709\ttotal: 45.5s\tremaining: 1m 4s\n",
            "328:\tlearn: 83.5747815\ttotal: 45.6s\tremaining: 1m 4s\n",
            "329:\tlearn: 83.2345399\ttotal: 45.8s\tremaining: 1m 4s\n",
            "330:\tlearn: 83.1009248\ttotal: 45.9s\tremaining: 1m 3s\n",
            "331:\tlearn: 83.0555108\ttotal: 46s\tremaining: 1m 3s\n",
            "332:\tlearn: 82.9577545\ttotal: 46.2s\tremaining: 1m 3s\n",
            "333:\tlearn: 82.8745350\ttotal: 46.4s\tremaining: 1m 3s\n",
            "334:\tlearn: 82.8456486\ttotal: 46.5s\tremaining: 1m 3s\n",
            "335:\tlearn: 82.8009643\ttotal: 46.7s\tremaining: 1m 3s\n",
            "336:\tlearn: 82.6796327\ttotal: 46.8s\tremaining: 1m 3s\n",
            "337:\tlearn: 82.6471595\ttotal: 46.9s\tremaining: 1m 3s\n",
            "338:\tlearn: 82.5979914\ttotal: 47.1s\tremaining: 1m 2s\n",
            "339:\tlearn: 82.5563679\ttotal: 47.2s\tremaining: 1m 2s\n",
            "340:\tlearn: 82.4677663\ttotal: 47.4s\tremaining: 1m 2s\n",
            "341:\tlearn: 82.2750726\ttotal: 47.5s\tremaining: 1m 2s\n",
            "342:\tlearn: 82.2310974\ttotal: 47.6s\tremaining: 1m 2s\n",
            "343:\tlearn: 82.1824708\ttotal: 47.8s\tremaining: 1m 2s\n",
            "344:\tlearn: 81.9636938\ttotal: 47.9s\tremaining: 1m 2s\n",
            "345:\tlearn: 81.8780595\ttotal: 48s\tremaining: 1m 1s\n",
            "346:\tlearn: 81.7334089\ttotal: 48.2s\tremaining: 1m 1s\n",
            "347:\tlearn: 81.6435730\ttotal: 48.3s\tremaining: 1m 1s\n",
            "348:\tlearn: 81.4583800\ttotal: 48.4s\tremaining: 1m 1s\n",
            "349:\tlearn: 81.2540126\ttotal: 48.6s\tremaining: 1m 1s\n",
            "350:\tlearn: 80.9853170\ttotal: 48.7s\tremaining: 1m 1s\n",
            "351:\tlearn: 80.9060687\ttotal: 48.8s\tremaining: 1m 1s\n",
            "352:\tlearn: 80.6547573\ttotal: 49s\tremaining: 1m\n",
            "353:\tlearn: 80.5485153\ttotal: 49.1s\tremaining: 1m\n",
            "354:\tlearn: 80.4477279\ttotal: 49.2s\tremaining: 1m\n",
            "355:\tlearn: 80.2885025\ttotal: 49.4s\tremaining: 1m\n",
            "356:\tlearn: 80.1365619\ttotal: 49.5s\tremaining: 1m\n",
            "357:\tlearn: 80.0741090\ttotal: 49.7s\tremaining: 1m\n",
            "358:\tlearn: 79.9313582\ttotal: 49.8s\tremaining: 1m\n",
            "359:\tlearn: 79.8402174\ttotal: 49.9s\tremaining: 59.9s\n",
            "360:\tlearn: 79.8051427\ttotal: 50.1s\tremaining: 59.8s\n",
            "361:\tlearn: 79.7320503\ttotal: 50.2s\tremaining: 59.6s\n",
            "362:\tlearn: 79.6501544\ttotal: 50.4s\tremaining: 59.5s\n",
            "363:\tlearn: 79.5826251\ttotal: 50.5s\tremaining: 59.4s\n",
            "364:\tlearn: 79.4850593\ttotal: 50.7s\tremaining: 59.3s\n",
            "365:\tlearn: 79.4398292\ttotal: 50.8s\tremaining: 59.1s\n",
            "366:\tlearn: 79.4017319\ttotal: 51s\tremaining: 59s\n",
            "367:\tlearn: 79.3645566\ttotal: 51.1s\tremaining: 58.9s\n",
            "368:\tlearn: 79.3113147\ttotal: 51.2s\tremaining: 58.7s\n",
            "369:\tlearn: 79.2915525\ttotal: 51.4s\tremaining: 58.6s\n",
            "370:\tlearn: 79.2724540\ttotal: 51.5s\tremaining: 58.5s\n",
            "371:\tlearn: 79.2477565\ttotal: 51.7s\tremaining: 58.3s\n",
            "372:\tlearn: 79.1177308\ttotal: 51.8s\tremaining: 58.2s\n",
            "373:\tlearn: 79.0846326\ttotal: 52s\tremaining: 58.1s\n",
            "374:\tlearn: 79.0342010\ttotal: 52.1s\tremaining: 58s\n",
            "375:\tlearn: 78.8171296\ttotal: 52.3s\tremaining: 57.8s\n",
            "376:\tlearn: 78.5978990\ttotal: 52.4s\tremaining: 57.7s\n",
            "377:\tlearn: 78.4459374\ttotal: 52.6s\tremaining: 57.6s\n",
            "378:\tlearn: 78.1735751\ttotal: 52.7s\tremaining: 57.4s\n",
            "379:\tlearn: 78.0136050\ttotal: 52.8s\tremaining: 57.3s\n",
            "380:\tlearn: 77.9503475\ttotal: 53s\tremaining: 57.1s\n",
            "381:\tlearn: 77.7959099\ttotal: 53.1s\tremaining: 57s\n",
            "382:\tlearn: 77.7330849\ttotal: 53.2s\tremaining: 56.8s\n",
            "383:\tlearn: 77.6405469\ttotal: 53.4s\tremaining: 56.7s\n",
            "384:\tlearn: 77.5804824\ttotal: 53.5s\tremaining: 56.6s\n",
            "385:\tlearn: 77.5447644\ttotal: 53.7s\tremaining: 56.4s\n",
            "386:\tlearn: 77.4870655\ttotal: 53.8s\tremaining: 56.3s\n",
            "387:\tlearn: 77.4472273\ttotal: 53.9s\tremaining: 56.2s\n",
            "388:\tlearn: 77.3766107\ttotal: 54.1s\tremaining: 56s\n",
            "389:\tlearn: 77.2724003\ttotal: 54.2s\tremaining: 55.9s\n",
            "390:\tlearn: 77.2205656\ttotal: 54.4s\tremaining: 55.8s\n",
            "391:\tlearn: 77.0691593\ttotal: 54.5s\tremaining: 55.7s\n",
            "392:\tlearn: 76.9968090\ttotal: 54.7s\tremaining: 55.5s\n",
            "393:\tlearn: 76.9529848\ttotal: 54.8s\tremaining: 55.4s\n",
            "394:\tlearn: 76.9116059\ttotal: 55s\tremaining: 55.2s\n",
            "395:\tlearn: 76.7923924\ttotal: 55.1s\tremaining: 55.1s\n",
            "396:\tlearn: 76.6737542\ttotal: 55.2s\tremaining: 55s\n",
            "397:\tlearn: 76.6348262\ttotal: 55.4s\tremaining: 54.9s\n",
            "398:\tlearn: 76.5682582\ttotal: 55.6s\tremaining: 54.7s\n",
            "399:\tlearn: 76.4101807\ttotal: 55.7s\tremaining: 54.6s\n",
            "400:\tlearn: 76.3141457\ttotal: 55.8s\tremaining: 54.4s\n",
            "401:\tlearn: 76.2031585\ttotal: 56s\tremaining: 54.3s\n",
            "402:\tlearn: 76.1622903\ttotal: 56.1s\tremaining: 54.2s\n",
            "403:\tlearn: 76.1117252\ttotal: 56.3s\tremaining: 54s\n",
            "404:\tlearn: 76.0523039\ttotal: 56.4s\tremaining: 53.9s\n",
            "405:\tlearn: 76.0016501\ttotal: 56.5s\tremaining: 53.7s\n",
            "406:\tlearn: 75.9943911\ttotal: 56.7s\tremaining: 53.6s\n",
            "407:\tlearn: 75.8604529\ttotal: 56.8s\tremaining: 53.5s\n",
            "408:\tlearn: 75.6249110\ttotal: 56.9s\tremaining: 53.3s\n",
            "409:\tlearn: 75.5424635\ttotal: 57.1s\tremaining: 53.2s\n",
            "410:\tlearn: 75.4639407\ttotal: 57.2s\tremaining: 53s\n",
            "411:\tlearn: 75.3983138\ttotal: 57.3s\tremaining: 52.9s\n",
            "412:\tlearn: 75.2921931\ttotal: 57.5s\tremaining: 52.7s\n",
            "413:\tlearn: 75.1248316\ttotal: 57.6s\tremaining: 52.6s\n",
            "414:\tlearn: 75.0867803\ttotal: 57.8s\tremaining: 52.5s\n",
            "415:\tlearn: 75.0179058\ttotal: 57.9s\tremaining: 52.3s\n",
            "416:\tlearn: 74.9746241\ttotal: 58.1s\tremaining: 52.2s\n",
            "417:\tlearn: 74.8247660\ttotal: 58.2s\tremaining: 52.1s\n",
            "418:\tlearn: 74.8031363\ttotal: 58.3s\tremaining: 51.9s\n",
            "419:\tlearn: 74.6609451\ttotal: 58.5s\tremaining: 51.8s\n",
            "420:\tlearn: 74.6016595\ttotal: 58.7s\tremaining: 51.7s\n",
            "421:\tlearn: 74.6011135\ttotal: 58.8s\tremaining: 51.6s\n",
            "422:\tlearn: 74.5460513\ttotal: 59s\tremaining: 51.4s\n",
            "423:\tlearn: 74.4769403\ttotal: 59.1s\tremaining: 51.3s\n",
            "424:\tlearn: 74.3680078\ttotal: 59.2s\tremaining: 51.2s\n",
            "425:\tlearn: 74.3662350\ttotal: 59.4s\tremaining: 51s\n",
            "426:\tlearn: 74.3250904\ttotal: 59.6s\tremaining: 50.9s\n",
            "427:\tlearn: 74.2331580\ttotal: 59.7s\tremaining: 50.8s\n",
            "428:\tlearn: 74.0479383\ttotal: 59.8s\tremaining: 50.6s\n",
            "429:\tlearn: 74.0082953\ttotal: 60s\tremaining: 50.5s\n",
            "430:\tlearn: 73.9409199\ttotal: 1m\tremaining: 50.4s\n",
            "431:\tlearn: 73.8988610\ttotal: 1m\tremaining: 50.2s\n",
            "432:\tlearn: 73.8965182\ttotal: 1m\tremaining: 50.1s\n",
            "433:\tlearn: 73.8251291\ttotal: 1m\tremaining: 50s\n",
            "434:\tlearn: 73.7360814\ttotal: 1m\tremaining: 49.8s\n",
            "435:\tlearn: 73.6016035\ttotal: 1m\tremaining: 49.7s\n",
            "436:\tlearn: 73.4376540\ttotal: 1m\tremaining: 49.5s\n",
            "437:\tlearn: 73.3664067\ttotal: 1m 1s\tremaining: 49.4s\n",
            "438:\tlearn: 73.3358121\ttotal: 1m 1s\tremaining: 49.3s\n",
            "439:\tlearn: 73.2832545\ttotal: 1m 1s\tremaining: 49.1s\n",
            "440:\tlearn: 73.2471984\ttotal: 1m 1s\tremaining: 49s\n",
            "441:\tlearn: 73.1903418\ttotal: 1m 1s\tremaining: 48.9s\n",
            "442:\tlearn: 73.0441281\ttotal: 1m 1s\tremaining: 48.7s\n",
            "443:\tlearn: 73.0270613\ttotal: 1m 2s\tremaining: 48.6s\n",
            "444:\tlearn: 73.0050855\ttotal: 1m 2s\tremaining: 48.5s\n",
            "445:\tlearn: 72.8780010\ttotal: 1m 2s\tremaining: 48.3s\n",
            "446:\tlearn: 72.8443946\ttotal: 1m 2s\tremaining: 48.2s\n",
            "447:\tlearn: 72.7229265\ttotal: 1m 2s\tremaining: 48.1s\n",
            "448:\tlearn: 72.6669421\ttotal: 1m 2s\tremaining: 47.9s\n",
            "449:\tlearn: 72.4824414\ttotal: 1m 2s\tremaining: 47.8s\n",
            "450:\tlearn: 72.4041128\ttotal: 1m 3s\tremaining: 47.6s\n",
            "451:\tlearn: 72.2986963\ttotal: 1m 3s\tremaining: 47.5s\n",
            "452:\tlearn: 72.2648535\ttotal: 1m 3s\tremaining: 47.4s\n",
            "453:\tlearn: 72.1917517\ttotal: 1m 3s\tremaining: 47.2s\n",
            "454:\tlearn: 72.1329614\ttotal: 1m 3s\tremaining: 47.1s\n",
            "455:\tlearn: 72.1165889\ttotal: 1m 3s\tremaining: 46.9s\n",
            "456:\tlearn: 71.9472045\ttotal: 1m 3s\tremaining: 46.8s\n",
            "457:\tlearn: 71.8560349\ttotal: 1m 3s\tremaining: 46.6s\n",
            "458:\tlearn: 71.8225058\ttotal: 1m 4s\tremaining: 46.5s\n",
            "459:\tlearn: 71.6630837\ttotal: 1m 4s\tremaining: 46.4s\n",
            "460:\tlearn: 71.6174450\ttotal: 1m 4s\tremaining: 46.2s\n",
            "461:\tlearn: 71.5445162\ttotal: 1m 4s\tremaining: 46.1s\n",
            "462:\tlearn: 71.5070721\ttotal: 1m 4s\tremaining: 46s\n",
            "463:\tlearn: 71.4572478\ttotal: 1m 4s\tremaining: 45.8s\n",
            "464:\tlearn: 71.2493042\ttotal: 1m 4s\tremaining: 45.7s\n",
            "465:\tlearn: 71.2022383\ttotal: 1m 5s\tremaining: 45.6s\n",
            "466:\tlearn: 71.0979199\ttotal: 1m 5s\tremaining: 45.4s\n",
            "467:\tlearn: 71.0552381\ttotal: 1m 5s\tremaining: 45.3s\n",
            "468:\tlearn: 71.0119020\ttotal: 1m 5s\tremaining: 45.2s\n",
            "469:\tlearn: 70.9733483\ttotal: 1m 5s\tremaining: 45.1s\n",
            "470:\tlearn: 70.9709226\ttotal: 1m 5s\tremaining: 44.9s\n",
            "471:\tlearn: 70.9290368\ttotal: 1m 6s\tremaining: 44.8s\n",
            "472:\tlearn: 70.9108040\ttotal: 1m 6s\tremaining: 44.7s\n",
            "473:\tlearn: 70.8395554\ttotal: 1m 6s\tremaining: 44.5s\n",
            "474:\tlearn: 70.8390769\ttotal: 1m 6s\tremaining: 44.4s\n",
            "475:\tlearn: 70.8056060\ttotal: 1m 6s\tremaining: 44.3s\n",
            "476:\tlearn: 70.6626189\ttotal: 1m 6s\tremaining: 44.1s\n",
            "477:\tlearn: 70.5731035\ttotal: 1m 6s\tremaining: 44s\n",
            "478:\tlearn: 70.4904589\ttotal: 1m 7s\tremaining: 43.8s\n",
            "479:\tlearn: 70.3927080\ttotal: 1m 7s\tremaining: 43.7s\n",
            "480:\tlearn: 70.2874175\ttotal: 1m 7s\tremaining: 43.5s\n",
            "481:\tlearn: 70.1556552\ttotal: 1m 7s\tremaining: 43.4s\n",
            "482:\tlearn: 70.1021025\ttotal: 1m 7s\tremaining: 43.3s\n",
            "483:\tlearn: 70.0353710\ttotal: 1m 7s\tremaining: 43.1s\n",
            "484:\tlearn: 69.9940671\ttotal: 1m 7s\tremaining: 43s\n",
            "485:\tlearn: 69.9459247\ttotal: 1m 8s\tremaining: 42.8s\n",
            "486:\tlearn: 69.8121789\ttotal: 1m 8s\tremaining: 42.7s\n",
            "487:\tlearn: 69.7466782\ttotal: 1m 8s\tremaining: 42.6s\n",
            "488:\tlearn: 69.6390879\ttotal: 1m 8s\tremaining: 42.4s\n",
            "489:\tlearn: 69.4488232\ttotal: 1m 8s\tremaining: 42.3s\n",
            "490:\tlearn: 69.3816829\ttotal: 1m 8s\tremaining: 42.1s\n",
            "491:\tlearn: 69.1836276\ttotal: 1m 8s\tremaining: 42s\n",
            "492:\tlearn: 69.1230936\ttotal: 1m 9s\tremaining: 41.9s\n",
            "493:\tlearn: 68.9690766\ttotal: 1m 9s\tremaining: 41.7s\n",
            "494:\tlearn: 68.9354255\ttotal: 1m 9s\tremaining: 41.6s\n",
            "495:\tlearn: 68.8183321\ttotal: 1m 9s\tremaining: 41.4s\n",
            "496:\tlearn: 68.7767292\ttotal: 1m 9s\tremaining: 41.3s\n",
            "497:\tlearn: 68.5839995\ttotal: 1m 9s\tremaining: 41.2s\n",
            "498:\tlearn: 68.4758046\ttotal: 1m 9s\tremaining: 41s\n",
            "499:\tlearn: 68.3635430\ttotal: 1m 9s\tremaining: 40.9s\n",
            "500:\tlearn: 68.2849603\ttotal: 1m 10s\tremaining: 40.7s\n",
            "501:\tlearn: 68.2585574\ttotal: 1m 10s\tremaining: 40.6s\n",
            "502:\tlearn: 68.2289922\ttotal: 1m 10s\tremaining: 40.5s\n",
            "503:\tlearn: 68.1776745\ttotal: 1m 10s\tremaining: 40.3s\n",
            "504:\tlearn: 68.1572915\ttotal: 1m 10s\tremaining: 40.2s\n",
            "505:\tlearn: 68.1031967\ttotal: 1m 10s\tremaining: 40.1s\n",
            "506:\tlearn: 67.9908737\ttotal: 1m 10s\tremaining: 39.9s\n",
            "507:\tlearn: 67.9370951\ttotal: 1m 11s\tremaining: 39.8s\n",
            "508:\tlearn: 67.7839160\ttotal: 1m 11s\tremaining: 39.6s\n",
            "509:\tlearn: 67.7298678\ttotal: 1m 11s\tremaining: 39.5s\n",
            "510:\tlearn: 67.6104168\ttotal: 1m 11s\tremaining: 39.4s\n",
            "511:\tlearn: 67.5336265\ttotal: 1m 11s\tremaining: 39.2s\n",
            "512:\tlearn: 67.4858269\ttotal: 1m 11s\tremaining: 39.1s\n",
            "513:\tlearn: 67.2671376\ttotal: 1m 11s\tremaining: 38.9s\n",
            "514:\tlearn: 67.2411281\ttotal: 1m 12s\tremaining: 38.8s\n",
            "515:\tlearn: 67.2184066\ttotal: 1m 12s\tremaining: 38.7s\n",
            "516:\tlearn: 67.1327680\ttotal: 1m 12s\tremaining: 38.5s\n",
            "517:\tlearn: 67.0992348\ttotal: 1m 12s\tremaining: 38.4s\n",
            "518:\tlearn: 67.0343408\ttotal: 1m 12s\tremaining: 38.2s\n",
            "519:\tlearn: 66.9249168\ttotal: 1m 12s\tremaining: 38.1s\n",
            "520:\tlearn: 66.8752357\ttotal: 1m 12s\tremaining: 38s\n",
            "521:\tlearn: 66.8527306\ttotal: 1m 13s\tremaining: 37.8s\n",
            "522:\tlearn: 66.8025527\ttotal: 1m 13s\tremaining: 37.7s\n",
            "523:\tlearn: 66.7519086\ttotal: 1m 13s\tremaining: 37.5s\n",
            "524:\tlearn: 66.7039152\ttotal: 1m 13s\tremaining: 37.4s\n",
            "525:\tlearn: 66.6309345\ttotal: 1m 13s\tremaining: 37.3s\n",
            "526:\tlearn: 66.5507670\ttotal: 1m 13s\tremaining: 37.1s\n",
            "527:\tlearn: 66.3949932\ttotal: 1m 13s\tremaining: 37s\n",
            "528:\tlearn: 66.3405780\ttotal: 1m 14s\tremaining: 36.8s\n",
            "529:\tlearn: 66.2411039\ttotal: 1m 14s\tremaining: 36.7s\n",
            "530:\tlearn: 66.1421116\ttotal: 1m 14s\tremaining: 36.6s\n",
            "531:\tlearn: 66.0445200\ttotal: 1m 14s\tremaining: 36.4s\n",
            "532:\tlearn: 65.9795074\ttotal: 1m 14s\tremaining: 36.3s\n",
            "533:\tlearn: 65.9787213\ttotal: 1m 14s\tremaining: 36.1s\n",
            "534:\tlearn: 65.9359941\ttotal: 1m 14s\tremaining: 36s\n",
            "535:\tlearn: 65.8136643\ttotal: 1m 15s\tremaining: 35.9s\n",
            "536:\tlearn: 65.7535630\ttotal: 1m 15s\tremaining: 35.7s\n",
            "537:\tlearn: 65.6485117\ttotal: 1m 15s\tremaining: 35.6s\n",
            "538:\tlearn: 65.5344329\ttotal: 1m 15s\tremaining: 35.4s\n",
            "539:\tlearn: 65.4731401\ttotal: 1m 15s\tremaining: 35.3s\n",
            "540:\tlearn: 65.4516568\ttotal: 1m 15s\tremaining: 35.2s\n",
            "541:\tlearn: 65.4148526\ttotal: 1m 15s\tremaining: 35s\n",
            "542:\tlearn: 65.3482105\ttotal: 1m 16s\tremaining: 34.9s\n",
            "543:\tlearn: 65.1455633\ttotal: 1m 16s\tremaining: 34.7s\n",
            "544:\tlearn: 64.9818270\ttotal: 1m 16s\tremaining: 34.6s\n",
            "545:\tlearn: 64.9297538\ttotal: 1m 16s\tremaining: 34.5s\n",
            "546:\tlearn: 64.8343040\ttotal: 1m 16s\tremaining: 34.3s\n",
            "547:\tlearn: 64.7065614\ttotal: 1m 16s\tremaining: 34.2s\n",
            "548:\tlearn: 64.6691363\ttotal: 1m 16s\tremaining: 34s\n",
            "549:\tlearn: 64.5966521\ttotal: 1m 17s\tremaining: 33.9s\n",
            "550:\tlearn: 64.5050819\ttotal: 1m 17s\tremaining: 33.8s\n",
            "551:\tlearn: 64.4369848\ttotal: 1m 17s\tremaining: 33.6s\n",
            "552:\tlearn: 64.3903815\ttotal: 1m 17s\tremaining: 33.5s\n",
            "553:\tlearn: 64.3488136\ttotal: 1m 17s\tremaining: 33.3s\n",
            "554:\tlearn: 64.3310980\ttotal: 1m 17s\tremaining: 33.2s\n",
            "555:\tlearn: 64.3004304\ttotal: 1m 17s\tremaining: 33.1s\n",
            "556:\tlearn: 64.2517186\ttotal: 1m 18s\tremaining: 32.9s\n",
            "557:\tlearn: 64.1820046\ttotal: 1m 18s\tremaining: 32.8s\n",
            "558:\tlearn: 64.1152299\ttotal: 1m 18s\tremaining: 32.6s\n",
            "559:\tlearn: 64.0779616\ttotal: 1m 18s\tremaining: 32.5s\n",
            "560:\tlearn: 64.0372780\ttotal: 1m 18s\tremaining: 32.4s\n",
            "561:\tlearn: 63.9711224\ttotal: 1m 18s\tremaining: 32.2s\n",
            "562:\tlearn: 63.9541621\ttotal: 1m 18s\tremaining: 32.1s\n",
            "563:\tlearn: 63.8997063\ttotal: 1m 18s\tremaining: 31.9s\n",
            "564:\tlearn: 63.8357844\ttotal: 1m 19s\tremaining: 31.8s\n",
            "565:\tlearn: 63.6946922\ttotal: 1m 19s\tremaining: 31.7s\n",
            "566:\tlearn: 63.6494588\ttotal: 1m 19s\tremaining: 31.5s\n",
            "567:\tlearn: 63.6025990\ttotal: 1m 19s\tremaining: 31.4s\n",
            "568:\tlearn: 63.5587479\ttotal: 1m 19s\tremaining: 31.2s\n",
            "569:\tlearn: 63.5004143\ttotal: 1m 19s\tremaining: 31.1s\n",
            "570:\tlearn: 63.4624969\ttotal: 1m 19s\tremaining: 31s\n",
            "571:\tlearn: 63.4056903\ttotal: 1m 20s\tremaining: 30.8s\n",
            "572:\tlearn: 63.3679137\ttotal: 1m 20s\tremaining: 30.7s\n",
            "573:\tlearn: 63.3033794\ttotal: 1m 20s\tremaining: 30.5s\n",
            "574:\tlearn: 63.2431832\ttotal: 1m 20s\tremaining: 30.4s\n",
            "575:\tlearn: 63.0889838\ttotal: 1m 20s\tremaining: 30.3s\n",
            "576:\tlearn: 62.9585318\ttotal: 1m 20s\tremaining: 30.1s\n",
            "577:\tlearn: 62.8105519\ttotal: 1m 20s\tremaining: 30s\n",
            "578:\tlearn: 62.7798390\ttotal: 1m 21s\tremaining: 29.8s\n",
            "579:\tlearn: 62.7016830\ttotal: 1m 21s\tremaining: 29.7s\n",
            "580:\tlearn: 62.6429932\ttotal: 1m 21s\tremaining: 29.5s\n",
            "581:\tlearn: 62.6005027\ttotal: 1m 21s\tremaining: 29.4s\n",
            "582:\tlearn: 62.5999811\ttotal: 1m 21s\tremaining: 29.3s\n",
            "583:\tlearn: 62.5667990\ttotal: 1m 21s\tremaining: 29.1s\n",
            "584:\tlearn: 62.5072122\ttotal: 1m 21s\tremaining: 29s\n",
            "585:\tlearn: 62.4657459\ttotal: 1m 22s\tremaining: 28.8s\n",
            "586:\tlearn: 62.4358917\ttotal: 1m 22s\tremaining: 28.7s\n",
            "587:\tlearn: 62.3462895\ttotal: 1m 22s\tremaining: 28.6s\n",
            "588:\tlearn: 62.3285501\ttotal: 1m 22s\tremaining: 28.4s\n",
            "589:\tlearn: 62.2328385\ttotal: 1m 22s\tremaining: 28.3s\n",
            "590:\tlearn: 62.1326551\ttotal: 1m 22s\tremaining: 28.2s\n",
            "591:\tlearn: 62.1158818\ttotal: 1m 22s\tremaining: 28s\n",
            "592:\tlearn: 62.1145517\ttotal: 1m 23s\tremaining: 27.9s\n",
            "593:\tlearn: 62.1139691\ttotal: 1m 23s\tremaining: 27.8s\n",
            "594:\tlearn: 62.0624302\ttotal: 1m 23s\tremaining: 27.6s\n",
            "595:\tlearn: 61.9998415\ttotal: 1m 23s\tremaining: 27.5s\n",
            "596:\tlearn: 61.9072026\ttotal: 1m 23s\tremaining: 27.3s\n",
            "597:\tlearn: 61.8803013\ttotal: 1m 23s\tremaining: 27.2s\n",
            "598:\tlearn: 61.8422169\ttotal: 1m 23s\tremaining: 27s\n",
            "599:\tlearn: 61.7681182\ttotal: 1m 24s\tremaining: 26.9s\n",
            "600:\tlearn: 61.7154320\ttotal: 1m 24s\tremaining: 26.8s\n",
            "601:\tlearn: 61.6946354\ttotal: 1m 24s\tremaining: 26.6s\n",
            "602:\tlearn: 61.6633122\ttotal: 1m 24s\tremaining: 26.5s\n",
            "603:\tlearn: 61.6632006\ttotal: 1m 24s\tremaining: 26.4s\n",
            "604:\tlearn: 61.5265087\ttotal: 1m 24s\tremaining: 26.2s\n",
            "605:\tlearn: 61.3867282\ttotal: 1m 24s\tremaining: 26.1s\n",
            "606:\tlearn: 61.3443516\ttotal: 1m 25s\tremaining: 25.9s\n",
            "607:\tlearn: 61.2820864\ttotal: 1m 25s\tremaining: 25.8s\n",
            "608:\tlearn: 61.2146205\ttotal: 1m 25s\tremaining: 25.7s\n",
            "609:\tlearn: 61.1306671\ttotal: 1m 25s\tremaining: 25.5s\n",
            "610:\tlearn: 61.0720161\ttotal: 1m 25s\tremaining: 25.4s\n",
            "611:\tlearn: 60.9831173\ttotal: 1m 25s\tremaining: 25.2s\n",
            "612:\tlearn: 60.9481307\ttotal: 1m 25s\tremaining: 25.1s\n",
            "613:\tlearn: 60.9197588\ttotal: 1m 26s\tremaining: 24.9s\n",
            "614:\tlearn: 60.8898651\ttotal: 1m 26s\tremaining: 24.8s\n",
            "615:\tlearn: 60.8481397\ttotal: 1m 26s\tremaining: 24.7s\n",
            "616:\tlearn: 60.7914096\ttotal: 1m 26s\tremaining: 24.5s\n",
            "617:\tlearn: 60.7240701\ttotal: 1m 26s\tremaining: 24.4s\n",
            "618:\tlearn: 60.6980103\ttotal: 1m 26s\tremaining: 24.3s\n",
            "619:\tlearn: 60.6386289\ttotal: 1m 26s\tremaining: 24.1s\n",
            "620:\tlearn: 60.5958519\ttotal: 1m 27s\tremaining: 24s\n",
            "621:\tlearn: 60.4992616\ttotal: 1m 27s\tremaining: 23.8s\n",
            "622:\tlearn: 60.4549816\ttotal: 1m 27s\tremaining: 23.7s\n",
            "623:\tlearn: 60.3450776\ttotal: 1m 27s\tremaining: 23.5s\n",
            "624:\tlearn: 60.2637184\ttotal: 1m 27s\tremaining: 23.4s\n",
            "625:\tlearn: 60.1905188\ttotal: 1m 27s\tremaining: 23.3s\n",
            "626:\tlearn: 60.1458314\ttotal: 1m 27s\tremaining: 23.1s\n",
            "627:\tlearn: 60.0745560\ttotal: 1m 27s\tremaining: 23s\n",
            "628:\tlearn: 60.0403213\ttotal: 1m 28s\tremaining: 22.8s\n",
            "629:\tlearn: 60.0184613\ttotal: 1m 28s\tremaining: 22.7s\n",
            "630:\tlearn: 59.9920419\ttotal: 1m 28s\tremaining: 22.6s\n",
            "631:\tlearn: 59.9565557\ttotal: 1m 28s\tremaining: 22.4s\n",
            "632:\tlearn: 59.9550921\ttotal: 1m 28s\tremaining: 22.3s\n",
            "633:\tlearn: 59.9189334\ttotal: 1m 28s\tremaining: 22.2s\n",
            "634:\tlearn: 59.8955844\ttotal: 1m 29s\tremaining: 22s\n",
            "635:\tlearn: 59.8645277\ttotal: 1m 29s\tremaining: 21.9s\n",
            "636:\tlearn: 59.7656402\ttotal: 1m 29s\tremaining: 21.7s\n",
            "637:\tlearn: 59.7654740\ttotal: 1m 29s\tremaining: 21.6s\n",
            "638:\tlearn: 59.7554027\ttotal: 1m 29s\tremaining: 21.5s\n",
            "639:\tlearn: 59.6821942\ttotal: 1m 29s\tremaining: 21.3s\n",
            "640:\tlearn: 59.6391539\ttotal: 1m 29s\tremaining: 21.2s\n",
            "641:\tlearn: 59.6036983\ttotal: 1m 30s\tremaining: 21s\n",
            "642:\tlearn: 59.5413581\ttotal: 1m 30s\tremaining: 20.9s\n",
            "643:\tlearn: 59.5127111\ttotal: 1m 30s\tremaining: 20.8s\n",
            "644:\tlearn: 59.4528132\ttotal: 1m 30s\tremaining: 20.6s\n",
            "645:\tlearn: 59.4521673\ttotal: 1m 30s\tremaining: 20.5s\n",
            "646:\tlearn: 59.4107720\ttotal: 1m 30s\tremaining: 20.3s\n",
            "647:\tlearn: 59.3878654\ttotal: 1m 30s\tremaining: 20.2s\n",
            "648:\tlearn: 59.3712053\ttotal: 1m 31s\tremaining: 20.1s\n",
            "649:\tlearn: 59.3383775\ttotal: 1m 31s\tremaining: 19.9s\n",
            "650:\tlearn: 59.2965812\ttotal: 1m 31s\tremaining: 19.8s\n",
            "651:\tlearn: 59.2516849\ttotal: 1m 31s\tremaining: 19.6s\n",
            "652:\tlearn: 59.1375575\ttotal: 1m 31s\tremaining: 19.5s\n",
            "653:\tlearn: 59.0972976\ttotal: 1m 31s\tremaining: 19.4s\n",
            "654:\tlearn: 59.0645640\ttotal: 1m 31s\tremaining: 19.2s\n",
            "655:\tlearn: 59.0107776\ttotal: 1m 32s\tremaining: 19.1s\n",
            "656:\tlearn: 58.9832020\ttotal: 1m 32s\tremaining: 18.9s\n",
            "657:\tlearn: 58.9565773\ttotal: 1m 32s\tremaining: 18.8s\n",
            "658:\tlearn: 58.9019787\ttotal: 1m 32s\tremaining: 18.7s\n",
            "659:\tlearn: 58.8115698\ttotal: 1m 32s\tremaining: 18.5s\n",
            "660:\tlearn: 58.7197729\ttotal: 1m 32s\tremaining: 18.4s\n",
            "661:\tlearn: 58.6404916\ttotal: 1m 32s\tremaining: 18.2s\n",
            "662:\tlearn: 58.6100751\ttotal: 1m 33s\tremaining: 18.1s\n",
            "663:\tlearn: 58.5994354\ttotal: 1m 33s\tremaining: 18s\n",
            "664:\tlearn: 58.5991957\ttotal: 1m 33s\tremaining: 17.8s\n",
            "665:\tlearn: 58.5674656\ttotal: 1m 33s\tremaining: 17.7s\n",
            "666:\tlearn: 58.4954470\ttotal: 1m 33s\tremaining: 17.5s\n",
            "667:\tlearn: 58.4290025\ttotal: 1m 33s\tremaining: 17.4s\n",
            "668:\tlearn: 58.4073805\ttotal: 1m 33s\tremaining: 17.3s\n",
            "669:\tlearn: 58.3292054\ttotal: 1m 34s\tremaining: 17.1s\n",
            "670:\tlearn: 58.2838827\ttotal: 1m 34s\tremaining: 17s\n",
            "671:\tlearn: 58.2485499\ttotal: 1m 34s\tremaining: 16.8s\n",
            "672:\tlearn: 58.1586069\ttotal: 1m 34s\tremaining: 16.7s\n",
            "673:\tlearn: 58.1206902\ttotal: 1m 34s\tremaining: 16.6s\n",
            "674:\tlearn: 58.1025971\ttotal: 1m 34s\tremaining: 16.4s\n",
            "675:\tlearn: 58.0147787\ttotal: 1m 34s\tremaining: 16.3s\n",
            "676:\tlearn: 57.9878416\ttotal: 1m 35s\tremaining: 16.1s\n",
            "677:\tlearn: 57.9590058\ttotal: 1m 35s\tremaining: 16s\n",
            "678:\tlearn: 57.9285904\ttotal: 1m 35s\tremaining: 15.9s\n",
            "679:\tlearn: 57.8389171\ttotal: 1m 35s\tremaining: 15.7s\n",
            "680:\tlearn: 57.7970998\ttotal: 1m 35s\tremaining: 15.6s\n",
            "681:\tlearn: 57.6274711\ttotal: 1m 35s\tremaining: 15.4s\n",
            "682:\tlearn: 57.5890612\ttotal: 1m 35s\tremaining: 15.3s\n",
            "683:\tlearn: 57.5117155\ttotal: 1m 35s\tremaining: 15.2s\n",
            "684:\tlearn: 57.4801208\ttotal: 1m 36s\tremaining: 15s\n",
            "685:\tlearn: 57.3716855\ttotal: 1m 36s\tremaining: 14.9s\n",
            "686:\tlearn: 57.3349832\ttotal: 1m 36s\tremaining: 14.7s\n",
            "687:\tlearn: 57.2974939\ttotal: 1m 36s\tremaining: 14.6s\n",
            "688:\tlearn: 57.2410447\ttotal: 1m 36s\tremaining: 14.5s\n",
            "689:\tlearn: 57.2025623\ttotal: 1m 36s\tremaining: 14.3s\n",
            "690:\tlearn: 57.1561669\ttotal: 1m 36s\tremaining: 14.2s\n",
            "691:\tlearn: 57.1098823\ttotal: 1m 37s\tremaining: 14s\n",
            "692:\tlearn: 57.0746328\ttotal: 1m 37s\tremaining: 13.9s\n",
            "693:\tlearn: 57.0712014\ttotal: 1m 37s\tremaining: 13.8s\n",
            "694:\tlearn: 57.0458687\ttotal: 1m 37s\tremaining: 13.6s\n",
            "695:\tlearn: 57.0001953\ttotal: 1m 37s\tremaining: 13.5s\n",
            "696:\tlearn: 56.9740003\ttotal: 1m 37s\tremaining: 13.3s\n",
            "697:\tlearn: 56.8819576\ttotal: 1m 37s\tremaining: 13.2s\n",
            "698:\tlearn: 56.7906202\ttotal: 1m 38s\tremaining: 13.1s\n",
            "699:\tlearn: 56.7670339\ttotal: 1m 38s\tremaining: 12.9s\n",
            "700:\tlearn: 56.5818046\ttotal: 1m 38s\tremaining: 12.8s\n",
            "701:\tlearn: 56.5058365\ttotal: 1m 38s\tremaining: 12.6s\n",
            "702:\tlearn: 56.4910414\ttotal: 1m 38s\tremaining: 12.5s\n",
            "703:\tlearn: 56.4678645\ttotal: 1m 38s\tremaining: 12.4s\n",
            "704:\tlearn: 56.4476849\ttotal: 1m 39s\tremaining: 12.2s\n",
            "705:\tlearn: 56.3986061\ttotal: 1m 39s\tremaining: 12.1s\n",
            "706:\tlearn: 56.3705047\ttotal: 1m 39s\tremaining: 11.9s\n",
            "707:\tlearn: 56.2654420\ttotal: 1m 39s\tremaining: 11.8s\n",
            "708:\tlearn: 56.2145947\ttotal: 1m 39s\tremaining: 11.7s\n",
            "709:\tlearn: 56.1870986\ttotal: 1m 39s\tremaining: 11.5s\n",
            "710:\tlearn: 56.1340806\ttotal: 1m 39s\tremaining: 11.4s\n",
            "711:\tlearn: 56.0991101\ttotal: 1m 40s\tremaining: 11.2s\n",
            "712:\tlearn: 55.9801230\ttotal: 1m 40s\tremaining: 11.1s\n",
            "713:\tlearn: 55.9501902\ttotal: 1m 40s\tremaining: 11s\n",
            "714:\tlearn: 55.8792845\ttotal: 1m 40s\tremaining: 10.8s\n",
            "715:\tlearn: 55.8781958\ttotal: 1m 40s\tremaining: 10.7s\n",
            "716:\tlearn: 55.8685649\ttotal: 1m 40s\tremaining: 10.5s\n",
            "717:\tlearn: 55.8230608\ttotal: 1m 40s\tremaining: 10.4s\n",
            "718:\tlearn: 55.8030532\ttotal: 1m 41s\tremaining: 10.3s\n",
            "719:\tlearn: 55.7031226\ttotal: 1m 41s\tremaining: 10.1s\n",
            "720:\tlearn: 55.6329107\ttotal: 1m 41s\tremaining: 9.98s\n",
            "721:\tlearn: 55.5993995\ttotal: 1m 41s\tremaining: 9.83s\n",
            "722:\tlearn: 55.5798983\ttotal: 1m 41s\tremaining: 9.69s\n",
            "723:\tlearn: 55.5429299\ttotal: 1m 41s\tremaining: 9.55s\n",
            "724:\tlearn: 55.5020664\ttotal: 1m 41s\tremaining: 9.41s\n",
            "725:\tlearn: 55.4793305\ttotal: 1m 41s\tremaining: 9.27s\n",
            "726:\tlearn: 55.4580257\ttotal: 1m 42s\tremaining: 9.13s\n",
            "727:\tlearn: 55.4219571\ttotal: 1m 42s\tremaining: 8.99s\n",
            "728:\tlearn: 55.3876325\ttotal: 1m 42s\tremaining: 8.85s\n",
            "729:\tlearn: 55.3653148\ttotal: 1m 42s\tremaining: 8.72s\n",
            "730:\tlearn: 55.3251254\ttotal: 1m 42s\tremaining: 8.58s\n",
            "731:\tlearn: 55.3012479\ttotal: 1m 42s\tremaining: 8.44s\n",
            "732:\tlearn: 55.2933218\ttotal: 1m 43s\tremaining: 8.3s\n",
            "733:\tlearn: 55.2643949\ttotal: 1m 43s\tremaining: 8.16s\n",
            "734:\tlearn: 55.2104012\ttotal: 1m 43s\tremaining: 8.02s\n",
            "735:\tlearn: 55.1680201\ttotal: 1m 43s\tremaining: 7.88s\n",
            "736:\tlearn: 55.1196615\ttotal: 1m 43s\tremaining: 7.73s\n",
            "737:\tlearn: 55.0890845\ttotal: 1m 43s\tremaining: 7.59s\n",
            "738:\tlearn: 55.0709156\ttotal: 1m 43s\tremaining: 7.45s\n",
            "739:\tlearn: 55.0418371\ttotal: 1m 44s\tremaining: 7.31s\n",
            "740:\tlearn: 55.0092318\ttotal: 1m 44s\tremaining: 7.17s\n",
            "741:\tlearn: 54.9639214\ttotal: 1m 44s\tremaining: 7.03s\n",
            "742:\tlearn: 54.9446077\ttotal: 1m 44s\tremaining: 6.89s\n",
            "743:\tlearn: 54.8607749\ttotal: 1m 44s\tremaining: 6.75s\n",
            "744:\tlearn: 54.8252929\ttotal: 1m 44s\tremaining: 6.61s\n",
            "745:\tlearn: 54.7948859\ttotal: 1m 44s\tremaining: 6.47s\n",
            "746:\tlearn: 54.7498540\ttotal: 1m 45s\tremaining: 6.33s\n",
            "747:\tlearn: 54.7072270\ttotal: 1m 45s\tremaining: 6.19s\n",
            "748:\tlearn: 54.6451664\ttotal: 1m 45s\tremaining: 6.05s\n",
            "749:\tlearn: 54.5965624\ttotal: 1m 45s\tremaining: 5.91s\n",
            "750:\tlearn: 54.5790229\ttotal: 1m 45s\tremaining: 5.76s\n",
            "751:\tlearn: 54.5415091\ttotal: 1m 45s\tremaining: 5.63s\n",
            "752:\tlearn: 54.5230983\ttotal: 1m 45s\tremaining: 5.49s\n",
            "753:\tlearn: 54.4763546\ttotal: 1m 46s\tremaining: 5.34s\n",
            "754:\tlearn: 54.3826679\ttotal: 1m 46s\tremaining: 5.2s\n",
            "755:\tlearn: 54.2979825\ttotal: 1m 46s\tremaining: 5.06s\n",
            "756:\tlearn: 54.2510973\ttotal: 1m 46s\tremaining: 4.92s\n",
            "757:\tlearn: 54.1360433\ttotal: 1m 46s\tremaining: 4.78s\n",
            "758:\tlearn: 54.0976123\ttotal: 1m 46s\tremaining: 4.64s\n",
            "759:\tlearn: 54.0590655\ttotal: 1m 46s\tremaining: 4.5s\n",
            "760:\tlearn: 54.0168278\ttotal: 1m 47s\tremaining: 4.36s\n",
            "761:\tlearn: 53.9712913\ttotal: 1m 47s\tremaining: 4.22s\n",
            "762:\tlearn: 53.9704534\ttotal: 1m 47s\tremaining: 4.08s\n",
            "763:\tlearn: 53.9562095\ttotal: 1m 47s\tremaining: 3.94s\n",
            "764:\tlearn: 53.9285823\ttotal: 1m 47s\tremaining: 3.8s\n",
            "765:\tlearn: 53.8945865\ttotal: 1m 47s\tremaining: 3.65s\n",
            "766:\tlearn: 53.8476251\ttotal: 1m 47s\tremaining: 3.51s\n",
            "767:\tlearn: 53.7727747\ttotal: 1m 47s\tremaining: 3.37s\n",
            "768:\tlearn: 53.7428036\ttotal: 1m 48s\tremaining: 3.23s\n",
            "769:\tlearn: 53.6516019\ttotal: 1m 48s\tremaining: 3.09s\n",
            "770:\tlearn: 53.6372747\ttotal: 1m 48s\tremaining: 2.95s\n",
            "771:\tlearn: 53.6065705\ttotal: 1m 48s\tremaining: 2.81s\n",
            "772:\tlearn: 53.6053941\ttotal: 1m 48s\tremaining: 2.67s\n",
            "773:\tlearn: 53.5056563\ttotal: 1m 48s\tremaining: 2.53s\n",
            "774:\tlearn: 53.4724680\ttotal: 1m 48s\tremaining: 2.39s\n",
            "775:\tlearn: 53.3382876\ttotal: 1m 49s\tremaining: 2.25s\n",
            "776:\tlearn: 53.2998527\ttotal: 1m 49s\tremaining: 2.11s\n",
            "777:\tlearn: 53.2787192\ttotal: 1m 49s\tremaining: 1.97s\n",
            "778:\tlearn: 53.2242874\ttotal: 1m 49s\tremaining: 1.83s\n",
            "779:\tlearn: 53.1719417\ttotal: 1m 49s\tremaining: 1.69s\n",
            "780:\tlearn: 53.1060424\ttotal: 1m 49s\tremaining: 1.54s\n",
            "781:\tlearn: 53.0742274\ttotal: 1m 49s\tremaining: 1.41s\n",
            "782:\tlearn: 53.0624505\ttotal: 1m 50s\tremaining: 1.26s\n",
            "783:\tlearn: 52.9911780\ttotal: 1m 50s\tremaining: 1.12s\n",
            "784:\tlearn: 52.9752041\ttotal: 1m 50s\tremaining: 984ms\n",
            "785:\tlearn: 52.9645784\ttotal: 1m 50s\tremaining: 843ms\n",
            "786:\tlearn: 52.8950278\ttotal: 1m 50s\tremaining: 703ms\n",
            "787:\tlearn: 52.8124991\ttotal: 1m 50s\tremaining: 562ms\n",
            "788:\tlearn: 52.7877428\ttotal: 1m 50s\tremaining: 422ms\n",
            "789:\tlearn: 52.7655689\ttotal: 1m 51s\tremaining: 281ms\n",
            "790:\tlearn: 52.7245685\ttotal: 1m 51s\tremaining: 141ms\n",
            "791:\tlearn: 52.6426412\ttotal: 1m 51s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 00:47:49,969]\u001b[0m A new study created in memory with name: no-name-34d199d9-2296-4ac7-aab4-4affaab2fe81\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 00:49:50,662]\u001b[0m Trial 0 finished with value: 55.43807050501562 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 55.43807050501562.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:51:58,839]\u001b[0m Trial 1 finished with value: 46.94340540096268 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 46.94340540096268.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:54:03,178]\u001b[0m Trial 2 finished with value: 46.6410394064305 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:09:49,764]\u001b[0m Trial 3 finished with value: 71.68530020741635 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:11:56,972]\u001b[0m Trial 4 finished with value: 55.37986954123148 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:28:07,671]\u001b[0m Trial 5 finished with value: 69.1836400384031 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:31:20,332]\u001b[0m Trial 6 finished with value: 46.56191676648264 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:34:22,077]\u001b[0m Trial 7 finished with value: 49.97255745046673 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:35:58,206]\u001b[0m Trial 8 finished with value: 68.5849081431324 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:56:01,550]\u001b[0m Trial 9 finished with value: 55.86651997949391 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:57:04,850]\u001b[0m Trial 10 finished with value: 83.54449536524108 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:58:40,351]\u001b[0m Trial 11 finished with value: 57.436170968311984 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:02:19,647]\u001b[0m Trial 12 finished with value: 49.018176081587626 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:04:46,095]\u001b[0m Trial 13 finished with value: 46.163782205728744 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:06:49,189]\u001b[0m Trial 14 finished with value: 63.39437740801802 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:08:08,780]\u001b[0m Trial 15 finished with value: 64.41581750617603 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:09:29,684]\u001b[0m Trial 16 finished with value: 67.755824227023 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:12:08,220]\u001b[0m Trial 17 finished with value: 44.239773522743256 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 44.239773522743256.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:19:15,919]\u001b[0m Trial 18 finished with value: 51.7384520797867 and parameters: {'iterations': 764, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 86.15280749591527, 'od_wait': 64, 'leaf_estimation_iterations': 15, 'bagging_temperature': 56.02552081166322, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 44.239773522743256.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:21:35,650]\u001b[0m Trial 19 finished with value: 45.86744070140521 and parameters: {'iterations': 673, 'learning_rate': 0.3989455666851299, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 84.99456385371188, 'od_wait': 25, 'leaf_estimation_iterations': 17, 'bagging_temperature': 9.35910112420739, 'colsample_bylevel': 0.49862570897953884}. Best is trial 17 with value: 44.239773522743256.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 2: 44.239773522743256\n",
            "Best hyperparameters for fold 2: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 207.3189292\ttotal: 170ms\tremaining: 2m 14s\n",
            "1:\tlearn: 206.0674098\ttotal: 293ms\tremaining: 1m 55s\n",
            "2:\tlearn: 205.6772481\ttotal: 479ms\tremaining: 2m 6s\n",
            "3:\tlearn: 205.4774880\ttotal: 558ms\tremaining: 1m 49s\n",
            "4:\tlearn: 204.5742147\ttotal: 739ms\tremaining: 1m 56s\n",
            "5:\tlearn: 204.3597055\ttotal: 888ms\tremaining: 1m 56s\n",
            "6:\tlearn: 203.5728966\ttotal: 1.03s\tremaining: 1m 55s\n",
            "7:\tlearn: 203.5728702\ttotal: 1.1s\tremaining: 1m 48s\n",
            "8:\tlearn: 203.5404589\ttotal: 1.21s\tremaining: 1m 45s\n",
            "9:\tlearn: 202.6561341\ttotal: 1.37s\tremaining: 1m 47s\n",
            "10:\tlearn: 202.6514240\ttotal: 1.43s\tremaining: 1m 41s\n",
            "11:\tlearn: 202.4720807\ttotal: 1.6s\tremaining: 1m 44s\n",
            "12:\tlearn: 201.8706778\ttotal: 1.77s\tremaining: 1m 46s\n",
            "13:\tlearn: 201.2110156\ttotal: 1.88s\tremaining: 1m 44s\n",
            "14:\tlearn: 201.1152666\ttotal: 2.06s\tremaining: 1m 46s\n",
            "15:\tlearn: 201.1141910\ttotal: 2.15s\tremaining: 1m 44s\n",
            "16:\tlearn: 201.0668308\ttotal: 2.27s\tremaining: 1m 43s\n",
            "17:\tlearn: 200.5607246\ttotal: 2.43s\tremaining: 1m 44s\n",
            "18:\tlearn: 199.7489055\ttotal: 2.59s\tremaining: 1m 45s\n",
            "19:\tlearn: 199.6448655\ttotal: 2.65s\tremaining: 1m 42s\n",
            "20:\tlearn: 199.4965538\ttotal: 2.8s\tremaining: 1m 42s\n",
            "21:\tlearn: 199.4630398\ttotal: 2.9s\tremaining: 1m 41s\n",
            "22:\tlearn: 199.2697364\ttotal: 3.06s\tremaining: 1m 42s\n",
            "23:\tlearn: 199.2697364\ttotal: 3.11s\tremaining: 1m 39s\n",
            "24:\tlearn: 198.8305705\ttotal: 3.26s\tremaining: 1m 40s\n",
            "25:\tlearn: 198.5897160\ttotal: 3.44s\tremaining: 1m 41s\n",
            "26:\tlearn: 198.5143668\ttotal: 3.61s\tremaining: 1m 42s\n",
            "27:\tlearn: 198.5143668\ttotal: 3.65s\tremaining: 1m 39s\n",
            "28:\tlearn: 198.4399385\ttotal: 3.78s\tremaining: 1m 39s\n",
            "29:\tlearn: 198.2047314\ttotal: 3.94s\tremaining: 1m 40s\n",
            "30:\tlearn: 198.0866774\ttotal: 4.05s\tremaining: 1m 39s\n",
            "31:\tlearn: 198.0739424\ttotal: 4.11s\tremaining: 1m 37s\n",
            "32:\tlearn: 197.9247174\ttotal: 4.29s\tremaining: 1m 38s\n",
            "33:\tlearn: 197.9247174\ttotal: 4.34s\tremaining: 1m 36s\n",
            "34:\tlearn: 196.9729549\ttotal: 4.48s\tremaining: 1m 36s\n",
            "35:\tlearn: 196.0524651\ttotal: 4.55s\tremaining: 1m 35s\n",
            "36:\tlearn: 194.9066093\ttotal: 4.69s\tremaining: 1m 35s\n",
            "37:\tlearn: 193.0964621\ttotal: 4.86s\tremaining: 1m 36s\n",
            "38:\tlearn: 191.2790210\ttotal: 5.02s\tremaining: 1m 36s\n",
            "39:\tlearn: 190.0543386\ttotal: 5.19s\tremaining: 1m 37s\n",
            "40:\tlearn: 187.4116179\ttotal: 5.34s\tremaining: 1m 37s\n",
            "41:\tlearn: 186.4954937\ttotal: 5.48s\tremaining: 1m 37s\n",
            "42:\tlearn: 185.3311921\ttotal: 5.66s\tremaining: 1m 38s\n",
            "43:\tlearn: 184.8825898\ttotal: 5.83s\tremaining: 1m 39s\n",
            "44:\tlearn: 184.1993113\ttotal: 5.98s\tremaining: 1m 39s\n",
            "45:\tlearn: 183.5924554\ttotal: 6.12s\tremaining: 1m 39s\n",
            "46:\tlearn: 183.0416070\ttotal: 6.29s\tremaining: 1m 39s\n",
            "47:\tlearn: 181.4337417\ttotal: 6.44s\tremaining: 1m 39s\n",
            "48:\tlearn: 181.0443297\ttotal: 6.59s\tremaining: 1m 39s\n",
            "49:\tlearn: 180.6837432\ttotal: 6.76s\tremaining: 1m 40s\n",
            "50:\tlearn: 179.1259059\ttotal: 6.9s\tremaining: 1m 40s\n",
            "51:\tlearn: 178.7081892\ttotal: 7.06s\tremaining: 1m 40s\n",
            "52:\tlearn: 178.3231941\ttotal: 7.22s\tremaining: 1m 40s\n",
            "53:\tlearn: 175.5725203\ttotal: 7.37s\tremaining: 1m 40s\n",
            "54:\tlearn: 175.3052818\ttotal: 7.52s\tremaining: 1m 40s\n",
            "55:\tlearn: 173.5898817\ttotal: 7.66s\tremaining: 1m 40s\n",
            "56:\tlearn: 173.0771917\ttotal: 7.8s\tremaining: 1m 40s\n",
            "57:\tlearn: 172.7771094\ttotal: 7.97s\tremaining: 1m 40s\n",
            "58:\tlearn: 170.7573477\ttotal: 8.11s\tremaining: 1m 40s\n",
            "59:\tlearn: 170.3393400\ttotal: 8.26s\tremaining: 1m 40s\n",
            "60:\tlearn: 169.7098525\ttotal: 8.4s\tremaining: 1m 40s\n",
            "61:\tlearn: 169.5801832\ttotal: 8.57s\tremaining: 1m 40s\n",
            "62:\tlearn: 168.8364086\ttotal: 8.72s\tremaining: 1m 40s\n",
            "63:\tlearn: 167.3478433\ttotal: 8.86s\tremaining: 1m 40s\n",
            "64:\tlearn: 167.0485031\ttotal: 9.02s\tremaining: 1m 40s\n",
            "65:\tlearn: 166.0908931\ttotal: 9.16s\tremaining: 1m 40s\n",
            "66:\tlearn: 164.8926295\ttotal: 9.31s\tremaining: 1m 40s\n",
            "67:\tlearn: 163.9220266\ttotal: 9.45s\tremaining: 1m 40s\n",
            "68:\tlearn: 163.5746354\ttotal: 9.6s\tremaining: 1m 40s\n",
            "69:\tlearn: 162.8425386\ttotal: 9.76s\tremaining: 1m 40s\n",
            "70:\tlearn: 162.6698663\ttotal: 9.92s\tremaining: 1m 40s\n",
            "71:\tlearn: 161.4314943\ttotal: 10.1s\tremaining: 1m 40s\n",
            "72:\tlearn: 161.1543149\ttotal: 10.2s\tremaining: 1m 40s\n",
            "73:\tlearn: 160.3692914\ttotal: 10.4s\tremaining: 1m 40s\n",
            "74:\tlearn: 159.1687554\ttotal: 10.5s\tremaining: 1m 40s\n",
            "75:\tlearn: 158.0129164\ttotal: 10.7s\tremaining: 1m 40s\n",
            "76:\tlearn: 157.1621244\ttotal: 10.8s\tremaining: 1m 40s\n",
            "77:\tlearn: 156.1621105\ttotal: 11s\tremaining: 1m 40s\n",
            "78:\tlearn: 155.5190055\ttotal: 11.1s\tremaining: 1m 40s\n",
            "79:\tlearn: 154.8394672\ttotal: 11.3s\tremaining: 1m 40s\n",
            "80:\tlearn: 154.6914146\ttotal: 11.4s\tremaining: 1m 40s\n",
            "81:\tlearn: 154.2493012\ttotal: 11.6s\tremaining: 1m 40s\n",
            "82:\tlearn: 152.1032323\ttotal: 11.7s\tremaining: 1m 40s\n",
            "83:\tlearn: 151.2019445\ttotal: 11.9s\tremaining: 1m 40s\n",
            "84:\tlearn: 150.3560170\ttotal: 12s\tremaining: 1m 39s\n",
            "85:\tlearn: 149.2767084\ttotal: 12.2s\tremaining: 1m 39s\n",
            "86:\tlearn: 149.0447459\ttotal: 12.3s\tremaining: 1m 40s\n",
            "87:\tlearn: 148.4729548\ttotal: 12.5s\tremaining: 1m 39s\n",
            "88:\tlearn: 148.1559388\ttotal: 12.6s\tremaining: 1m 39s\n",
            "89:\tlearn: 147.9929772\ttotal: 12.8s\tremaining: 1m 39s\n",
            "90:\tlearn: 147.7046693\ttotal: 12.9s\tremaining: 1m 39s\n",
            "91:\tlearn: 147.1562018\ttotal: 13.1s\tremaining: 1m 39s\n",
            "92:\tlearn: 146.8782324\ttotal: 13.2s\tremaining: 1m 39s\n",
            "93:\tlearn: 145.9687810\ttotal: 13.4s\tremaining: 1m 39s\n",
            "94:\tlearn: 145.1540416\ttotal: 13.5s\tremaining: 1m 39s\n",
            "95:\tlearn: 144.9911316\ttotal: 13.7s\tremaining: 1m 39s\n",
            "96:\tlearn: 144.4096673\ttotal: 13.8s\tremaining: 1m 39s\n",
            "97:\tlearn: 143.8359914\ttotal: 13.9s\tremaining: 1m 38s\n",
            "98:\tlearn: 143.3878256\ttotal: 14.1s\tremaining: 1m 38s\n",
            "99:\tlearn: 141.8598332\ttotal: 14.2s\tremaining: 1m 38s\n",
            "100:\tlearn: 141.3499718\ttotal: 14.4s\tremaining: 1m 38s\n",
            "101:\tlearn: 140.9399011\ttotal: 14.5s\tremaining: 1m 38s\n",
            "102:\tlearn: 140.3626285\ttotal: 14.7s\tremaining: 1m 38s\n",
            "103:\tlearn: 139.7188246\ttotal: 14.8s\tremaining: 1m 37s\n",
            "104:\tlearn: 138.8029567\ttotal: 14.9s\tremaining: 1m 37s\n",
            "105:\tlearn: 137.8949568\ttotal: 15.1s\tremaining: 1m 37s\n",
            "106:\tlearn: 137.6331879\ttotal: 15.2s\tremaining: 1m 37s\n",
            "107:\tlearn: 137.2656788\ttotal: 15.4s\tremaining: 1m 37s\n",
            "108:\tlearn: 136.7935215\ttotal: 15.5s\tremaining: 1m 37s\n",
            "109:\tlearn: 136.4413677\ttotal: 15.7s\tremaining: 1m 37s\n",
            "110:\tlearn: 135.8561884\ttotal: 15.8s\tremaining: 1m 37s\n",
            "111:\tlearn: 135.5011915\ttotal: 16s\tremaining: 1m 37s\n",
            "112:\tlearn: 135.1649080\ttotal: 16.1s\tremaining: 1m 36s\n",
            "113:\tlearn: 134.9943756\ttotal: 16.3s\tremaining: 1m 36s\n",
            "114:\tlearn: 134.7696851\ttotal: 16.4s\tremaining: 1m 36s\n",
            "115:\tlearn: 134.3082649\ttotal: 16.6s\tremaining: 1m 36s\n",
            "116:\tlearn: 134.1072828\ttotal: 16.7s\tremaining: 1m 36s\n",
            "117:\tlearn: 133.2223736\ttotal: 16.9s\tremaining: 1m 36s\n",
            "118:\tlearn: 133.1445360\ttotal: 17s\tremaining: 1m 36s\n",
            "119:\tlearn: 132.9822970\ttotal: 17.2s\tremaining: 1m 36s\n",
            "120:\tlearn: 132.5903188\ttotal: 17.4s\tremaining: 1m 36s\n",
            "121:\tlearn: 131.4112228\ttotal: 17.5s\tremaining: 1m 36s\n",
            "122:\tlearn: 131.2322269\ttotal: 17.6s\tremaining: 1m 35s\n",
            "123:\tlearn: 130.3775810\ttotal: 17.8s\tremaining: 1m 35s\n",
            "124:\tlearn: 129.7496537\ttotal: 17.9s\tremaining: 1m 35s\n",
            "125:\tlearn: 129.1766605\ttotal: 18s\tremaining: 1m 35s\n",
            "126:\tlearn: 128.9965344\ttotal: 18.2s\tremaining: 1m 35s\n",
            "127:\tlearn: 127.8121777\ttotal: 18.3s\tremaining: 1m 35s\n",
            "128:\tlearn: 127.3913962\ttotal: 18.5s\tremaining: 1m 35s\n",
            "129:\tlearn: 126.9176991\ttotal: 18.6s\tremaining: 1m 34s\n",
            "130:\tlearn: 126.4399393\ttotal: 18.8s\tremaining: 1m 34s\n",
            "131:\tlearn: 126.3295270\ttotal: 18.9s\tremaining: 1m 34s\n",
            "132:\tlearn: 125.8252241\ttotal: 19.1s\tremaining: 1m 34s\n",
            "133:\tlearn: 125.2911464\ttotal: 19.2s\tremaining: 1m 34s\n",
            "134:\tlearn: 125.1911507\ttotal: 19.4s\tremaining: 1m 34s\n",
            "135:\tlearn: 125.0658681\ttotal: 19.5s\tremaining: 1m 34s\n",
            "136:\tlearn: 124.4600676\ttotal: 19.7s\tremaining: 1m 34s\n",
            "137:\tlearn: 124.2496493\ttotal: 19.8s\tremaining: 1m 33s\n",
            "138:\tlearn: 124.1073951\ttotal: 20s\tremaining: 1m 33s\n",
            "139:\tlearn: 123.4270305\ttotal: 20.1s\tremaining: 1m 33s\n",
            "140:\tlearn: 123.3018297\ttotal: 20.3s\tremaining: 1m 33s\n",
            "141:\tlearn: 123.0550246\ttotal: 20.4s\tremaining: 1m 33s\n",
            "142:\tlearn: 122.7832903\ttotal: 20.6s\tremaining: 1m 33s\n",
            "143:\tlearn: 122.6738424\ttotal: 20.8s\tremaining: 1m 33s\n",
            "144:\tlearn: 122.4610573\ttotal: 20.9s\tremaining: 1m 33s\n",
            "145:\tlearn: 122.3268046\ttotal: 21.1s\tremaining: 1m 33s\n",
            "146:\tlearn: 121.9011051\ttotal: 21.2s\tremaining: 1m 33s\n",
            "147:\tlearn: 121.3676734\ttotal: 21.3s\tremaining: 1m 32s\n",
            "148:\tlearn: 121.0600502\ttotal: 21.5s\tremaining: 1m 32s\n",
            "149:\tlearn: 120.7287593\ttotal: 21.6s\tremaining: 1m 32s\n",
            "150:\tlearn: 120.6229109\ttotal: 21.8s\tremaining: 1m 32s\n",
            "151:\tlearn: 120.2036248\ttotal: 21.9s\tremaining: 1m 32s\n",
            "152:\tlearn: 119.5076807\ttotal: 22.1s\tremaining: 1m 32s\n",
            "153:\tlearn: 118.6088388\ttotal: 22.2s\tremaining: 1m 32s\n",
            "154:\tlearn: 118.4954111\ttotal: 22.4s\tremaining: 1m 32s\n",
            "155:\tlearn: 118.2511144\ttotal: 22.5s\tremaining: 1m 31s\n",
            "156:\tlearn: 118.0776658\ttotal: 22.7s\tremaining: 1m 31s\n",
            "157:\tlearn: 117.7881618\ttotal: 22.9s\tremaining: 1m 31s\n",
            "158:\tlearn: 117.5000907\ttotal: 23s\tremaining: 1m 31s\n",
            "159:\tlearn: 117.3646745\ttotal: 23.2s\tremaining: 1m 31s\n",
            "160:\tlearn: 117.1682155\ttotal: 23.3s\tremaining: 1m 31s\n",
            "161:\tlearn: 116.9450584\ttotal: 23.5s\tremaining: 1m 31s\n",
            "162:\tlearn: 116.6877940\ttotal: 23.6s\tremaining: 1m 31s\n",
            "163:\tlearn: 116.6862129\ttotal: 23.8s\tremaining: 1m 30s\n",
            "164:\tlearn: 116.5319202\ttotal: 23.9s\tremaining: 1m 30s\n",
            "165:\tlearn: 116.3414486\ttotal: 24.1s\tremaining: 1m 30s\n",
            "166:\tlearn: 116.2072167\ttotal: 24.2s\tremaining: 1m 30s\n",
            "167:\tlearn: 116.0728173\ttotal: 24.3s\tremaining: 1m 30s\n",
            "168:\tlearn: 115.9923454\ttotal: 24.5s\tremaining: 1m 30s\n",
            "169:\tlearn: 115.9800620\ttotal: 24.7s\tremaining: 1m 30s\n",
            "170:\tlearn: 115.8818624\ttotal: 24.8s\tremaining: 1m 30s\n",
            "171:\tlearn: 114.9830658\ttotal: 25s\tremaining: 1m 30s\n",
            "172:\tlearn: 114.7435424\ttotal: 25.1s\tremaining: 1m 29s\n",
            "173:\tlearn: 113.3602485\ttotal: 25.3s\tremaining: 1m 29s\n",
            "174:\tlearn: 113.1233917\ttotal: 25.4s\tremaining: 1m 29s\n",
            "175:\tlearn: 112.9928173\ttotal: 25.6s\tremaining: 1m 29s\n",
            "176:\tlearn: 112.7230750\ttotal: 25.7s\tremaining: 1m 29s\n",
            "177:\tlearn: 111.9089072\ttotal: 25.8s\tremaining: 1m 29s\n",
            "178:\tlearn: 111.8058720\ttotal: 26s\tremaining: 1m 29s\n",
            "179:\tlearn: 111.6667102\ttotal: 26.2s\tremaining: 1m 29s\n",
            "180:\tlearn: 111.5487201\ttotal: 26.3s\tremaining: 1m 28s\n",
            "181:\tlearn: 111.0949453\ttotal: 26.5s\tremaining: 1m 28s\n",
            "182:\tlearn: 111.0172006\ttotal: 26.6s\tremaining: 1m 28s\n",
            "183:\tlearn: 110.7278591\ttotal: 26.8s\tremaining: 1m 28s\n",
            "184:\tlearn: 110.7156872\ttotal: 26.9s\tremaining: 1m 28s\n",
            "185:\tlearn: 110.6089028\ttotal: 27.1s\tremaining: 1m 28s\n",
            "186:\tlearn: 110.1717436\ttotal: 27.3s\tremaining: 1m 28s\n",
            "187:\tlearn: 109.8709657\ttotal: 27.4s\tremaining: 1m 28s\n",
            "188:\tlearn: 109.6831459\ttotal: 27.5s\tremaining: 1m 27s\n",
            "189:\tlearn: 109.3083435\ttotal: 27.7s\tremaining: 1m 27s\n",
            "190:\tlearn: 109.2127890\ttotal: 27.8s\tremaining: 1m 27s\n",
            "191:\tlearn: 108.5495821\ttotal: 28s\tremaining: 1m 27s\n",
            "192:\tlearn: 107.9401197\ttotal: 28.1s\tremaining: 1m 27s\n",
            "193:\tlearn: 107.7260251\ttotal: 28.2s\tremaining: 1m 27s\n",
            "194:\tlearn: 107.3060951\ttotal: 28.4s\tremaining: 1m 26s\n",
            "195:\tlearn: 106.7233220\ttotal: 28.5s\tremaining: 1m 26s\n",
            "196:\tlearn: 106.5105435\ttotal: 28.7s\tremaining: 1m 26s\n",
            "197:\tlearn: 106.3400247\ttotal: 28.8s\tremaining: 1m 26s\n",
            "198:\tlearn: 105.1791843\ttotal: 29s\tremaining: 1m 26s\n",
            "199:\tlearn: 105.0158709\ttotal: 29.1s\tremaining: 1m 26s\n",
            "200:\tlearn: 104.7157046\ttotal: 29.3s\tremaining: 1m 26s\n",
            "201:\tlearn: 104.5959886\ttotal: 29.4s\tremaining: 1m 25s\n",
            "202:\tlearn: 104.5242989\ttotal: 29.6s\tremaining: 1m 25s\n",
            "203:\tlearn: 104.3305191\ttotal: 29.7s\tremaining: 1m 25s\n",
            "204:\tlearn: 104.2103069\ttotal: 29.9s\tremaining: 1m 25s\n",
            "205:\tlearn: 104.1339838\ttotal: 30.1s\tremaining: 1m 25s\n",
            "206:\tlearn: 104.0517517\ttotal: 30.2s\tremaining: 1m 25s\n",
            "207:\tlearn: 103.7730602\ttotal: 30.4s\tremaining: 1m 25s\n",
            "208:\tlearn: 103.3031139\ttotal: 30.5s\tremaining: 1m 25s\n",
            "209:\tlearn: 103.0815648\ttotal: 30.6s\tremaining: 1m 24s\n",
            "210:\tlearn: 102.8151437\ttotal: 30.8s\tremaining: 1m 24s\n",
            "211:\tlearn: 102.6505102\ttotal: 31s\tremaining: 1m 24s\n",
            "212:\tlearn: 102.5950226\ttotal: 31.1s\tremaining: 1m 24s\n",
            "213:\tlearn: 102.5739522\ttotal: 31.3s\tremaining: 1m 24s\n",
            "214:\tlearn: 102.0733297\ttotal: 31.4s\tremaining: 1m 24s\n",
            "215:\tlearn: 101.9507017\ttotal: 31.6s\tremaining: 1m 24s\n",
            "216:\tlearn: 101.5749903\ttotal: 31.7s\tremaining: 1m 24s\n",
            "217:\tlearn: 101.4990375\ttotal: 31.9s\tremaining: 1m 23s\n",
            "218:\tlearn: 101.4486285\ttotal: 32s\tremaining: 1m 23s\n",
            "219:\tlearn: 101.2585277\ttotal: 32.2s\tremaining: 1m 23s\n",
            "220:\tlearn: 101.1137226\ttotal: 32.3s\tremaining: 1m 23s\n",
            "221:\tlearn: 100.8649268\ttotal: 32.5s\tremaining: 1m 23s\n",
            "222:\tlearn: 100.5411176\ttotal: 32.6s\tremaining: 1m 23s\n",
            "223:\tlearn: 100.4672248\ttotal: 32.8s\tremaining: 1m 23s\n",
            "224:\tlearn: 100.2452783\ttotal: 32.9s\tremaining: 1m 22s\n",
            "225:\tlearn: 100.0073020\ttotal: 33.1s\tremaining: 1m 22s\n",
            "226:\tlearn: 99.7956331\ttotal: 33.2s\tremaining: 1m 22s\n",
            "227:\tlearn: 99.6336281\ttotal: 33.4s\tremaining: 1m 22s\n",
            "228:\tlearn: 99.5295725\ttotal: 33.5s\tremaining: 1m 22s\n",
            "229:\tlearn: 99.3592864\ttotal: 33.7s\tremaining: 1m 22s\n",
            "230:\tlearn: 99.3544755\ttotal: 33.8s\tremaining: 1m 22s\n",
            "231:\tlearn: 99.2926387\ttotal: 34s\tremaining: 1m 22s\n",
            "232:\tlearn: 99.0528817\ttotal: 34.1s\tremaining: 1m 21s\n",
            "233:\tlearn: 98.8891519\ttotal: 34.3s\tremaining: 1m 21s\n",
            "234:\tlearn: 98.7642814\ttotal: 34.4s\tremaining: 1m 21s\n",
            "235:\tlearn: 98.6105087\ttotal: 34.6s\tremaining: 1m 21s\n",
            "236:\tlearn: 98.3824696\ttotal: 34.7s\tremaining: 1m 21s\n",
            "237:\tlearn: 98.1380445\ttotal: 34.9s\tremaining: 1m 21s\n",
            "238:\tlearn: 98.0531341\ttotal: 35s\tremaining: 1m 21s\n",
            "239:\tlearn: 98.0098502\ttotal: 35.2s\tremaining: 1m 20s\n",
            "240:\tlearn: 97.9661124\ttotal: 35.3s\tremaining: 1m 20s\n",
            "241:\tlearn: 97.8628586\ttotal: 35.5s\tremaining: 1m 20s\n",
            "242:\tlearn: 97.8580105\ttotal: 35.7s\tremaining: 1m 20s\n",
            "243:\tlearn: 97.5167563\ttotal: 35.8s\tremaining: 1m 20s\n",
            "244:\tlearn: 97.5123194\ttotal: 36s\tremaining: 1m 20s\n",
            "245:\tlearn: 97.5117912\ttotal: 36.1s\tremaining: 1m 20s\n",
            "246:\tlearn: 97.4685390\ttotal: 36.3s\tremaining: 1m 20s\n",
            "247:\tlearn: 97.0490440\ttotal: 36.4s\tremaining: 1m 19s\n",
            "248:\tlearn: 96.9881033\ttotal: 36.6s\tremaining: 1m 19s\n",
            "249:\tlearn: 96.8288468\ttotal: 36.7s\tremaining: 1m 19s\n",
            "250:\tlearn: 96.7669031\ttotal: 36.9s\tremaining: 1m 19s\n",
            "251:\tlearn: 96.3881407\ttotal: 37.1s\tremaining: 1m 19s\n",
            "252:\tlearn: 96.0364950\ttotal: 37.2s\tremaining: 1m 19s\n",
            "253:\tlearn: 95.9434419\ttotal: 37.4s\tremaining: 1m 19s\n",
            "254:\tlearn: 95.7304462\ttotal: 37.5s\tremaining: 1m 18s\n",
            "255:\tlearn: 95.6211994\ttotal: 37.6s\tremaining: 1m 18s\n",
            "256:\tlearn: 95.4792752\ttotal: 37.8s\tremaining: 1m 18s\n",
            "257:\tlearn: 95.3226175\ttotal: 37.9s\tremaining: 1m 18s\n",
            "258:\tlearn: 95.2596977\ttotal: 38.1s\tremaining: 1m 18s\n",
            "259:\tlearn: 95.0825837\ttotal: 38.2s\tremaining: 1m 18s\n",
            "260:\tlearn: 94.9892201\ttotal: 38.4s\tremaining: 1m 18s\n",
            "261:\tlearn: 94.7993220\ttotal: 38.6s\tremaining: 1m 18s\n",
            "262:\tlearn: 94.6948507\ttotal: 38.7s\tremaining: 1m 17s\n",
            "263:\tlearn: 94.4275976\ttotal: 38.9s\tremaining: 1m 17s\n",
            "264:\tlearn: 94.0115750\ttotal: 39s\tremaining: 1m 17s\n",
            "265:\tlearn: 93.4523899\ttotal: 39.2s\tremaining: 1m 17s\n",
            "266:\tlearn: 93.2487203\ttotal: 39.3s\tremaining: 1m 17s\n",
            "267:\tlearn: 93.0207667\ttotal: 39.5s\tremaining: 1m 17s\n",
            "268:\tlearn: 92.9283092\ttotal: 39.6s\tremaining: 1m 17s\n",
            "269:\tlearn: 92.8274150\ttotal: 39.8s\tremaining: 1m 16s\n",
            "270:\tlearn: 92.5070951\ttotal: 39.9s\tremaining: 1m 16s\n",
            "271:\tlearn: 92.4053594\ttotal: 40.1s\tremaining: 1m 16s\n",
            "272:\tlearn: 92.2106283\ttotal: 40.3s\tremaining: 1m 16s\n",
            "273:\tlearn: 92.0270660\ttotal: 40.4s\tremaining: 1m 16s\n",
            "274:\tlearn: 91.9411537\ttotal: 40.6s\tremaining: 1m 16s\n",
            "275:\tlearn: 91.8027362\ttotal: 40.7s\tremaining: 1m 16s\n",
            "276:\tlearn: 91.5476815\ttotal: 40.8s\tremaining: 1m 15s\n",
            "277:\tlearn: 91.4645165\ttotal: 41s\tremaining: 1m 15s\n",
            "278:\tlearn: 91.3990574\ttotal: 41.2s\tremaining: 1m 15s\n",
            "279:\tlearn: 91.3486359\ttotal: 41.3s\tremaining: 1m 15s\n",
            "280:\tlearn: 91.1969389\ttotal: 41.5s\tremaining: 1m 15s\n",
            "281:\tlearn: 91.1629909\ttotal: 41.6s\tremaining: 1m 15s\n",
            "282:\tlearn: 91.1005790\ttotal: 41.8s\tremaining: 1m 15s\n",
            "283:\tlearn: 90.7584979\ttotal: 41.9s\tremaining: 1m 14s\n",
            "284:\tlearn: 90.5140819\ttotal: 42.1s\tremaining: 1m 14s\n",
            "285:\tlearn: 90.2980837\ttotal: 42.2s\tremaining: 1m 14s\n",
            "286:\tlearn: 90.1826513\ttotal: 42.4s\tremaining: 1m 14s\n",
            "287:\tlearn: 90.0904984\ttotal: 42.5s\tremaining: 1m 14s\n",
            "288:\tlearn: 89.8865480\ttotal: 42.7s\tremaining: 1m 14s\n",
            "289:\tlearn: 89.6608165\ttotal: 42.8s\tremaining: 1m 14s\n",
            "290:\tlearn: 89.2760127\ttotal: 43s\tremaining: 1m 13s\n",
            "291:\tlearn: 89.1867341\ttotal: 43.1s\tremaining: 1m 13s\n",
            "292:\tlearn: 89.0933744\ttotal: 43.3s\tremaining: 1m 13s\n",
            "293:\tlearn: 88.9033050\ttotal: 43.4s\tremaining: 1m 13s\n",
            "294:\tlearn: 88.5213281\ttotal: 43.6s\tremaining: 1m 13s\n",
            "295:\tlearn: 88.4146214\ttotal: 43.7s\tremaining: 1m 13s\n",
            "296:\tlearn: 88.3746024\ttotal: 43.9s\tremaining: 1m 13s\n",
            "297:\tlearn: 88.1787440\ttotal: 44s\tremaining: 1m 12s\n",
            "298:\tlearn: 88.0771716\ttotal: 44.2s\tremaining: 1m 12s\n",
            "299:\tlearn: 88.0308848\ttotal: 44.3s\tremaining: 1m 12s\n",
            "300:\tlearn: 87.9516740\ttotal: 44.5s\tremaining: 1m 12s\n",
            "301:\tlearn: 87.8637653\ttotal: 44.6s\tremaining: 1m 12s\n",
            "302:\tlearn: 87.7867297\ttotal: 44.8s\tremaining: 1m 12s\n",
            "303:\tlearn: 87.7215045\ttotal: 44.9s\tremaining: 1m 12s\n",
            "304:\tlearn: 87.5108833\ttotal: 45s\tremaining: 1m 11s\n",
            "305:\tlearn: 87.3525269\ttotal: 45.2s\tremaining: 1m 11s\n",
            "306:\tlearn: 87.3098283\ttotal: 45.4s\tremaining: 1m 11s\n",
            "307:\tlearn: 87.0593383\ttotal: 45.5s\tremaining: 1m 11s\n",
            "308:\tlearn: 86.9706309\ttotal: 45.7s\tremaining: 1m 11s\n",
            "309:\tlearn: 86.9526580\ttotal: 45.9s\tremaining: 1m 11s\n",
            "310:\tlearn: 86.9519251\ttotal: 46s\tremaining: 1m 11s\n",
            "311:\tlearn: 86.8826114\ttotal: 46.2s\tremaining: 1m 11s\n",
            "312:\tlearn: 86.7999332\ttotal: 46.3s\tremaining: 1m 10s\n",
            "313:\tlearn: 86.6064807\ttotal: 46.5s\tremaining: 1m 10s\n",
            "314:\tlearn: 86.5175543\ttotal: 46.7s\tremaining: 1m 10s\n",
            "315:\tlearn: 86.3412422\ttotal: 46.8s\tremaining: 1m 10s\n",
            "316:\tlearn: 86.1438812\ttotal: 46.9s\tremaining: 1m 10s\n",
            "317:\tlearn: 85.7901996\ttotal: 47.1s\tremaining: 1m 10s\n",
            "318:\tlearn: 85.5714340\ttotal: 47.2s\tremaining: 1m 10s\n",
            "319:\tlearn: 85.4363200\ttotal: 47.4s\tremaining: 1m 9s\n",
            "320:\tlearn: 85.3555500\ttotal: 47.5s\tremaining: 1m 9s\n",
            "321:\tlearn: 85.3143394\ttotal: 47.7s\tremaining: 1m 9s\n",
            "322:\tlearn: 85.2264483\ttotal: 47.8s\tremaining: 1m 9s\n",
            "323:\tlearn: 85.1927684\ttotal: 48s\tremaining: 1m 9s\n",
            "324:\tlearn: 85.0671457\ttotal: 48.2s\tremaining: 1m 9s\n",
            "325:\tlearn: 84.9578549\ttotal: 48.3s\tremaining: 1m 9s\n",
            "326:\tlearn: 84.7155426\ttotal: 48.5s\tremaining: 1m 8s\n",
            "327:\tlearn: 84.6207366\ttotal: 48.6s\tremaining: 1m 8s\n",
            "328:\tlearn: 84.5188891\ttotal: 48.8s\tremaining: 1m 8s\n",
            "329:\tlearn: 84.5159757\ttotal: 48.9s\tremaining: 1m 8s\n",
            "330:\tlearn: 84.4619233\ttotal: 49.1s\tremaining: 1m 8s\n",
            "331:\tlearn: 84.3792099\ttotal: 49.3s\tremaining: 1m 8s\n",
            "332:\tlearn: 84.2524557\ttotal: 49.4s\tremaining: 1m 8s\n",
            "333:\tlearn: 84.2146146\ttotal: 49.6s\tremaining: 1m 8s\n",
            "334:\tlearn: 84.1339364\ttotal: 49.7s\tremaining: 1m 7s\n",
            "335:\tlearn: 84.0788092\ttotal: 49.9s\tremaining: 1m 7s\n",
            "336:\tlearn: 84.0745579\ttotal: 50.1s\tremaining: 1m 7s\n",
            "337:\tlearn: 83.9702088\ttotal: 50.2s\tremaining: 1m 7s\n",
            "338:\tlearn: 83.9263560\ttotal: 50.4s\tremaining: 1m 7s\n",
            "339:\tlearn: 83.7288960\ttotal: 50.5s\tremaining: 1m 7s\n",
            "340:\tlearn: 83.6269711\ttotal: 50.7s\tremaining: 1m 7s\n",
            "341:\tlearn: 83.4221344\ttotal: 50.8s\tremaining: 1m 6s\n",
            "342:\tlearn: 83.3069874\ttotal: 51s\tremaining: 1m 6s\n",
            "343:\tlearn: 83.2143734\ttotal: 51.1s\tremaining: 1m 6s\n",
            "344:\tlearn: 83.1776183\ttotal: 51.3s\tremaining: 1m 6s\n",
            "345:\tlearn: 82.9506557\ttotal: 51.4s\tremaining: 1m 6s\n",
            "346:\tlearn: 82.7775972\ttotal: 51.6s\tremaining: 1m 6s\n",
            "347:\tlearn: 82.4466101\ttotal: 51.7s\tremaining: 1m 5s\n",
            "348:\tlearn: 82.3519303\ttotal: 51.9s\tremaining: 1m 5s\n",
            "349:\tlearn: 82.1898811\ttotal: 52s\tremaining: 1m 5s\n",
            "350:\tlearn: 82.1471183\ttotal: 52.2s\tremaining: 1m 5s\n",
            "351:\tlearn: 82.0743422\ttotal: 52.3s\tremaining: 1m 5s\n",
            "352:\tlearn: 82.0175059\ttotal: 52.5s\tremaining: 1m 5s\n",
            "353:\tlearn: 81.9792333\ttotal: 52.6s\tremaining: 1m 5s\n",
            "354:\tlearn: 81.8522688\ttotal: 52.8s\tremaining: 1m 4s\n",
            "355:\tlearn: 81.7732447\ttotal: 52.9s\tremaining: 1m 4s\n",
            "356:\tlearn: 81.7130812\ttotal: 53.1s\tremaining: 1m 4s\n",
            "357:\tlearn: 81.3465186\ttotal: 53.2s\tremaining: 1m 4s\n",
            "358:\tlearn: 81.2934839\ttotal: 53.4s\tremaining: 1m 4s\n",
            "359:\tlearn: 81.2178580\ttotal: 53.5s\tremaining: 1m 4s\n",
            "360:\tlearn: 81.0162552\ttotal: 53.7s\tremaining: 1m 4s\n",
            "361:\tlearn: 80.7392868\ttotal: 53.8s\tremaining: 1m 3s\n",
            "362:\tlearn: 80.3872163\ttotal: 54s\tremaining: 1m 3s\n",
            "363:\tlearn: 80.2952290\ttotal: 54.2s\tremaining: 1m 3s\n",
            "364:\tlearn: 80.2588149\ttotal: 54.3s\tremaining: 1m 3s\n",
            "365:\tlearn: 80.1966241\ttotal: 54.5s\tremaining: 1m 3s\n",
            "366:\tlearn: 80.1135251\ttotal: 54.6s\tremaining: 1m 3s\n",
            "367:\tlearn: 80.0641571\ttotal: 54.8s\tremaining: 1m 3s\n",
            "368:\tlearn: 80.0508187\ttotal: 54.9s\tremaining: 1m 2s\n",
            "369:\tlearn: 80.0115097\ttotal: 55.1s\tremaining: 1m 2s\n",
            "370:\tlearn: 79.9557946\ttotal: 55.3s\tremaining: 1m 2s\n",
            "371:\tlearn: 79.8184672\ttotal: 55.4s\tremaining: 1m 2s\n",
            "372:\tlearn: 79.7467162\ttotal: 55.6s\tremaining: 1m 2s\n",
            "373:\tlearn: 79.6563808\ttotal: 55.7s\tremaining: 1m 2s\n",
            "374:\tlearn: 79.5728333\ttotal: 55.9s\tremaining: 1m 2s\n",
            "375:\tlearn: 79.4776355\ttotal: 56s\tremaining: 1m 1s\n",
            "376:\tlearn: 79.4035352\ttotal: 56.2s\tremaining: 1m 1s\n",
            "377:\tlearn: 79.3352076\ttotal: 56.3s\tremaining: 1m 1s\n",
            "378:\tlearn: 79.2704774\ttotal: 56.4s\tremaining: 1m 1s\n",
            "379:\tlearn: 79.2241345\ttotal: 56.6s\tremaining: 1m 1s\n",
            "380:\tlearn: 78.9603651\ttotal: 56.7s\tremaining: 1m 1s\n",
            "381:\tlearn: 78.8601306\ttotal: 56.9s\tremaining: 1m 1s\n",
            "382:\tlearn: 78.6955564\ttotal: 57s\tremaining: 1m\n",
            "383:\tlearn: 78.6353136\ttotal: 57.2s\tremaining: 1m\n",
            "384:\tlearn: 78.4458232\ttotal: 57.3s\tremaining: 1m\n",
            "385:\tlearn: 78.3874763\ttotal: 57.5s\tremaining: 1m\n",
            "386:\tlearn: 78.2023802\ttotal: 57.6s\tremaining: 1m\n",
            "387:\tlearn: 78.1120164\ttotal: 57.8s\tremaining: 1m\n",
            "388:\tlearn: 77.9932295\ttotal: 58s\tremaining: 1m\n",
            "389:\tlearn: 77.9414863\ttotal: 58.1s\tremaining: 59.9s\n",
            "390:\tlearn: 77.8912874\ttotal: 58.3s\tremaining: 59.8s\n",
            "391:\tlearn: 77.8367959\ttotal: 58.4s\tremaining: 59.6s\n",
            "392:\tlearn: 77.8354547\ttotal: 58.6s\tremaining: 59.5s\n",
            "393:\tlearn: 77.8002679\ttotal: 58.8s\tremaining: 59.4s\n",
            "394:\tlearn: 77.6848658\ttotal: 58.9s\tremaining: 59.2s\n",
            "395:\tlearn: 77.6561757\ttotal: 59.1s\tremaining: 59.1s\n",
            "396:\tlearn: 77.6056265\ttotal: 59.3s\tremaining: 59s\n",
            "397:\tlearn: 77.5069173\ttotal: 59.4s\tremaining: 58.8s\n",
            "398:\tlearn: 77.4678775\ttotal: 59.5s\tremaining: 58.6s\n",
            "399:\tlearn: 77.4671632\ttotal: 59.7s\tremaining: 58.5s\n",
            "400:\tlearn: 77.4197619\ttotal: 59.9s\tremaining: 58.4s\n",
            "401:\tlearn: 77.3605058\ttotal: 1m\tremaining: 58.3s\n",
            "402:\tlearn: 77.3600602\ttotal: 1m\tremaining: 58.1s\n",
            "403:\tlearn: 77.2124063\ttotal: 1m\tremaining: 57.9s\n",
            "404:\tlearn: 77.1334974\ttotal: 1m\tremaining: 57.8s\n",
            "405:\tlearn: 76.8048902\ttotal: 1m\tremaining: 57.7s\n",
            "406:\tlearn: 76.7234469\ttotal: 1m\tremaining: 57.5s\n",
            "407:\tlearn: 76.6626875\ttotal: 1m\tremaining: 57.4s\n",
            "408:\tlearn: 76.6171096\ttotal: 1m 1s\tremaining: 57.2s\n",
            "409:\tlearn: 76.5871267\ttotal: 1m 1s\tremaining: 57s\n",
            "410:\tlearn: 76.5420419\ttotal: 1m 1s\tremaining: 56.9s\n",
            "411:\tlearn: 76.2360269\ttotal: 1m 1s\tremaining: 56.8s\n",
            "412:\tlearn: 76.0123563\ttotal: 1m 1s\tremaining: 56.6s\n",
            "413:\tlearn: 75.9656165\ttotal: 1m 1s\tremaining: 56.5s\n",
            "414:\tlearn: 75.6957323\ttotal: 1m 1s\tremaining: 56.3s\n",
            "415:\tlearn: 75.6528665\ttotal: 1m 2s\tremaining: 56.2s\n",
            "416:\tlearn: 75.5904638\ttotal: 1m 2s\tremaining: 56s\n",
            "417:\tlearn: 75.5890211\ttotal: 1m 2s\tremaining: 55.9s\n",
            "418:\tlearn: 75.4532776\ttotal: 1m 2s\tremaining: 55.7s\n",
            "419:\tlearn: 75.3570886\ttotal: 1m 2s\tremaining: 55.6s\n",
            "420:\tlearn: 75.2020403\ttotal: 1m 2s\tremaining: 55.4s\n",
            "421:\tlearn: 75.1719990\ttotal: 1m 3s\tremaining: 55.3s\n",
            "422:\tlearn: 75.0233465\ttotal: 1m 3s\tremaining: 55.2s\n",
            "423:\tlearn: 74.9768875\ttotal: 1m 3s\tremaining: 55s\n",
            "424:\tlearn: 74.9477799\ttotal: 1m 3s\tremaining: 54.9s\n",
            "425:\tlearn: 74.9190711\ttotal: 1m 3s\tremaining: 54.8s\n",
            "426:\tlearn: 74.8900805\ttotal: 1m 3s\tremaining: 54.6s\n",
            "427:\tlearn: 74.6054962\ttotal: 1m 4s\tremaining: 54.5s\n",
            "428:\tlearn: 74.5407376\ttotal: 1m 4s\tremaining: 54.3s\n",
            "429:\tlearn: 74.3768581\ttotal: 1m 4s\tremaining: 54.2s\n",
            "430:\tlearn: 74.3045957\ttotal: 1m 4s\tremaining: 54s\n",
            "431:\tlearn: 74.1841060\ttotal: 1m 4s\tremaining: 53.8s\n",
            "432:\tlearn: 74.1286454\ttotal: 1m 4s\tremaining: 53.7s\n",
            "433:\tlearn: 74.0154078\ttotal: 1m 4s\tremaining: 53.5s\n",
            "434:\tlearn: 73.8725455\ttotal: 1m 5s\tremaining: 53.4s\n",
            "435:\tlearn: 73.7412247\ttotal: 1m 5s\tremaining: 53.2s\n",
            "436:\tlearn: 73.6870950\ttotal: 1m 5s\tremaining: 53.1s\n",
            "437:\tlearn: 73.5585262\ttotal: 1m 5s\tremaining: 53s\n",
            "438:\tlearn: 73.4502171\ttotal: 1m 5s\tremaining: 52.8s\n",
            "439:\tlearn: 73.3886845\ttotal: 1m 5s\tremaining: 52.7s\n",
            "440:\tlearn: 73.3171466\ttotal: 1m 5s\tremaining: 52.5s\n",
            "441:\tlearn: 73.2680959\ttotal: 1m 6s\tremaining: 52.4s\n",
            "442:\tlearn: 73.2293984\ttotal: 1m 6s\tremaining: 52.2s\n",
            "443:\tlearn: 73.1958972\ttotal: 1m 6s\tremaining: 52.1s\n",
            "444:\tlearn: 73.1435270\ttotal: 1m 6s\tremaining: 52s\n",
            "445:\tlearn: 73.1094052\ttotal: 1m 6s\tremaining: 51.8s\n",
            "446:\tlearn: 73.0608408\ttotal: 1m 6s\tremaining: 51.6s\n",
            "447:\tlearn: 72.9441887\ttotal: 1m 7s\tremaining: 51.5s\n",
            "448:\tlearn: 72.8852082\ttotal: 1m 7s\tremaining: 51.4s\n",
            "449:\tlearn: 72.7120800\ttotal: 1m 7s\tremaining: 51.2s\n",
            "450:\tlearn: 72.6947795\ttotal: 1m 7s\tremaining: 51.1s\n",
            "451:\tlearn: 72.6018802\ttotal: 1m 7s\tremaining: 50.9s\n",
            "452:\tlearn: 72.5467190\ttotal: 1m 7s\tremaining: 50.8s\n",
            "453:\tlearn: 72.4789287\ttotal: 1m 8s\tremaining: 50.6s\n",
            "454:\tlearn: 72.4189491\ttotal: 1m 8s\tremaining: 50.5s\n",
            "455:\tlearn: 72.3855960\ttotal: 1m 8s\tremaining: 50.4s\n",
            "456:\tlearn: 72.3847928\ttotal: 1m 8s\tremaining: 50.2s\n",
            "457:\tlearn: 72.2461631\ttotal: 1m 8s\tremaining: 50.1s\n",
            "458:\tlearn: 72.1670944\ttotal: 1m 8s\tremaining: 49.9s\n",
            "459:\tlearn: 72.1296207\ttotal: 1m 8s\tremaining: 49.8s\n",
            "460:\tlearn: 72.0556237\ttotal: 1m 9s\tremaining: 49.7s\n",
            "461:\tlearn: 72.0433549\ttotal: 1m 9s\tremaining: 49.5s\n",
            "462:\tlearn: 71.9692307\ttotal: 1m 9s\tremaining: 49.3s\n",
            "463:\tlearn: 71.9272550\ttotal: 1m 9s\tremaining: 49.2s\n",
            "464:\tlearn: 71.7658056\ttotal: 1m 9s\tremaining: 49s\n",
            "465:\tlearn: 71.5969479\ttotal: 1m 9s\tremaining: 48.9s\n",
            "466:\tlearn: 71.5464238\ttotal: 1m 10s\tremaining: 48.7s\n",
            "467:\tlearn: 71.5079710\ttotal: 1m 10s\tremaining: 48.6s\n",
            "468:\tlearn: 71.4427883\ttotal: 1m 10s\tremaining: 48.4s\n",
            "469:\tlearn: 71.4126379\ttotal: 1m 10s\tremaining: 48.3s\n",
            "470:\tlearn: 71.2787961\ttotal: 1m 10s\tremaining: 48.1s\n",
            "471:\tlearn: 71.1493504\ttotal: 1m 10s\tremaining: 48s\n",
            "472:\tlearn: 71.1182103\ttotal: 1m 10s\tremaining: 47.9s\n",
            "473:\tlearn: 71.0716917\ttotal: 1m 11s\tremaining: 47.7s\n",
            "474:\tlearn: 70.8745918\ttotal: 1m 11s\tremaining: 47.6s\n",
            "475:\tlearn: 70.8388786\ttotal: 1m 11s\tremaining: 47.4s\n",
            "476:\tlearn: 70.8154596\ttotal: 1m 11s\tremaining: 47.3s\n",
            "477:\tlearn: 70.8118483\ttotal: 1m 11s\tremaining: 47.1s\n",
            "478:\tlearn: 70.8081061\ttotal: 1m 11s\tremaining: 47s\n",
            "479:\tlearn: 70.6558778\ttotal: 1m 12s\tremaining: 46.8s\n",
            "480:\tlearn: 70.6547680\ttotal: 1m 12s\tremaining: 46.7s\n",
            "481:\tlearn: 70.5235477\ttotal: 1m 12s\tremaining: 46.5s\n",
            "482:\tlearn: 70.4983719\ttotal: 1m 12s\tremaining: 46.4s\n",
            "483:\tlearn: 70.3921825\ttotal: 1m 12s\tremaining: 46.3s\n",
            "484:\tlearn: 70.3659288\ttotal: 1m 12s\tremaining: 46.1s\n",
            "485:\tlearn: 70.2243928\ttotal: 1m 12s\tremaining: 46s\n",
            "486:\tlearn: 70.1856317\ttotal: 1m 13s\tremaining: 45.8s\n",
            "487:\tlearn: 70.1321296\ttotal: 1m 13s\tremaining: 45.7s\n",
            "488:\tlearn: 70.0662454\ttotal: 1m 13s\tremaining: 45.5s\n",
            "489:\tlearn: 70.0512648\ttotal: 1m 13s\tremaining: 45.4s\n",
            "490:\tlearn: 70.0489853\ttotal: 1m 13s\tremaining: 45.2s\n",
            "491:\tlearn: 70.0324959\ttotal: 1m 13s\tremaining: 45.1s\n",
            "492:\tlearn: 69.9907489\ttotal: 1m 14s\tremaining: 45s\n",
            "493:\tlearn: 69.8834242\ttotal: 1m 14s\tremaining: 44.8s\n",
            "494:\tlearn: 69.8152209\ttotal: 1m 14s\tremaining: 44.7s\n",
            "495:\tlearn: 69.6969167\ttotal: 1m 14s\tremaining: 44.5s\n",
            "496:\tlearn: 69.5261387\ttotal: 1m 14s\tremaining: 44.3s\n",
            "497:\tlearn: 69.3654221\ttotal: 1m 14s\tremaining: 44.2s\n",
            "498:\tlearn: 69.3327856\ttotal: 1m 15s\tremaining: 44.1s\n",
            "499:\tlearn: 69.3134337\ttotal: 1m 15s\tremaining: 43.9s\n",
            "500:\tlearn: 69.2424902\ttotal: 1m 15s\tremaining: 43.8s\n",
            "501:\tlearn: 69.1870317\ttotal: 1m 15s\tremaining: 43.6s\n",
            "502:\tlearn: 69.0128975\ttotal: 1m 15s\tremaining: 43.5s\n",
            "503:\tlearn: 68.9847852\ttotal: 1m 15s\tremaining: 43.3s\n",
            "504:\tlearn: 68.9111119\ttotal: 1m 15s\tremaining: 43.2s\n",
            "505:\tlearn: 68.8471063\ttotal: 1m 16s\tremaining: 43s\n",
            "506:\tlearn: 68.7260797\ttotal: 1m 16s\tremaining: 42.9s\n",
            "507:\tlearn: 68.6557661\ttotal: 1m 16s\tremaining: 42.7s\n",
            "508:\tlearn: 68.5885245\ttotal: 1m 16s\tremaining: 42.6s\n",
            "509:\tlearn: 68.5214375\ttotal: 1m 16s\tremaining: 42.4s\n",
            "510:\tlearn: 68.3959342\ttotal: 1m 16s\tremaining: 42.2s\n",
            "511:\tlearn: 68.3453252\ttotal: 1m 16s\tremaining: 42.1s\n",
            "512:\tlearn: 68.2687541\ttotal: 1m 17s\tremaining: 41.9s\n",
            "513:\tlearn: 68.2432628\ttotal: 1m 17s\tremaining: 41.8s\n",
            "514:\tlearn: 68.0901681\ttotal: 1m 17s\tremaining: 41.6s\n",
            "515:\tlearn: 68.0447756\ttotal: 1m 17s\tremaining: 41.5s\n",
            "516:\tlearn: 67.9266206\ttotal: 1m 17s\tremaining: 41.3s\n",
            "517:\tlearn: 67.8196776\ttotal: 1m 17s\tremaining: 41.2s\n",
            "518:\tlearn: 67.7577953\ttotal: 1m 17s\tremaining: 41s\n",
            "519:\tlearn: 67.6633584\ttotal: 1m 18s\tremaining: 40.8s\n",
            "520:\tlearn: 67.6133948\ttotal: 1m 18s\tremaining: 40.7s\n",
            "521:\tlearn: 67.5635941\ttotal: 1m 18s\tremaining: 40.6s\n",
            "522:\tlearn: 67.4339220\ttotal: 1m 18s\tremaining: 40.4s\n",
            "523:\tlearn: 67.3320620\ttotal: 1m 18s\tremaining: 40.2s\n",
            "524:\tlearn: 67.3078110\ttotal: 1m 18s\tremaining: 40.1s\n",
            "525:\tlearn: 67.2818714\ttotal: 1m 19s\tremaining: 40s\n",
            "526:\tlearn: 67.2269748\ttotal: 1m 19s\tremaining: 39.8s\n",
            "527:\tlearn: 67.1615952\ttotal: 1m 19s\tremaining: 39.6s\n",
            "528:\tlearn: 67.1275572\ttotal: 1m 19s\tremaining: 39.5s\n",
            "529:\tlearn: 66.9846293\ttotal: 1m 19s\tremaining: 39.3s\n",
            "530:\tlearn: 66.9414054\ttotal: 1m 19s\tremaining: 39.2s\n",
            "531:\tlearn: 66.8717126\ttotal: 1m 19s\tremaining: 39s\n",
            "532:\tlearn: 66.8388947\ttotal: 1m 20s\tremaining: 38.9s\n",
            "533:\tlearn: 66.7486145\ttotal: 1m 20s\tremaining: 38.7s\n",
            "534:\tlearn: 66.7236644\ttotal: 1m 20s\tremaining: 38.6s\n",
            "535:\tlearn: 66.5948547\ttotal: 1m 20s\tremaining: 38.4s\n",
            "536:\tlearn: 66.5744655\ttotal: 1m 20s\tremaining: 38.3s\n",
            "537:\tlearn: 66.5398017\ttotal: 1m 20s\tremaining: 38.1s\n",
            "538:\tlearn: 66.5213207\ttotal: 1m 20s\tremaining: 38s\n",
            "539:\tlearn: 66.4904514\ttotal: 1m 21s\tremaining: 37.8s\n",
            "540:\tlearn: 66.4601950\ttotal: 1m 21s\tremaining: 37.7s\n",
            "541:\tlearn: 66.4475657\ttotal: 1m 21s\tremaining: 37.6s\n",
            "542:\tlearn: 66.3838896\ttotal: 1m 21s\tremaining: 37.4s\n",
            "543:\tlearn: 66.2895332\ttotal: 1m 21s\tremaining: 37.3s\n",
            "544:\tlearn: 66.2318285\ttotal: 1m 21s\tremaining: 37.1s\n",
            "545:\tlearn: 66.1818211\ttotal: 1m 22s\tremaining: 37s\n",
            "546:\tlearn: 66.0599689\ttotal: 1m 22s\tremaining: 36.8s\n",
            "547:\tlearn: 66.0204694\ttotal: 1m 22s\tremaining: 36.7s\n",
            "548:\tlearn: 65.9956555\ttotal: 1m 22s\tremaining: 36.5s\n",
            "549:\tlearn: 65.9539654\ttotal: 1m 22s\tremaining: 36.4s\n",
            "550:\tlearn: 65.9086468\ttotal: 1m 22s\tremaining: 36.2s\n",
            "551:\tlearn: 65.6977893\ttotal: 1m 22s\tremaining: 36.1s\n",
            "552:\tlearn: 65.5603481\ttotal: 1m 23s\tremaining: 35.9s\n",
            "553:\tlearn: 65.4913559\ttotal: 1m 23s\tremaining: 35.8s\n",
            "554:\tlearn: 65.3949853\ttotal: 1m 23s\tremaining: 35.6s\n",
            "555:\tlearn: 65.3102731\ttotal: 1m 23s\tremaining: 35.5s\n",
            "556:\tlearn: 65.2837681\ttotal: 1m 23s\tremaining: 35.3s\n",
            "557:\tlearn: 65.2519499\ttotal: 1m 23s\tremaining: 35.2s\n",
            "558:\tlearn: 65.1486239\ttotal: 1m 24s\tremaining: 35s\n",
            "559:\tlearn: 65.1471704\ttotal: 1m 24s\tremaining: 34.9s\n",
            "560:\tlearn: 65.0928283\ttotal: 1m 24s\tremaining: 34.7s\n",
            "561:\tlearn: 64.9895317\ttotal: 1m 24s\tremaining: 34.6s\n",
            "562:\tlearn: 64.9352753\ttotal: 1m 24s\tremaining: 34.4s\n",
            "563:\tlearn: 64.8755634\ttotal: 1m 24s\tremaining: 34.3s\n",
            "564:\tlearn: 64.8674961\ttotal: 1m 24s\tremaining: 34.1s\n",
            "565:\tlearn: 64.7773536\ttotal: 1m 25s\tremaining: 34s\n",
            "566:\tlearn: 64.7479228\ttotal: 1m 25s\tremaining: 33.8s\n",
            "567:\tlearn: 64.6393295\ttotal: 1m 25s\tremaining: 33.7s\n",
            "568:\tlearn: 64.5280432\ttotal: 1m 25s\tremaining: 33.5s\n",
            "569:\tlearn: 64.4139549\ttotal: 1m 25s\tremaining: 33.4s\n",
            "570:\tlearn: 64.3353139\ttotal: 1m 25s\tremaining: 33.2s\n",
            "571:\tlearn: 64.2960970\ttotal: 1m 26s\tremaining: 33.1s\n",
            "572:\tlearn: 64.2488933\ttotal: 1m 26s\tremaining: 32.9s\n",
            "573:\tlearn: 64.1204391\ttotal: 1m 26s\tremaining: 32.8s\n",
            "574:\tlearn: 64.0822894\ttotal: 1m 26s\tremaining: 32.6s\n",
            "575:\tlearn: 63.9746225\ttotal: 1m 26s\tremaining: 32.5s\n",
            "576:\tlearn: 63.9328335\ttotal: 1m 26s\tremaining: 32.3s\n",
            "577:\tlearn: 63.8925741\ttotal: 1m 26s\tremaining: 32.2s\n",
            "578:\tlearn: 63.8802931\ttotal: 1m 27s\tremaining: 32s\n",
            "579:\tlearn: 63.7626148\ttotal: 1m 27s\tremaining: 31.9s\n",
            "580:\tlearn: 63.7448373\ttotal: 1m 27s\tremaining: 31.7s\n",
            "581:\tlearn: 63.5958646\ttotal: 1m 27s\tremaining: 31.6s\n",
            "582:\tlearn: 63.5408869\ttotal: 1m 27s\tremaining: 31.4s\n",
            "583:\tlearn: 63.4699391\ttotal: 1m 27s\tremaining: 31.3s\n",
            "584:\tlearn: 63.2255358\ttotal: 1m 27s\tremaining: 31.1s\n",
            "585:\tlearn: 63.1007753\ttotal: 1m 28s\tremaining: 31s\n",
            "586:\tlearn: 63.0752347\ttotal: 1m 28s\tremaining: 30.8s\n",
            "587:\tlearn: 62.9690318\ttotal: 1m 28s\tremaining: 30.7s\n",
            "588:\tlearn: 62.9083674\ttotal: 1m 28s\tremaining: 30.5s\n",
            "589:\tlearn: 62.8723160\ttotal: 1m 28s\tremaining: 30.4s\n",
            "590:\tlearn: 62.8472020\ttotal: 1m 28s\tremaining: 30.2s\n",
            "591:\tlearn: 62.8098079\ttotal: 1m 29s\tremaining: 30.1s\n",
            "592:\tlearn: 62.7093011\ttotal: 1m 29s\tremaining: 29.9s\n",
            "593:\tlearn: 62.6651596\ttotal: 1m 29s\tremaining: 29.8s\n",
            "594:\tlearn: 62.5738286\ttotal: 1m 29s\tremaining: 29.6s\n",
            "595:\tlearn: 62.3993952\ttotal: 1m 29s\tremaining: 29.5s\n",
            "596:\tlearn: 62.3730649\ttotal: 1m 29s\tremaining: 29.3s\n",
            "597:\tlearn: 62.3117323\ttotal: 1m 29s\tremaining: 29.2s\n",
            "598:\tlearn: 62.2963968\ttotal: 1m 30s\tremaining: 29s\n",
            "599:\tlearn: 62.2527585\ttotal: 1m 30s\tremaining: 28.9s\n",
            "600:\tlearn: 62.2145813\ttotal: 1m 30s\tremaining: 28.7s\n",
            "601:\tlearn: 62.1884513\ttotal: 1m 30s\tremaining: 28.6s\n",
            "602:\tlearn: 62.1501105\ttotal: 1m 30s\tremaining: 28.4s\n",
            "603:\tlearn: 62.0734607\ttotal: 1m 30s\tremaining: 28.3s\n",
            "604:\tlearn: 62.0433579\ttotal: 1m 30s\tremaining: 28.1s\n",
            "605:\tlearn: 61.9753397\ttotal: 1m 31s\tremaining: 28s\n",
            "606:\tlearn: 61.8624706\ttotal: 1m 31s\tremaining: 27.8s\n",
            "607:\tlearn: 61.7856003\ttotal: 1m 31s\tremaining: 27.7s\n",
            "608:\tlearn: 61.7327925\ttotal: 1m 31s\tremaining: 27.5s\n",
            "609:\tlearn: 61.6796728\ttotal: 1m 31s\tremaining: 27.4s\n",
            "610:\tlearn: 61.4924835\ttotal: 1m 31s\tremaining: 27.2s\n",
            "611:\tlearn: 61.4779055\ttotal: 1m 31s\tremaining: 27.1s\n",
            "612:\tlearn: 61.4052427\ttotal: 1m 32s\tremaining: 26.9s\n",
            "613:\tlearn: 61.3633354\ttotal: 1m 32s\tremaining: 26.8s\n",
            "614:\tlearn: 61.3123023\ttotal: 1m 32s\tremaining: 26.6s\n",
            "615:\tlearn: 61.2902841\ttotal: 1m 32s\tremaining: 26.4s\n",
            "616:\tlearn: 61.2519389\ttotal: 1m 32s\tremaining: 26.3s\n",
            "617:\tlearn: 61.1516715\ttotal: 1m 32s\tremaining: 26.1s\n",
            "618:\tlearn: 61.1133446\ttotal: 1m 33s\tremaining: 26s\n",
            "619:\tlearn: 61.0903203\ttotal: 1m 33s\tremaining: 25.8s\n",
            "620:\tlearn: 61.0120648\ttotal: 1m 33s\tremaining: 25.7s\n",
            "621:\tlearn: 60.9085990\ttotal: 1m 33s\tremaining: 25.5s\n",
            "622:\tlearn: 60.8158935\ttotal: 1m 33s\tremaining: 25.4s\n",
            "623:\tlearn: 60.7335184\ttotal: 1m 33s\tremaining: 25.2s\n",
            "624:\tlearn: 60.6860588\ttotal: 1m 33s\tremaining: 25.1s\n",
            "625:\tlearn: 60.6423713\ttotal: 1m 33s\tremaining: 24.9s\n",
            "626:\tlearn: 60.6026865\ttotal: 1m 34s\tremaining: 24.8s\n",
            "627:\tlearn: 60.5731953\ttotal: 1m 34s\tremaining: 24.6s\n",
            "628:\tlearn: 60.4761356\ttotal: 1m 34s\tremaining: 24.5s\n",
            "629:\tlearn: 60.4380868\ttotal: 1m 34s\tremaining: 24.3s\n",
            "630:\tlearn: 60.4126426\ttotal: 1m 34s\tremaining: 24.2s\n",
            "631:\tlearn: 60.2495225\ttotal: 1m 34s\tremaining: 24s\n",
            "632:\tlearn: 60.2031811\ttotal: 1m 35s\tremaining: 23.9s\n",
            "633:\tlearn: 60.1420447\ttotal: 1m 35s\tremaining: 23.7s\n",
            "634:\tlearn: 60.1172396\ttotal: 1m 35s\tremaining: 23.6s\n",
            "635:\tlearn: 60.0843716\ttotal: 1m 35s\tremaining: 23.4s\n",
            "636:\tlearn: 60.0821848\ttotal: 1m 35s\tremaining: 23.3s\n",
            "637:\tlearn: 60.0068713\ttotal: 1m 35s\tremaining: 23.1s\n",
            "638:\tlearn: 59.9788876\ttotal: 1m 36s\tremaining: 23s\n",
            "639:\tlearn: 59.9355286\ttotal: 1m 36s\tremaining: 22.8s\n",
            "640:\tlearn: 59.8534257\ttotal: 1m 36s\tremaining: 22.7s\n",
            "641:\tlearn: 59.8209096\ttotal: 1m 36s\tremaining: 22.5s\n",
            "642:\tlearn: 59.7806008\ttotal: 1m 36s\tremaining: 22.4s\n",
            "643:\tlearn: 59.7243420\ttotal: 1m 36s\tremaining: 22.2s\n",
            "644:\tlearn: 59.6473515\ttotal: 1m 36s\tremaining: 22.1s\n",
            "645:\tlearn: 59.6109413\ttotal: 1m 37s\tremaining: 21.9s\n",
            "646:\tlearn: 59.5777544\ttotal: 1m 37s\tremaining: 21.8s\n",
            "647:\tlearn: 59.5213040\ttotal: 1m 37s\tremaining: 21.6s\n",
            "648:\tlearn: 59.4935507\ttotal: 1m 37s\tremaining: 21.5s\n",
            "649:\tlearn: 59.4524741\ttotal: 1m 37s\tremaining: 21.3s\n",
            "650:\tlearn: 59.4225276\ttotal: 1m 37s\tremaining: 21.2s\n",
            "651:\tlearn: 59.3989956\ttotal: 1m 38s\tremaining: 21.1s\n",
            "652:\tlearn: 59.3361503\ttotal: 1m 38s\tremaining: 20.9s\n",
            "653:\tlearn: 59.2842942\ttotal: 1m 38s\tremaining: 20.8s\n",
            "654:\tlearn: 59.1815895\ttotal: 1m 38s\tremaining: 20.6s\n",
            "655:\tlearn: 59.1467799\ttotal: 1m 38s\tremaining: 20.5s\n",
            "656:\tlearn: 59.1139890\ttotal: 1m 38s\tremaining: 20.3s\n",
            "657:\tlearn: 58.9876651\ttotal: 1m 38s\tremaining: 20.2s\n",
            "658:\tlearn: 58.9068323\ttotal: 1m 39s\tremaining: 20s\n",
            "659:\tlearn: 58.8010102\ttotal: 1m 39s\tremaining: 19.8s\n",
            "660:\tlearn: 58.7192743\ttotal: 1m 39s\tremaining: 19.7s\n",
            "661:\tlearn: 58.6683270\ttotal: 1m 39s\tremaining: 19.5s\n",
            "662:\tlearn: 58.6205930\ttotal: 1m 39s\tremaining: 19.4s\n",
            "663:\tlearn: 58.6059041\ttotal: 1m 39s\tremaining: 19.3s\n",
            "664:\tlearn: 58.5882802\ttotal: 1m 40s\tremaining: 19.1s\n",
            "665:\tlearn: 58.5462704\ttotal: 1m 40s\tremaining: 19s\n",
            "666:\tlearn: 58.5449693\ttotal: 1m 40s\tremaining: 18.8s\n",
            "667:\tlearn: 58.4710488\ttotal: 1m 40s\tremaining: 18.7s\n",
            "668:\tlearn: 58.4195511\ttotal: 1m 40s\tremaining: 18.5s\n",
            "669:\tlearn: 58.3545128\ttotal: 1m 40s\tremaining: 18.4s\n",
            "670:\tlearn: 58.3215914\ttotal: 1m 40s\tremaining: 18.2s\n",
            "671:\tlearn: 58.2905127\ttotal: 1m 41s\tremaining: 18.1s\n",
            "672:\tlearn: 58.2545748\ttotal: 1m 41s\tremaining: 17.9s\n",
            "673:\tlearn: 58.2064067\ttotal: 1m 41s\tremaining: 17.8s\n",
            "674:\tlearn: 58.1645067\ttotal: 1m 41s\tremaining: 17.6s\n",
            "675:\tlearn: 58.1155077\ttotal: 1m 41s\tremaining: 17.5s\n",
            "676:\tlearn: 58.0880304\ttotal: 1m 41s\tremaining: 17.3s\n",
            "677:\tlearn: 58.0670630\ttotal: 1m 42s\tremaining: 17.2s\n",
            "678:\tlearn: 58.0068730\ttotal: 1m 42s\tremaining: 17s\n",
            "679:\tlearn: 57.9785130\ttotal: 1m 42s\tremaining: 16.9s\n",
            "680:\tlearn: 57.9628230\ttotal: 1m 42s\tremaining: 16.7s\n",
            "681:\tlearn: 57.9402357\ttotal: 1m 42s\tremaining: 16.6s\n",
            "682:\tlearn: 57.8943826\ttotal: 1m 42s\tremaining: 16.4s\n",
            "683:\tlearn: 57.8471344\ttotal: 1m 42s\tremaining: 16.3s\n",
            "684:\tlearn: 57.8107207\ttotal: 1m 43s\tremaining: 16.1s\n",
            "685:\tlearn: 57.7514963\ttotal: 1m 43s\tremaining: 16s\n",
            "686:\tlearn: 57.6856963\ttotal: 1m 43s\tremaining: 15.8s\n",
            "687:\tlearn: 57.6700490\ttotal: 1m 43s\tremaining: 15.7s\n",
            "688:\tlearn: 57.6288379\ttotal: 1m 43s\tremaining: 15.5s\n",
            "689:\tlearn: 57.4866857\ttotal: 1m 43s\tremaining: 15.4s\n",
            "690:\tlearn: 57.4591073\ttotal: 1m 44s\tremaining: 15.2s\n",
            "691:\tlearn: 57.4155018\ttotal: 1m 44s\tremaining: 15.1s\n",
            "692:\tlearn: 57.3978684\ttotal: 1m 44s\tremaining: 14.9s\n",
            "693:\tlearn: 57.3779160\ttotal: 1m 44s\tremaining: 14.8s\n",
            "694:\tlearn: 57.3322754\ttotal: 1m 44s\tremaining: 14.6s\n",
            "695:\tlearn: 57.3182991\ttotal: 1m 44s\tremaining: 14.5s\n",
            "696:\tlearn: 57.2872002\ttotal: 1m 45s\tremaining: 14.3s\n",
            "697:\tlearn: 57.2773492\ttotal: 1m 45s\tremaining: 14.2s\n",
            "698:\tlearn: 57.2209959\ttotal: 1m 45s\tremaining: 14s\n",
            "699:\tlearn: 57.1804815\ttotal: 1m 45s\tremaining: 13.9s\n",
            "700:\tlearn: 57.1044015\ttotal: 1m 45s\tremaining: 13.7s\n",
            "701:\tlearn: 57.0821913\ttotal: 1m 45s\tremaining: 13.6s\n",
            "702:\tlearn: 57.0507765\ttotal: 1m 45s\tremaining: 13.4s\n",
            "703:\tlearn: 57.0190706\ttotal: 1m 46s\tremaining: 13.3s\n",
            "704:\tlearn: 57.0172968\ttotal: 1m 46s\tremaining: 13.1s\n",
            "705:\tlearn: 56.9795759\ttotal: 1m 46s\tremaining: 13s\n",
            "706:\tlearn: 56.9583459\ttotal: 1m 46s\tremaining: 12.8s\n",
            "707:\tlearn: 56.9005922\ttotal: 1m 46s\tremaining: 12.7s\n",
            "708:\tlearn: 56.8473511\ttotal: 1m 46s\tremaining: 12.5s\n",
            "709:\tlearn: 56.8116289\ttotal: 1m 46s\tremaining: 12.4s\n",
            "710:\tlearn: 56.7741894\ttotal: 1m 47s\tremaining: 12.2s\n",
            "711:\tlearn: 56.7358238\ttotal: 1m 47s\tremaining: 12.1s\n",
            "712:\tlearn: 56.6560106\ttotal: 1m 47s\tremaining: 11.9s\n",
            "713:\tlearn: 56.5992626\ttotal: 1m 47s\tremaining: 11.7s\n",
            "714:\tlearn: 56.5077671\ttotal: 1m 47s\tremaining: 11.6s\n",
            "715:\tlearn: 56.4705614\ttotal: 1m 47s\tremaining: 11.4s\n",
            "716:\tlearn: 56.4417852\ttotal: 1m 47s\tremaining: 11.3s\n",
            "717:\tlearn: 56.4145759\ttotal: 1m 48s\tremaining: 11.1s\n",
            "718:\tlearn: 56.3687730\ttotal: 1m 48s\tremaining: 11s\n",
            "719:\tlearn: 56.2424644\ttotal: 1m 48s\tremaining: 10.8s\n",
            "720:\tlearn: 56.2213884\ttotal: 1m 48s\tremaining: 10.7s\n",
            "721:\tlearn: 56.1114587\ttotal: 1m 48s\tremaining: 10.5s\n",
            "722:\tlearn: 56.0335653\ttotal: 1m 48s\tremaining: 10.4s\n",
            "723:\tlearn: 55.9152012\ttotal: 1m 48s\tremaining: 10.2s\n",
            "724:\tlearn: 55.8821581\ttotal: 1m 49s\tremaining: 10.1s\n",
            "725:\tlearn: 55.8578533\ttotal: 1m 49s\tremaining: 9.94s\n",
            "726:\tlearn: 55.8290164\ttotal: 1m 49s\tremaining: 9.79s\n",
            "727:\tlearn: 55.7969141\ttotal: 1m 49s\tremaining: 9.64s\n",
            "728:\tlearn: 55.7611010\ttotal: 1m 49s\tremaining: 9.48s\n",
            "729:\tlearn: 55.6400077\ttotal: 1m 49s\tremaining: 9.33s\n",
            "730:\tlearn: 55.6192448\ttotal: 1m 50s\tremaining: 9.18s\n",
            "731:\tlearn: 55.5913750\ttotal: 1m 50s\tremaining: 9.03s\n",
            "732:\tlearn: 55.4717827\ttotal: 1m 50s\tremaining: 8.88s\n",
            "733:\tlearn: 55.4413314\ttotal: 1m 50s\tremaining: 8.73s\n",
            "734:\tlearn: 55.4023023\ttotal: 1m 50s\tremaining: 8.58s\n",
            "735:\tlearn: 55.3484237\ttotal: 1m 50s\tremaining: 8.43s\n",
            "736:\tlearn: 55.3147769\ttotal: 1m 50s\tremaining: 8.28s\n",
            "737:\tlearn: 55.3091649\ttotal: 1m 51s\tremaining: 8.13s\n",
            "738:\tlearn: 55.2323259\ttotal: 1m 51s\tremaining: 7.98s\n",
            "739:\tlearn: 55.2309599\ttotal: 1m 51s\tremaining: 7.83s\n",
            "740:\tlearn: 55.2236126\ttotal: 1m 51s\tremaining: 7.68s\n",
            "741:\tlearn: 55.1755755\ttotal: 1m 51s\tremaining: 7.53s\n",
            "742:\tlearn: 55.1429143\ttotal: 1m 51s\tremaining: 7.38s\n",
            "743:\tlearn: 55.1223539\ttotal: 1m 52s\tremaining: 7.23s\n",
            "744:\tlearn: 55.0577661\ttotal: 1m 52s\tremaining: 7.08s\n",
            "745:\tlearn: 55.0101098\ttotal: 1m 52s\tremaining: 6.93s\n",
            "746:\tlearn: 54.9474286\ttotal: 1m 52s\tremaining: 6.78s\n",
            "747:\tlearn: 54.8943380\ttotal: 1m 52s\tremaining: 6.63s\n",
            "748:\tlearn: 54.7357437\ttotal: 1m 52s\tremaining: 6.48s\n",
            "749:\tlearn: 54.7142745\ttotal: 1m 53s\tremaining: 6.33s\n",
            "750:\tlearn: 54.6827009\ttotal: 1m 53s\tremaining: 6.18s\n",
            "751:\tlearn: 54.6435019\ttotal: 1m 53s\tremaining: 6.03s\n",
            "752:\tlearn: 54.6197514\ttotal: 1m 53s\tremaining: 5.88s\n",
            "753:\tlearn: 54.5611196\ttotal: 1m 53s\tremaining: 5.72s\n",
            "754:\tlearn: 54.5576137\ttotal: 1m 53s\tremaining: 5.58s\n",
            "755:\tlearn: 54.5165090\ttotal: 1m 53s\tremaining: 5.42s\n",
            "756:\tlearn: 54.4642443\ttotal: 1m 54s\tremaining: 5.28s\n",
            "757:\tlearn: 54.4339027\ttotal: 1m 54s\tremaining: 5.13s\n",
            "758:\tlearn: 54.3957187\ttotal: 1m 54s\tremaining: 4.97s\n",
            "759:\tlearn: 54.3573833\ttotal: 1m 54s\tremaining: 4.82s\n",
            "760:\tlearn: 54.2590156\ttotal: 1m 54s\tremaining: 4.67s\n",
            "761:\tlearn: 54.2223249\ttotal: 1m 54s\tremaining: 4.52s\n",
            "762:\tlearn: 54.1865843\ttotal: 1m 54s\tremaining: 4.37s\n",
            "763:\tlearn: 54.1216611\ttotal: 1m 55s\tremaining: 4.22s\n",
            "764:\tlearn: 54.0731914\ttotal: 1m 55s\tremaining: 4.07s\n",
            "765:\tlearn: 53.9370095\ttotal: 1m 55s\tremaining: 3.92s\n",
            "766:\tlearn: 53.8880924\ttotal: 1m 55s\tremaining: 3.77s\n",
            "767:\tlearn: 53.7906889\ttotal: 1m 55s\tremaining: 3.62s\n",
            "768:\tlearn: 53.7561247\ttotal: 1m 55s\tremaining: 3.46s\n",
            "769:\tlearn: 53.6904364\ttotal: 1m 56s\tremaining: 3.31s\n",
            "770:\tlearn: 53.6303696\ttotal: 1m 56s\tremaining: 3.16s\n",
            "771:\tlearn: 53.5856239\ttotal: 1m 56s\tremaining: 3.01s\n",
            "772:\tlearn: 53.5409146\ttotal: 1m 56s\tremaining: 2.86s\n",
            "773:\tlearn: 53.5158382\ttotal: 1m 56s\tremaining: 2.71s\n",
            "774:\tlearn: 53.4813933\ttotal: 1m 56s\tremaining: 2.56s\n",
            "775:\tlearn: 53.4117044\ttotal: 1m 56s\tremaining: 2.41s\n",
            "776:\tlearn: 53.3816654\ttotal: 1m 57s\tremaining: 2.26s\n",
            "777:\tlearn: 53.3548192\ttotal: 1m 57s\tremaining: 2.11s\n",
            "778:\tlearn: 53.3374309\ttotal: 1m 57s\tremaining: 1.96s\n",
            "779:\tlearn: 53.3095298\ttotal: 1m 57s\tremaining: 1.81s\n",
            "780:\tlearn: 53.2510847\ttotal: 1m 57s\tremaining: 1.66s\n",
            "781:\tlearn: 53.2216396\ttotal: 1m 57s\tremaining: 1.51s\n",
            "782:\tlearn: 53.2071499\ttotal: 1m 57s\tremaining: 1.36s\n",
            "783:\tlearn: 53.1815504\ttotal: 1m 58s\tremaining: 1.21s\n",
            "784:\tlearn: 53.1617357\ttotal: 1m 58s\tremaining: 1.05s\n",
            "785:\tlearn: 53.1441068\ttotal: 1m 58s\tremaining: 904ms\n",
            "786:\tlearn: 53.0857113\ttotal: 1m 58s\tremaining: 753ms\n",
            "787:\tlearn: 53.0227823\ttotal: 1m 58s\tremaining: 603ms\n",
            "788:\tlearn: 53.0204284\ttotal: 1m 58s\tremaining: 452ms\n",
            "789:\tlearn: 52.9526208\ttotal: 1m 59s\tremaining: 301ms\n",
            "790:\tlearn: 52.9268508\ttotal: 1m 59s\tremaining: 151ms\n",
            "791:\tlearn: 52.9028114\ttotal: 1m 59s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 02:23:36,942]\u001b[0m A new study created in memory with name: no-name-5351219e-c4b3-43f8-8013-c84339c2e463\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 02:26:34,238]\u001b[0m Trial 0 finished with value: 57.310794461375295 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 57.310794461375295.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:28:51,072]\u001b[0m Trial 1 finished with value: 47.0897552436914 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 47.0897552436914.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:31:01,406]\u001b[0m Trial 2 finished with value: 47.05657757276818 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:47:16,135]\u001b[0m Trial 3 finished with value: 81.95072617976903 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:49:32,814]\u001b[0m Trial 4 finished with value: 54.42270842004076 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:03:02,452]\u001b[0m Trial 5 finished with value: 67.60950008855522 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:06:20,646]\u001b[0m Trial 6 finished with value: 45.34453529999927 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:09:27,294]\u001b[0m Trial 7 finished with value: 49.79700780661634 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:11:07,027]\u001b[0m Trial 8 finished with value: 67.60945391304371 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:30:37,914]\u001b[0m Trial 9 finished with value: 57.40686585917658 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:31:40,957]\u001b[0m Trial 10 finished with value: 79.97723508750317 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:33:20,226]\u001b[0m Trial 11 finished with value: 55.87401108749162 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:37:00,994]\u001b[0m Trial 12 finished with value: 47.7410907686109 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:39:29,033]\u001b[0m Trial 13 finished with value: 45.428184253183424 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:41:33,668]\u001b[0m Trial 14 finished with value: 62.39304137708308 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:42:55,613]\u001b[0m Trial 15 finished with value: 65.63417833517829 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:44:17,984]\u001b[0m Trial 16 finished with value: 64.89807632320789 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:46:58,965]\u001b[0m Trial 17 finished with value: 42.615525740274286 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 42.615525740274286.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:55:23,514]\u001b[0m Trial 18 finished with value: 51.59795686679941 and parameters: {'iterations': 838, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 87.60982739927447, 'od_wait': 86, 'leaf_estimation_iterations': 15, 'bagging_temperature': 55.57442205671075, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 42.615525740274286.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:58:51,789]\u001b[0m Trial 19 finished with value: 45.50729628393904 and parameters: {'iterations': 996, 'learning_rate': 0.18484933160405892, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 45.629127031175706, 'od_wait': 63, 'leaf_estimation_iterations': 17, 'bagging_temperature': 50.428094719702216, 'colsample_bylevel': 0.46751740730087066}. Best is trial 17 with value: 42.615525740274286.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 3: 42.615525740274286\n",
            "Best hyperparameters for fold 3: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 206.8902255\ttotal: 156ms\tremaining: 2m 3s\n",
            "1:\tlearn: 205.6392514\ttotal: 241ms\tremaining: 1m 35s\n",
            "2:\tlearn: 205.2129312\ttotal: 411ms\tremaining: 1m 48s\n",
            "3:\tlearn: 205.0020045\ttotal: 483ms\tremaining: 1m 35s\n",
            "4:\tlearn: 205.0020045\ttotal: 531ms\tremaining: 1m 23s\n",
            "5:\tlearn: 204.6891131\ttotal: 685ms\tremaining: 1m 29s\n",
            "6:\tlearn: 202.8235990\ttotal: 827ms\tremaining: 1m 32s\n",
            "7:\tlearn: 201.9483903\ttotal: 992ms\tremaining: 1m 37s\n",
            "8:\tlearn: 201.7532797\ttotal: 1.16s\tremaining: 1m 41s\n",
            "9:\tlearn: 200.7952509\ttotal: 1.34s\tremaining: 1m 44s\n",
            "10:\tlearn: 200.6132614\ttotal: 1.51s\tremaining: 1m 47s\n",
            "11:\tlearn: 200.4544639\ttotal: 1.68s\tremaining: 1m 48s\n",
            "12:\tlearn: 200.1654063\ttotal: 1.86s\tremaining: 1m 51s\n",
            "13:\tlearn: 199.1776330\ttotal: 2.02s\tremaining: 1m 52s\n",
            "14:\tlearn: 198.8835189\ttotal: 2.16s\tremaining: 1m 51s\n",
            "15:\tlearn: 198.8639423\ttotal: 2.23s\tremaining: 1m 48s\n",
            "16:\tlearn: 198.8639423\ttotal: 2.28s\tremaining: 1m 44s\n",
            "17:\tlearn: 198.6969025\ttotal: 2.44s\tremaining: 1m 45s\n",
            "18:\tlearn: 198.4024319\ttotal: 2.61s\tremaining: 1m 46s\n",
            "19:\tlearn: 198.2202286\ttotal: 2.76s\tremaining: 1m 46s\n",
            "20:\tlearn: 198.2107069\ttotal: 2.85s\tremaining: 1m 44s\n",
            "21:\tlearn: 198.1207887\ttotal: 3.01s\tremaining: 1m 45s\n",
            "22:\tlearn: 198.1132433\ttotal: 3.17s\tremaining: 1m 46s\n",
            "23:\tlearn: 197.9495169\ttotal: 3.35s\tremaining: 1m 47s\n",
            "24:\tlearn: 197.9439679\ttotal: 3.43s\tremaining: 1m 45s\n",
            "25:\tlearn: 197.7103389\ttotal: 3.59s\tremaining: 1m 45s\n",
            "26:\tlearn: 197.6208727\ttotal: 3.75s\tremaining: 1m 46s\n",
            "27:\tlearn: 197.4794694\ttotal: 3.9s\tremaining: 1m 46s\n",
            "28:\tlearn: 197.1917484\ttotal: 4.06s\tremaining: 1m 46s\n",
            "29:\tlearn: 196.8383169\ttotal: 4.22s\tremaining: 1m 47s\n",
            "30:\tlearn: 196.4169178\ttotal: 4.38s\tremaining: 1m 47s\n",
            "31:\tlearn: 196.4063698\ttotal: 4.49s\tremaining: 1m 46s\n",
            "32:\tlearn: 196.4062202\ttotal: 4.55s\tremaining: 1m 44s\n",
            "33:\tlearn: 195.1546875\ttotal: 4.71s\tremaining: 1m 45s\n",
            "34:\tlearn: 195.0625592\ttotal: 4.8s\tremaining: 1m 43s\n",
            "35:\tlearn: 194.9681388\ttotal: 4.92s\tremaining: 1m 43s\n",
            "36:\tlearn: 194.1581317\ttotal: 5.11s\tremaining: 1m 44s\n",
            "37:\tlearn: 193.7497749\ttotal: 5.25s\tremaining: 1m 44s\n",
            "38:\tlearn: 192.5442887\ttotal: 5.42s\tremaining: 1m 44s\n",
            "39:\tlearn: 191.6649301\ttotal: 5.56s\tremaining: 1m 44s\n",
            "40:\tlearn: 191.0057641\ttotal: 5.73s\tremaining: 1m 44s\n",
            "41:\tlearn: 189.9348080\ttotal: 5.88s\tremaining: 1m 44s\n",
            "42:\tlearn: 188.8478074\ttotal: 6.03s\tremaining: 1m 44s\n",
            "43:\tlearn: 187.3441756\ttotal: 6.19s\tremaining: 1m 45s\n",
            "44:\tlearn: 186.7637561\ttotal: 6.37s\tremaining: 1m 45s\n",
            "45:\tlearn: 186.0066543\ttotal: 6.5s\tremaining: 1m 45s\n",
            "46:\tlearn: 185.3711297\ttotal: 6.66s\tremaining: 1m 45s\n",
            "47:\tlearn: 182.9434125\ttotal: 6.81s\tremaining: 1m 45s\n",
            "48:\tlearn: 182.6146454\ttotal: 6.97s\tremaining: 1m 45s\n",
            "49:\tlearn: 182.0242536\ttotal: 7.12s\tremaining: 1m 45s\n",
            "50:\tlearn: 181.3492257\ttotal: 7.26s\tremaining: 1m 45s\n",
            "51:\tlearn: 180.4482234\ttotal: 7.41s\tremaining: 1m 45s\n",
            "52:\tlearn: 176.6291158\ttotal: 7.55s\tremaining: 1m 45s\n",
            "53:\tlearn: 176.2144210\ttotal: 7.69s\tremaining: 1m 45s\n",
            "54:\tlearn: 175.1619255\ttotal: 7.87s\tremaining: 1m 45s\n",
            "55:\tlearn: 174.7826420\ttotal: 8.03s\tremaining: 1m 45s\n",
            "56:\tlearn: 172.7933818\ttotal: 8.18s\tremaining: 1m 45s\n",
            "57:\tlearn: 172.2798516\ttotal: 8.34s\tremaining: 1m 45s\n",
            "58:\tlearn: 171.3181269\ttotal: 8.48s\tremaining: 1m 45s\n",
            "59:\tlearn: 170.9609921\ttotal: 8.64s\tremaining: 1m 45s\n",
            "60:\tlearn: 169.3626367\ttotal: 8.78s\tremaining: 1m 45s\n",
            "61:\tlearn: 168.9950792\ttotal: 8.94s\tremaining: 1m 45s\n",
            "62:\tlearn: 168.0322078\ttotal: 9.09s\tremaining: 1m 45s\n",
            "63:\tlearn: 167.0369041\ttotal: 9.23s\tremaining: 1m 45s\n",
            "64:\tlearn: 165.9629175\ttotal: 9.38s\tremaining: 1m 44s\n",
            "65:\tlearn: 164.5177694\ttotal: 9.54s\tremaining: 1m 44s\n",
            "66:\tlearn: 162.8896455\ttotal: 9.69s\tremaining: 1m 44s\n",
            "67:\tlearn: 161.9993379\ttotal: 9.84s\tremaining: 1m 44s\n",
            "68:\tlearn: 161.8255491\ttotal: 10s\tremaining: 1m 44s\n",
            "69:\tlearn: 161.5904644\ttotal: 10.2s\tremaining: 1m 44s\n",
            "70:\tlearn: 160.2652759\ttotal: 10.3s\tremaining: 1m 44s\n",
            "71:\tlearn: 159.1745166\ttotal: 10.4s\tremaining: 1m 44s\n",
            "72:\tlearn: 158.8883445\ttotal: 10.6s\tremaining: 1m 44s\n",
            "73:\tlearn: 157.8994367\ttotal: 10.7s\tremaining: 1m 44s\n",
            "74:\tlearn: 157.3566433\ttotal: 10.9s\tremaining: 1m 43s\n",
            "75:\tlearn: 157.0205308\ttotal: 11s\tremaining: 1m 43s\n",
            "76:\tlearn: 156.7022287\ttotal: 11.2s\tremaining: 1m 43s\n",
            "77:\tlearn: 155.9631742\ttotal: 11.3s\tremaining: 1m 43s\n",
            "78:\tlearn: 155.5260561\ttotal: 11.5s\tremaining: 1m 44s\n",
            "79:\tlearn: 154.0386846\ttotal: 11.7s\tremaining: 1m 43s\n",
            "80:\tlearn: 153.3865299\ttotal: 11.8s\tremaining: 1m 43s\n",
            "81:\tlearn: 152.9774436\ttotal: 12s\tremaining: 1m 43s\n",
            "82:\tlearn: 152.6286780\ttotal: 12.1s\tremaining: 1m 43s\n",
            "83:\tlearn: 152.2429189\ttotal: 12.3s\tremaining: 1m 43s\n",
            "84:\tlearn: 152.1219204\ttotal: 12.4s\tremaining: 1m 43s\n",
            "85:\tlearn: 152.0783994\ttotal: 12.6s\tremaining: 1m 43s\n",
            "86:\tlearn: 151.9130701\ttotal: 12.7s\tremaining: 1m 43s\n",
            "87:\tlearn: 149.8393503\ttotal: 12.9s\tremaining: 1m 43s\n",
            "88:\tlearn: 148.9828375\ttotal: 13s\tremaining: 1m 42s\n",
            "89:\tlearn: 147.9167296\ttotal: 13.2s\tremaining: 1m 42s\n",
            "90:\tlearn: 146.9141285\ttotal: 13.3s\tremaining: 1m 42s\n",
            "91:\tlearn: 145.0581605\ttotal: 13.5s\tremaining: 1m 42s\n",
            "92:\tlearn: 143.9172940\ttotal: 13.6s\tremaining: 1m 42s\n",
            "93:\tlearn: 143.6588012\ttotal: 13.8s\tremaining: 1m 42s\n",
            "94:\tlearn: 142.7840739\ttotal: 13.9s\tremaining: 1m 41s\n",
            "95:\tlearn: 142.2205615\ttotal: 14s\tremaining: 1m 41s\n",
            "96:\tlearn: 141.4983687\ttotal: 14.2s\tremaining: 1m 41s\n",
            "97:\tlearn: 141.3187732\ttotal: 14.3s\tremaining: 1m 41s\n",
            "98:\tlearn: 140.7390184\ttotal: 14.5s\tremaining: 1m 41s\n",
            "99:\tlearn: 139.6305459\ttotal: 14.6s\tremaining: 1m 41s\n",
            "100:\tlearn: 139.4135681\ttotal: 14.8s\tremaining: 1m 41s\n",
            "101:\tlearn: 139.2241650\ttotal: 14.9s\tremaining: 1m 40s\n",
            "102:\tlearn: 138.5318584\ttotal: 15s\tremaining: 1m 40s\n",
            "103:\tlearn: 136.9513248\ttotal: 15.2s\tremaining: 1m 40s\n",
            "104:\tlearn: 136.1400065\ttotal: 15.3s\tremaining: 1m 40s\n",
            "105:\tlearn: 135.7086747\ttotal: 15.5s\tremaining: 1m 40s\n",
            "106:\tlearn: 135.4059110\ttotal: 15.6s\tremaining: 1m 39s\n",
            "107:\tlearn: 135.2556730\ttotal: 15.8s\tremaining: 1m 39s\n",
            "108:\tlearn: 133.6723308\ttotal: 15.9s\tremaining: 1m 39s\n",
            "109:\tlearn: 133.4531229\ttotal: 16.1s\tremaining: 1m 39s\n",
            "110:\tlearn: 132.7378460\ttotal: 16.2s\tremaining: 1m 39s\n",
            "111:\tlearn: 132.2354894\ttotal: 16.4s\tremaining: 1m 39s\n",
            "112:\tlearn: 132.0846901\ttotal: 16.5s\tremaining: 1m 39s\n",
            "113:\tlearn: 131.9443341\ttotal: 16.6s\tremaining: 1m 38s\n",
            "114:\tlearn: 131.5993670\ttotal: 16.8s\tremaining: 1m 38s\n",
            "115:\tlearn: 131.4557152\ttotal: 17s\tremaining: 1m 38s\n",
            "116:\tlearn: 131.4338894\ttotal: 17.1s\tremaining: 1m 38s\n",
            "117:\tlearn: 131.0955826\ttotal: 17.3s\tremaining: 1m 38s\n",
            "118:\tlearn: 129.8195342\ttotal: 17.4s\tremaining: 1m 38s\n",
            "119:\tlearn: 128.8877850\ttotal: 17.6s\tremaining: 1m 38s\n",
            "120:\tlearn: 128.2788655\ttotal: 17.7s\tremaining: 1m 38s\n",
            "121:\tlearn: 128.1538525\ttotal: 17.9s\tremaining: 1m 38s\n",
            "122:\tlearn: 127.8406776\ttotal: 18s\tremaining: 1m 37s\n",
            "123:\tlearn: 127.5245606\ttotal: 18.1s\tremaining: 1m 37s\n",
            "124:\tlearn: 126.6490246\ttotal: 18.3s\tremaining: 1m 37s\n",
            "125:\tlearn: 126.4362391\ttotal: 18.4s\tremaining: 1m 37s\n",
            "126:\tlearn: 125.9354467\ttotal: 18.5s\tremaining: 1m 37s\n",
            "127:\tlearn: 125.7521905\ttotal: 18.7s\tremaining: 1m 36s\n",
            "128:\tlearn: 125.6305745\ttotal: 18.9s\tremaining: 1m 36s\n",
            "129:\tlearn: 124.8457599\ttotal: 19s\tremaining: 1m 36s\n",
            "130:\tlearn: 124.5740362\ttotal: 19.1s\tremaining: 1m 36s\n",
            "131:\tlearn: 124.5088849\ttotal: 19.3s\tremaining: 1m 36s\n",
            "132:\tlearn: 124.2979371\ttotal: 19.4s\tremaining: 1m 36s\n",
            "133:\tlearn: 123.9403428\ttotal: 19.6s\tremaining: 1m 36s\n",
            "134:\tlearn: 123.9355680\ttotal: 19.7s\tremaining: 1m 36s\n",
            "135:\tlearn: 123.8466319\ttotal: 19.9s\tremaining: 1m 36s\n",
            "136:\tlearn: 123.0447056\ttotal: 20.1s\tremaining: 1m 35s\n",
            "137:\tlearn: 122.4589710\ttotal: 20.2s\tremaining: 1m 35s\n",
            "138:\tlearn: 122.3118803\ttotal: 20.4s\tremaining: 1m 35s\n",
            "139:\tlearn: 122.1532905\ttotal: 20.5s\tremaining: 1m 35s\n",
            "140:\tlearn: 121.8532331\ttotal: 20.7s\tremaining: 1m 35s\n",
            "141:\tlearn: 121.3418511\ttotal: 20.8s\tremaining: 1m 35s\n",
            "142:\tlearn: 120.9414413\ttotal: 21s\tremaining: 1m 35s\n",
            "143:\tlearn: 120.8075781\ttotal: 21.1s\tremaining: 1m 35s\n",
            "144:\tlearn: 120.6746304\ttotal: 21.3s\tremaining: 1m 35s\n",
            "145:\tlearn: 120.4726448\ttotal: 21.5s\tremaining: 1m 34s\n",
            "146:\tlearn: 120.2844815\ttotal: 21.6s\tremaining: 1m 34s\n",
            "147:\tlearn: 119.3997950\ttotal: 21.7s\tremaining: 1m 34s\n",
            "148:\tlearn: 118.9983348\ttotal: 21.9s\tremaining: 1m 34s\n",
            "149:\tlearn: 118.8427553\ttotal: 22s\tremaining: 1m 34s\n",
            "150:\tlearn: 118.4199098\ttotal: 22.2s\tremaining: 1m 34s\n",
            "151:\tlearn: 117.8925037\ttotal: 22.3s\tremaining: 1m 33s\n",
            "152:\tlearn: 117.7447950\ttotal: 22.5s\tremaining: 1m 33s\n",
            "153:\tlearn: 117.5399591\ttotal: 22.6s\tremaining: 1m 33s\n",
            "154:\tlearn: 117.3195936\ttotal: 22.8s\tremaining: 1m 33s\n",
            "155:\tlearn: 117.0729454\ttotal: 22.9s\tremaining: 1m 33s\n",
            "156:\tlearn: 116.4185154\ttotal: 23.1s\tremaining: 1m 33s\n",
            "157:\tlearn: 116.1724094\ttotal: 23.2s\tremaining: 1m 33s\n",
            "158:\tlearn: 115.9537737\ttotal: 23.4s\tremaining: 1m 32s\n",
            "159:\tlearn: 115.7480674\ttotal: 23.5s\tremaining: 1m 32s\n",
            "160:\tlearn: 115.5920091\ttotal: 23.6s\tremaining: 1m 32s\n",
            "161:\tlearn: 115.0825131\ttotal: 23.8s\tremaining: 1m 32s\n",
            "162:\tlearn: 114.6897783\ttotal: 23.9s\tremaining: 1m 32s\n",
            "163:\tlearn: 113.8460237\ttotal: 24.1s\tremaining: 1m 32s\n",
            "164:\tlearn: 113.7533845\ttotal: 24.2s\tremaining: 1m 31s\n",
            "165:\tlearn: 113.3287437\ttotal: 24.3s\tremaining: 1m 31s\n",
            "166:\tlearn: 113.0621873\ttotal: 24.5s\tremaining: 1m 31s\n",
            "167:\tlearn: 112.8737050\ttotal: 24.6s\tremaining: 1m 31s\n",
            "168:\tlearn: 112.4365597\ttotal: 24.8s\tremaining: 1m 31s\n",
            "169:\tlearn: 112.3869856\ttotal: 24.9s\tremaining: 1m 31s\n",
            "170:\tlearn: 112.2968278\ttotal: 25.1s\tremaining: 1m 31s\n",
            "171:\tlearn: 110.9704132\ttotal: 25.2s\tremaining: 1m 30s\n",
            "172:\tlearn: 110.9001029\ttotal: 25.4s\tremaining: 1m 30s\n",
            "173:\tlearn: 110.6970631\ttotal: 25.5s\tremaining: 1m 30s\n",
            "174:\tlearn: 110.5604118\ttotal: 25.7s\tremaining: 1m 30s\n",
            "175:\tlearn: 110.4187746\ttotal: 25.8s\tremaining: 1m 30s\n",
            "176:\tlearn: 109.9900680\ttotal: 26s\tremaining: 1m 30s\n",
            "177:\tlearn: 109.7996202\ttotal: 26.1s\tremaining: 1m 30s\n",
            "178:\tlearn: 109.4862103\ttotal: 26.3s\tremaining: 1m 29s\n",
            "179:\tlearn: 109.4126521\ttotal: 26.4s\tremaining: 1m 29s\n",
            "180:\tlearn: 109.3064310\ttotal: 26.6s\tremaining: 1m 29s\n",
            "181:\tlearn: 109.1845727\ttotal: 26.8s\tremaining: 1m 29s\n",
            "182:\tlearn: 108.8324980\ttotal: 26.9s\tremaining: 1m 29s\n",
            "183:\tlearn: 108.6887635\ttotal: 27.1s\tremaining: 1m 29s\n",
            "184:\tlearn: 108.3069292\ttotal: 27.2s\tremaining: 1m 29s\n",
            "185:\tlearn: 108.1182515\ttotal: 27.3s\tremaining: 1m 29s\n",
            "186:\tlearn: 107.6603677\ttotal: 27.5s\tremaining: 1m 28s\n",
            "187:\tlearn: 107.6027332\ttotal: 27.6s\tremaining: 1m 28s\n",
            "188:\tlearn: 107.4004566\ttotal: 27.8s\tremaining: 1m 28s\n",
            "189:\tlearn: 107.3283414\ttotal: 28s\tremaining: 1m 28s\n",
            "190:\tlearn: 107.1256179\ttotal: 28.1s\tremaining: 1m 28s\n",
            "191:\tlearn: 107.0241951\ttotal: 28.3s\tremaining: 1m 28s\n",
            "192:\tlearn: 106.8593720\ttotal: 28.4s\tremaining: 1m 28s\n",
            "193:\tlearn: 106.6719897\ttotal: 28.6s\tremaining: 1m 28s\n",
            "194:\tlearn: 106.6603439\ttotal: 28.8s\tremaining: 1m 28s\n",
            "195:\tlearn: 106.5898198\ttotal: 28.9s\tremaining: 1m 27s\n",
            "196:\tlearn: 106.4562045\ttotal: 29.1s\tremaining: 1m 27s\n",
            "197:\tlearn: 106.2760801\ttotal: 29.2s\tremaining: 1m 27s\n",
            "198:\tlearn: 106.0396063\ttotal: 29.4s\tremaining: 1m 27s\n",
            "199:\tlearn: 105.6650060\ttotal: 29.5s\tremaining: 1m 27s\n",
            "200:\tlearn: 105.2415395\ttotal: 29.7s\tremaining: 1m 27s\n",
            "201:\tlearn: 105.0150441\ttotal: 29.8s\tremaining: 1m 27s\n",
            "202:\tlearn: 104.8671962\ttotal: 30s\tremaining: 1m 26s\n",
            "203:\tlearn: 104.3872287\ttotal: 30.1s\tremaining: 1m 26s\n",
            "204:\tlearn: 104.2104913\ttotal: 30.3s\tremaining: 1m 26s\n",
            "205:\tlearn: 104.0020890\ttotal: 30.4s\tremaining: 1m 26s\n",
            "206:\tlearn: 103.8754676\ttotal: 30.5s\tremaining: 1m 26s\n",
            "207:\tlearn: 103.7770379\ttotal: 30.7s\tremaining: 1m 26s\n",
            "208:\tlearn: 103.1879959\ttotal: 30.8s\tremaining: 1m 26s\n",
            "209:\tlearn: 103.1112392\ttotal: 31s\tremaining: 1m 25s\n",
            "210:\tlearn: 102.9339125\ttotal: 31.2s\tremaining: 1m 25s\n",
            "211:\tlearn: 102.6373262\ttotal: 31.3s\tremaining: 1m 25s\n",
            "212:\tlearn: 102.5664684\ttotal: 31.4s\tremaining: 1m 25s\n",
            "213:\tlearn: 102.2543688\ttotal: 31.6s\tremaining: 1m 25s\n",
            "214:\tlearn: 101.9960037\ttotal: 31.7s\tremaining: 1m 25s\n",
            "215:\tlearn: 101.9085062\ttotal: 31.9s\tremaining: 1m 25s\n",
            "216:\tlearn: 101.7901424\ttotal: 32s\tremaining: 1m 24s\n",
            "217:\tlearn: 101.6792822\ttotal: 32.2s\tremaining: 1m 24s\n",
            "218:\tlearn: 101.5621810\ttotal: 32.3s\tremaining: 1m 24s\n",
            "219:\tlearn: 101.4909640\ttotal: 32.5s\tremaining: 1m 24s\n",
            "220:\tlearn: 101.1315158\ttotal: 32.6s\tremaining: 1m 24s\n",
            "221:\tlearn: 101.0755171\ttotal: 32.8s\tremaining: 1m 24s\n",
            "222:\tlearn: 100.6828619\ttotal: 32.9s\tremaining: 1m 23s\n",
            "223:\tlearn: 100.6353907\ttotal: 33.1s\tremaining: 1m 23s\n",
            "224:\tlearn: 100.5603639\ttotal: 33.2s\tremaining: 1m 23s\n",
            "225:\tlearn: 100.3847617\ttotal: 33.4s\tremaining: 1m 23s\n",
            "226:\tlearn: 100.1700490\ttotal: 33.5s\tremaining: 1m 23s\n",
            "227:\tlearn: 99.9989666\ttotal: 33.6s\tremaining: 1m 23s\n",
            "228:\tlearn: 99.9028703\ttotal: 33.8s\tremaining: 1m 23s\n",
            "229:\tlearn: 99.6638038\ttotal: 34s\tremaining: 1m 22s\n",
            "230:\tlearn: 99.5934120\ttotal: 34.1s\tremaining: 1m 22s\n",
            "231:\tlearn: 99.4851465\ttotal: 34.3s\tremaining: 1m 22s\n",
            "232:\tlearn: 99.3495668\ttotal: 34.4s\tremaining: 1m 22s\n",
            "233:\tlearn: 99.1003155\ttotal: 34.5s\tremaining: 1m 22s\n",
            "234:\tlearn: 98.9782578\ttotal: 34.7s\tremaining: 1m 22s\n",
            "235:\tlearn: 98.8709145\ttotal: 34.8s\tremaining: 1m 22s\n",
            "236:\tlearn: 98.5457967\ttotal: 35s\tremaining: 1m 21s\n",
            "237:\tlearn: 98.4373993\ttotal: 35.2s\tremaining: 1m 21s\n",
            "238:\tlearn: 98.4259425\ttotal: 35.3s\tremaining: 1m 21s\n",
            "239:\tlearn: 98.3510186\ttotal: 35.5s\tremaining: 1m 21s\n",
            "240:\tlearn: 98.1160668\ttotal: 35.6s\tremaining: 1m 21s\n",
            "241:\tlearn: 97.9282072\ttotal: 35.8s\tremaining: 1m 21s\n",
            "242:\tlearn: 97.6298342\ttotal: 35.9s\tremaining: 1m 21s\n",
            "243:\tlearn: 97.3904273\ttotal: 36s\tremaining: 1m 20s\n",
            "244:\tlearn: 97.1765267\ttotal: 36.2s\tremaining: 1m 20s\n",
            "245:\tlearn: 97.0328248\ttotal: 36.3s\tremaining: 1m 20s\n",
            "246:\tlearn: 96.8350516\ttotal: 36.5s\tremaining: 1m 20s\n",
            "247:\tlearn: 96.8349411\ttotal: 36.6s\tremaining: 1m 20s\n",
            "248:\tlearn: 96.6915457\ttotal: 36.8s\tremaining: 1m 20s\n",
            "249:\tlearn: 96.6333414\ttotal: 37s\tremaining: 1m 20s\n",
            "250:\tlearn: 96.4476953\ttotal: 37.1s\tremaining: 1m 19s\n",
            "251:\tlearn: 96.4221416\ttotal: 37.3s\tremaining: 1m 19s\n",
            "252:\tlearn: 96.4032524\ttotal: 37.4s\tremaining: 1m 19s\n",
            "253:\tlearn: 96.3103910\ttotal: 37.6s\tremaining: 1m 19s\n",
            "254:\tlearn: 96.2492256\ttotal: 37.7s\tremaining: 1m 19s\n",
            "255:\tlearn: 96.1786579\ttotal: 37.9s\tremaining: 1m 19s\n",
            "256:\tlearn: 96.0367422\ttotal: 38.1s\tremaining: 1m 19s\n",
            "257:\tlearn: 95.9994060\ttotal: 38.2s\tremaining: 1m 19s\n",
            "258:\tlearn: 95.5877056\ttotal: 38.4s\tremaining: 1m 18s\n",
            "259:\tlearn: 95.3188152\ttotal: 38.5s\tremaining: 1m 18s\n",
            "260:\tlearn: 95.2224080\ttotal: 38.7s\tremaining: 1m 18s\n",
            "261:\tlearn: 94.9860330\ttotal: 38.8s\tremaining: 1m 18s\n",
            "262:\tlearn: 94.9533163\ttotal: 39s\tremaining: 1m 18s\n",
            "263:\tlearn: 94.8695420\ttotal: 39.1s\tremaining: 1m 18s\n",
            "264:\tlearn: 94.5955161\ttotal: 39.2s\tremaining: 1m 18s\n",
            "265:\tlearn: 94.5172588\ttotal: 39.4s\tremaining: 1m 17s\n",
            "266:\tlearn: 94.4451206\ttotal: 39.5s\tremaining: 1m 17s\n",
            "267:\tlearn: 94.3672296\ttotal: 39.7s\tremaining: 1m 17s\n",
            "268:\tlearn: 94.2947628\ttotal: 39.8s\tremaining: 1m 17s\n",
            "269:\tlearn: 93.8631437\ttotal: 40s\tremaining: 1m 17s\n",
            "270:\tlearn: 93.6747945\ttotal: 40.1s\tremaining: 1m 17s\n",
            "271:\tlearn: 93.6486646\ttotal: 40.3s\tremaining: 1m 17s\n",
            "272:\tlearn: 93.5495657\ttotal: 40.5s\tremaining: 1m 16s\n",
            "273:\tlearn: 93.4405803\ttotal: 40.6s\tremaining: 1m 16s\n",
            "274:\tlearn: 93.2244617\ttotal: 40.8s\tremaining: 1m 16s\n",
            "275:\tlearn: 92.9501383\ttotal: 40.9s\tremaining: 1m 16s\n",
            "276:\tlearn: 92.7445063\ttotal: 41.1s\tremaining: 1m 16s\n",
            "277:\tlearn: 92.6116944\ttotal: 41.2s\tremaining: 1m 16s\n",
            "278:\tlearn: 92.5385921\ttotal: 41.4s\tremaining: 1m 16s\n",
            "279:\tlearn: 92.4703802\ttotal: 41.5s\tremaining: 1m 15s\n",
            "280:\tlearn: 92.2622655\ttotal: 41.6s\tremaining: 1m 15s\n",
            "281:\tlearn: 92.1360761\ttotal: 41.8s\tremaining: 1m 15s\n",
            "282:\tlearn: 92.0440380\ttotal: 41.9s\tremaining: 1m 15s\n",
            "283:\tlearn: 91.9528834\ttotal: 42.1s\tremaining: 1m 15s\n",
            "284:\tlearn: 91.8269017\ttotal: 42.2s\tremaining: 1m 15s\n",
            "285:\tlearn: 91.4202892\ttotal: 42.4s\tremaining: 1m 14s\n",
            "286:\tlearn: 91.2787726\ttotal: 42.5s\tremaining: 1m 14s\n",
            "287:\tlearn: 91.0195598\ttotal: 42.7s\tremaining: 1m 14s\n",
            "288:\tlearn: 90.6762080\ttotal: 42.8s\tremaining: 1m 14s\n",
            "289:\tlearn: 90.5829839\ttotal: 43s\tremaining: 1m 14s\n",
            "290:\tlearn: 90.4736520\ttotal: 43.1s\tremaining: 1m 14s\n",
            "291:\tlearn: 90.3576181\ttotal: 43.3s\tremaining: 1m 14s\n",
            "292:\tlearn: 90.1832392\ttotal: 43.4s\tremaining: 1m 13s\n",
            "293:\tlearn: 90.1808093\ttotal: 43.6s\tremaining: 1m 13s\n",
            "294:\tlearn: 90.1282222\ttotal: 43.7s\tremaining: 1m 13s\n",
            "295:\tlearn: 90.0964900\ttotal: 43.9s\tremaining: 1m 13s\n",
            "296:\tlearn: 89.8515917\ttotal: 44s\tremaining: 1m 13s\n",
            "297:\tlearn: 89.8388336\ttotal: 44.2s\tremaining: 1m 13s\n",
            "298:\tlearn: 89.7579490\ttotal: 44.4s\tremaining: 1m 13s\n",
            "299:\tlearn: 89.4822597\ttotal: 44.5s\tremaining: 1m 13s\n",
            "300:\tlearn: 89.4029721\ttotal: 44.7s\tremaining: 1m 12s\n",
            "301:\tlearn: 89.1732040\ttotal: 44.8s\tremaining: 1m 12s\n",
            "302:\tlearn: 89.0358059\ttotal: 45s\tremaining: 1m 12s\n",
            "303:\tlearn: 88.9064472\ttotal: 45.1s\tremaining: 1m 12s\n",
            "304:\tlearn: 88.7930382\ttotal: 45.3s\tremaining: 1m 12s\n",
            "305:\tlearn: 88.7003791\ttotal: 45.4s\tremaining: 1m 12s\n",
            "306:\tlearn: 88.4833207\ttotal: 45.5s\tremaining: 1m 11s\n",
            "307:\tlearn: 88.4164348\ttotal: 45.7s\tremaining: 1m 11s\n",
            "308:\tlearn: 88.3070279\ttotal: 45.8s\tremaining: 1m 11s\n",
            "309:\tlearn: 88.2083386\ttotal: 46s\tremaining: 1m 11s\n",
            "310:\tlearn: 88.1408269\ttotal: 46.2s\tremaining: 1m 11s\n",
            "311:\tlearn: 87.9885962\ttotal: 46.3s\tremaining: 1m 11s\n",
            "312:\tlearn: 87.6122118\ttotal: 46.4s\tremaining: 1m 11s\n",
            "313:\tlearn: 87.1820474\ttotal: 46.6s\tremaining: 1m 10s\n",
            "314:\tlearn: 87.0821084\ttotal: 46.7s\tremaining: 1m 10s\n",
            "315:\tlearn: 86.9892713\ttotal: 46.8s\tremaining: 1m 10s\n",
            "316:\tlearn: 86.7201655\ttotal: 47s\tremaining: 1m 10s\n",
            "317:\tlearn: 86.5903265\ttotal: 47.1s\tremaining: 1m 10s\n",
            "318:\tlearn: 86.4369695\ttotal: 47.3s\tremaining: 1m 10s\n",
            "319:\tlearn: 86.3715174\ttotal: 47.5s\tremaining: 1m 10s\n",
            "320:\tlearn: 86.2845246\ttotal: 47.6s\tremaining: 1m 9s\n",
            "321:\tlearn: 86.2205288\ttotal: 47.8s\tremaining: 1m 9s\n",
            "322:\tlearn: 86.0680748\ttotal: 47.9s\tremaining: 1m 9s\n",
            "323:\tlearn: 85.9473395\ttotal: 48.1s\tremaining: 1m 9s\n",
            "324:\tlearn: 85.8894794\ttotal: 48.2s\tremaining: 1m 9s\n",
            "325:\tlearn: 85.8611749\ttotal: 48.4s\tremaining: 1m 9s\n",
            "326:\tlearn: 85.5246435\ttotal: 48.5s\tremaining: 1m 8s\n",
            "327:\tlearn: 85.4325004\ttotal: 48.6s\tremaining: 1m 8s\n",
            "328:\tlearn: 85.3382732\ttotal: 48.8s\tremaining: 1m 8s\n",
            "329:\tlearn: 85.2414225\ttotal: 48.9s\tremaining: 1m 8s\n",
            "330:\tlearn: 85.1689274\ttotal: 49.1s\tremaining: 1m 8s\n",
            "331:\tlearn: 85.1173416\ttotal: 49.3s\tremaining: 1m 8s\n",
            "332:\tlearn: 84.8273293\ttotal: 49.4s\tremaining: 1m 8s\n",
            "333:\tlearn: 84.7085120\ttotal: 49.5s\tremaining: 1m 7s\n",
            "334:\tlearn: 84.6296846\ttotal: 49.7s\tremaining: 1m 7s\n",
            "335:\tlearn: 84.6104878\ttotal: 49.9s\tremaining: 1m 7s\n",
            "336:\tlearn: 84.5652283\ttotal: 50s\tremaining: 1m 7s\n",
            "337:\tlearn: 84.4154978\ttotal: 50.2s\tremaining: 1m 7s\n",
            "338:\tlearn: 84.3714545\ttotal: 50.4s\tremaining: 1m 7s\n",
            "339:\tlearn: 84.2995983\ttotal: 50.5s\tremaining: 1m 7s\n",
            "340:\tlearn: 84.0482510\ttotal: 50.7s\tremaining: 1m 7s\n",
            "341:\tlearn: 83.7654567\ttotal: 50.8s\tremaining: 1m 6s\n",
            "342:\tlearn: 83.6511698\ttotal: 51s\tremaining: 1m 6s\n",
            "343:\tlearn: 83.5600891\ttotal: 51.1s\tremaining: 1m 6s\n",
            "344:\tlearn: 83.4726972\ttotal: 51.2s\tremaining: 1m 6s\n",
            "345:\tlearn: 83.4170374\ttotal: 51.4s\tremaining: 1m 6s\n",
            "346:\tlearn: 83.3460747\ttotal: 51.6s\tremaining: 1m 6s\n",
            "347:\tlearn: 83.2518751\ttotal: 51.7s\tremaining: 1m 5s\n",
            "348:\tlearn: 83.1944207\ttotal: 51.9s\tremaining: 1m 5s\n",
            "349:\tlearn: 83.1297432\ttotal: 52s\tremaining: 1m 5s\n",
            "350:\tlearn: 82.9667466\ttotal: 52.1s\tremaining: 1m 5s\n",
            "351:\tlearn: 82.9601428\ttotal: 52.3s\tremaining: 1m 5s\n",
            "352:\tlearn: 82.8284653\ttotal: 52.4s\tremaining: 1m 5s\n",
            "353:\tlearn: 82.7473183\ttotal: 52.6s\tremaining: 1m 5s\n",
            "354:\tlearn: 82.6618049\ttotal: 52.7s\tremaining: 1m 4s\n",
            "355:\tlearn: 82.6598719\ttotal: 52.9s\tremaining: 1m 4s\n",
            "356:\tlearn: 82.5320074\ttotal: 53s\tremaining: 1m 4s\n",
            "357:\tlearn: 82.2322653\ttotal: 53.2s\tremaining: 1m 4s\n",
            "358:\tlearn: 82.1664055\ttotal: 53.3s\tremaining: 1m 4s\n",
            "359:\tlearn: 81.9807217\ttotal: 53.5s\tremaining: 1m 4s\n",
            "360:\tlearn: 81.9570556\ttotal: 53.6s\tremaining: 1m 4s\n",
            "361:\tlearn: 81.9394049\ttotal: 53.8s\tremaining: 1m 3s\n",
            "362:\tlearn: 81.7031851\ttotal: 53.9s\tremaining: 1m 3s\n",
            "363:\tlearn: 81.6599398\ttotal: 54.1s\tremaining: 1m 3s\n",
            "364:\tlearn: 81.1329738\ttotal: 54.2s\tremaining: 1m 3s\n",
            "365:\tlearn: 81.0117193\ttotal: 54.3s\tremaining: 1m 3s\n",
            "366:\tlearn: 80.9479129\ttotal: 54.5s\tremaining: 1m 3s\n",
            "367:\tlearn: 80.8073055\ttotal: 54.6s\tremaining: 1m 2s\n",
            "368:\tlearn: 80.5829305\ttotal: 54.8s\tremaining: 1m 2s\n",
            "369:\tlearn: 80.5032450\ttotal: 54.9s\tremaining: 1m 2s\n",
            "370:\tlearn: 80.4290947\ttotal: 55.1s\tremaining: 1m 2s\n",
            "371:\tlearn: 80.3566669\ttotal: 55.2s\tremaining: 1m 2s\n",
            "372:\tlearn: 80.3463676\ttotal: 55.4s\tremaining: 1m 2s\n",
            "373:\tlearn: 80.3391543\ttotal: 55.6s\tremaining: 1m 2s\n",
            "374:\tlearn: 80.3019665\ttotal: 55.7s\tremaining: 1m 1s\n",
            "375:\tlearn: 80.1415042\ttotal: 55.9s\tremaining: 1m 1s\n",
            "376:\tlearn: 80.0930675\ttotal: 56s\tremaining: 1m 1s\n",
            "377:\tlearn: 79.8635684\ttotal: 56.2s\tremaining: 1m 1s\n",
            "378:\tlearn: 79.7182401\ttotal: 56.3s\tremaining: 1m 1s\n",
            "379:\tlearn: 79.6701442\ttotal: 56.5s\tremaining: 1m 1s\n",
            "380:\tlearn: 79.6593125\ttotal: 56.7s\tremaining: 1m 1s\n",
            "381:\tlearn: 79.3711191\ttotal: 56.8s\tremaining: 1m\n",
            "382:\tlearn: 79.3708890\ttotal: 56.9s\tremaining: 1m\n",
            "383:\tlearn: 79.2212887\ttotal: 57.1s\tremaining: 1m\n",
            "384:\tlearn: 79.1572183\ttotal: 57.3s\tremaining: 1m\n",
            "385:\tlearn: 79.0290135\ttotal: 57.4s\tremaining: 1m\n",
            "386:\tlearn: 79.0021538\ttotal: 57.6s\tremaining: 1m\n",
            "387:\tlearn: 78.8316389\ttotal: 57.7s\tremaining: 1m\n",
            "388:\tlearn: 78.7531418\ttotal: 57.9s\tremaining: 59.9s\n",
            "389:\tlearn: 78.6769840\ttotal: 58s\tremaining: 59.8s\n",
            "390:\tlearn: 78.5908695\ttotal: 58.2s\tremaining: 59.7s\n",
            "391:\tlearn: 78.5001028\ttotal: 58.3s\tremaining: 59.5s\n",
            "392:\tlearn: 78.3106683\ttotal: 58.5s\tremaining: 59.4s\n",
            "393:\tlearn: 78.1674443\ttotal: 58.6s\tremaining: 59.2s\n",
            "394:\tlearn: 78.0879649\ttotal: 58.8s\tremaining: 59.1s\n",
            "395:\tlearn: 77.9472816\ttotal: 58.9s\tremaining: 58.9s\n",
            "396:\tlearn: 77.8945595\ttotal: 59s\tremaining: 58.7s\n",
            "397:\tlearn: 77.7389903\ttotal: 59.2s\tremaining: 58.6s\n",
            "398:\tlearn: 77.6912064\ttotal: 59.4s\tremaining: 58.5s\n",
            "399:\tlearn: 77.6081640\ttotal: 59.5s\tremaining: 58.3s\n",
            "400:\tlearn: 77.5504878\ttotal: 59.7s\tremaining: 58.2s\n",
            "401:\tlearn: 77.4109501\ttotal: 59.8s\tremaining: 58s\n",
            "402:\tlearn: 77.3196474\ttotal: 60s\tremaining: 57.9s\n",
            "403:\tlearn: 77.2699435\ttotal: 1m\tremaining: 57.8s\n",
            "404:\tlearn: 77.1134408\ttotal: 1m\tremaining: 57.6s\n",
            "405:\tlearn: 77.0955902\ttotal: 1m\tremaining: 57.5s\n",
            "406:\tlearn: 77.0023766\ttotal: 1m\tremaining: 57.3s\n",
            "407:\tlearn: 76.9323248\ttotal: 1m\tremaining: 57.2s\n",
            "408:\tlearn: 76.8662924\ttotal: 1m\tremaining: 57.1s\n",
            "409:\tlearn: 76.8179676\ttotal: 1m 1s\tremaining: 56.9s\n",
            "410:\tlearn: 76.7931142\ttotal: 1m 1s\tremaining: 56.8s\n",
            "411:\tlearn: 76.7353676\ttotal: 1m 1s\tremaining: 56.6s\n",
            "412:\tlearn: 76.4329389\ttotal: 1m 1s\tremaining: 56.5s\n",
            "413:\tlearn: 76.3542210\ttotal: 1m 1s\tremaining: 56.3s\n",
            "414:\tlearn: 76.1922303\ttotal: 1m 1s\tremaining: 56.2s\n",
            "415:\tlearn: 76.1032143\ttotal: 1m 1s\tremaining: 56s\n",
            "416:\tlearn: 75.9842247\ttotal: 1m 2s\tremaining: 55.9s\n",
            "417:\tlearn: 75.9338427\ttotal: 1m 2s\tremaining: 55.7s\n",
            "418:\tlearn: 75.8551632\ttotal: 1m 2s\tremaining: 55.6s\n",
            "419:\tlearn: 75.8545974\ttotal: 1m 2s\tremaining: 55.4s\n",
            "420:\tlearn: 75.6751904\ttotal: 1m 2s\tremaining: 55.3s\n",
            "421:\tlearn: 75.6226158\ttotal: 1m 2s\tremaining: 55.1s\n",
            "422:\tlearn: 75.4507935\ttotal: 1m 3s\tremaining: 55s\n",
            "423:\tlearn: 75.3535551\ttotal: 1m 3s\tremaining: 54.8s\n",
            "424:\tlearn: 75.2371487\ttotal: 1m 3s\tremaining: 54.7s\n",
            "425:\tlearn: 75.2038443\ttotal: 1m 3s\tremaining: 54.5s\n",
            "426:\tlearn: 75.1470321\ttotal: 1m 3s\tremaining: 54.4s\n",
            "427:\tlearn: 75.1426272\ttotal: 1m 3s\tremaining: 54.3s\n",
            "428:\tlearn: 74.9048615\ttotal: 1m 3s\tremaining: 54.1s\n",
            "429:\tlearn: 74.6985741\ttotal: 1m 4s\tremaining: 54s\n",
            "430:\tlearn: 74.6744885\ttotal: 1m 4s\tremaining: 53.8s\n",
            "431:\tlearn: 74.6253760\ttotal: 1m 4s\tremaining: 53.7s\n",
            "432:\tlearn: 74.4992442\ttotal: 1m 4s\tremaining: 53.5s\n",
            "433:\tlearn: 74.3327805\ttotal: 1m 4s\tremaining: 53.4s\n",
            "434:\tlearn: 74.2764786\ttotal: 1m 4s\tremaining: 53.2s\n",
            "435:\tlearn: 74.2079247\ttotal: 1m 4s\tremaining: 53.1s\n",
            "436:\tlearn: 73.9176289\ttotal: 1m 5s\tremaining: 52.9s\n",
            "437:\tlearn: 73.7254108\ttotal: 1m 5s\tremaining: 52.8s\n",
            "438:\tlearn: 73.6783372\ttotal: 1m 5s\tremaining: 52.6s\n",
            "439:\tlearn: 73.6329615\ttotal: 1m 5s\tremaining: 52.5s\n",
            "440:\tlearn: 73.6121525\ttotal: 1m 5s\tremaining: 52.3s\n",
            "441:\tlearn: 73.5657866\ttotal: 1m 5s\tremaining: 52.2s\n",
            "442:\tlearn: 73.4946116\ttotal: 1m 6s\tremaining: 52.1s\n",
            "443:\tlearn: 73.4621464\ttotal: 1m 6s\tremaining: 51.9s\n",
            "444:\tlearn: 73.3976671\ttotal: 1m 6s\tremaining: 51.8s\n",
            "445:\tlearn: 73.3572239\ttotal: 1m 6s\tremaining: 51.6s\n",
            "446:\tlearn: 73.1903999\ttotal: 1m 6s\tremaining: 51.5s\n",
            "447:\tlearn: 73.0182414\ttotal: 1m 6s\tremaining: 51.3s\n",
            "448:\tlearn: 72.8647781\ttotal: 1m 6s\tremaining: 51.2s\n",
            "449:\tlearn: 72.7852901\ttotal: 1m 7s\tremaining: 51s\n",
            "450:\tlearn: 72.7841354\ttotal: 1m 7s\tremaining: 50.9s\n",
            "451:\tlearn: 72.6675969\ttotal: 1m 7s\tremaining: 50.7s\n",
            "452:\tlearn: 72.5646887\ttotal: 1m 7s\tremaining: 50.6s\n",
            "453:\tlearn: 72.5087819\ttotal: 1m 7s\tremaining: 50.4s\n",
            "454:\tlearn: 72.4322609\ttotal: 1m 7s\tremaining: 50.3s\n",
            "455:\tlearn: 72.4047128\ttotal: 1m 8s\tremaining: 50.1s\n",
            "456:\tlearn: 72.2911469\ttotal: 1m 8s\tremaining: 50s\n",
            "457:\tlearn: 72.2341047\ttotal: 1m 8s\tremaining: 49.9s\n",
            "458:\tlearn: 72.1604322\ttotal: 1m 8s\tremaining: 49.7s\n",
            "459:\tlearn: 72.0860090\ttotal: 1m 8s\tremaining: 49.6s\n",
            "460:\tlearn: 72.0653624\ttotal: 1m 8s\tremaining: 49.4s\n",
            "461:\tlearn: 72.0014998\ttotal: 1m 9s\tremaining: 49.3s\n",
            "462:\tlearn: 71.8892406\ttotal: 1m 9s\tremaining: 49.1s\n",
            "463:\tlearn: 71.8674452\ttotal: 1m 9s\tremaining: 49s\n",
            "464:\tlearn: 71.8291036\ttotal: 1m 9s\tremaining: 48.9s\n",
            "465:\tlearn: 71.7623627\ttotal: 1m 9s\tremaining: 48.7s\n",
            "466:\tlearn: 71.7237926\ttotal: 1m 9s\tremaining: 48.6s\n",
            "467:\tlearn: 71.6226351\ttotal: 1m 9s\tremaining: 48.4s\n",
            "468:\tlearn: 71.4020699\ttotal: 1m 10s\tremaining: 48.3s\n",
            "469:\tlearn: 71.3622927\ttotal: 1m 10s\tremaining: 48.1s\n",
            "470:\tlearn: 71.3223796\ttotal: 1m 10s\tremaining: 48s\n",
            "471:\tlearn: 71.2002544\ttotal: 1m 10s\tremaining: 47.8s\n",
            "472:\tlearn: 71.1760813\ttotal: 1m 10s\tremaining: 47.7s\n",
            "473:\tlearn: 71.1384927\ttotal: 1m 10s\tremaining: 47.6s\n",
            "474:\tlearn: 71.0521628\ttotal: 1m 11s\tremaining: 47.4s\n",
            "475:\tlearn: 71.0121390\ttotal: 1m 11s\tremaining: 47.3s\n",
            "476:\tlearn: 70.9718612\ttotal: 1m 11s\tremaining: 47.1s\n",
            "477:\tlearn: 70.8114934\ttotal: 1m 11s\tremaining: 47s\n",
            "478:\tlearn: 70.8107858\ttotal: 1m 11s\tremaining: 46.8s\n",
            "479:\tlearn: 70.7801866\ttotal: 1m 11s\tremaining: 46.7s\n",
            "480:\tlearn: 70.7079399\ttotal: 1m 11s\tremaining: 46.5s\n",
            "481:\tlearn: 70.4784133\ttotal: 1m 12s\tremaining: 46.4s\n",
            "482:\tlearn: 70.3629719\ttotal: 1m 12s\tremaining: 46.2s\n",
            "483:\tlearn: 70.3251947\ttotal: 1m 12s\tremaining: 46.1s\n",
            "484:\tlearn: 70.2525605\ttotal: 1m 12s\tremaining: 45.9s\n",
            "485:\tlearn: 70.0142244\ttotal: 1m 12s\tremaining: 45.8s\n",
            "486:\tlearn: 69.9709311\ttotal: 1m 12s\tremaining: 45.6s\n",
            "487:\tlearn: 69.9471180\ttotal: 1m 13s\tremaining: 45.5s\n",
            "488:\tlearn: 69.8211232\ttotal: 1m 13s\tremaining: 45.3s\n",
            "489:\tlearn: 69.7963942\ttotal: 1m 13s\tremaining: 45.2s\n",
            "490:\tlearn: 69.7956150\ttotal: 1m 13s\tremaining: 45.1s\n",
            "491:\tlearn: 69.6925226\ttotal: 1m 13s\tremaining: 44.9s\n",
            "492:\tlearn: 69.6748408\ttotal: 1m 13s\tremaining: 44.8s\n",
            "493:\tlearn: 69.5886053\ttotal: 1m 13s\tremaining: 44.6s\n",
            "494:\tlearn: 69.5415871\ttotal: 1m 14s\tremaining: 44.5s\n",
            "495:\tlearn: 69.4973343\ttotal: 1m 14s\tremaining: 44.3s\n",
            "496:\tlearn: 69.2762945\ttotal: 1m 14s\tremaining: 44.2s\n",
            "497:\tlearn: 69.2289063\ttotal: 1m 14s\tremaining: 44s\n",
            "498:\tlearn: 69.1990859\ttotal: 1m 14s\tremaining: 43.9s\n",
            "499:\tlearn: 69.1750306\ttotal: 1m 14s\tremaining: 43.7s\n",
            "500:\tlearn: 69.1501525\ttotal: 1m 15s\tremaining: 43.6s\n",
            "501:\tlearn: 69.1120847\ttotal: 1m 15s\tremaining: 43.4s\n",
            "502:\tlearn: 69.0548332\ttotal: 1m 15s\tremaining: 43.3s\n",
            "503:\tlearn: 68.9978371\ttotal: 1m 15s\tremaining: 43.1s\n",
            "504:\tlearn: 68.8088272\ttotal: 1m 15s\tremaining: 43s\n",
            "505:\tlearn: 68.7669479\ttotal: 1m 15s\tremaining: 42.8s\n",
            "506:\tlearn: 68.6804046\ttotal: 1m 15s\tremaining: 42.7s\n",
            "507:\tlearn: 68.6326751\ttotal: 1m 16s\tremaining: 42.5s\n",
            "508:\tlearn: 68.6157660\ttotal: 1m 16s\tremaining: 42.4s\n",
            "509:\tlearn: 68.5609704\ttotal: 1m 16s\tremaining: 42.3s\n",
            "510:\tlearn: 68.5382284\ttotal: 1m 16s\tremaining: 42.1s\n",
            "511:\tlearn: 68.5138307\ttotal: 1m 16s\tremaining: 42s\n",
            "512:\tlearn: 68.4585237\ttotal: 1m 16s\tremaining: 41.8s\n",
            "513:\tlearn: 68.2404133\ttotal: 1m 17s\tremaining: 41.7s\n",
            "514:\tlearn: 68.1493098\ttotal: 1m 17s\tremaining: 41.5s\n",
            "515:\tlearn: 68.1059037\ttotal: 1m 17s\tremaining: 41.4s\n",
            "516:\tlearn: 68.0831977\ttotal: 1m 17s\tremaining: 41.2s\n",
            "517:\tlearn: 68.0181853\ttotal: 1m 17s\tremaining: 41.1s\n",
            "518:\tlearn: 67.9821123\ttotal: 1m 17s\tremaining: 40.9s\n",
            "519:\tlearn: 67.9538091\ttotal: 1m 17s\tremaining: 40.8s\n",
            "520:\tlearn: 67.8025979\ttotal: 1m 18s\tremaining: 40.6s\n",
            "521:\tlearn: 67.7297634\ttotal: 1m 18s\tremaining: 40.5s\n",
            "522:\tlearn: 67.6875207\ttotal: 1m 18s\tremaining: 40.3s\n",
            "523:\tlearn: 67.6360405\ttotal: 1m 18s\tremaining: 40.2s\n",
            "524:\tlearn: 67.5343274\ttotal: 1m 18s\tremaining: 40s\n",
            "525:\tlearn: 67.4953206\ttotal: 1m 18s\tremaining: 39.9s\n",
            "526:\tlearn: 67.3462740\ttotal: 1m 19s\tremaining: 39.7s\n",
            "527:\tlearn: 67.3003277\ttotal: 1m 19s\tremaining: 39.6s\n",
            "528:\tlearn: 67.2293263\ttotal: 1m 19s\tremaining: 39.4s\n",
            "529:\tlearn: 67.1608158\ttotal: 1m 19s\tremaining: 39.3s\n",
            "530:\tlearn: 67.1358010\ttotal: 1m 19s\tremaining: 39.1s\n",
            "531:\tlearn: 66.9947506\ttotal: 1m 19s\tremaining: 39s\n",
            "532:\tlearn: 66.9768343\ttotal: 1m 19s\tremaining: 38.8s\n",
            "533:\tlearn: 66.8466582\ttotal: 1m 20s\tremaining: 38.7s\n",
            "534:\tlearn: 66.7778280\ttotal: 1m 20s\tremaining: 38.5s\n",
            "535:\tlearn: 66.6881391\ttotal: 1m 20s\tremaining: 38.4s\n",
            "536:\tlearn: 66.6752683\ttotal: 1m 20s\tremaining: 38.2s\n",
            "537:\tlearn: 66.5858307\ttotal: 1m 20s\tremaining: 38.1s\n",
            "538:\tlearn: 66.5665373\ttotal: 1m 20s\tremaining: 37.9s\n",
            "539:\tlearn: 66.4877652\ttotal: 1m 20s\tremaining: 37.8s\n",
            "540:\tlearn: 66.3927651\ttotal: 1m 21s\tremaining: 37.6s\n",
            "541:\tlearn: 66.2973214\ttotal: 1m 21s\tremaining: 37.5s\n",
            "542:\tlearn: 66.2325578\ttotal: 1m 21s\tremaining: 37.3s\n",
            "543:\tlearn: 66.2064142\ttotal: 1m 21s\tremaining: 37.2s\n",
            "544:\tlearn: 66.1100224\ttotal: 1m 21s\tremaining: 37s\n",
            "545:\tlearn: 66.1084607\ttotal: 1m 21s\tremaining: 36.9s\n",
            "546:\tlearn: 66.0704686\ttotal: 1m 22s\tremaining: 36.8s\n",
            "547:\tlearn: 66.0433948\ttotal: 1m 22s\tremaining: 36.6s\n",
            "548:\tlearn: 66.0077423\ttotal: 1m 22s\tremaining: 36.5s\n",
            "549:\tlearn: 65.9539043\ttotal: 1m 22s\tremaining: 36.3s\n",
            "550:\tlearn: 65.8838248\ttotal: 1m 22s\tremaining: 36.2s\n",
            "551:\tlearn: 65.8095263\ttotal: 1m 22s\tremaining: 36s\n",
            "552:\tlearn: 65.5912784\ttotal: 1m 22s\tremaining: 35.9s\n",
            "553:\tlearn: 65.5381376\ttotal: 1m 23s\tremaining: 35.7s\n",
            "554:\tlearn: 65.5152868\ttotal: 1m 23s\tremaining: 35.6s\n",
            "555:\tlearn: 65.4591637\ttotal: 1m 23s\tremaining: 35.4s\n",
            "556:\tlearn: 65.3470208\ttotal: 1m 23s\tremaining: 35.3s\n",
            "557:\tlearn: 65.2728688\ttotal: 1m 23s\tremaining: 35.1s\n",
            "558:\tlearn: 65.2152219\ttotal: 1m 23s\tremaining: 34.9s\n",
            "559:\tlearn: 65.0983516\ttotal: 1m 23s\tremaining: 34.8s\n",
            "560:\tlearn: 64.9557175\ttotal: 1m 24s\tremaining: 34.6s\n",
            "561:\tlearn: 64.9277968\ttotal: 1m 24s\tremaining: 34.5s\n",
            "562:\tlearn: 64.9080708\ttotal: 1m 24s\tremaining: 34.4s\n",
            "563:\tlearn: 64.8584024\ttotal: 1m 24s\tremaining: 34.2s\n",
            "564:\tlearn: 64.8337468\ttotal: 1m 24s\tremaining: 34.1s\n",
            "565:\tlearn: 64.8145597\ttotal: 1m 24s\tremaining: 33.9s\n",
            "566:\tlearn: 64.7642416\ttotal: 1m 25s\tremaining: 33.8s\n",
            "567:\tlearn: 64.7009259\ttotal: 1m 25s\tremaining: 33.6s\n",
            "568:\tlearn: 64.6328490\ttotal: 1m 25s\tremaining: 33.5s\n",
            "569:\tlearn: 64.3723389\ttotal: 1m 25s\tremaining: 33.3s\n",
            "570:\tlearn: 64.3171092\ttotal: 1m 25s\tremaining: 33.2s\n",
            "571:\tlearn: 64.2669726\ttotal: 1m 25s\tremaining: 33s\n",
            "572:\tlearn: 64.2313185\ttotal: 1m 26s\tremaining: 32.9s\n",
            "573:\tlearn: 64.1143079\ttotal: 1m 26s\tremaining: 32.7s\n",
            "574:\tlearn: 64.0750872\ttotal: 1m 26s\tremaining: 32.6s\n",
            "575:\tlearn: 64.0739909\ttotal: 1m 26s\tremaining: 32.4s\n",
            "576:\tlearn: 63.9882226\ttotal: 1m 26s\tremaining: 32.3s\n",
            "577:\tlearn: 63.8951252\ttotal: 1m 26s\tremaining: 32.1s\n",
            "578:\tlearn: 63.8231264\ttotal: 1m 26s\tremaining: 32s\n",
            "579:\tlearn: 63.6509290\ttotal: 1m 27s\tremaining: 31.8s\n",
            "580:\tlearn: 63.5597414\ttotal: 1m 27s\tremaining: 31.7s\n",
            "581:\tlearn: 63.5146243\ttotal: 1m 27s\tremaining: 31.5s\n",
            "582:\tlearn: 63.4488495\ttotal: 1m 27s\tremaining: 31.4s\n",
            "583:\tlearn: 63.3821708\ttotal: 1m 27s\tremaining: 31.2s\n",
            "584:\tlearn: 63.3289882\ttotal: 1m 27s\tremaining: 31.1s\n",
            "585:\tlearn: 63.2949480\ttotal: 1m 27s\tremaining: 30.9s\n",
            "586:\tlearn: 63.1947783\ttotal: 1m 28s\tremaining: 30.8s\n",
            "587:\tlearn: 63.1648244\ttotal: 1m 28s\tremaining: 30.6s\n",
            "588:\tlearn: 63.0680305\ttotal: 1m 28s\tremaining: 30.5s\n",
            "589:\tlearn: 62.9403196\ttotal: 1m 28s\tremaining: 30.3s\n",
            "590:\tlearn: 62.9207603\ttotal: 1m 28s\tremaining: 30.2s\n",
            "591:\tlearn: 62.8695981\ttotal: 1m 28s\tremaining: 30s\n",
            "592:\tlearn: 62.8188905\ttotal: 1m 29s\tremaining: 29.9s\n",
            "593:\tlearn: 62.7720614\ttotal: 1m 29s\tremaining: 29.7s\n",
            "594:\tlearn: 62.7337453\ttotal: 1m 29s\tremaining: 29.6s\n",
            "595:\tlearn: 62.7223078\ttotal: 1m 29s\tremaining: 29.4s\n",
            "596:\tlearn: 62.6894447\ttotal: 1m 29s\tremaining: 29.3s\n",
            "597:\tlearn: 62.6726844\ttotal: 1m 29s\tremaining: 29.1s\n",
            "598:\tlearn: 62.6250489\ttotal: 1m 30s\tremaining: 29s\n",
            "599:\tlearn: 62.5303688\ttotal: 1m 30s\tremaining: 28.9s\n",
            "600:\tlearn: 62.5193631\ttotal: 1m 30s\tremaining: 28.7s\n",
            "601:\tlearn: 62.4888299\ttotal: 1m 30s\tremaining: 28.6s\n",
            "602:\tlearn: 62.4657141\ttotal: 1m 30s\tremaining: 28.4s\n",
            "603:\tlearn: 62.4318199\ttotal: 1m 30s\tremaining: 28.3s\n",
            "604:\tlearn: 62.4146547\ttotal: 1m 30s\tremaining: 28.1s\n",
            "605:\tlearn: 62.2002559\ttotal: 1m 31s\tremaining: 28s\n",
            "606:\tlearn: 62.1538250\ttotal: 1m 31s\tremaining: 27.8s\n",
            "607:\tlearn: 62.1348974\ttotal: 1m 31s\tremaining: 27.7s\n",
            "608:\tlearn: 62.1164705\ttotal: 1m 31s\tremaining: 27.5s\n",
            "609:\tlearn: 62.0453240\ttotal: 1m 31s\tremaining: 27.4s\n",
            "610:\tlearn: 62.0035492\ttotal: 1m 31s\tremaining: 27.2s\n",
            "611:\tlearn: 61.9656195\ttotal: 1m 32s\tremaining: 27.1s\n",
            "612:\tlearn: 61.9228726\ttotal: 1m 32s\tremaining: 26.9s\n",
            "613:\tlearn: 61.8584799\ttotal: 1m 32s\tremaining: 26.8s\n",
            "614:\tlearn: 61.7683886\ttotal: 1m 32s\tremaining: 26.6s\n",
            "615:\tlearn: 61.7050312\ttotal: 1m 32s\tremaining: 26.5s\n",
            "616:\tlearn: 61.6664991\ttotal: 1m 32s\tremaining: 26.3s\n",
            "617:\tlearn: 61.6361800\ttotal: 1m 32s\tremaining: 26.2s\n",
            "618:\tlearn: 61.5395575\ttotal: 1m 33s\tremaining: 26s\n",
            "619:\tlearn: 61.5241631\ttotal: 1m 33s\tremaining: 25.9s\n",
            "620:\tlearn: 61.4825976\ttotal: 1m 33s\tremaining: 25.7s\n",
            "621:\tlearn: 61.4804631\ttotal: 1m 33s\tremaining: 25.6s\n",
            "622:\tlearn: 61.4426674\ttotal: 1m 33s\tremaining: 25.4s\n",
            "623:\tlearn: 61.3288137\ttotal: 1m 33s\tremaining: 25.3s\n",
            "624:\tlearn: 61.2098345\ttotal: 1m 33s\tremaining: 25.1s\n",
            "625:\tlearn: 61.1115458\ttotal: 1m 34s\tremaining: 25s\n",
            "626:\tlearn: 61.0797924\ttotal: 1m 34s\tremaining: 24.8s\n",
            "627:\tlearn: 61.0465758\ttotal: 1m 34s\tremaining: 24.7s\n",
            "628:\tlearn: 60.9643590\ttotal: 1m 34s\tremaining: 24.5s\n",
            "629:\tlearn: 60.9146124\ttotal: 1m 34s\tremaining: 24.4s\n",
            "630:\tlearn: 60.8774540\ttotal: 1m 34s\tremaining: 24.2s\n",
            "631:\tlearn: 60.7684066\ttotal: 1m 35s\tremaining: 24.1s\n",
            "632:\tlearn: 60.7284830\ttotal: 1m 35s\tremaining: 23.9s\n",
            "633:\tlearn: 60.6961272\ttotal: 1m 35s\tremaining: 23.8s\n",
            "634:\tlearn: 60.5773853\ttotal: 1m 35s\tremaining: 23.6s\n",
            "635:\tlearn: 60.5408047\ttotal: 1m 35s\tremaining: 23.5s\n",
            "636:\tlearn: 60.5269213\ttotal: 1m 35s\tremaining: 23.3s\n",
            "637:\tlearn: 60.4812002\ttotal: 1m 35s\tremaining: 23.2s\n",
            "638:\tlearn: 60.4308167\ttotal: 1m 36s\tremaining: 23s\n",
            "639:\tlearn: 60.4109097\ttotal: 1m 36s\tremaining: 22.9s\n",
            "640:\tlearn: 60.3829123\ttotal: 1m 36s\tremaining: 22.7s\n",
            "641:\tlearn: 60.3528786\ttotal: 1m 36s\tremaining: 22.6s\n",
            "642:\tlearn: 60.3250400\ttotal: 1m 36s\tremaining: 22.4s\n",
            "643:\tlearn: 60.2824621\ttotal: 1m 36s\tremaining: 22.3s\n",
            "644:\tlearn: 60.2631550\ttotal: 1m 37s\tremaining: 22.1s\n",
            "645:\tlearn: 60.1860704\ttotal: 1m 37s\tremaining: 22s\n",
            "646:\tlearn: 60.1703842\ttotal: 1m 37s\tremaining: 21.8s\n",
            "647:\tlearn: 60.1299593\ttotal: 1m 37s\tremaining: 21.7s\n",
            "648:\tlearn: 60.0608565\ttotal: 1m 37s\tremaining: 21.5s\n",
            "649:\tlearn: 60.0355031\ttotal: 1m 37s\tremaining: 21.4s\n",
            "650:\tlearn: 59.9838783\ttotal: 1m 37s\tremaining: 21.2s\n",
            "651:\tlearn: 59.9415225\ttotal: 1m 38s\tremaining: 21.1s\n",
            "652:\tlearn: 59.9274380\ttotal: 1m 38s\tremaining: 20.9s\n",
            "653:\tlearn: 59.8672531\ttotal: 1m 38s\tremaining: 20.8s\n",
            "654:\tlearn: 59.8667785\ttotal: 1m 38s\tremaining: 20.6s\n",
            "655:\tlearn: 59.8512061\ttotal: 1m 38s\tremaining: 20.5s\n",
            "656:\tlearn: 59.8253108\ttotal: 1m 38s\tremaining: 20.3s\n",
            "657:\tlearn: 59.8015867\ttotal: 1m 39s\tremaining: 20.2s\n",
            "658:\tlearn: 59.7194894\ttotal: 1m 39s\tremaining: 20s\n",
            "659:\tlearn: 59.6772114\ttotal: 1m 39s\tremaining: 19.9s\n",
            "660:\tlearn: 59.6466313\ttotal: 1m 39s\tremaining: 19.7s\n",
            "661:\tlearn: 59.5997519\ttotal: 1m 39s\tremaining: 19.6s\n",
            "662:\tlearn: 59.5823948\ttotal: 1m 39s\tremaining: 19.4s\n",
            "663:\tlearn: 59.5064813\ttotal: 1m 39s\tremaining: 19.3s\n",
            "664:\tlearn: 59.4929095\ttotal: 1m 40s\tremaining: 19.1s\n",
            "665:\tlearn: 59.3843621\ttotal: 1m 40s\tremaining: 19s\n",
            "666:\tlearn: 59.3661954\ttotal: 1m 40s\tremaining: 18.8s\n",
            "667:\tlearn: 59.2796795\ttotal: 1m 40s\tremaining: 18.7s\n",
            "668:\tlearn: 59.2505295\ttotal: 1m 40s\tremaining: 18.5s\n",
            "669:\tlearn: 59.1856755\ttotal: 1m 40s\tremaining: 18.3s\n",
            "670:\tlearn: 59.1663057\ttotal: 1m 40s\tremaining: 18.2s\n",
            "671:\tlearn: 59.1379062\ttotal: 1m 41s\tremaining: 18s\n",
            "672:\tlearn: 59.0857343\ttotal: 1m 41s\tremaining: 17.9s\n",
            "673:\tlearn: 59.0345739\ttotal: 1m 41s\tremaining: 17.7s\n",
            "674:\tlearn: 59.0165295\ttotal: 1m 41s\tremaining: 17.6s\n",
            "675:\tlearn: 59.0042328\ttotal: 1m 41s\tremaining: 17.4s\n",
            "676:\tlearn: 58.9784965\ttotal: 1m 41s\tremaining: 17.3s\n",
            "677:\tlearn: 58.8972944\ttotal: 1m 41s\tremaining: 17.1s\n",
            "678:\tlearn: 58.8544425\ttotal: 1m 42s\tremaining: 17s\n",
            "679:\tlearn: 58.8089180\ttotal: 1m 42s\tremaining: 16.8s\n",
            "680:\tlearn: 58.7817486\ttotal: 1m 42s\tremaining: 16.7s\n",
            "681:\tlearn: 58.7475033\ttotal: 1m 42s\tremaining: 16.5s\n",
            "682:\tlearn: 58.6780441\ttotal: 1m 42s\tremaining: 16.4s\n",
            "683:\tlearn: 58.6518705\ttotal: 1m 42s\tremaining: 16.2s\n",
            "684:\tlearn: 58.6119065\ttotal: 1m 43s\tremaining: 16.1s\n",
            "685:\tlearn: 58.5327653\ttotal: 1m 43s\tremaining: 15.9s\n",
            "686:\tlearn: 58.4302607\ttotal: 1m 43s\tremaining: 15.8s\n",
            "687:\tlearn: 58.4063456\ttotal: 1m 43s\tremaining: 15.6s\n",
            "688:\tlearn: 58.3571522\ttotal: 1m 43s\tremaining: 15.5s\n",
            "689:\tlearn: 58.2978310\ttotal: 1m 43s\tremaining: 15.3s\n",
            "690:\tlearn: 58.2384520\ttotal: 1m 43s\tremaining: 15.2s\n",
            "691:\tlearn: 58.1591973\ttotal: 1m 44s\tremaining: 15s\n",
            "692:\tlearn: 57.9539144\ttotal: 1m 44s\tremaining: 14.9s\n",
            "693:\tlearn: 57.9229674\ttotal: 1m 44s\tremaining: 14.7s\n",
            "694:\tlearn: 57.8920378\ttotal: 1m 44s\tremaining: 14.6s\n",
            "695:\tlearn: 57.8450812\ttotal: 1m 44s\tremaining: 14.4s\n",
            "696:\tlearn: 57.8120298\ttotal: 1m 44s\tremaining: 14.3s\n",
            "697:\tlearn: 57.7484224\ttotal: 1m 44s\tremaining: 14.1s\n",
            "698:\tlearn: 57.7360015\ttotal: 1m 45s\tremaining: 14s\n",
            "699:\tlearn: 57.7126198\ttotal: 1m 45s\tremaining: 13.8s\n",
            "700:\tlearn: 57.6792203\ttotal: 1m 45s\tremaining: 13.7s\n",
            "701:\tlearn: 57.6347351\ttotal: 1m 45s\tremaining: 13.5s\n",
            "702:\tlearn: 57.5832070\ttotal: 1m 45s\tremaining: 13.4s\n",
            "703:\tlearn: 57.5406867\ttotal: 1m 45s\tremaining: 13.2s\n",
            "704:\tlearn: 57.5156240\ttotal: 1m 46s\tremaining: 13.1s\n",
            "705:\tlearn: 57.4346694\ttotal: 1m 46s\tremaining: 12.9s\n",
            "706:\tlearn: 57.3538091\ttotal: 1m 46s\tremaining: 12.8s\n",
            "707:\tlearn: 57.3362635\ttotal: 1m 46s\tremaining: 12.6s\n",
            "708:\tlearn: 57.2890571\ttotal: 1m 46s\tremaining: 12.5s\n",
            "709:\tlearn: 57.2560761\ttotal: 1m 46s\tremaining: 12.3s\n",
            "710:\tlearn: 57.2147830\ttotal: 1m 46s\tremaining: 12.2s\n",
            "711:\tlearn: 57.1581585\ttotal: 1m 47s\tremaining: 12s\n",
            "712:\tlearn: 57.0934106\ttotal: 1m 47s\tremaining: 11.9s\n",
            "713:\tlearn: 57.0696193\ttotal: 1m 47s\tremaining: 11.7s\n",
            "714:\tlearn: 57.0346079\ttotal: 1m 47s\tremaining: 11.6s\n",
            "715:\tlearn: 56.9472049\ttotal: 1m 47s\tremaining: 11.4s\n",
            "716:\tlearn: 56.9355978\ttotal: 1m 47s\tremaining: 11.3s\n",
            "717:\tlearn: 56.8498456\ttotal: 1m 47s\tremaining: 11.1s\n",
            "718:\tlearn: 56.8210261\ttotal: 1m 48s\tremaining: 11s\n",
            "719:\tlearn: 56.7870798\ttotal: 1m 48s\tremaining: 10.8s\n",
            "720:\tlearn: 56.7223674\ttotal: 1m 48s\tremaining: 10.7s\n",
            "721:\tlearn: 56.6808196\ttotal: 1m 48s\tremaining: 10.5s\n",
            "722:\tlearn: 56.6589115\ttotal: 1m 48s\tremaining: 10.4s\n",
            "723:\tlearn: 56.6210889\ttotal: 1m 48s\tremaining: 10.2s\n",
            "724:\tlearn: 56.5844718\ttotal: 1m 49s\tremaining: 10.1s\n",
            "725:\tlearn: 56.4734088\ttotal: 1m 49s\tremaining: 9.92s\n",
            "726:\tlearn: 56.4530539\ttotal: 1m 49s\tremaining: 9.78s\n",
            "727:\tlearn: 56.4070583\ttotal: 1m 49s\tremaining: 9.63s\n",
            "728:\tlearn: 56.3197862\ttotal: 1m 49s\tremaining: 9.47s\n",
            "729:\tlearn: 56.2780683\ttotal: 1m 49s\tremaining: 9.32s\n",
            "730:\tlearn: 56.2763798\ttotal: 1m 49s\tremaining: 9.17s\n",
            "731:\tlearn: 56.0948157\ttotal: 1m 50s\tremaining: 9.02s\n",
            "732:\tlearn: 56.0788425\ttotal: 1m 50s\tremaining: 8.87s\n",
            "733:\tlearn: 56.0396974\ttotal: 1m 50s\tremaining: 8.72s\n",
            "734:\tlearn: 56.0076756\ttotal: 1m 50s\tremaining: 8.57s\n",
            "735:\tlearn: 55.9901515\ttotal: 1m 50s\tremaining: 8.42s\n",
            "736:\tlearn: 55.9548496\ttotal: 1m 50s\tremaining: 8.27s\n",
            "737:\tlearn: 55.9366282\ttotal: 1m 51s\tremaining: 8.13s\n",
            "738:\tlearn: 55.9112734\ttotal: 1m 51s\tremaining: 7.98s\n",
            "739:\tlearn: 55.8253201\ttotal: 1m 51s\tremaining: 7.83s\n",
            "740:\tlearn: 55.7318036\ttotal: 1m 51s\tremaining: 7.67s\n",
            "741:\tlearn: 55.6360196\ttotal: 1m 51s\tremaining: 7.53s\n",
            "742:\tlearn: 55.5928491\ttotal: 1m 51s\tremaining: 7.37s\n",
            "743:\tlearn: 55.5759645\ttotal: 1m 51s\tremaining: 7.22s\n",
            "744:\tlearn: 55.5419556\ttotal: 1m 52s\tremaining: 7.07s\n",
            "745:\tlearn: 55.4849999\ttotal: 1m 52s\tremaining: 6.92s\n",
            "746:\tlearn: 55.4593454\ttotal: 1m 52s\tremaining: 6.77s\n",
            "747:\tlearn: 55.4344739\ttotal: 1m 52s\tremaining: 6.62s\n",
            "748:\tlearn: 55.3462059\ttotal: 1m 52s\tremaining: 6.47s\n",
            "749:\tlearn: 55.3060255\ttotal: 1m 52s\tremaining: 6.32s\n",
            "750:\tlearn: 55.2683837\ttotal: 1m 53s\tremaining: 6.17s\n",
            "751:\tlearn: 55.2370368\ttotal: 1m 53s\tremaining: 6.02s\n",
            "752:\tlearn: 55.2037808\ttotal: 1m 53s\tremaining: 5.87s\n",
            "753:\tlearn: 55.1671962\ttotal: 1m 53s\tremaining: 5.72s\n",
            "754:\tlearn: 55.1344308\ttotal: 1m 53s\tremaining: 5.57s\n",
            "755:\tlearn: 55.0434004\ttotal: 1m 53s\tremaining: 5.42s\n",
            "756:\tlearn: 54.9528650\ttotal: 1m 53s\tremaining: 5.27s\n",
            "757:\tlearn: 54.9247260\ttotal: 1m 54s\tremaining: 5.12s\n",
            "758:\tlearn: 54.8892476\ttotal: 1m 54s\tremaining: 4.96s\n",
            "759:\tlearn: 54.8480123\ttotal: 1m 54s\tremaining: 4.81s\n",
            "760:\tlearn: 54.7989190\ttotal: 1m 54s\tremaining: 4.66s\n",
            "761:\tlearn: 54.7250160\ttotal: 1m 54s\tremaining: 4.51s\n",
            "762:\tlearn: 54.7005795\ttotal: 1m 54s\tremaining: 4.36s\n",
            "763:\tlearn: 54.6726342\ttotal: 1m 54s\tremaining: 4.21s\n",
            "764:\tlearn: 54.5991405\ttotal: 1m 55s\tremaining: 4.06s\n",
            "765:\tlearn: 54.5874012\ttotal: 1m 55s\tremaining: 3.91s\n",
            "766:\tlearn: 54.5433206\ttotal: 1m 55s\tremaining: 3.76s\n",
            "767:\tlearn: 54.4812856\ttotal: 1m 55s\tremaining: 3.61s\n",
            "768:\tlearn: 54.4168004\ttotal: 1m 55s\tremaining: 3.46s\n",
            "769:\tlearn: 54.3710377\ttotal: 1m 55s\tremaining: 3.31s\n",
            "770:\tlearn: 54.3277668\ttotal: 1m 55s\tremaining: 3.16s\n",
            "771:\tlearn: 54.3048673\ttotal: 1m 56s\tremaining: 3.01s\n",
            "772:\tlearn: 54.2577509\ttotal: 1m 56s\tremaining: 2.86s\n",
            "773:\tlearn: 54.1802684\ttotal: 1m 56s\tremaining: 2.71s\n",
            "774:\tlearn: 54.1326787\ttotal: 1m 56s\tremaining: 2.56s\n",
            "775:\tlearn: 54.0830190\ttotal: 1m 56s\tremaining: 2.41s\n",
            "776:\tlearn: 54.0552940\ttotal: 1m 56s\tremaining: 2.26s\n",
            "777:\tlearn: 54.0304038\ttotal: 1m 57s\tremaining: 2.11s\n",
            "778:\tlearn: 53.9917166\ttotal: 1m 57s\tremaining: 1.96s\n",
            "779:\tlearn: 53.9722685\ttotal: 1m 57s\tremaining: 1.8s\n",
            "780:\tlearn: 53.9033459\ttotal: 1m 57s\tremaining: 1.66s\n",
            "781:\tlearn: 53.8664348\ttotal: 1m 57s\tremaining: 1.5s\n",
            "782:\tlearn: 53.8228746\ttotal: 1m 57s\tremaining: 1.35s\n",
            "783:\tlearn: 53.7858978\ttotal: 1m 58s\tremaining: 1.2s\n",
            "784:\tlearn: 53.7710718\ttotal: 1m 58s\tremaining: 1.05s\n",
            "785:\tlearn: 53.6461301\ttotal: 1m 58s\tremaining: 903ms\n",
            "786:\tlearn: 53.6058188\ttotal: 1m 58s\tremaining: 753ms\n",
            "787:\tlearn: 53.5699031\ttotal: 1m 58s\tremaining: 602ms\n",
            "788:\tlearn: 53.5381477\ttotal: 1m 58s\tremaining: 452ms\n",
            "789:\tlearn: 53.4968932\ttotal: 1m 58s\tremaining: 301ms\n",
            "790:\tlearn: 53.4747725\ttotal: 1m 59s\tremaining: 151ms\n",
            "791:\tlearn: 53.4572627\ttotal: 1m 59s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 04:00:53,005]\u001b[0m A new study created in memory with name: no-name-52a19541-ec62-42c8-8ecc-c0e0a26c940c\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 04:02:52,475]\u001b[0m Trial 0 finished with value: 51.7893636644697 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 51.7893636644697.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:04:56,646]\u001b[0m Trial 1 finished with value: 47.890309680958346 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 47.890309680958346.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:06:59,321]\u001b[0m Trial 2 finished with value: 46.47266476434371 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:20:05,794]\u001b[0m Trial 3 finished with value: 76.818324755595 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:22:10,930]\u001b[0m Trial 4 finished with value: 55.521883975037014 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:39:10,617]\u001b[0m Trial 5 finished with value: 66.89882463877937 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:42:16,969]\u001b[0m Trial 6 finished with value: 44.76931509209572 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:45:16,453]\u001b[0m Trial 7 finished with value: 49.37052039590518 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:46:52,092]\u001b[0m Trial 8 finished with value: 68.84913884673526 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:06:49,587]\u001b[0m Trial 9 finished with value: 56.34296597184787 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:07:51,594]\u001b[0m Trial 10 finished with value: 80.2135405424639 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:09:23,997]\u001b[0m Trial 11 finished with value: 57.11747038357349 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:13:05,127]\u001b[0m Trial 12 finished with value: 48.80378894553733 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:15:26,415]\u001b[0m Trial 13 finished with value: 45.73475134979331 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:17:25,048]\u001b[0m Trial 14 finished with value: 63.4674509634417 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:18:38,817]\u001b[0m Trial 15 finished with value: 63.7197746479203 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:19:56,582]\u001b[0m Trial 16 finished with value: 66.69283825760108 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:22:25,795]\u001b[0m Trial 17 finished with value: 42.99479569369386 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 42.99479569369386.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:30:39,371]\u001b[0m Trial 18 finished with value: 51.47277960627142 and parameters: {'iterations': 838, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 87.60982739927447, 'od_wait': 86, 'leaf_estimation_iterations': 15, 'bagging_temperature': 55.57442205671075, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 42.99479569369386.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:33:53,286]\u001b[0m Trial 19 finished with value: 44.85393140095243 and parameters: {'iterations': 996, 'learning_rate': 0.18484933160405892, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 45.629127031175706, 'od_wait': 63, 'leaf_estimation_iterations': 17, 'bagging_temperature': 50.428094719702216, 'colsample_bylevel': 0.46751740730087066}. Best is trial 17 with value: 42.99479569369386.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 4: 42.99479569369386\n",
            "Best hyperparameters for fold 4: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 207.5462183\ttotal: 148ms\tremaining: 1m 56s\n",
            "1:\tlearn: 206.2793843\ttotal: 235ms\tremaining: 1m 32s\n",
            "2:\tlearn: 206.2774985\ttotal: 287ms\tremaining: 1m 15s\n",
            "3:\tlearn: 204.7584634\ttotal: 371ms\tremaining: 1m 13s\n",
            "4:\tlearn: 204.1595926\ttotal: 541ms\tremaining: 1m 25s\n",
            "5:\tlearn: 204.1418121\ttotal: 604ms\tremaining: 1m 19s\n",
            "6:\tlearn: 204.1093265\ttotal: 700ms\tremaining: 1m 18s\n",
            "7:\tlearn: 202.6380865\ttotal: 845ms\tremaining: 1m 22s\n",
            "8:\tlearn: 202.1132612\ttotal: 1.02s\tremaining: 1m 28s\n",
            "9:\tlearn: 201.9492653\ttotal: 1.1s\tremaining: 1m 26s\n",
            "10:\tlearn: 201.9431037\ttotal: 1.16s\tremaining: 1m 22s\n",
            "11:\tlearn: 201.9431037\ttotal: 1.21s\tremaining: 1m 18s\n",
            "12:\tlearn: 201.7805569\ttotal: 1.38s\tremaining: 1m 22s\n",
            "13:\tlearn: 201.7276653\ttotal: 1.45s\tremaining: 1m 20s\n",
            "14:\tlearn: 201.6003051\ttotal: 1.61s\tremaining: 1m 23s\n",
            "15:\tlearn: 201.0473364\ttotal: 1.76s\tremaining: 1m 25s\n",
            "16:\tlearn: 200.8695627\ttotal: 1.94s\tremaining: 1m 28s\n",
            "17:\tlearn: 200.7557947\ttotal: 2.02s\tremaining: 1m 27s\n",
            "18:\tlearn: 200.7245038\ttotal: 2.19s\tremaining: 1m 29s\n",
            "19:\tlearn: 200.6383023\ttotal: 2.33s\tremaining: 1m 29s\n",
            "20:\tlearn: 200.4611226\ttotal: 2.5s\tremaining: 1m 31s\n",
            "21:\tlearn: 200.1483520\ttotal: 2.67s\tremaining: 1m 33s\n",
            "22:\tlearn: 200.1354411\ttotal: 2.77s\tremaining: 1m 32s\n",
            "23:\tlearn: 199.8170020\ttotal: 2.92s\tremaining: 1m 33s\n",
            "24:\tlearn: 199.3498327\ttotal: 3.09s\tremaining: 1m 34s\n",
            "25:\tlearn: 199.2248663\ttotal: 3.26s\tremaining: 1m 36s\n",
            "26:\tlearn: 199.2248663\ttotal: 3.31s\tremaining: 1m 33s\n",
            "27:\tlearn: 198.9408717\ttotal: 3.47s\tremaining: 1m 34s\n",
            "28:\tlearn: 198.8351729\ttotal: 3.63s\tremaining: 1m 35s\n",
            "29:\tlearn: 198.8351695\ttotal: 3.68s\tremaining: 1m 33s\n",
            "30:\tlearn: 198.7721750\ttotal: 3.81s\tremaining: 1m 33s\n",
            "31:\tlearn: 198.6799692\ttotal: 3.98s\tremaining: 1m 34s\n",
            "32:\tlearn: 198.2863211\ttotal: 4.14s\tremaining: 1m 35s\n",
            "33:\tlearn: 198.2052423\ttotal: 4.23s\tremaining: 1m 34s\n",
            "34:\tlearn: 198.0718448\ttotal: 4.33s\tremaining: 1m 33s\n",
            "35:\tlearn: 197.3348876\ttotal: 4.4s\tremaining: 1m 32s\n",
            "36:\tlearn: 196.6597088\ttotal: 4.51s\tremaining: 1m 32s\n",
            "37:\tlearn: 194.8084120\ttotal: 4.67s\tremaining: 1m 32s\n",
            "38:\tlearn: 193.6421588\ttotal: 4.83s\tremaining: 1m 33s\n",
            "39:\tlearn: 192.8883522\ttotal: 4.96s\tremaining: 1m 33s\n",
            "40:\tlearn: 192.0891796\ttotal: 5.12s\tremaining: 1m 33s\n",
            "41:\tlearn: 190.5702000\ttotal: 5.26s\tremaining: 1m 33s\n",
            "42:\tlearn: 189.1541167\ttotal: 5.4s\tremaining: 1m 34s\n",
            "43:\tlearn: 188.5088820\ttotal: 5.55s\tremaining: 1m 34s\n",
            "44:\tlearn: 185.7162432\ttotal: 5.71s\tremaining: 1m 34s\n",
            "45:\tlearn: 185.0355572\ttotal: 5.85s\tremaining: 1m 34s\n",
            "46:\tlearn: 184.3849653\ttotal: 6.01s\tremaining: 1m 35s\n",
            "47:\tlearn: 183.9684108\ttotal: 6.17s\tremaining: 1m 35s\n",
            "48:\tlearn: 182.4375774\ttotal: 6.31s\tremaining: 1m 35s\n",
            "49:\tlearn: 181.1160130\ttotal: 6.48s\tremaining: 1m 36s\n",
            "50:\tlearn: 180.4135666\ttotal: 6.63s\tremaining: 1m 36s\n",
            "51:\tlearn: 179.9923688\ttotal: 6.78s\tremaining: 1m 36s\n",
            "52:\tlearn: 178.8088927\ttotal: 6.93s\tremaining: 1m 36s\n",
            "53:\tlearn: 175.8334264\ttotal: 7.08s\tremaining: 1m 36s\n",
            "54:\tlearn: 175.4143278\ttotal: 7.23s\tremaining: 1m 36s\n",
            "55:\tlearn: 174.1932464\ttotal: 7.37s\tremaining: 1m 36s\n",
            "56:\tlearn: 172.5426550\ttotal: 7.5s\tremaining: 1m 36s\n",
            "57:\tlearn: 170.3574276\ttotal: 7.66s\tremaining: 1m 36s\n",
            "58:\tlearn: 169.3762980\ttotal: 7.8s\tremaining: 1m 36s\n",
            "59:\tlearn: 167.1019145\ttotal: 7.94s\tremaining: 1m 36s\n",
            "60:\tlearn: 166.7321771\ttotal: 8.1s\tremaining: 1m 37s\n",
            "61:\tlearn: 164.1894266\ttotal: 8.23s\tremaining: 1m 36s\n",
            "62:\tlearn: 162.1737440\ttotal: 8.37s\tremaining: 1m 36s\n",
            "63:\tlearn: 161.6068300\ttotal: 8.53s\tremaining: 1m 36s\n",
            "64:\tlearn: 161.2873708\ttotal: 8.7s\tremaining: 1m 37s\n",
            "65:\tlearn: 159.7644790\ttotal: 8.87s\tremaining: 1m 37s\n",
            "66:\tlearn: 159.4411468\ttotal: 9.03s\tremaining: 1m 37s\n",
            "67:\tlearn: 157.3687971\ttotal: 9.17s\tremaining: 1m 37s\n",
            "68:\tlearn: 156.9700727\ttotal: 9.29s\tremaining: 1m 37s\n",
            "69:\tlearn: 156.7597518\ttotal: 9.45s\tremaining: 1m 37s\n",
            "70:\tlearn: 155.8871650\ttotal: 9.6s\tremaining: 1m 37s\n",
            "71:\tlearn: 154.4040132\ttotal: 9.75s\tremaining: 1m 37s\n",
            "72:\tlearn: 153.8230585\ttotal: 9.91s\tremaining: 1m 37s\n",
            "73:\tlearn: 153.1784035\ttotal: 10.1s\tremaining: 1m 37s\n",
            "74:\tlearn: 152.8869159\ttotal: 10.2s\tremaining: 1m 37s\n",
            "75:\tlearn: 151.3246873\ttotal: 10.4s\tremaining: 1m 37s\n",
            "76:\tlearn: 150.4802671\ttotal: 10.5s\tremaining: 1m 37s\n",
            "77:\tlearn: 150.2805027\ttotal: 10.7s\tremaining: 1m 37s\n",
            "78:\tlearn: 150.1486663\ttotal: 10.8s\tremaining: 1m 37s\n",
            "79:\tlearn: 150.0656522\ttotal: 11s\tremaining: 1m 37s\n",
            "80:\tlearn: 149.7706518\ttotal: 11.1s\tremaining: 1m 37s\n",
            "81:\tlearn: 149.6609064\ttotal: 11.3s\tremaining: 1m 37s\n",
            "82:\tlearn: 149.2600367\ttotal: 11.4s\tremaining: 1m 37s\n",
            "83:\tlearn: 148.0045997\ttotal: 11.6s\tremaining: 1m 37s\n",
            "84:\tlearn: 147.7883968\ttotal: 11.8s\tremaining: 1m 37s\n",
            "85:\tlearn: 147.4656667\ttotal: 11.9s\tremaining: 1m 38s\n",
            "86:\tlearn: 147.2195432\ttotal: 12.1s\tremaining: 1m 38s\n",
            "87:\tlearn: 145.7592730\ttotal: 12.3s\tremaining: 1m 38s\n",
            "88:\tlearn: 145.4749549\ttotal: 12.4s\tremaining: 1m 38s\n",
            "89:\tlearn: 144.9949157\ttotal: 12.6s\tremaining: 1m 38s\n",
            "90:\tlearn: 144.9570511\ttotal: 12.7s\tremaining: 1m 38s\n",
            "91:\tlearn: 144.1834087\ttotal: 12.9s\tremaining: 1m 38s\n",
            "92:\tlearn: 143.4052038\ttotal: 13s\tremaining: 1m 37s\n",
            "93:\tlearn: 143.1618193\ttotal: 13.2s\tremaining: 1m 37s\n",
            "94:\tlearn: 142.6011885\ttotal: 13.3s\tremaining: 1m 37s\n",
            "95:\tlearn: 141.0298550\ttotal: 13.5s\tremaining: 1m 37s\n",
            "96:\tlearn: 140.0791656\ttotal: 13.6s\tremaining: 1m 37s\n",
            "97:\tlearn: 139.8654670\ttotal: 13.8s\tremaining: 1m 37s\n",
            "98:\tlearn: 139.6526450\ttotal: 13.9s\tremaining: 1m 37s\n",
            "99:\tlearn: 138.6596989\ttotal: 14.1s\tremaining: 1m 37s\n",
            "100:\tlearn: 136.7394058\ttotal: 14.2s\tremaining: 1m 37s\n",
            "101:\tlearn: 136.2401101\ttotal: 14.4s\tremaining: 1m 37s\n",
            "102:\tlearn: 135.9783857\ttotal: 14.5s\tremaining: 1m 37s\n",
            "103:\tlearn: 135.4440864\ttotal: 14.7s\tremaining: 1m 37s\n",
            "104:\tlearn: 135.2442035\ttotal: 14.9s\tremaining: 1m 37s\n",
            "105:\tlearn: 134.5833829\ttotal: 15s\tremaining: 1m 37s\n",
            "106:\tlearn: 133.9280251\ttotal: 15.1s\tremaining: 1m 36s\n",
            "107:\tlearn: 133.1157807\ttotal: 15.3s\tremaining: 1m 36s\n",
            "108:\tlearn: 132.8651380\ttotal: 15.4s\tremaining: 1m 36s\n",
            "109:\tlearn: 132.6927822\ttotal: 15.6s\tremaining: 1m 36s\n",
            "110:\tlearn: 132.3454401\ttotal: 15.7s\tremaining: 1m 36s\n",
            "111:\tlearn: 131.1321992\ttotal: 15.9s\tremaining: 1m 36s\n",
            "112:\tlearn: 130.9107680\ttotal: 16s\tremaining: 1m 36s\n",
            "113:\tlearn: 130.3050587\ttotal: 16.2s\tremaining: 1m 36s\n",
            "114:\tlearn: 130.0297226\ttotal: 16.3s\tremaining: 1m 36s\n",
            "115:\tlearn: 129.3432868\ttotal: 16.5s\tremaining: 1m 35s\n",
            "116:\tlearn: 129.0524138\ttotal: 16.6s\tremaining: 1m 35s\n",
            "117:\tlearn: 128.7877637\ttotal: 16.8s\tremaining: 1m 35s\n",
            "118:\tlearn: 128.5784514\ttotal: 16.9s\tremaining: 1m 35s\n",
            "119:\tlearn: 128.2769540\ttotal: 17.1s\tremaining: 1m 35s\n",
            "120:\tlearn: 128.1230852\ttotal: 17.2s\tremaining: 1m 35s\n",
            "121:\tlearn: 128.1098680\ttotal: 17.4s\tremaining: 1m 35s\n",
            "122:\tlearn: 126.9225046\ttotal: 17.6s\tremaining: 1m 35s\n",
            "123:\tlearn: 126.3890721\ttotal: 17.7s\tremaining: 1m 35s\n",
            "124:\tlearn: 126.2838809\ttotal: 17.9s\tremaining: 1m 35s\n",
            "125:\tlearn: 125.6187679\ttotal: 18s\tremaining: 1m 35s\n",
            "126:\tlearn: 125.0039309\ttotal: 18.2s\tremaining: 1m 35s\n",
            "127:\tlearn: 123.8834791\ttotal: 18.3s\tremaining: 1m 34s\n",
            "128:\tlearn: 123.7233662\ttotal: 18.5s\tremaining: 1m 34s\n",
            "129:\tlearn: 123.4302312\ttotal: 18.6s\tremaining: 1m 34s\n",
            "130:\tlearn: 122.5667072\ttotal: 18.7s\tremaining: 1m 34s\n",
            "131:\tlearn: 122.3135098\ttotal: 18.9s\tremaining: 1m 34s\n",
            "132:\tlearn: 121.9941735\ttotal: 19.1s\tremaining: 1m 34s\n",
            "133:\tlearn: 121.7417432\ttotal: 19.2s\tremaining: 1m 34s\n",
            "134:\tlearn: 121.6541706\ttotal: 19.4s\tremaining: 1m 34s\n",
            "135:\tlearn: 121.3811288\ttotal: 19.5s\tremaining: 1m 34s\n",
            "136:\tlearn: 120.9895294\ttotal: 19.7s\tremaining: 1m 34s\n",
            "137:\tlearn: 119.3650058\ttotal: 19.8s\tremaining: 1m 34s\n",
            "138:\tlearn: 118.7639624\ttotal: 20s\tremaining: 1m 33s\n",
            "139:\tlearn: 118.5530425\ttotal: 20.1s\tremaining: 1m 33s\n",
            "140:\tlearn: 118.3409872\ttotal: 20.3s\tremaining: 1m 33s\n",
            "141:\tlearn: 117.9661261\ttotal: 20.4s\tremaining: 1m 33s\n",
            "142:\tlearn: 117.7865426\ttotal: 20.6s\tremaining: 1m 33s\n",
            "143:\tlearn: 117.6776834\ttotal: 20.7s\tremaining: 1m 33s\n",
            "144:\tlearn: 117.5357194\ttotal: 20.9s\tremaining: 1m 33s\n",
            "145:\tlearn: 117.3692188\ttotal: 21s\tremaining: 1m 33s\n",
            "146:\tlearn: 117.2501206\ttotal: 21.2s\tremaining: 1m 33s\n",
            "147:\tlearn: 116.8905241\ttotal: 21.3s\tremaining: 1m 32s\n",
            "148:\tlearn: 116.2355697\ttotal: 21.5s\tremaining: 1m 32s\n",
            "149:\tlearn: 115.9807208\ttotal: 21.6s\tremaining: 1m 32s\n",
            "150:\tlearn: 115.8451299\ttotal: 21.8s\tremaining: 1m 32s\n",
            "151:\tlearn: 115.3066585\ttotal: 21.9s\tremaining: 1m 32s\n",
            "152:\tlearn: 115.1997879\ttotal: 22.1s\tremaining: 1m 32s\n",
            "153:\tlearn: 114.8580534\ttotal: 22.2s\tremaining: 1m 32s\n",
            "154:\tlearn: 114.5550005\ttotal: 22.4s\tremaining: 1m 31s\n",
            "155:\tlearn: 114.1907969\ttotal: 22.5s\tremaining: 1m 31s\n",
            "156:\tlearn: 113.6580586\ttotal: 22.7s\tremaining: 1m 31s\n",
            "157:\tlearn: 113.5520017\ttotal: 22.8s\tremaining: 1m 31s\n",
            "158:\tlearn: 113.2242917\ttotal: 22.9s\tremaining: 1m 31s\n",
            "159:\tlearn: 113.0721903\ttotal: 23.1s\tremaining: 1m 31s\n",
            "160:\tlearn: 112.9043039\ttotal: 23.2s\tremaining: 1m 31s\n",
            "161:\tlearn: 112.7784037\ttotal: 23.4s\tremaining: 1m 30s\n",
            "162:\tlearn: 112.6992728\ttotal: 23.5s\tremaining: 1m 30s\n",
            "163:\tlearn: 112.5497374\ttotal: 23.7s\tremaining: 1m 30s\n",
            "164:\tlearn: 112.4707878\ttotal: 23.8s\tremaining: 1m 30s\n",
            "165:\tlearn: 112.3814107\ttotal: 24s\tremaining: 1m 30s\n",
            "166:\tlearn: 112.3550513\ttotal: 24.2s\tremaining: 1m 30s\n",
            "167:\tlearn: 112.1413659\ttotal: 24.3s\tremaining: 1m 30s\n",
            "168:\tlearn: 111.6700568\ttotal: 24.4s\tremaining: 1m 30s\n",
            "169:\tlearn: 111.4415039\ttotal: 24.6s\tremaining: 1m 29s\n",
            "170:\tlearn: 110.6233081\ttotal: 24.7s\tremaining: 1m 29s\n",
            "171:\tlearn: 110.1612568\ttotal: 24.8s\tremaining: 1m 29s\n",
            "172:\tlearn: 109.6891887\ttotal: 25s\tremaining: 1m 29s\n",
            "173:\tlearn: 109.3229355\ttotal: 25.1s\tremaining: 1m 29s\n",
            "174:\tlearn: 109.2250350\ttotal: 25.3s\tremaining: 1m 29s\n",
            "175:\tlearn: 109.1183335\ttotal: 25.5s\tremaining: 1m 29s\n",
            "176:\tlearn: 108.8102626\ttotal: 25.6s\tremaining: 1m 28s\n",
            "177:\tlearn: 108.5511565\ttotal: 25.7s\tremaining: 1m 28s\n",
            "178:\tlearn: 108.4425326\ttotal: 25.9s\tremaining: 1m 28s\n",
            "179:\tlearn: 108.2340045\ttotal: 26.1s\tremaining: 1m 28s\n",
            "180:\tlearn: 108.0553137\ttotal: 26.2s\tremaining: 1m 28s\n",
            "181:\tlearn: 107.8980852\ttotal: 26.3s\tremaining: 1m 28s\n",
            "182:\tlearn: 107.7791274\ttotal: 26.5s\tremaining: 1m 28s\n",
            "183:\tlearn: 107.6612676\ttotal: 26.6s\tremaining: 1m 28s\n",
            "184:\tlearn: 107.5268236\ttotal: 26.8s\tremaining: 1m 27s\n",
            "185:\tlearn: 107.4181724\ttotal: 26.9s\tremaining: 1m 27s\n",
            "186:\tlearn: 107.1446460\ttotal: 27.1s\tremaining: 1m 27s\n",
            "187:\tlearn: 106.6475482\ttotal: 27.2s\tremaining: 1m 27s\n",
            "188:\tlearn: 106.4213769\ttotal: 27.3s\tremaining: 1m 27s\n",
            "189:\tlearn: 106.0390756\ttotal: 27.5s\tremaining: 1m 27s\n",
            "190:\tlearn: 105.9280120\ttotal: 27.6s\tremaining: 1m 26s\n",
            "191:\tlearn: 105.8173518\ttotal: 27.8s\tremaining: 1m 26s\n",
            "192:\tlearn: 105.5341316\ttotal: 27.9s\tremaining: 1m 26s\n",
            "193:\tlearn: 105.4153958\ttotal: 28.1s\tremaining: 1m 26s\n",
            "194:\tlearn: 105.3976284\ttotal: 28.2s\tremaining: 1m 26s\n",
            "195:\tlearn: 105.3288950\ttotal: 28.4s\tremaining: 1m 26s\n",
            "196:\tlearn: 105.2083885\ttotal: 28.5s\tremaining: 1m 26s\n",
            "197:\tlearn: 105.0733091\ttotal: 28.6s\tremaining: 1m 25s\n",
            "198:\tlearn: 104.6532081\ttotal: 28.8s\tremaining: 1m 25s\n",
            "199:\tlearn: 103.9183688\ttotal: 28.9s\tremaining: 1m 25s\n",
            "200:\tlearn: 103.7494385\ttotal: 29.1s\tremaining: 1m 25s\n",
            "201:\tlearn: 103.6688238\ttotal: 29.2s\tremaining: 1m 25s\n",
            "202:\tlearn: 103.4469529\ttotal: 29.4s\tremaining: 1m 25s\n",
            "203:\tlearn: 103.3514962\ttotal: 29.5s\tremaining: 1m 25s\n",
            "204:\tlearn: 103.2598669\ttotal: 29.6s\tremaining: 1m 24s\n",
            "205:\tlearn: 103.1501362\ttotal: 29.9s\tremaining: 1m 24s\n",
            "206:\tlearn: 103.0125458\ttotal: 30s\tremaining: 1m 24s\n",
            "207:\tlearn: 102.8792501\ttotal: 30.2s\tremaining: 1m 24s\n",
            "208:\tlearn: 102.7264122\ttotal: 30.3s\tremaining: 1m 24s\n",
            "209:\tlearn: 102.6034505\ttotal: 30.5s\tremaining: 1m 24s\n",
            "210:\tlearn: 102.6001783\ttotal: 30.6s\tremaining: 1m 24s\n",
            "211:\tlearn: 102.1856078\ttotal: 30.8s\tremaining: 1m 24s\n",
            "212:\tlearn: 102.0161231\ttotal: 30.9s\tremaining: 1m 24s\n",
            "213:\tlearn: 101.9591320\ttotal: 31.1s\tremaining: 1m 23s\n",
            "214:\tlearn: 101.7622583\ttotal: 31.3s\tremaining: 1m 23s\n",
            "215:\tlearn: 101.6955291\ttotal: 31.4s\tremaining: 1m 23s\n",
            "216:\tlearn: 101.4224817\ttotal: 31.6s\tremaining: 1m 23s\n",
            "217:\tlearn: 101.3366631\ttotal: 31.7s\tremaining: 1m 23s\n",
            "218:\tlearn: 101.0271836\ttotal: 31.9s\tremaining: 1m 23s\n",
            "219:\tlearn: 100.9970916\ttotal: 32s\tremaining: 1m 23s\n",
            "220:\tlearn: 100.7655248\ttotal: 32.2s\tremaining: 1m 23s\n",
            "221:\tlearn: 100.5035228\ttotal: 32.3s\tremaining: 1m 22s\n",
            "222:\tlearn: 100.0303629\ttotal: 32.5s\tremaining: 1m 22s\n",
            "223:\tlearn: 99.9497806\ttotal: 32.6s\tremaining: 1m 22s\n",
            "224:\tlearn: 99.8096755\ttotal: 32.8s\tremaining: 1m 22s\n",
            "225:\tlearn: 99.7529767\ttotal: 32.9s\tremaining: 1m 22s\n",
            "226:\tlearn: 99.5786169\ttotal: 33.1s\tremaining: 1m 22s\n",
            "227:\tlearn: 99.5225290\ttotal: 33.3s\tremaining: 1m 22s\n",
            "228:\tlearn: 99.4541021\ttotal: 33.4s\tremaining: 1m 22s\n",
            "229:\tlearn: 99.4019049\ttotal: 33.6s\tremaining: 1m 22s\n",
            "230:\tlearn: 99.3090813\ttotal: 33.8s\tremaining: 1m 21s\n",
            "231:\tlearn: 99.2820987\ttotal: 33.9s\tremaining: 1m 21s\n",
            "232:\tlearn: 98.5182953\ttotal: 34.1s\tremaining: 1m 21s\n",
            "233:\tlearn: 98.1868660\ttotal: 34.2s\tremaining: 1m 21s\n",
            "234:\tlearn: 98.1774112\ttotal: 34.4s\tremaining: 1m 21s\n",
            "235:\tlearn: 97.9827569\ttotal: 34.5s\tremaining: 1m 21s\n",
            "236:\tlearn: 97.9005777\ttotal: 34.7s\tremaining: 1m 21s\n",
            "237:\tlearn: 97.8024721\ttotal: 34.8s\tremaining: 1m 21s\n",
            "238:\tlearn: 97.7659922\ttotal: 35s\tremaining: 1m 20s\n",
            "239:\tlearn: 97.6875199\ttotal: 35.1s\tremaining: 1m 20s\n",
            "240:\tlearn: 97.5870813\ttotal: 35.3s\tremaining: 1m 20s\n",
            "241:\tlearn: 97.3840073\ttotal: 35.4s\tremaining: 1m 20s\n",
            "242:\tlearn: 97.3833427\ttotal: 35.6s\tremaining: 1m 20s\n",
            "243:\tlearn: 97.2759730\ttotal: 35.8s\tremaining: 1m 20s\n",
            "244:\tlearn: 97.2662257\ttotal: 35.9s\tremaining: 1m 20s\n",
            "245:\tlearn: 97.2312778\ttotal: 36.1s\tremaining: 1m 20s\n",
            "246:\tlearn: 96.8922012\ttotal: 36.2s\tremaining: 1m 19s\n",
            "247:\tlearn: 96.5098756\ttotal: 36.4s\tremaining: 1m 19s\n",
            "248:\tlearn: 96.2857730\ttotal: 36.5s\tremaining: 1m 19s\n",
            "249:\tlearn: 96.2187482\ttotal: 36.7s\tremaining: 1m 19s\n",
            "250:\tlearn: 96.1752168\ttotal: 36.8s\tremaining: 1m 19s\n",
            "251:\tlearn: 96.0379858\ttotal: 37s\tremaining: 1m 19s\n",
            "252:\tlearn: 95.6623032\ttotal: 37.1s\tremaining: 1m 19s\n",
            "253:\tlearn: 95.6011590\ttotal: 37.3s\tremaining: 1m 18s\n",
            "254:\tlearn: 95.5185529\ttotal: 37.4s\tremaining: 1m 18s\n",
            "255:\tlearn: 95.2733452\ttotal: 37.5s\tremaining: 1m 18s\n",
            "256:\tlearn: 95.2204321\ttotal: 37.7s\tremaining: 1m 18s\n",
            "257:\tlearn: 95.1367229\ttotal: 37.9s\tremaining: 1m 18s\n",
            "258:\tlearn: 94.8141703\ttotal: 38s\tremaining: 1m 18s\n",
            "259:\tlearn: 94.7336568\ttotal: 38.1s\tremaining: 1m 18s\n",
            "260:\tlearn: 94.7107507\ttotal: 38.3s\tremaining: 1m 17s\n",
            "261:\tlearn: 94.5726808\ttotal: 38.5s\tremaining: 1m 17s\n",
            "262:\tlearn: 94.4733066\ttotal: 38.6s\tremaining: 1m 17s\n",
            "263:\tlearn: 94.4722590\ttotal: 38.8s\tremaining: 1m 17s\n",
            "264:\tlearn: 94.2394499\ttotal: 38.9s\tremaining: 1m 17s\n",
            "265:\tlearn: 94.0215413\ttotal: 39.1s\tremaining: 1m 17s\n",
            "266:\tlearn: 93.9808368\ttotal: 39.2s\tremaining: 1m 17s\n",
            "267:\tlearn: 93.9219478\ttotal: 39.4s\tremaining: 1m 17s\n",
            "268:\tlearn: 93.5489909\ttotal: 39.5s\tremaining: 1m 16s\n",
            "269:\tlearn: 93.1211267\ttotal: 39.7s\tremaining: 1m 16s\n",
            "270:\tlearn: 92.9734187\ttotal: 39.8s\tremaining: 1m 16s\n",
            "271:\tlearn: 92.7984594\ttotal: 40s\tremaining: 1m 16s\n",
            "272:\tlearn: 92.6790243\ttotal: 40.1s\tremaining: 1m 16s\n",
            "273:\tlearn: 92.5715995\ttotal: 40.3s\tremaining: 1m 16s\n",
            "274:\tlearn: 92.1278996\ttotal: 40.4s\tremaining: 1m 15s\n",
            "275:\tlearn: 92.0377010\ttotal: 40.6s\tremaining: 1m 15s\n",
            "276:\tlearn: 91.9218361\ttotal: 40.7s\tremaining: 1m 15s\n",
            "277:\tlearn: 91.7591616\ttotal: 40.8s\tremaining: 1m 15s\n",
            "278:\tlearn: 91.7027624\ttotal: 41s\tremaining: 1m 15s\n",
            "279:\tlearn: 91.6278989\ttotal: 41.1s\tremaining: 1m 15s\n",
            "280:\tlearn: 91.4998372\ttotal: 41.3s\tremaining: 1m 15s\n",
            "281:\tlearn: 91.4957249\ttotal: 41.4s\tremaining: 1m 14s\n",
            "282:\tlearn: 91.3946178\ttotal: 41.6s\tremaining: 1m 14s\n",
            "283:\tlearn: 91.1303788\ttotal: 41.8s\tremaining: 1m 14s\n",
            "284:\tlearn: 91.0527536\ttotal: 41.9s\tremaining: 1m 14s\n",
            "285:\tlearn: 90.9883530\ttotal: 42.1s\tremaining: 1m 14s\n",
            "286:\tlearn: 90.8781647\ttotal: 42.2s\tremaining: 1m 14s\n",
            "287:\tlearn: 90.5300847\ttotal: 42.4s\tremaining: 1m 14s\n",
            "288:\tlearn: 90.3227689\ttotal: 42.5s\tremaining: 1m 14s\n",
            "289:\tlearn: 90.2400772\ttotal: 42.7s\tremaining: 1m 13s\n",
            "290:\tlearn: 90.1156682\ttotal: 42.8s\tremaining: 1m 13s\n",
            "291:\tlearn: 90.0371552\ttotal: 43s\tremaining: 1m 13s\n",
            "292:\tlearn: 89.9611059\ttotal: 43.1s\tremaining: 1m 13s\n",
            "293:\tlearn: 89.9220410\ttotal: 43.3s\tremaining: 1m 13s\n",
            "294:\tlearn: 89.7944521\ttotal: 43.5s\tremaining: 1m 13s\n",
            "295:\tlearn: 89.7201598\ttotal: 43.6s\tremaining: 1m 13s\n",
            "296:\tlearn: 89.7153449\ttotal: 43.7s\tremaining: 1m 12s\n",
            "297:\tlearn: 89.7081029\ttotal: 43.9s\tremaining: 1m 12s\n",
            "298:\tlearn: 89.6205231\ttotal: 44s\tremaining: 1m 12s\n",
            "299:\tlearn: 89.4644154\ttotal: 44.2s\tremaining: 1m 12s\n",
            "300:\tlearn: 89.1605594\ttotal: 44.3s\tremaining: 1m 12s\n",
            "301:\tlearn: 88.9144520\ttotal: 44.4s\tremaining: 1m 12s\n",
            "302:\tlearn: 88.8344896\ttotal: 44.6s\tremaining: 1m 11s\n",
            "303:\tlearn: 88.7181794\ttotal: 44.7s\tremaining: 1m 11s\n",
            "304:\tlearn: 88.6593371\ttotal: 44.9s\tremaining: 1m 11s\n",
            "305:\tlearn: 88.5551673\ttotal: 45s\tremaining: 1m 11s\n",
            "306:\tlearn: 88.4759103\ttotal: 45.2s\tremaining: 1m 11s\n",
            "307:\tlearn: 88.3881344\ttotal: 45.3s\tremaining: 1m 11s\n",
            "308:\tlearn: 88.2785865\ttotal: 45.5s\tremaining: 1m 11s\n",
            "309:\tlearn: 88.1978618\ttotal: 45.6s\tremaining: 1m 10s\n",
            "310:\tlearn: 87.9505495\ttotal: 45.8s\tremaining: 1m 10s\n",
            "311:\tlearn: 87.8138048\ttotal: 45.9s\tremaining: 1m 10s\n",
            "312:\tlearn: 87.3225870\ttotal: 46.1s\tremaining: 1m 10s\n",
            "313:\tlearn: 86.9643405\ttotal: 46.2s\tremaining: 1m 10s\n",
            "314:\tlearn: 86.7350507\ttotal: 46.4s\tremaining: 1m 10s\n",
            "315:\tlearn: 86.6680960\ttotal: 46.5s\tremaining: 1m 10s\n",
            "316:\tlearn: 86.5588177\ttotal: 46.7s\tremaining: 1m 9s\n",
            "317:\tlearn: 86.5194651\ttotal: 46.8s\tremaining: 1m 9s\n",
            "318:\tlearn: 86.2786990\ttotal: 47s\tremaining: 1m 9s\n",
            "319:\tlearn: 86.1395707\ttotal: 47.1s\tremaining: 1m 9s\n",
            "320:\tlearn: 86.0552651\ttotal: 47.3s\tremaining: 1m 9s\n",
            "321:\tlearn: 85.9877862\ttotal: 47.5s\tremaining: 1m 9s\n",
            "322:\tlearn: 85.8531654\ttotal: 47.6s\tremaining: 1m 9s\n",
            "323:\tlearn: 85.8073612\ttotal: 47.8s\tremaining: 1m 9s\n",
            "324:\tlearn: 85.2200180\ttotal: 47.9s\tremaining: 1m 8s\n",
            "325:\tlearn: 84.8169301\ttotal: 48s\tremaining: 1m 8s\n",
            "326:\tlearn: 84.6717821\ttotal: 48.2s\tremaining: 1m 8s\n",
            "327:\tlearn: 84.4787832\ttotal: 48.3s\tremaining: 1m 8s\n",
            "328:\tlearn: 84.3814415\ttotal: 48.5s\tremaining: 1m 8s\n",
            "329:\tlearn: 84.2142512\ttotal: 48.6s\tremaining: 1m 8s\n",
            "330:\tlearn: 84.0738631\ttotal: 48.7s\tremaining: 1m 7s\n",
            "331:\tlearn: 84.0154443\ttotal: 48.9s\tremaining: 1m 7s\n",
            "332:\tlearn: 83.9493090\ttotal: 49s\tremaining: 1m 7s\n",
            "333:\tlearn: 83.5991540\ttotal: 49.2s\tremaining: 1m 7s\n",
            "334:\tlearn: 83.4888919\ttotal: 49.3s\tremaining: 1m 7s\n",
            "335:\tlearn: 83.4451540\ttotal: 49.5s\tremaining: 1m 7s\n",
            "336:\tlearn: 83.3221104\ttotal: 49.6s\tremaining: 1m 7s\n",
            "337:\tlearn: 83.2347889\ttotal: 49.8s\tremaining: 1m 6s\n",
            "338:\tlearn: 83.1075018\ttotal: 49.9s\tremaining: 1m 6s\n",
            "339:\tlearn: 82.9701239\ttotal: 50.1s\tremaining: 1m 6s\n",
            "340:\tlearn: 82.8683366\ttotal: 50.2s\tremaining: 1m 6s\n",
            "341:\tlearn: 82.7934609\ttotal: 50.4s\tremaining: 1m 6s\n",
            "342:\tlearn: 82.5624680\ttotal: 50.5s\tremaining: 1m 6s\n",
            "343:\tlearn: 82.5602853\ttotal: 50.7s\tremaining: 1m 6s\n",
            "344:\tlearn: 82.4501056\ttotal: 50.8s\tremaining: 1m 5s\n",
            "345:\tlearn: 82.2985234\ttotal: 51s\tremaining: 1m 5s\n",
            "346:\tlearn: 82.2646524\ttotal: 51.2s\tremaining: 1m 5s\n",
            "347:\tlearn: 82.2643207\ttotal: 51.3s\tremaining: 1m 5s\n",
            "348:\tlearn: 82.2031719\ttotal: 51.5s\tremaining: 1m 5s\n",
            "349:\tlearn: 82.1095009\ttotal: 51.7s\tremaining: 1m 5s\n",
            "350:\tlearn: 82.0535290\ttotal: 51.8s\tremaining: 1m 5s\n",
            "351:\tlearn: 81.8780015\ttotal: 52s\tremaining: 1m 4s\n",
            "352:\tlearn: 81.6623355\ttotal: 52.1s\tremaining: 1m 4s\n",
            "353:\tlearn: 81.5962824\ttotal: 52.3s\tremaining: 1m 4s\n",
            "354:\tlearn: 81.3504247\ttotal: 52.4s\tremaining: 1m 4s\n",
            "355:\tlearn: 80.9910031\ttotal: 52.6s\tremaining: 1m 4s\n",
            "356:\tlearn: 80.9509632\ttotal: 52.7s\tremaining: 1m 4s\n",
            "357:\tlearn: 80.8558068\ttotal: 52.9s\tremaining: 1m 4s\n",
            "358:\tlearn: 80.8201945\ttotal: 53s\tremaining: 1m 3s\n",
            "359:\tlearn: 80.7716081\ttotal: 53.2s\tremaining: 1m 3s\n",
            "360:\tlearn: 80.6976144\ttotal: 53.4s\tremaining: 1m 3s\n",
            "361:\tlearn: 80.5930859\ttotal: 53.5s\tremaining: 1m 3s\n",
            "362:\tlearn: 80.5214214\ttotal: 53.6s\tremaining: 1m 3s\n",
            "363:\tlearn: 80.4206540\ttotal: 53.8s\tremaining: 1m 3s\n",
            "364:\tlearn: 80.3682529\ttotal: 53.9s\tremaining: 1m 3s\n",
            "365:\tlearn: 80.2300677\ttotal: 54.1s\tremaining: 1m 2s\n",
            "366:\tlearn: 80.0939804\ttotal: 54.2s\tremaining: 1m 2s\n",
            "367:\tlearn: 79.9605811\ttotal: 54.4s\tremaining: 1m 2s\n",
            "368:\tlearn: 79.9157891\ttotal: 54.5s\tremaining: 1m 2s\n",
            "369:\tlearn: 79.8712330\ttotal: 54.7s\tremaining: 1m 2s\n",
            "370:\tlearn: 79.7932362\ttotal: 54.8s\tremaining: 1m 2s\n",
            "371:\tlearn: 79.7061807\ttotal: 55s\tremaining: 1m 2s\n",
            "372:\tlearn: 79.6609997\ttotal: 55.1s\tremaining: 1m 1s\n",
            "373:\tlearn: 79.6545303\ttotal: 55.3s\tremaining: 1m 1s\n",
            "374:\tlearn: 79.6125368\ttotal: 55.5s\tremaining: 1m 1s\n",
            "375:\tlearn: 79.4284897\ttotal: 55.6s\tremaining: 1m 1s\n",
            "376:\tlearn: 79.2145146\ttotal: 55.7s\tremaining: 1m 1s\n",
            "377:\tlearn: 79.1963148\ttotal: 55.9s\tremaining: 1m 1s\n",
            "378:\tlearn: 79.1031524\ttotal: 56s\tremaining: 1m 1s\n",
            "379:\tlearn: 79.0250670\ttotal: 56.2s\tremaining: 1m\n",
            "380:\tlearn: 78.9749864\ttotal: 56.3s\tremaining: 1m\n",
            "381:\tlearn: 78.7598383\ttotal: 56.5s\tremaining: 1m\n",
            "382:\tlearn: 78.7431097\ttotal: 56.6s\tremaining: 1m\n",
            "383:\tlearn: 78.5703953\ttotal: 56.8s\tremaining: 1m\n",
            "384:\tlearn: 78.2338917\ttotal: 56.9s\tremaining: 1m\n",
            "385:\tlearn: 78.1267990\ttotal: 57s\tremaining: 60s\n",
            "386:\tlearn: 78.0699812\ttotal: 57.2s\tremaining: 59.8s\n",
            "387:\tlearn: 78.0013917\ttotal: 57.3s\tremaining: 59.7s\n",
            "388:\tlearn: 77.7607793\ttotal: 57.5s\tremaining: 59.5s\n",
            "389:\tlearn: 77.5311864\ttotal: 57.6s\tremaining: 59.4s\n",
            "390:\tlearn: 77.4445444\ttotal: 57.8s\tremaining: 59.2s\n",
            "391:\tlearn: 77.3698900\ttotal: 57.9s\tremaining: 59.1s\n",
            "392:\tlearn: 77.2252431\ttotal: 58.1s\tremaining: 58.9s\n",
            "393:\tlearn: 77.0408563\ttotal: 58.2s\tremaining: 58.8s\n",
            "394:\tlearn: 76.9365635\ttotal: 58.3s\tremaining: 58.6s\n",
            "395:\tlearn: 76.8410904\ttotal: 58.5s\tremaining: 58.5s\n",
            "396:\tlearn: 76.7776930\ttotal: 58.6s\tremaining: 58.3s\n",
            "397:\tlearn: 76.6675940\ttotal: 58.8s\tremaining: 58.2s\n",
            "398:\tlearn: 76.3435188\ttotal: 58.9s\tremaining: 58s\n",
            "399:\tlearn: 76.2718991\ttotal: 59.1s\tremaining: 57.9s\n",
            "400:\tlearn: 76.0154116\ttotal: 59.2s\tremaining: 57.7s\n",
            "401:\tlearn: 75.9480849\ttotal: 59.4s\tremaining: 57.6s\n",
            "402:\tlearn: 75.8376442\ttotal: 59.5s\tremaining: 57.4s\n",
            "403:\tlearn: 75.7536804\ttotal: 59.6s\tremaining: 57.3s\n",
            "404:\tlearn: 75.6539004\ttotal: 59.8s\tremaining: 57.1s\n",
            "405:\tlearn: 75.5671498\ttotal: 59.9s\tremaining: 57s\n",
            "406:\tlearn: 75.4614396\ttotal: 1m\tremaining: 56.8s\n",
            "407:\tlearn: 75.4188711\ttotal: 1m\tremaining: 56.6s\n",
            "408:\tlearn: 75.2535326\ttotal: 1m\tremaining: 56.5s\n",
            "409:\tlearn: 75.1746248\ttotal: 1m\tremaining: 56.3s\n",
            "410:\tlearn: 74.9660638\ttotal: 1m\tremaining: 56.2s\n",
            "411:\tlearn: 74.9054766\ttotal: 1m\tremaining: 56s\n",
            "412:\tlearn: 74.8481025\ttotal: 1m\tremaining: 55.9s\n",
            "413:\tlearn: 74.8338738\ttotal: 1m 1s\tremaining: 55.8s\n",
            "414:\tlearn: 74.7994456\ttotal: 1m 1s\tremaining: 55.7s\n",
            "415:\tlearn: 74.5536699\ttotal: 1m 1s\tremaining: 55.5s\n",
            "416:\tlearn: 74.4834023\ttotal: 1m 1s\tremaining: 55.4s\n",
            "417:\tlearn: 74.3634477\ttotal: 1m 1s\tremaining: 55.2s\n",
            "418:\tlearn: 74.2566037\ttotal: 1m 1s\tremaining: 55.1s\n",
            "419:\tlearn: 74.2156916\ttotal: 1m 2s\tremaining: 54.9s\n",
            "420:\tlearn: 74.1598726\ttotal: 1m 2s\tremaining: 54.8s\n",
            "421:\tlearn: 74.0919724\ttotal: 1m 2s\tremaining: 54.6s\n",
            "422:\tlearn: 73.9867116\ttotal: 1m 2s\tremaining: 54.5s\n",
            "423:\tlearn: 73.8950888\ttotal: 1m 2s\tremaining: 54.3s\n",
            "424:\tlearn: 73.8514669\ttotal: 1m 2s\tremaining: 54.2s\n",
            "425:\tlearn: 73.8004503\ttotal: 1m 2s\tremaining: 54s\n",
            "426:\tlearn: 73.6933193\ttotal: 1m 3s\tremaining: 53.9s\n",
            "427:\tlearn: 73.6635239\ttotal: 1m 3s\tremaining: 53.7s\n",
            "428:\tlearn: 73.6239831\ttotal: 1m 3s\tremaining: 53.6s\n",
            "429:\tlearn: 73.5027602\ttotal: 1m 3s\tremaining: 53.4s\n",
            "430:\tlearn: 73.3926216\ttotal: 1m 3s\tremaining: 53.3s\n",
            "431:\tlearn: 73.2745490\ttotal: 1m 3s\tremaining: 53.1s\n",
            "432:\tlearn: 73.2692059\ttotal: 1m 3s\tremaining: 53s\n",
            "433:\tlearn: 73.2285452\ttotal: 1m 4s\tremaining: 52.8s\n",
            "434:\tlearn: 73.1516571\ttotal: 1m 4s\tremaining: 52.7s\n",
            "435:\tlearn: 73.1034384\ttotal: 1m 4s\tremaining: 52.6s\n",
            "436:\tlearn: 72.8619408\ttotal: 1m 4s\tremaining: 52.4s\n",
            "437:\tlearn: 72.7188531\ttotal: 1m 4s\tremaining: 52.3s\n",
            "438:\tlearn: 72.6315453\ttotal: 1m 4s\tremaining: 52.1s\n",
            "439:\tlearn: 72.5371088\ttotal: 1m 4s\tremaining: 52s\n",
            "440:\tlearn: 72.4377213\ttotal: 1m 5s\tremaining: 51.8s\n",
            "441:\tlearn: 72.3519042\ttotal: 1m 5s\tremaining: 51.7s\n",
            "442:\tlearn: 72.3283601\ttotal: 1m 5s\tremaining: 51.5s\n",
            "443:\tlearn: 72.2726373\ttotal: 1m 5s\tremaining: 51.4s\n",
            "444:\tlearn: 72.2264974\ttotal: 1m 5s\tremaining: 51.2s\n",
            "445:\tlearn: 72.2026619\ttotal: 1m 5s\tremaining: 51.1s\n",
            "446:\tlearn: 72.1807682\ttotal: 1m 6s\tremaining: 51s\n",
            "447:\tlearn: 72.0598561\ttotal: 1m 6s\tremaining: 50.8s\n",
            "448:\tlearn: 72.0013809\ttotal: 1m 6s\tremaining: 50.7s\n",
            "449:\tlearn: 71.9592538\ttotal: 1m 6s\tremaining: 50.5s\n",
            "450:\tlearn: 71.8062549\ttotal: 1m 6s\tremaining: 50.4s\n",
            "451:\tlearn: 71.7951190\ttotal: 1m 6s\tremaining: 50.2s\n",
            "452:\tlearn: 71.7303330\ttotal: 1m 6s\tremaining: 50.1s\n",
            "453:\tlearn: 71.5896824\ttotal: 1m 7s\tremaining: 49.9s\n",
            "454:\tlearn: 71.5032083\ttotal: 1m 7s\tremaining: 49.8s\n",
            "455:\tlearn: 71.4495080\ttotal: 1m 7s\tremaining: 49.7s\n",
            "456:\tlearn: 71.4230684\ttotal: 1m 7s\tremaining: 49.5s\n",
            "457:\tlearn: 71.3921374\ttotal: 1m 7s\tremaining: 49.4s\n",
            "458:\tlearn: 71.3536959\ttotal: 1m 7s\tremaining: 49.2s\n",
            "459:\tlearn: 71.2867868\ttotal: 1m 8s\tremaining: 49.1s\n",
            "460:\tlearn: 71.2149531\ttotal: 1m 8s\tremaining: 48.9s\n",
            "461:\tlearn: 71.1410140\ttotal: 1m 8s\tremaining: 48.8s\n",
            "462:\tlearn: 71.0592109\ttotal: 1m 8s\tremaining: 48.6s\n",
            "463:\tlearn: 71.0344593\ttotal: 1m 8s\tremaining: 48.5s\n",
            "464:\tlearn: 70.9035884\ttotal: 1m 8s\tremaining: 48.3s\n",
            "465:\tlearn: 70.8767380\ttotal: 1m 8s\tremaining: 48.2s\n",
            "466:\tlearn: 70.8019216\ttotal: 1m 9s\tremaining: 48.1s\n",
            "467:\tlearn: 70.7626708\ttotal: 1m 9s\tremaining: 47.9s\n",
            "468:\tlearn: 70.6733548\ttotal: 1m 9s\tremaining: 47.8s\n",
            "469:\tlearn: 70.5920555\ttotal: 1m 9s\tremaining: 47.6s\n",
            "470:\tlearn: 70.5579926\ttotal: 1m 9s\tremaining: 47.5s\n",
            "471:\tlearn: 70.4864047\ttotal: 1m 9s\tremaining: 47.3s\n",
            "472:\tlearn: 70.4610804\ttotal: 1m 9s\tremaining: 47.2s\n",
            "473:\tlearn: 70.4585730\ttotal: 1m 10s\tremaining: 47.1s\n",
            "474:\tlearn: 70.4390434\ttotal: 1m 10s\tremaining: 46.9s\n",
            "475:\tlearn: 70.3598302\ttotal: 1m 10s\tremaining: 46.8s\n",
            "476:\tlearn: 70.3553867\ttotal: 1m 10s\tremaining: 46.6s\n",
            "477:\tlearn: 70.3241701\ttotal: 1m 10s\tremaining: 46.5s\n",
            "478:\tlearn: 70.2596619\ttotal: 1m 10s\tremaining: 46.4s\n",
            "479:\tlearn: 70.1860849\ttotal: 1m 11s\tremaining: 46.2s\n",
            "480:\tlearn: 70.1623578\ttotal: 1m 11s\tremaining: 46.1s\n",
            "481:\tlearn: 70.1226877\ttotal: 1m 11s\tremaining: 45.9s\n",
            "482:\tlearn: 70.0164097\ttotal: 1m 11s\tremaining: 45.8s\n",
            "483:\tlearn: 69.8274784\ttotal: 1m 11s\tremaining: 45.6s\n",
            "484:\tlearn: 69.7527069\ttotal: 1m 11s\tremaining: 45.5s\n",
            "485:\tlearn: 69.7198176\ttotal: 1m 11s\tremaining: 45.3s\n",
            "486:\tlearn: 69.6550516\ttotal: 1m 12s\tremaining: 45.2s\n",
            "487:\tlearn: 69.4767169\ttotal: 1m 12s\tremaining: 45s\n",
            "488:\tlearn: 69.4657775\ttotal: 1m 12s\tremaining: 44.9s\n",
            "489:\tlearn: 69.4225524\ttotal: 1m 12s\tremaining: 44.7s\n",
            "490:\tlearn: 69.3389459\ttotal: 1m 12s\tremaining: 44.6s\n",
            "491:\tlearn: 69.2342304\ttotal: 1m 12s\tremaining: 44.4s\n",
            "492:\tlearn: 69.2006648\ttotal: 1m 13s\tremaining: 44.3s\n",
            "493:\tlearn: 69.1583164\ttotal: 1m 13s\tremaining: 44.1s\n",
            "494:\tlearn: 69.1141141\ttotal: 1m 13s\tremaining: 44s\n",
            "495:\tlearn: 69.0603610\ttotal: 1m 13s\tremaining: 43.9s\n",
            "496:\tlearn: 69.0277419\ttotal: 1m 13s\tremaining: 43.7s\n",
            "497:\tlearn: 68.9719266\ttotal: 1m 13s\tremaining: 43.6s\n",
            "498:\tlearn: 68.9459739\ttotal: 1m 13s\tremaining: 43.4s\n",
            "499:\tlearn: 68.7877781\ttotal: 1m 14s\tremaining: 43.3s\n",
            "500:\tlearn: 68.7096118\ttotal: 1m 14s\tremaining: 43.1s\n",
            "501:\tlearn: 68.6664141\ttotal: 1m 14s\tremaining: 43s\n",
            "502:\tlearn: 68.6030400\ttotal: 1m 14s\tremaining: 42.8s\n",
            "503:\tlearn: 68.4512683\ttotal: 1m 14s\tremaining: 42.7s\n",
            "504:\tlearn: 68.2762145\ttotal: 1m 14s\tremaining: 42.5s\n",
            "505:\tlearn: 68.2533832\ttotal: 1m 15s\tremaining: 42.4s\n",
            "506:\tlearn: 68.2065689\ttotal: 1m 15s\tremaining: 42.3s\n",
            "507:\tlearn: 68.1286978\ttotal: 1m 15s\tremaining: 42.1s\n",
            "508:\tlearn: 68.0783187\ttotal: 1m 15s\tremaining: 42s\n",
            "509:\tlearn: 68.0302660\ttotal: 1m 15s\tremaining: 41.8s\n",
            "510:\tlearn: 67.9854472\ttotal: 1m 15s\tremaining: 41.7s\n",
            "511:\tlearn: 67.9273126\ttotal: 1m 15s\tremaining: 41.5s\n",
            "512:\tlearn: 67.8816565\ttotal: 1m 16s\tremaining: 41.4s\n",
            "513:\tlearn: 67.6378817\ttotal: 1m 16s\tremaining: 41.3s\n",
            "514:\tlearn: 67.5718734\ttotal: 1m 16s\tremaining: 41.1s\n",
            "515:\tlearn: 67.5025786\ttotal: 1m 16s\tremaining: 41s\n",
            "516:\tlearn: 67.3178973\ttotal: 1m 16s\tremaining: 40.8s\n",
            "517:\tlearn: 67.1964662\ttotal: 1m 16s\tremaining: 40.7s\n",
            "518:\tlearn: 67.1497239\ttotal: 1m 17s\tremaining: 40.5s\n",
            "519:\tlearn: 67.0915397\ttotal: 1m 17s\tremaining: 40.4s\n",
            "520:\tlearn: 66.9491171\ttotal: 1m 17s\tremaining: 40.2s\n",
            "521:\tlearn: 66.9091444\ttotal: 1m 17s\tremaining: 40.1s\n",
            "522:\tlearn: 66.8469012\ttotal: 1m 17s\tremaining: 39.9s\n",
            "523:\tlearn: 66.8251090\ttotal: 1m 17s\tremaining: 39.8s\n",
            "524:\tlearn: 66.7223228\ttotal: 1m 17s\tremaining: 39.6s\n",
            "525:\tlearn: 66.6536012\ttotal: 1m 18s\tremaining: 39.5s\n",
            "526:\tlearn: 66.6255925\ttotal: 1m 18s\tremaining: 39.3s\n",
            "527:\tlearn: 66.5466775\ttotal: 1m 18s\tremaining: 39.2s\n",
            "528:\tlearn: 66.4776888\ttotal: 1m 18s\tremaining: 39s\n",
            "529:\tlearn: 66.4586140\ttotal: 1m 18s\tremaining: 38.9s\n",
            "530:\tlearn: 66.2744884\ttotal: 1m 18s\tremaining: 38.7s\n",
            "531:\tlearn: 66.2465725\ttotal: 1m 18s\tremaining: 38.6s\n",
            "532:\tlearn: 66.2090583\ttotal: 1m 19s\tremaining: 38.4s\n",
            "533:\tlearn: 66.1304943\ttotal: 1m 19s\tremaining: 38.3s\n",
            "534:\tlearn: 66.1029954\ttotal: 1m 19s\tremaining: 38.1s\n",
            "535:\tlearn: 66.0175031\ttotal: 1m 19s\tremaining: 38s\n",
            "536:\tlearn: 65.9075623\ttotal: 1m 19s\tremaining: 37.8s\n",
            "537:\tlearn: 65.8700067\ttotal: 1m 19s\tremaining: 37.7s\n",
            "538:\tlearn: 65.7783342\ttotal: 1m 20s\tremaining: 37.6s\n",
            "539:\tlearn: 65.6888557\ttotal: 1m 20s\tremaining: 37.4s\n",
            "540:\tlearn: 65.6291920\ttotal: 1m 20s\tremaining: 37.2s\n",
            "541:\tlearn: 65.5814645\ttotal: 1m 20s\tremaining: 37.1s\n",
            "542:\tlearn: 65.4981758\ttotal: 1m 20s\tremaining: 36.9s\n",
            "543:\tlearn: 65.4324779\ttotal: 1m 20s\tremaining: 36.8s\n",
            "544:\tlearn: 65.4029371\ttotal: 1m 20s\tremaining: 36.7s\n",
            "545:\tlearn: 65.3206767\ttotal: 1m 21s\tremaining: 36.5s\n",
            "546:\tlearn: 65.3108467\ttotal: 1m 21s\tremaining: 36.4s\n",
            "547:\tlearn: 65.2219104\ttotal: 1m 21s\tremaining: 36.2s\n",
            "548:\tlearn: 65.1438860\ttotal: 1m 21s\tremaining: 36.1s\n",
            "549:\tlearn: 65.1395359\ttotal: 1m 21s\tremaining: 35.9s\n",
            "550:\tlearn: 65.0942185\ttotal: 1m 21s\tremaining: 35.8s\n",
            "551:\tlearn: 65.0776708\ttotal: 1m 21s\tremaining: 35.6s\n",
            "552:\tlearn: 64.9968008\ttotal: 1m 22s\tremaining: 35.5s\n",
            "553:\tlearn: 64.9346967\ttotal: 1m 22s\tremaining: 35.3s\n",
            "554:\tlearn: 64.9061095\ttotal: 1m 22s\tremaining: 35.2s\n",
            "555:\tlearn: 64.8662403\ttotal: 1m 22s\tremaining: 35s\n",
            "556:\tlearn: 64.7759368\ttotal: 1m 22s\tremaining: 34.9s\n",
            "557:\tlearn: 64.6889921\ttotal: 1m 22s\tremaining: 34.7s\n",
            "558:\tlearn: 64.6301909\ttotal: 1m 23s\tremaining: 34.6s\n",
            "559:\tlearn: 64.6070756\ttotal: 1m 23s\tremaining: 34.5s\n",
            "560:\tlearn: 64.5794081\ttotal: 1m 23s\tremaining: 34.3s\n",
            "561:\tlearn: 64.5211880\ttotal: 1m 23s\tremaining: 34.2s\n",
            "562:\tlearn: 64.4887046\ttotal: 1m 23s\tremaining: 34s\n",
            "563:\tlearn: 64.3602404\ttotal: 1m 23s\tremaining: 33.9s\n",
            "564:\tlearn: 64.3400158\ttotal: 1m 23s\tremaining: 33.7s\n",
            "565:\tlearn: 64.1834471\ttotal: 1m 24s\tremaining: 33.6s\n",
            "566:\tlearn: 64.0633410\ttotal: 1m 24s\tremaining: 33.4s\n",
            "567:\tlearn: 64.0346771\ttotal: 1m 24s\tremaining: 33.3s\n",
            "568:\tlearn: 63.9148379\ttotal: 1m 24s\tremaining: 33.1s\n",
            "569:\tlearn: 63.8685417\ttotal: 1m 24s\tremaining: 33s\n",
            "570:\tlearn: 63.8440863\ttotal: 1m 24s\tremaining: 32.8s\n",
            "571:\tlearn: 63.7781777\ttotal: 1m 24s\tremaining: 32.7s\n",
            "572:\tlearn: 63.7399446\ttotal: 1m 25s\tremaining: 32.5s\n",
            "573:\tlearn: 63.7201994\ttotal: 1m 25s\tremaining: 32.4s\n",
            "574:\tlearn: 63.7084472\ttotal: 1m 25s\tremaining: 32.3s\n",
            "575:\tlearn: 63.6980183\ttotal: 1m 25s\tremaining: 32.1s\n",
            "576:\tlearn: 63.6689791\ttotal: 1m 25s\tremaining: 32s\n",
            "577:\tlearn: 63.6206598\ttotal: 1m 25s\tremaining: 31.8s\n",
            "578:\tlearn: 63.5757026\ttotal: 1m 26s\tremaining: 31.7s\n",
            "579:\tlearn: 63.5144786\ttotal: 1m 26s\tremaining: 31.5s\n",
            "580:\tlearn: 63.4713557\ttotal: 1m 26s\tremaining: 31.4s\n",
            "581:\tlearn: 63.3563683\ttotal: 1m 26s\tremaining: 31.2s\n",
            "582:\tlearn: 63.3308914\ttotal: 1m 26s\tremaining: 31.1s\n",
            "583:\tlearn: 63.2621929\ttotal: 1m 26s\tremaining: 30.9s\n",
            "584:\tlearn: 63.2243384\ttotal: 1m 27s\tremaining: 30.8s\n",
            "585:\tlearn: 63.1940088\ttotal: 1m 27s\tremaining: 30.6s\n",
            "586:\tlearn: 62.9947282\ttotal: 1m 27s\tremaining: 30.5s\n",
            "587:\tlearn: 62.9591346\ttotal: 1m 27s\tremaining: 30.3s\n",
            "588:\tlearn: 62.8777838\ttotal: 1m 27s\tremaining: 30.2s\n",
            "589:\tlearn: 62.8406268\ttotal: 1m 27s\tremaining: 30.1s\n",
            "590:\tlearn: 62.7999800\ttotal: 1m 27s\tremaining: 29.9s\n",
            "591:\tlearn: 62.7214077\ttotal: 1m 28s\tremaining: 29.8s\n",
            "592:\tlearn: 62.5903120\ttotal: 1m 28s\tremaining: 29.6s\n",
            "593:\tlearn: 62.5614568\ttotal: 1m 28s\tremaining: 29.5s\n",
            "594:\tlearn: 62.4988769\ttotal: 1m 28s\tremaining: 29.3s\n",
            "595:\tlearn: 62.4123307\ttotal: 1m 28s\tremaining: 29.2s\n",
            "596:\tlearn: 62.3077849\ttotal: 1m 28s\tremaining: 29s\n",
            "597:\tlearn: 62.2278472\ttotal: 1m 28s\tremaining: 28.9s\n",
            "598:\tlearn: 62.1957753\ttotal: 1m 29s\tremaining: 28.7s\n",
            "599:\tlearn: 62.1794356\ttotal: 1m 29s\tremaining: 28.6s\n",
            "600:\tlearn: 62.1046427\ttotal: 1m 29s\tremaining: 28.4s\n",
            "601:\tlearn: 62.0795769\ttotal: 1m 29s\tremaining: 28.3s\n",
            "602:\tlearn: 62.0601662\ttotal: 1m 29s\tremaining: 28.1s\n",
            "603:\tlearn: 62.0582183\ttotal: 1m 29s\tremaining: 28s\n",
            "604:\tlearn: 61.8797658\ttotal: 1m 30s\tremaining: 27.8s\n",
            "605:\tlearn: 61.8287951\ttotal: 1m 30s\tremaining: 27.7s\n",
            "606:\tlearn: 61.8130084\ttotal: 1m 30s\tremaining: 27.5s\n",
            "607:\tlearn: 61.7944204\ttotal: 1m 30s\tremaining: 27.4s\n",
            "608:\tlearn: 61.7583825\ttotal: 1m 30s\tremaining: 27.2s\n",
            "609:\tlearn: 61.6638244\ttotal: 1m 30s\tremaining: 27.1s\n",
            "610:\tlearn: 61.6011462\ttotal: 1m 30s\tremaining: 26.9s\n",
            "611:\tlearn: 61.5613113\ttotal: 1m 31s\tremaining: 26.8s\n",
            "612:\tlearn: 61.5299703\ttotal: 1m 31s\tremaining: 26.6s\n",
            "613:\tlearn: 61.3909922\ttotal: 1m 31s\tremaining: 26.5s\n",
            "614:\tlearn: 61.3434607\ttotal: 1m 31s\tremaining: 26.3s\n",
            "615:\tlearn: 61.2792539\ttotal: 1m 31s\tremaining: 26.2s\n",
            "616:\tlearn: 61.2322313\ttotal: 1m 31s\tremaining: 26s\n",
            "617:\tlearn: 61.2116314\ttotal: 1m 31s\tremaining: 25.9s\n",
            "618:\tlearn: 61.1902456\ttotal: 1m 32s\tremaining: 25.7s\n",
            "619:\tlearn: 61.1632611\ttotal: 1m 32s\tremaining: 25.6s\n",
            "620:\tlearn: 61.1292888\ttotal: 1m 32s\tremaining: 25.4s\n",
            "621:\tlearn: 61.0710687\ttotal: 1m 32s\tremaining: 25.3s\n",
            "622:\tlearn: 61.0443849\ttotal: 1m 32s\tremaining: 25.1s\n",
            "623:\tlearn: 61.0397385\ttotal: 1m 32s\tremaining: 25s\n",
            "624:\tlearn: 60.8960374\ttotal: 1m 33s\tremaining: 24.9s\n",
            "625:\tlearn: 60.8588697\ttotal: 1m 33s\tremaining: 24.7s\n",
            "626:\tlearn: 60.8214775\ttotal: 1m 33s\tremaining: 24.6s\n",
            "627:\tlearn: 60.7441189\ttotal: 1m 33s\tremaining: 24.4s\n",
            "628:\tlearn: 60.7230199\ttotal: 1m 33s\tremaining: 24.3s\n",
            "629:\tlearn: 60.6268540\ttotal: 1m 33s\tremaining: 24.1s\n",
            "630:\tlearn: 60.5808943\ttotal: 1m 33s\tremaining: 24s\n",
            "631:\tlearn: 60.5486918\ttotal: 1m 34s\tremaining: 23.8s\n",
            "632:\tlearn: 60.4270401\ttotal: 1m 34s\tremaining: 23.7s\n",
            "633:\tlearn: 60.4090657\ttotal: 1m 34s\tremaining: 23.5s\n",
            "634:\tlearn: 60.3918415\ttotal: 1m 34s\tremaining: 23.4s\n",
            "635:\tlearn: 60.3412845\ttotal: 1m 34s\tremaining: 23.2s\n",
            "636:\tlearn: 60.3094091\ttotal: 1m 34s\tremaining: 23.1s\n",
            "637:\tlearn: 60.3034504\ttotal: 1m 35s\tremaining: 23s\n",
            "638:\tlearn: 60.2727142\ttotal: 1m 35s\tremaining: 22.8s\n",
            "639:\tlearn: 60.2044322\ttotal: 1m 35s\tremaining: 22.7s\n",
            "640:\tlearn: 60.1681822\ttotal: 1m 35s\tremaining: 22.5s\n",
            "641:\tlearn: 60.1244851\ttotal: 1m 35s\tremaining: 22.4s\n",
            "642:\tlearn: 60.0686040\ttotal: 1m 35s\tremaining: 22.2s\n",
            "643:\tlearn: 60.0364844\ttotal: 1m 36s\tremaining: 22.1s\n",
            "644:\tlearn: 60.0123012\ttotal: 1m 36s\tremaining: 21.9s\n",
            "645:\tlearn: 59.9916877\ttotal: 1m 36s\tremaining: 21.8s\n",
            "646:\tlearn: 59.8946089\ttotal: 1m 36s\tremaining: 21.6s\n",
            "647:\tlearn: 59.7995603\ttotal: 1m 36s\tremaining: 21.5s\n",
            "648:\tlearn: 59.7900228\ttotal: 1m 36s\tremaining: 21.3s\n",
            "649:\tlearn: 59.7536172\ttotal: 1m 36s\tremaining: 21.2s\n",
            "650:\tlearn: 59.7166573\ttotal: 1m 37s\tremaining: 21s\n",
            "651:\tlearn: 59.6882594\ttotal: 1m 37s\tremaining: 20.9s\n",
            "652:\tlearn: 59.6570391\ttotal: 1m 37s\tremaining: 20.7s\n",
            "653:\tlearn: 59.6308631\ttotal: 1m 37s\tremaining: 20.6s\n",
            "654:\tlearn: 59.6251519\ttotal: 1m 37s\tremaining: 20.4s\n",
            "655:\tlearn: 59.5998671\ttotal: 1m 37s\tremaining: 20.3s\n",
            "656:\tlearn: 59.5805994\ttotal: 1m 38s\tremaining: 20.2s\n",
            "657:\tlearn: 59.4975588\ttotal: 1m 38s\tremaining: 20s\n",
            "658:\tlearn: 59.4813758\ttotal: 1m 38s\tremaining: 19.9s\n",
            "659:\tlearn: 59.4463439\ttotal: 1m 38s\tremaining: 19.7s\n",
            "660:\tlearn: 59.4296842\ttotal: 1m 38s\tremaining: 19.6s\n",
            "661:\tlearn: 59.4055939\ttotal: 1m 38s\tremaining: 19.4s\n",
            "662:\tlearn: 59.3951530\ttotal: 1m 39s\tremaining: 19.3s\n",
            "663:\tlearn: 59.3584002\ttotal: 1m 39s\tremaining: 19.1s\n",
            "664:\tlearn: 59.3418952\ttotal: 1m 39s\tremaining: 19s\n",
            "665:\tlearn: 59.3217863\ttotal: 1m 39s\tremaining: 18.8s\n",
            "666:\tlearn: 59.3082222\ttotal: 1m 39s\tremaining: 18.7s\n",
            "667:\tlearn: 59.2790028\ttotal: 1m 39s\tremaining: 18.6s\n",
            "668:\tlearn: 59.2776910\ttotal: 1m 40s\tremaining: 18.4s\n",
            "669:\tlearn: 59.2606786\ttotal: 1m 40s\tremaining: 18.3s\n",
            "670:\tlearn: 59.2486485\ttotal: 1m 40s\tremaining: 18.1s\n",
            "671:\tlearn: 59.2038633\ttotal: 1m 40s\tremaining: 18s\n",
            "672:\tlearn: 59.1792847\ttotal: 1m 40s\tremaining: 17.8s\n",
            "673:\tlearn: 59.1541128\ttotal: 1m 40s\tremaining: 17.7s\n",
            "674:\tlearn: 59.0526889\ttotal: 1m 41s\tremaining: 17.5s\n",
            "675:\tlearn: 59.0147337\ttotal: 1m 41s\tremaining: 17.4s\n",
            "676:\tlearn: 58.9474677\ttotal: 1m 41s\tremaining: 17.2s\n",
            "677:\tlearn: 58.9014940\ttotal: 1m 41s\tremaining: 17.1s\n",
            "678:\tlearn: 58.8452067\ttotal: 1m 41s\tremaining: 16.9s\n",
            "679:\tlearn: 58.7357298\ttotal: 1m 41s\tremaining: 16.8s\n",
            "680:\tlearn: 58.6996121\ttotal: 1m 41s\tremaining: 16.6s\n",
            "681:\tlearn: 58.6773022\ttotal: 1m 42s\tremaining: 16.5s\n",
            "682:\tlearn: 58.6125615\ttotal: 1m 42s\tremaining: 16.3s\n",
            "683:\tlearn: 58.6021368\ttotal: 1m 42s\tremaining: 16.2s\n",
            "684:\tlearn: 58.5863307\ttotal: 1m 42s\tremaining: 16s\n",
            "685:\tlearn: 58.5724407\ttotal: 1m 42s\tremaining: 15.9s\n",
            "686:\tlearn: 58.5525507\ttotal: 1m 42s\tremaining: 15.7s\n",
            "687:\tlearn: 58.5230510\ttotal: 1m 43s\tremaining: 15.6s\n",
            "688:\tlearn: 58.4465654\ttotal: 1m 43s\tremaining: 15.4s\n",
            "689:\tlearn: 58.4038435\ttotal: 1m 43s\tremaining: 15.3s\n",
            "690:\tlearn: 58.3766375\ttotal: 1m 43s\tremaining: 15.1s\n",
            "691:\tlearn: 58.3104966\ttotal: 1m 43s\tremaining: 15s\n",
            "692:\tlearn: 58.2868385\ttotal: 1m 43s\tremaining: 14.8s\n",
            "693:\tlearn: 58.2705460\ttotal: 1m 43s\tremaining: 14.7s\n",
            "694:\tlearn: 58.1082391\ttotal: 1m 44s\tremaining: 14.5s\n",
            "695:\tlearn: 58.0790495\ttotal: 1m 44s\tremaining: 14.4s\n",
            "696:\tlearn: 57.9835542\ttotal: 1m 44s\tremaining: 14.2s\n",
            "697:\tlearn: 57.9623443\ttotal: 1m 44s\tremaining: 14.1s\n",
            "698:\tlearn: 57.9125582\ttotal: 1m 44s\tremaining: 13.9s\n",
            "699:\tlearn: 57.8910236\ttotal: 1m 44s\tremaining: 13.8s\n",
            "700:\tlearn: 57.8052398\ttotal: 1m 44s\tremaining: 13.6s\n",
            "701:\tlearn: 57.7920934\ttotal: 1m 45s\tremaining: 13.5s\n",
            "702:\tlearn: 57.6990277\ttotal: 1m 45s\tremaining: 13.3s\n",
            "703:\tlearn: 57.6801961\ttotal: 1m 45s\tremaining: 13.2s\n",
            "704:\tlearn: 57.6302206\ttotal: 1m 45s\tremaining: 13s\n",
            "705:\tlearn: 57.5792360\ttotal: 1m 45s\tremaining: 12.9s\n",
            "706:\tlearn: 57.5522718\ttotal: 1m 45s\tremaining: 12.7s\n",
            "707:\tlearn: 57.4624285\ttotal: 1m 46s\tremaining: 12.6s\n",
            "708:\tlearn: 57.4377976\ttotal: 1m 46s\tremaining: 12.4s\n",
            "709:\tlearn: 57.4124290\ttotal: 1m 46s\tremaining: 12.3s\n",
            "710:\tlearn: 57.2957364\ttotal: 1m 46s\tremaining: 12.1s\n",
            "711:\tlearn: 57.1937603\ttotal: 1m 46s\tremaining: 12s\n",
            "712:\tlearn: 57.1840419\ttotal: 1m 46s\tremaining: 11.8s\n",
            "713:\tlearn: 57.1207024\ttotal: 1m 46s\tremaining: 11.7s\n",
            "714:\tlearn: 57.0831535\ttotal: 1m 47s\tremaining: 11.5s\n",
            "715:\tlearn: 57.0134849\ttotal: 1m 47s\tremaining: 11.4s\n",
            "716:\tlearn: 56.9586341\ttotal: 1m 47s\tremaining: 11.2s\n",
            "717:\tlearn: 56.9307213\ttotal: 1m 47s\tremaining: 11.1s\n",
            "718:\tlearn: 56.8856564\ttotal: 1m 47s\tremaining: 10.9s\n",
            "719:\tlearn: 56.8555472\ttotal: 1m 47s\tremaining: 10.8s\n",
            "720:\tlearn: 56.8120241\ttotal: 1m 47s\tremaining: 10.6s\n",
            "721:\tlearn: 56.7917828\ttotal: 1m 48s\tremaining: 10.5s\n",
            "722:\tlearn: 56.7134307\ttotal: 1m 48s\tremaining: 10.3s\n",
            "723:\tlearn: 56.6502765\ttotal: 1m 48s\tremaining: 10.2s\n",
            "724:\tlearn: 56.5747488\ttotal: 1m 48s\tremaining: 10s\n",
            "725:\tlearn: 56.5153960\ttotal: 1m 48s\tremaining: 9.89s\n",
            "726:\tlearn: 56.4099682\ttotal: 1m 48s\tremaining: 9.74s\n",
            "727:\tlearn: 56.3040485\ttotal: 1m 49s\tremaining: 9.59s\n",
            "728:\tlearn: 56.2795497\ttotal: 1m 49s\tremaining: 9.44s\n",
            "729:\tlearn: 56.2583124\ttotal: 1m 49s\tremaining: 9.29s\n",
            "730:\tlearn: 56.2424874\ttotal: 1m 49s\tremaining: 9.14s\n",
            "731:\tlearn: 56.2092594\ttotal: 1m 49s\tremaining: 8.99s\n",
            "732:\tlearn: 56.1746238\ttotal: 1m 49s\tremaining: 8.84s\n",
            "733:\tlearn: 56.1563317\ttotal: 1m 50s\tremaining: 8.69s\n",
            "734:\tlearn: 56.0732016\ttotal: 1m 50s\tremaining: 8.54s\n",
            "735:\tlearn: 56.0677290\ttotal: 1m 50s\tremaining: 8.39s\n",
            "736:\tlearn: 55.9763308\ttotal: 1m 50s\tremaining: 8.24s\n",
            "737:\tlearn: 55.9407115\ttotal: 1m 50s\tremaining: 8.09s\n",
            "738:\tlearn: 55.8878520\ttotal: 1m 50s\tremaining: 7.94s\n",
            "739:\tlearn: 55.8842910\ttotal: 1m 50s\tremaining: 7.79s\n",
            "740:\tlearn: 55.8487898\ttotal: 1m 51s\tremaining: 7.65s\n",
            "741:\tlearn: 55.8206457\ttotal: 1m 51s\tremaining: 7.5s\n",
            "742:\tlearn: 55.7847898\ttotal: 1m 51s\tremaining: 7.35s\n",
            "743:\tlearn: 55.7499340\ttotal: 1m 51s\tremaining: 7.2s\n",
            "744:\tlearn: 55.7076257\ttotal: 1m 51s\tremaining: 7.05s\n",
            "745:\tlearn: 55.6911726\ttotal: 1m 51s\tremaining: 6.9s\n",
            "746:\tlearn: 55.6439496\ttotal: 1m 52s\tremaining: 6.75s\n",
            "747:\tlearn: 55.6166661\ttotal: 1m 52s\tremaining: 6.6s\n",
            "748:\tlearn: 55.5100894\ttotal: 1m 52s\tremaining: 6.45s\n",
            "749:\tlearn: 55.4564409\ttotal: 1m 52s\tremaining: 6.3s\n",
            "750:\tlearn: 55.4144073\ttotal: 1m 52s\tremaining: 6.15s\n",
            "751:\tlearn: 55.3668087\ttotal: 1m 52s\tremaining: 6s\n",
            "752:\tlearn: 55.3235270\ttotal: 1m 52s\tremaining: 5.85s\n",
            "753:\tlearn: 55.2950601\ttotal: 1m 53s\tremaining: 5.7s\n",
            "754:\tlearn: 55.2882467\ttotal: 1m 53s\tremaining: 5.55s\n",
            "755:\tlearn: 55.2493632\ttotal: 1m 53s\tremaining: 5.4s\n",
            "756:\tlearn: 55.2180204\ttotal: 1m 53s\tremaining: 5.25s\n",
            "757:\tlearn: 55.2059392\ttotal: 1m 53s\tremaining: 5.1s\n",
            "758:\tlearn: 55.1822252\ttotal: 1m 53s\tremaining: 4.95s\n",
            "759:\tlearn: 55.1726068\ttotal: 1m 54s\tremaining: 4.8s\n",
            "760:\tlearn: 55.1419340\ttotal: 1m 54s\tremaining: 4.65s\n",
            "761:\tlearn: 55.1081924\ttotal: 1m 54s\tremaining: 4.5s\n",
            "762:\tlearn: 55.0802693\ttotal: 1m 54s\tremaining: 4.35s\n",
            "763:\tlearn: 55.0344913\ttotal: 1m 54s\tremaining: 4.2s\n",
            "764:\tlearn: 55.0056173\ttotal: 1m 54s\tremaining: 4.05s\n",
            "765:\tlearn: 54.9927905\ttotal: 1m 54s\tremaining: 3.9s\n",
            "766:\tlearn: 54.9334475\ttotal: 1m 55s\tremaining: 3.75s\n",
            "767:\tlearn: 54.9011451\ttotal: 1m 55s\tremaining: 3.6s\n",
            "768:\tlearn: 54.8865945\ttotal: 1m 55s\tremaining: 3.45s\n",
            "769:\tlearn: 54.8541485\ttotal: 1m 55s\tremaining: 3.3s\n",
            "770:\tlearn: 54.8044033\ttotal: 1m 55s\tremaining: 3.15s\n",
            "771:\tlearn: 54.7210571\ttotal: 1m 55s\tremaining: 3s\n",
            "772:\tlearn: 54.6557696\ttotal: 1m 55s\tremaining: 2.85s\n",
            "773:\tlearn: 54.6036393\ttotal: 1m 56s\tremaining: 2.7s\n",
            "774:\tlearn: 54.5398570\ttotal: 1m 56s\tremaining: 2.55s\n",
            "775:\tlearn: 54.4884625\ttotal: 1m 56s\tremaining: 2.4s\n",
            "776:\tlearn: 54.4697884\ttotal: 1m 56s\tremaining: 2.25s\n",
            "777:\tlearn: 54.4297841\ttotal: 1m 56s\tremaining: 2.1s\n",
            "778:\tlearn: 54.3724399\ttotal: 1m 56s\tremaining: 1.95s\n",
            "779:\tlearn: 54.3517578\ttotal: 1m 56s\tremaining: 1.8s\n",
            "780:\tlearn: 54.3268702\ttotal: 1m 57s\tremaining: 1.65s\n",
            "781:\tlearn: 54.2944148\ttotal: 1m 57s\tremaining: 1.5s\n",
            "782:\tlearn: 54.2630789\ttotal: 1m 57s\tremaining: 1.35s\n",
            "783:\tlearn: 54.2591218\ttotal: 1m 57s\tremaining: 1.2s\n",
            "784:\tlearn: 54.1495151\ttotal: 1m 57s\tremaining: 1.05s\n",
            "785:\tlearn: 54.1048685\ttotal: 1m 57s\tremaining: 900ms\n",
            "786:\tlearn: 54.0450739\ttotal: 1m 58s\tremaining: 750ms\n",
            "787:\tlearn: 54.0245056\ttotal: 1m 58s\tremaining: 600ms\n",
            "788:\tlearn: 53.9673107\ttotal: 1m 58s\tremaining: 450ms\n",
            "789:\tlearn: 53.9240161\ttotal: 1m 58s\tremaining: 300ms\n",
            "790:\tlearn: 53.9001067\ttotal: 1m 58s\tremaining: 150ms\n",
            "791:\tlearn: 53.8704109\ttotal: 1m 58s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 05:35:54,142]\u001b[0m A new study created in memory with name: no-name-9a45990b-9b73-4ff6-9786-80073f391832\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 05:38:13,591]\u001b[0m Trial 0 finished with value: 57.20639981749408 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 57.20639981749408.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:40:26,699]\u001b[0m Trial 1 finished with value: 46.48582358805089 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:42:35,209]\u001b[0m Trial 2 finished with value: 46.53512271328078 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:57:39,962]\u001b[0m Trial 3 finished with value: 77.91200544407783 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:59:56,575]\u001b[0m Trial 4 finished with value: 55.88042439912455 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:17:25,326]\u001b[0m Trial 5 finished with value: 64.88535161091701 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:20:39,902]\u001b[0m Trial 6 finished with value: 46.001003133443334 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:23:23,039]\u001b[0m Trial 7 finished with value: 50.71947372107086 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:25:00,558]\u001b[0m Trial 8 finished with value: 67.84803326366799 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:41:41,840]\u001b[0m Trial 9 finished with value: 56.31175308973086 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:42:45,157]\u001b[0m Trial 10 finished with value: 80.50860446373237 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:44:59,863]\u001b[0m Trial 11 finished with value: 54.871069202346874 and parameters: {'iterations': 629, 'learning_rate': 0.21833643647270726, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 55.54864045530132, 'subsample': 0.9997353402587645, 'random_strength': 13.017338441324387, 'od_wait': 107, 'leaf_estimation_iterations': 20, 'bagging_temperature': 99.66157629574, 'colsample_bylevel': 0.6550388874489359}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:47:11,251]\u001b[0m Trial 12 finished with value: 47.42126402536329 and parameters: {'iterations': 732, 'learning_rate': 0.3353168481850217, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 55.768778894287756, 'subsample': 0.9669447478647194, 'random_strength': 17.767083773926824, 'od_wait': 148, 'leaf_estimation_iterations': 14, 'bagging_temperature': 12.841276123711907, 'colsample_bylevel': 0.5247297179758806}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:54:27,717]\u001b[0m Trial 13 finished with value: 45.38507370774545 and parameters: {'iterations': 533, 'learning_rate': 0.3582220796656632, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 69.46275838928238, 'subsample': 0.8540806581123832, 'random_strength': 28.159360754861588, 'od_wait': 125, 'leaf_estimation_iterations': 15, 'bagging_temperature': 49.644424887945355, 'colsample_bylevel': 0.7998758691220191}. Best is trial 13 with value: 45.38507370774545.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:08:20,730]\u001b[0m Trial 14 finished with value: 48.41706671254681 and parameters: {'iterations': 570, 'learning_rate': 0.3473522362771956, 'depth': 13, 'min_data_in_leaf': 27, 'reg_lambda': 79.01726191500387, 'subsample': 0.8397598233779698, 'random_strength': 67.36611186701461, 'od_wait': 91, 'leaf_estimation_iterations': 15, 'bagging_temperature': 36.02291324873896, 'colsample_bylevel': 0.7183006087448827}. Best is trial 13 with value: 45.38507370774545.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:18:01,062]\u001b[0m Trial 15 finished with value: 43.48887650381214 and parameters: {'iterations': 783, 'learning_rate': 0.2584880573013857, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 45.02231140060744, 'subsample': 0.6753500090427036, 'random_strength': 21.788829786258407, 'od_wait': 90, 'leaf_estimation_iterations': 16, 'bagging_temperature': 58.67357912276298, 'colsample_bylevel': 0.5379457040607726}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:25:34,068]\u001b[0m Trial 16 finished with value: 50.48731219910578 and parameters: {'iterations': 790, 'learning_rate': 0.36827523538520046, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 71.36224159383978, 'subsample': 0.6728869610125017, 'random_strength': 22.248322331820447, 'od_wait': 93, 'leaf_estimation_iterations': 16, 'bagging_temperature': 68.07824674104201, 'colsample_bylevel': 0.1566786862837674}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:34:49,001]\u001b[0m Trial 17 finished with value: 51.794375401241304 and parameters: {'iterations': 309, 'learning_rate': 0.23406832707723646, 'depth': 14, 'min_data_in_leaf': 30, 'reg_lambda': 43.32432635230013, 'subsample': 0.6160947515585349, 'random_strength': 10.274637826660694, 'od_wait': 128, 'leaf_estimation_iterations': 13, 'bagging_temperature': 59.49424678491784, 'colsample_bylevel': 0.4840114290491303}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:39:22,886]\u001b[0m Trial 18 finished with value: 55.79856928137143 and parameters: {'iterations': 527, 'learning_rate': 0.10874463140581817, 'depth': 12, 'min_data_in_leaf': 20, 'reg_lambda': 85.69461753584385, 'subsample': 0.7309339623530247, 'random_strength': 44.07889631754874, 'od_wait': 134, 'leaf_estimation_iterations': 16, 'bagging_temperature': 20.40268545253036, 'colsample_bylevel': 0.2640711272382002}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:57:40,488]\u001b[0m Trial 19 finished with value: 58.93536158593662 and parameters: {'iterations': 638, 'learning_rate': 0.4175147995093469, 'depth': 14, 'min_data_in_leaf': 26, 'reg_lambda': 43.908029962396185, 'subsample': 0.8923653279928291, 'random_strength': 81.67179289193317, 'od_wait': 102, 'leaf_estimation_iterations': 18, 'bagging_temperature': 49.18172333090726, 'colsample_bylevel': 0.5389911542559457}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 5: 43.48887650381214\n",
            "Best hyperparameters for fold 5: {'iterations': 783, 'learning_rate': 0.2584880573013857, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 45.02231140060744, 'subsample': 0.6753500090427036, 'random_strength': 21.788829786258407, 'od_wait': 90, 'leaf_estimation_iterations': 16, 'bagging_temperature': 58.67357912276298, 'colsample_bylevel': 0.5379457040607726}\n",
            "0:\tlearn: 205.2582218\ttotal: 697ms\tremaining: 9m 5s\n",
            "1:\tlearn: 204.2043019\ttotal: 886ms\tremaining: 5m 45s\n",
            "2:\tlearn: 203.0624090\ttotal: 1.01s\tremaining: 4m 23s\n",
            "3:\tlearn: 202.0806996\ttotal: 1.23s\tremaining: 3m 59s\n",
            "4:\tlearn: 200.1313413\ttotal: 1.92s\tremaining: 4m 59s\n",
            "5:\tlearn: 198.2753633\ttotal: 2.61s\tremaining: 5m 37s\n",
            "6:\tlearn: 197.5619923\ttotal: 3.39s\tremaining: 6m 16s\n",
            "7:\tlearn: 197.5466068\ttotal: 3.45s\tremaining: 5m 34s\n",
            "8:\tlearn: 197.1022517\ttotal: 3.64s\tremaining: 5m 13s\n",
            "9:\tlearn: 196.9068203\ttotal: 3.75s\tremaining: 4m 50s\n",
            "10:\tlearn: 196.8625524\ttotal: 3.9s\tremaining: 4m 33s\n",
            "11:\tlearn: 195.7974895\ttotal: 4.6s\tremaining: 4m 55s\n",
            "12:\tlearn: 195.5563617\ttotal: 5s\tremaining: 4m 56s\n",
            "13:\tlearn: 195.5528132\ttotal: 5.07s\tremaining: 4m 38s\n",
            "14:\tlearn: 195.4150949\ttotal: 5.32s\tremaining: 4m 32s\n",
            "15:\tlearn: 194.7677278\ttotal: 6.05s\tremaining: 4m 50s\n",
            "16:\tlearn: 194.7328622\ttotal: 6.19s\tremaining: 4m 38s\n",
            "17:\tlearn: 194.5833479\ttotal: 6.36s\tremaining: 4m 30s\n",
            "18:\tlearn: 194.5630793\ttotal: 6.44s\tremaining: 4m 18s\n",
            "19:\tlearn: 194.0990671\ttotal: 6.6s\tremaining: 4m 11s\n",
            "20:\tlearn: 194.0966539\ttotal: 6.66s\tremaining: 4m 1s\n",
            "21:\tlearn: 194.0334212\ttotal: 6.78s\tremaining: 3m 54s\n",
            "22:\tlearn: 193.8010498\ttotal: 7.53s\tremaining: 4m 8s\n",
            "23:\tlearn: 193.8005653\ttotal: 7.59s\tremaining: 4m\n",
            "24:\tlearn: 192.2456125\ttotal: 8.28s\tremaining: 4m 11s\n",
            "25:\tlearn: 191.4975149\ttotal: 8.98s\tremaining: 4m 21s\n",
            "26:\tlearn: 191.1153200\ttotal: 9.66s\tremaining: 4m 30s\n",
            "27:\tlearn: 191.0134430\ttotal: 10.4s\tremaining: 4m 40s\n",
            "28:\tlearn: 190.3881858\ttotal: 11.1s\tremaining: 4m 48s\n",
            "29:\tlearn: 190.3664000\ttotal: 11.2s\tremaining: 4m 41s\n",
            "30:\tlearn: 189.7721080\ttotal: 11.4s\tremaining: 4m 35s\n",
            "31:\tlearn: 189.3626907\ttotal: 11.5s\tremaining: 4m 28s\n",
            "32:\tlearn: 189.3574213\ttotal: 11.5s\tremaining: 4m 22s\n",
            "33:\tlearn: 189.3170066\ttotal: 11.7s\tremaining: 4m 17s\n",
            "34:\tlearn: 188.5953449\ttotal: 11.9s\tremaining: 4m 14s\n",
            "35:\tlearn: 188.5953445\ttotal: 12s\tremaining: 4m 8s\n",
            "36:\tlearn: 188.5952932\ttotal: 12s\tremaining: 4m 2s\n",
            "37:\tlearn: 188.5924858\ttotal: 12.1s\tremaining: 3m 57s\n",
            "38:\tlearn: 188.5851530\ttotal: 12.2s\tremaining: 3m 53s\n",
            "39:\tlearn: 188.5845305\ttotal: 12.3s\tremaining: 3m 48s\n",
            "40:\tlearn: 188.4670431\ttotal: 12.5s\tremaining: 3m 46s\n",
            "41:\tlearn: 188.0799683\ttotal: 13.2s\tremaining: 3m 52s\n",
            "42:\tlearn: 187.3781470\ttotal: 13.8s\tremaining: 3m 58s\n",
            "43:\tlearn: 187.0137253\ttotal: 14.5s\tremaining: 4m 4s\n",
            "44:\tlearn: 186.9213084\ttotal: 14.6s\tremaining: 4m\n",
            "45:\tlearn: 186.5733896\ttotal: 15.4s\tremaining: 4m 6s\n",
            "46:\tlearn: 186.5654044\ttotal: 15.5s\tremaining: 4m 2s\n",
            "47:\tlearn: 186.0539578\ttotal: 16.2s\tremaining: 4m 7s\n",
            "48:\tlearn: 185.9152955\ttotal: 16.9s\tremaining: 4m 13s\n",
            "49:\tlearn: 185.7445747\ttotal: 17.6s\tremaining: 4m 17s\n",
            "50:\tlearn: 185.0050928\ttotal: 17.7s\tremaining: 4m 14s\n",
            "51:\tlearn: 184.4358044\ttotal: 18.4s\tremaining: 4m 18s\n",
            "52:\tlearn: 184.3113793\ttotal: 18.5s\tremaining: 4m 15s\n",
            "53:\tlearn: 179.8821559\ttotal: 19.2s\tremaining: 4m 18s\n",
            "54:\tlearn: 179.4633260\ttotal: 19.9s\tremaining: 4m 23s\n",
            "55:\tlearn: 174.3650744\ttotal: 20.5s\tremaining: 4m 26s\n",
            "56:\tlearn: 174.3339013\ttotal: 20.6s\tremaining: 4m 22s\n",
            "57:\tlearn: 173.4821138\ttotal: 21.4s\tremaining: 4m 26s\n",
            "58:\tlearn: 170.6873931\ttotal: 22s\tremaining: 4m 29s\n",
            "59:\tlearn: 169.5589200\ttotal: 22.7s\tremaining: 4m 33s\n",
            "60:\tlearn: 168.7746577\ttotal: 23.3s\tremaining: 4m 35s\n",
            "61:\tlearn: 167.6970339\ttotal: 23.9s\tremaining: 4m 37s\n",
            "62:\tlearn: 166.7728626\ttotal: 24.5s\tremaining: 4m 39s\n",
            "63:\tlearn: 163.1831319\ttotal: 25.1s\tremaining: 4m 42s\n",
            "64:\tlearn: 159.9892709\ttotal: 25.7s\tremaining: 4m 43s\n",
            "65:\tlearn: 157.3274977\ttotal: 26.3s\tremaining: 4m 45s\n",
            "66:\tlearn: 155.5405567\ttotal: 26.9s\tremaining: 4m 47s\n",
            "67:\tlearn: 154.0512526\ttotal: 27.4s\tremaining: 4m 48s\n",
            "68:\tlearn: 152.5627027\ttotal: 28s\tremaining: 4m 49s\n",
            "69:\tlearn: 151.8313768\ttotal: 28.6s\tremaining: 4m 51s\n",
            "70:\tlearn: 150.6744616\ttotal: 29.2s\tremaining: 4m 53s\n",
            "71:\tlearn: 148.5478697\ttotal: 29.8s\tremaining: 4m 54s\n",
            "72:\tlearn: 146.1305968\ttotal: 30.3s\tremaining: 4m 54s\n",
            "73:\tlearn: 143.3174088\ttotal: 30.9s\tremaining: 4m 55s\n",
            "74:\tlearn: 142.3827071\ttotal: 31.6s\tremaining: 4m 58s\n",
            "75:\tlearn: 141.4858803\ttotal: 32.2s\tremaining: 4m 59s\n",
            "76:\tlearn: 140.0506573\ttotal: 32.8s\tremaining: 5m\n",
            "77:\tlearn: 139.1223777\ttotal: 33.4s\tremaining: 5m 1s\n",
            "78:\tlearn: 138.3133799\ttotal: 33.9s\tremaining: 5m 2s\n",
            "79:\tlearn: 137.4140883\ttotal: 34.6s\tremaining: 5m 4s\n",
            "80:\tlearn: 135.5996477\ttotal: 35.1s\tremaining: 5m 4s\n",
            "81:\tlearn: 133.3842636\ttotal: 35.6s\tremaining: 5m 4s\n",
            "82:\tlearn: 132.6716511\ttotal: 36.2s\tremaining: 5m 5s\n",
            "83:\tlearn: 131.9649255\ttotal: 36.9s\tremaining: 5m 7s\n",
            "84:\tlearn: 131.5524589\ttotal: 37.1s\tremaining: 5m 4s\n",
            "85:\tlearn: 130.3050949\ttotal: 37.6s\tremaining: 5m 5s\n",
            "86:\tlearn: 129.3506829\ttotal: 38.2s\tremaining: 5m 5s\n",
            "87:\tlearn: 128.8139323\ttotal: 38.9s\tremaining: 5m 7s\n",
            "88:\tlearn: 127.6534784\ttotal: 39.5s\tremaining: 5m 7s\n",
            "89:\tlearn: 127.1878359\ttotal: 40.3s\tremaining: 5m 9s\n",
            "90:\tlearn: 125.3087942\ttotal: 40.8s\tremaining: 5m 10s\n",
            "91:\tlearn: 124.6500328\ttotal: 41.5s\tremaining: 5m 11s\n",
            "92:\tlearn: 123.4858986\ttotal: 42.1s\tremaining: 5m 12s\n",
            "93:\tlearn: 122.6510694\ttotal: 42.7s\tremaining: 5m 12s\n",
            "94:\tlearn: 121.6690360\ttotal: 43.2s\tremaining: 5m 13s\n",
            "95:\tlearn: 120.7220411\ttotal: 43.8s\tremaining: 5m 13s\n",
            "96:\tlearn: 120.1693591\ttotal: 44.4s\tremaining: 5m 14s\n",
            "97:\tlearn: 119.6674074\ttotal: 45.1s\tremaining: 5m 15s\n",
            "98:\tlearn: 118.5808625\ttotal: 45.7s\tremaining: 5m 15s\n",
            "99:\tlearn: 117.8960115\ttotal: 46.3s\tremaining: 5m 16s\n",
            "100:\tlearn: 117.8365532\ttotal: 47s\tremaining: 5m 17s\n",
            "101:\tlearn: 117.4055944\ttotal: 47.7s\tremaining: 5m 18s\n",
            "102:\tlearn: 116.3774963\ttotal: 48.3s\tremaining: 5m 18s\n",
            "103:\tlearn: 115.7535861\ttotal: 48.9s\tremaining: 5m 19s\n",
            "104:\tlearn: 115.1051666\ttotal: 49.5s\tremaining: 5m 19s\n",
            "105:\tlearn: 114.1374441\ttotal: 50s\tremaining: 5m 19s\n",
            "106:\tlearn: 113.4097496\ttotal: 50.6s\tremaining: 5m 19s\n",
            "107:\tlearn: 113.1207241\ttotal: 51.2s\tremaining: 5m 19s\n",
            "108:\tlearn: 112.7457337\ttotal: 51.8s\tremaining: 5m 20s\n",
            "109:\tlearn: 112.2990859\ttotal: 52.4s\tremaining: 5m 20s\n",
            "110:\tlearn: 111.8520897\ttotal: 53s\tremaining: 5m 20s\n",
            "111:\tlearn: 111.1440661\ttotal: 53.6s\tremaining: 5m 20s\n",
            "112:\tlearn: 110.8162098\ttotal: 54.1s\tremaining: 5m 20s\n",
            "113:\tlearn: 110.3511621\ttotal: 54.7s\tremaining: 5m 21s\n",
            "114:\tlearn: 109.6671280\ttotal: 55.3s\tremaining: 5m 21s\n",
            "115:\tlearn: 108.2785391\ttotal: 55.9s\tremaining: 5m 21s\n",
            "116:\tlearn: 107.8243424\ttotal: 56.5s\tremaining: 5m 21s\n",
            "117:\tlearn: 107.5440460\ttotal: 57.1s\tremaining: 5m 21s\n",
            "118:\tlearn: 107.0519033\ttotal: 57.7s\tremaining: 5m 22s\n",
            "119:\tlearn: 106.2106836\ttotal: 58.3s\tremaining: 5m 22s\n",
            "120:\tlearn: 105.2724782\ttotal: 58.8s\tremaining: 5m 21s\n",
            "121:\tlearn: 104.7771442\ttotal: 59.4s\tremaining: 5m 21s\n",
            "122:\tlearn: 104.4603940\ttotal: 1m\tremaining: 5m 22s\n",
            "123:\tlearn: 103.6761125\ttotal: 1m\tremaining: 5m 21s\n",
            "124:\tlearn: 103.1935313\ttotal: 1m 1s\tremaining: 5m 22s\n",
            "125:\tlearn: 102.3790608\ttotal: 1m 1s\tremaining: 5m 22s\n",
            "126:\tlearn: 102.0425058\ttotal: 1m 2s\tremaining: 5m 22s\n",
            "127:\tlearn: 101.4040974\ttotal: 1m 3s\tremaining: 5m 22s\n",
            "128:\tlearn: 100.3391653\ttotal: 1m 3s\tremaining: 5m 22s\n",
            "129:\tlearn: 99.7830425\ttotal: 1m 4s\tremaining: 5m 22s\n",
            "130:\tlearn: 99.2970352\ttotal: 1m 4s\tremaining: 5m 22s\n",
            "131:\tlearn: 99.0233446\ttotal: 1m 5s\tremaining: 5m 22s\n",
            "132:\tlearn: 98.7678563\ttotal: 1m 6s\tremaining: 5m 22s\n",
            "133:\tlearn: 98.4083157\ttotal: 1m 6s\tremaining: 5m 22s\n",
            "134:\tlearn: 98.1371940\ttotal: 1m 7s\tremaining: 5m 22s\n",
            "135:\tlearn: 97.6258881\ttotal: 1m 7s\tremaining: 5m 22s\n",
            "136:\tlearn: 96.7808313\ttotal: 1m 8s\tremaining: 5m 22s\n",
            "137:\tlearn: 96.1054475\ttotal: 1m 9s\tremaining: 5m 22s\n",
            "138:\tlearn: 95.6788122\ttotal: 1m 9s\tremaining: 5m 22s\n",
            "139:\tlearn: 95.3078639\ttotal: 1m 10s\tremaining: 5m 22s\n",
            "140:\tlearn: 94.9477548\ttotal: 1m 10s\tremaining: 5m 22s\n",
            "141:\tlearn: 94.4250681\ttotal: 1m 11s\tremaining: 5m 22s\n",
            "142:\tlearn: 94.2216089\ttotal: 1m 12s\tremaining: 5m 22s\n",
            "143:\tlearn: 93.7533412\ttotal: 1m 12s\tremaining: 5m 22s\n",
            "144:\tlearn: 93.5447589\ttotal: 1m 13s\tremaining: 5m 21s\n",
            "145:\tlearn: 92.9855469\ttotal: 1m 13s\tremaining: 5m 21s\n",
            "146:\tlearn: 92.6639535\ttotal: 1m 14s\tremaining: 5m 21s\n",
            "147:\tlearn: 91.8475118\ttotal: 1m 14s\tremaining: 5m 21s\n",
            "148:\tlearn: 91.6232433\ttotal: 1m 15s\tremaining: 5m 21s\n",
            "149:\tlearn: 91.2798941\ttotal: 1m 16s\tremaining: 5m 21s\n",
            "150:\tlearn: 90.6670996\ttotal: 1m 16s\tremaining: 5m 20s\n",
            "151:\tlearn: 90.4724180\ttotal: 1m 17s\tremaining: 5m 21s\n",
            "152:\tlearn: 90.0291767\ttotal: 1m 17s\tremaining: 5m 20s\n",
            "153:\tlearn: 89.4812569\ttotal: 1m 18s\tremaining: 5m 20s\n",
            "154:\tlearn: 89.1915029\ttotal: 1m 19s\tremaining: 5m 20s\n",
            "155:\tlearn: 88.7095959\ttotal: 1m 19s\tremaining: 5m 19s\n",
            "156:\tlearn: 88.3278851\ttotal: 1m 20s\tremaining: 5m 19s\n",
            "157:\tlearn: 87.8235846\ttotal: 1m 20s\tremaining: 5m 19s\n",
            "158:\tlearn: 87.5532449\ttotal: 1m 21s\tremaining: 5m 19s\n",
            "159:\tlearn: 87.2677775\ttotal: 1m 21s\tremaining: 5m 19s\n",
            "160:\tlearn: 87.0726203\ttotal: 1m 22s\tremaining: 5m 19s\n",
            "161:\tlearn: 86.6094932\ttotal: 1m 23s\tremaining: 5m 19s\n",
            "162:\tlearn: 86.4273053\ttotal: 1m 23s\tremaining: 5m 19s\n",
            "163:\tlearn: 85.7673184\ttotal: 1m 24s\tremaining: 5m 19s\n",
            "164:\tlearn: 85.4040354\ttotal: 1m 25s\tremaining: 5m 18s\n",
            "165:\tlearn: 85.1999491\ttotal: 1m 25s\tremaining: 5m 18s\n",
            "166:\tlearn: 84.9621772\ttotal: 1m 26s\tremaining: 5m 18s\n",
            "167:\tlearn: 84.6062781\ttotal: 1m 27s\tremaining: 5m 18s\n",
            "168:\tlearn: 84.1794285\ttotal: 1m 27s\tremaining: 5m 18s\n",
            "169:\tlearn: 83.7615996\ttotal: 1m 28s\tremaining: 5m 18s\n",
            "170:\tlearn: 82.9398595\ttotal: 1m 28s\tremaining: 5m 17s\n",
            "171:\tlearn: 82.6194478\ttotal: 1m 29s\tremaining: 5m 17s\n",
            "172:\tlearn: 81.7443117\ttotal: 1m 29s\tremaining: 5m 17s\n",
            "173:\tlearn: 81.4662223\ttotal: 1m 30s\tremaining: 5m 17s\n",
            "174:\tlearn: 81.3467401\ttotal: 1m 31s\tremaining: 5m 16s\n",
            "175:\tlearn: 80.8408738\ttotal: 1m 31s\tremaining: 5m 16s\n",
            "176:\tlearn: 80.6436904\ttotal: 1m 32s\tremaining: 5m 16s\n",
            "177:\tlearn: 80.2419585\ttotal: 1m 32s\tremaining: 5m 15s\n",
            "178:\tlearn: 80.0795610\ttotal: 1m 33s\tremaining: 5m 15s\n",
            "179:\tlearn: 79.7376477\ttotal: 1m 34s\tremaining: 5m 15s\n",
            "180:\tlearn: 79.5050825\ttotal: 1m 34s\tremaining: 5m 15s\n",
            "181:\tlearn: 79.2301648\ttotal: 1m 35s\tremaining: 5m 14s\n",
            "182:\tlearn: 79.0876139\ttotal: 1m 35s\tremaining: 5m 14s\n",
            "183:\tlearn: 78.9345669\ttotal: 1m 36s\tremaining: 5m 14s\n",
            "184:\tlearn: 78.7648952\ttotal: 1m 37s\tremaining: 5m 13s\n",
            "185:\tlearn: 78.6266448\ttotal: 1m 37s\tremaining: 5m 13s\n",
            "186:\tlearn: 78.4909269\ttotal: 1m 38s\tremaining: 5m 13s\n",
            "187:\tlearn: 78.2841773\ttotal: 1m 39s\tremaining: 5m 13s\n",
            "188:\tlearn: 77.9203038\ttotal: 1m 39s\tremaining: 5m 13s\n",
            "189:\tlearn: 77.6024264\ttotal: 1m 40s\tremaining: 5m 12s\n",
            "190:\tlearn: 77.4380813\ttotal: 1m 40s\tremaining: 5m 12s\n",
            "191:\tlearn: 77.2022733\ttotal: 1m 41s\tremaining: 5m 12s\n",
            "192:\tlearn: 77.0057350\ttotal: 1m 42s\tremaining: 5m 12s\n",
            "193:\tlearn: 76.6233452\ttotal: 1m 42s\tremaining: 5m 11s\n",
            "194:\tlearn: 76.3701704\ttotal: 1m 43s\tremaining: 5m 11s\n",
            "195:\tlearn: 76.2505206\ttotal: 1m 43s\tremaining: 5m 11s\n",
            "196:\tlearn: 76.1133439\ttotal: 1m 44s\tremaining: 5m 10s\n",
            "197:\tlearn: 75.9119226\ttotal: 1m 45s\tremaining: 5m 10s\n",
            "198:\tlearn: 75.5994094\ttotal: 1m 45s\tremaining: 5m 10s\n",
            "199:\tlearn: 75.4896616\ttotal: 1m 46s\tremaining: 5m 10s\n",
            "200:\tlearn: 75.1414828\ttotal: 1m 46s\tremaining: 5m 9s\n",
            "201:\tlearn: 75.0120142\ttotal: 1m 47s\tremaining: 5m 8s\n",
            "202:\tlearn: 74.5245649\ttotal: 1m 47s\tremaining: 5m 8s\n",
            "203:\tlearn: 74.1408265\ttotal: 1m 48s\tremaining: 5m 8s\n",
            "204:\tlearn: 73.8293844\ttotal: 1m 49s\tremaining: 5m 7s\n",
            "205:\tlearn: 73.5035190\ttotal: 1m 49s\tremaining: 5m 7s\n",
            "206:\tlearn: 73.4377219\ttotal: 1m 50s\tremaining: 5m 7s\n",
            "207:\tlearn: 73.2475093\ttotal: 1m 51s\tremaining: 5m 7s\n",
            "208:\tlearn: 72.9557016\ttotal: 1m 51s\tremaining: 5m 7s\n",
            "209:\tlearn: 72.7434648\ttotal: 1m 52s\tremaining: 5m 6s\n",
            "210:\tlearn: 72.3213175\ttotal: 1m 52s\tremaining: 5m 6s\n",
            "211:\tlearn: 72.1219031\ttotal: 1m 53s\tremaining: 5m 5s\n",
            "212:\tlearn: 72.1200984\ttotal: 1m 54s\tremaining: 5m 5s\n",
            "213:\tlearn: 71.8601575\ttotal: 1m 54s\tremaining: 5m 4s\n",
            "214:\tlearn: 71.6209994\ttotal: 1m 55s\tremaining: 5m 4s\n",
            "215:\tlearn: 71.4438724\ttotal: 1m 55s\tremaining: 5m 4s\n",
            "216:\tlearn: 71.1588179\ttotal: 1m 56s\tremaining: 5m 4s\n",
            "217:\tlearn: 71.0286674\ttotal: 1m 57s\tremaining: 5m 3s\n",
            "218:\tlearn: 70.7043551\ttotal: 1m 57s\tremaining: 5m 3s\n",
            "219:\tlearn: 70.2919778\ttotal: 1m 58s\tremaining: 5m 2s\n",
            "220:\tlearn: 69.9596030\ttotal: 1m 58s\tremaining: 5m 2s\n",
            "221:\tlearn: 69.7639830\ttotal: 1m 59s\tremaining: 5m 2s\n",
            "222:\tlearn: 69.6062866\ttotal: 2m\tremaining: 5m 2s\n",
            "223:\tlearn: 69.4224223\ttotal: 2m\tremaining: 5m 1s\n",
            "224:\tlearn: 69.2980152\ttotal: 2m 1s\tremaining: 5m 1s\n",
            "225:\tlearn: 69.1732114\ttotal: 2m 2s\tremaining: 5m\n",
            "226:\tlearn: 69.0263715\ttotal: 2m 2s\tremaining: 5m\n",
            "227:\tlearn: 68.8954782\ttotal: 2m 3s\tremaining: 4m 59s\n",
            "228:\tlearn: 68.6263961\ttotal: 2m 3s\tremaining: 4m 59s\n",
            "229:\tlearn: 68.2997282\ttotal: 2m 4s\tremaining: 4m 59s\n",
            "230:\tlearn: 67.8958630\ttotal: 2m 5s\tremaining: 4m 58s\n",
            "231:\tlearn: 67.6973274\ttotal: 2m 5s\tremaining: 4m 58s\n",
            "232:\tlearn: 67.5026703\ttotal: 2m 6s\tremaining: 4m 57s\n",
            "233:\tlearn: 67.1734353\ttotal: 2m 6s\tremaining: 4m 57s\n",
            "234:\tlearn: 66.8735067\ttotal: 2m 7s\tremaining: 4m 57s\n",
            "235:\tlearn: 66.7390613\ttotal: 2m 8s\tremaining: 4m 56s\n",
            "236:\tlearn: 66.5672454\ttotal: 2m 8s\tremaining: 4m 56s\n",
            "237:\tlearn: 66.4180904\ttotal: 2m 9s\tremaining: 4m 55s\n",
            "238:\tlearn: 66.2449955\ttotal: 2m 9s\tremaining: 4m 55s\n",
            "239:\tlearn: 65.9296421\ttotal: 2m 10s\tremaining: 4m 55s\n",
            "240:\tlearn: 65.4417331\ttotal: 2m 11s\tremaining: 4m 54s\n",
            "241:\tlearn: 65.1857699\ttotal: 2m 11s\tremaining: 4m 54s\n",
            "242:\tlearn: 65.0784167\ttotal: 2m 12s\tremaining: 4m 53s\n",
            "243:\tlearn: 64.8013356\ttotal: 2m 12s\tremaining: 4m 53s\n",
            "244:\tlearn: 64.6948768\ttotal: 2m 13s\tremaining: 4m 52s\n",
            "245:\tlearn: 64.5520728\ttotal: 2m 14s\tremaining: 4m 52s\n",
            "246:\tlearn: 64.4337424\ttotal: 2m 14s\tremaining: 4m 52s\n",
            "247:\tlearn: 64.1116615\ttotal: 2m 15s\tremaining: 4m 51s\n",
            "248:\tlearn: 63.9751352\ttotal: 2m 15s\tremaining: 4m 51s\n",
            "249:\tlearn: 63.8415193\ttotal: 2m 16s\tremaining: 4m 50s\n",
            "250:\tlearn: 63.7471994\ttotal: 2m 17s\tremaining: 4m 50s\n",
            "251:\tlearn: 63.5493159\ttotal: 2m 17s\tremaining: 4m 50s\n",
            "252:\tlearn: 63.4133729\ttotal: 2m 18s\tremaining: 4m 49s\n",
            "253:\tlearn: 63.1958374\ttotal: 2m 18s\tremaining: 4m 49s\n",
            "254:\tlearn: 62.9385122\ttotal: 2m 19s\tremaining: 4m 48s\n",
            "255:\tlearn: 62.6812261\ttotal: 2m 20s\tremaining: 4m 48s\n",
            "256:\tlearn: 62.5480673\ttotal: 2m 20s\tremaining: 4m 48s\n",
            "257:\tlearn: 62.4433851\ttotal: 2m 21s\tremaining: 4m 47s\n",
            "258:\tlearn: 62.3552898\ttotal: 2m 21s\tremaining: 4m 47s\n",
            "259:\tlearn: 62.3184480\ttotal: 2m 22s\tremaining: 4m 47s\n",
            "260:\tlearn: 62.1212563\ttotal: 2m 23s\tremaining: 4m 46s\n",
            "261:\tlearn: 62.0131807\ttotal: 2m 23s\tremaining: 4m 46s\n",
            "262:\tlearn: 61.8327061\ttotal: 2m 24s\tremaining: 4m 45s\n",
            "263:\tlearn: 61.8326044\ttotal: 2m 25s\tremaining: 4m 45s\n",
            "264:\tlearn: 61.7009173\ttotal: 2m 25s\tremaining: 4m 44s\n",
            "265:\tlearn: 61.5030348\ttotal: 2m 26s\tremaining: 4m 44s\n",
            "266:\tlearn: 61.3975080\ttotal: 2m 26s\tremaining: 4m 43s\n",
            "267:\tlearn: 61.2833033\ttotal: 2m 27s\tremaining: 4m 43s\n",
            "268:\tlearn: 61.0442427\ttotal: 2m 28s\tremaining: 4m 43s\n",
            "269:\tlearn: 60.9920533\ttotal: 2m 28s\tremaining: 4m 42s\n",
            "270:\tlearn: 60.8325594\ttotal: 2m 29s\tremaining: 4m 42s\n",
            "271:\tlearn: 60.6411420\ttotal: 2m 29s\tremaining: 4m 41s\n",
            "272:\tlearn: 60.3792489\ttotal: 2m 30s\tremaining: 4m 41s\n",
            "273:\tlearn: 60.0730274\ttotal: 2m 31s\tremaining: 4m 40s\n",
            "274:\tlearn: 59.8936159\ttotal: 2m 31s\tremaining: 4m 40s\n",
            "275:\tlearn: 59.7501090\ttotal: 2m 32s\tremaining: 4m 40s\n",
            "276:\tlearn: 59.5935088\ttotal: 2m 33s\tremaining: 4m 39s\n",
            "277:\tlearn: 59.4706105\ttotal: 2m 33s\tremaining: 4m 39s\n",
            "278:\tlearn: 59.3533539\ttotal: 2m 34s\tremaining: 4m 38s\n",
            "279:\tlearn: 59.1665585\ttotal: 2m 34s\tremaining: 4m 37s\n",
            "280:\tlearn: 58.9983335\ttotal: 2m 35s\tremaining: 4m 37s\n",
            "281:\tlearn: 58.9175665\ttotal: 2m 36s\tremaining: 4m 37s\n",
            "282:\tlearn: 58.8109693\ttotal: 2m 36s\tremaining: 4m 36s\n",
            "283:\tlearn: 58.6416658\ttotal: 2m 37s\tremaining: 4m 36s\n",
            "284:\tlearn: 58.4258467\ttotal: 2m 37s\tremaining: 4m 35s\n",
            "285:\tlearn: 58.3008729\ttotal: 2m 38s\tremaining: 4m 35s\n",
            "286:\tlearn: 58.1850105\ttotal: 2m 39s\tremaining: 4m 34s\n",
            "287:\tlearn: 57.9738421\ttotal: 2m 39s\tremaining: 4m 34s\n",
            "288:\tlearn: 57.8822201\ttotal: 2m 40s\tremaining: 4m 34s\n",
            "289:\tlearn: 57.7672981\ttotal: 2m 41s\tremaining: 4m 33s\n",
            "290:\tlearn: 57.6376092\ttotal: 2m 41s\tremaining: 4m 33s\n",
            "291:\tlearn: 57.4511121\ttotal: 2m 42s\tremaining: 4m 32s\n",
            "292:\tlearn: 57.2835052\ttotal: 2m 42s\tremaining: 4m 32s\n",
            "293:\tlearn: 57.2334263\ttotal: 2m 43s\tremaining: 4m 32s\n",
            "294:\tlearn: 57.0448236\ttotal: 2m 44s\tremaining: 4m 31s\n",
            "295:\tlearn: 56.9284192\ttotal: 2m 44s\tremaining: 4m 31s\n",
            "296:\tlearn: 56.8148455\ttotal: 2m 45s\tremaining: 4m 31s\n",
            "297:\tlearn: 56.7227764\ttotal: 2m 46s\tremaining: 4m 30s\n",
            "298:\tlearn: 56.6075377\ttotal: 2m 47s\tremaining: 4m 30s\n",
            "299:\tlearn: 56.4951889\ttotal: 2m 47s\tremaining: 4m 30s\n",
            "300:\tlearn: 56.3217334\ttotal: 2m 48s\tremaining: 4m 29s\n",
            "301:\tlearn: 56.1786437\ttotal: 2m 48s\tremaining: 4m 29s\n",
            "302:\tlearn: 56.0257693\ttotal: 2m 49s\tremaining: 4m 28s\n",
            "303:\tlearn: 55.8901595\ttotal: 2m 50s\tremaining: 4m 28s\n",
            "304:\tlearn: 55.6466852\ttotal: 2m 50s\tremaining: 4m 27s\n",
            "305:\tlearn: 55.4640753\ttotal: 2m 51s\tremaining: 4m 27s\n",
            "306:\tlearn: 55.3420473\ttotal: 2m 52s\tremaining: 4m 26s\n",
            "307:\tlearn: 55.1481608\ttotal: 2m 52s\tremaining: 4m 26s\n",
            "308:\tlearn: 54.8838766\ttotal: 2m 53s\tremaining: 4m 25s\n",
            "309:\tlearn: 54.7434110\ttotal: 2m 54s\tremaining: 4m 25s\n",
            "310:\tlearn: 54.6887406\ttotal: 2m 54s\tremaining: 4m 25s\n",
            "311:\tlearn: 54.5403488\ttotal: 2m 55s\tremaining: 4m 24s\n",
            "312:\tlearn: 54.4358357\ttotal: 2m 55s\tremaining: 4m 24s\n",
            "313:\tlearn: 54.3328303\ttotal: 2m 56s\tremaining: 4m 23s\n",
            "314:\tlearn: 54.2060019\ttotal: 2m 57s\tremaining: 4m 23s\n",
            "315:\tlearn: 54.0997470\ttotal: 2m 57s\tremaining: 4m 22s\n",
            "316:\tlearn: 54.0825751\ttotal: 2m 58s\tremaining: 4m 22s\n",
            "317:\tlearn: 53.9828187\ttotal: 2m 59s\tremaining: 4m 22s\n",
            "318:\tlearn: 53.8109674\ttotal: 2m 59s\tremaining: 4m 21s\n",
            "319:\tlearn: 53.7114162\ttotal: 3m\tremaining: 4m 21s\n",
            "320:\tlearn: 53.5806337\ttotal: 3m 1s\tremaining: 4m 20s\n",
            "321:\tlearn: 53.4658978\ttotal: 3m 1s\tremaining: 4m 20s\n",
            "322:\tlearn: 53.2791965\ttotal: 3m 2s\tremaining: 4m 19s\n",
            "323:\tlearn: 53.1304162\ttotal: 3m 2s\tremaining: 4m 19s\n",
            "324:\tlearn: 53.0362012\ttotal: 3m 3s\tremaining: 4m 18s\n",
            "325:\tlearn: 52.8812414\ttotal: 3m 4s\tremaining: 4m 18s\n",
            "326:\tlearn: 52.7585154\ttotal: 3m 4s\tremaining: 4m 17s\n",
            "327:\tlearn: 52.6381064\ttotal: 3m 5s\tremaining: 4m 17s\n",
            "328:\tlearn: 52.5150130\ttotal: 3m 6s\tremaining: 4m 16s\n",
            "329:\tlearn: 52.4588342\ttotal: 3m 6s\tremaining: 4m 16s\n",
            "330:\tlearn: 52.3107730\ttotal: 3m 7s\tremaining: 4m 15s\n",
            "331:\tlearn: 52.2243787\ttotal: 3m 7s\tremaining: 4m 15s\n",
            "332:\tlearn: 52.1681769\ttotal: 3m 8s\tremaining: 4m 15s\n",
            "333:\tlearn: 51.9784938\ttotal: 3m 9s\tremaining: 4m 14s\n",
            "334:\tlearn: 51.7418145\ttotal: 3m 9s\tremaining: 4m 13s\n",
            "335:\tlearn: 51.6132037\ttotal: 3m 10s\tremaining: 4m 13s\n",
            "336:\tlearn: 51.5193613\ttotal: 3m 11s\tremaining: 4m 12s\n",
            "337:\tlearn: 51.3325317\ttotal: 3m 11s\tremaining: 4m 12s\n",
            "338:\tlearn: 51.2193444\ttotal: 3m 12s\tremaining: 4m 11s\n",
            "339:\tlearn: 51.1075617\ttotal: 3m 12s\tremaining: 4m 11s\n",
            "340:\tlearn: 50.9660630\ttotal: 3m 13s\tremaining: 4m 10s\n",
            "341:\tlearn: 50.8184161\ttotal: 3m 14s\tremaining: 4m 10s\n",
            "342:\tlearn: 50.7136941\ttotal: 3m 14s\tremaining: 4m 9s\n",
            "343:\tlearn: 50.6577014\ttotal: 3m 15s\tremaining: 4m 9s\n",
            "344:\tlearn: 50.6167361\ttotal: 3m 16s\tremaining: 4m 8s\n",
            "345:\tlearn: 50.3996103\ttotal: 3m 16s\tremaining: 4m 8s\n",
            "346:\tlearn: 50.2533757\ttotal: 3m 17s\tremaining: 4m 7s\n",
            "347:\tlearn: 50.1108041\ttotal: 3m 18s\tremaining: 4m 7s\n",
            "348:\tlearn: 49.9908996\ttotal: 3m 18s\tremaining: 4m 7s\n",
            "349:\tlearn: 49.8751518\ttotal: 3m 19s\tremaining: 4m 6s\n",
            "350:\tlearn: 49.7967723\ttotal: 3m 20s\tremaining: 4m 6s\n",
            "351:\tlearn: 49.7478935\ttotal: 3m 20s\tremaining: 4m 5s\n",
            "352:\tlearn: 49.5562822\ttotal: 3m 21s\tremaining: 4m 5s\n",
            "353:\tlearn: 49.4503309\ttotal: 3m 22s\tremaining: 4m 4s\n",
            "354:\tlearn: 49.3435443\ttotal: 3m 22s\tremaining: 4m 4s\n",
            "355:\tlearn: 49.2295291\ttotal: 3m 23s\tremaining: 4m 3s\n",
            "356:\tlearn: 49.1161940\ttotal: 3m 23s\tremaining: 4m 3s\n",
            "357:\tlearn: 49.0571713\ttotal: 3m 24s\tremaining: 4m 2s\n",
            "358:\tlearn: 48.8931824\ttotal: 3m 25s\tremaining: 4m 2s\n",
            "359:\tlearn: 48.8040361\ttotal: 3m 25s\tremaining: 4m 1s\n",
            "360:\tlearn: 48.7509036\ttotal: 3m 26s\tremaining: 4m 1s\n",
            "361:\tlearn: 48.6092374\ttotal: 3m 26s\tremaining: 4m\n",
            "362:\tlearn: 48.4460293\ttotal: 3m 27s\tremaining: 4m\n",
            "363:\tlearn: 48.2962638\ttotal: 3m 28s\tremaining: 3m 59s\n",
            "364:\tlearn: 48.2545890\ttotal: 3m 28s\tremaining: 3m 59s\n",
            "365:\tlearn: 48.1996235\ttotal: 3m 29s\tremaining: 3m 58s\n",
            "366:\tlearn: 48.1341680\ttotal: 3m 30s\tremaining: 3m 58s\n",
            "367:\tlearn: 48.0348406\ttotal: 3m 30s\tremaining: 3m 57s\n",
            "368:\tlearn: 47.9828649\ttotal: 3m 31s\tremaining: 3m 57s\n",
            "369:\tlearn: 47.8883112\ttotal: 3m 32s\tremaining: 3m 56s\n",
            "370:\tlearn: 47.7837308\ttotal: 3m 32s\tremaining: 3m 56s\n",
            "371:\tlearn: 47.7547532\ttotal: 3m 33s\tremaining: 3m 55s\n",
            "372:\tlearn: 47.5806700\ttotal: 3m 34s\tremaining: 3m 55s\n",
            "373:\tlearn: 47.5558835\ttotal: 3m 34s\tremaining: 3m 54s\n",
            "374:\tlearn: 47.4062656\ttotal: 3m 35s\tremaining: 3m 54s\n",
            "375:\tlearn: 47.2780979\ttotal: 3m 36s\tremaining: 3m 53s\n",
            "376:\tlearn: 47.1824122\ttotal: 3m 36s\tremaining: 3m 53s\n",
            "377:\tlearn: 47.0440734\ttotal: 3m 37s\tremaining: 3m 52s\n",
            "378:\tlearn: 46.8550782\ttotal: 3m 38s\tremaining: 3m 52s\n",
            "379:\tlearn: 46.7728933\ttotal: 3m 38s\tremaining: 3m 51s\n",
            "380:\tlearn: 46.6164256\ttotal: 3m 39s\tremaining: 3m 51s\n",
            "381:\tlearn: 46.5564667\ttotal: 3m 39s\tremaining: 3m 50s\n",
            "382:\tlearn: 46.4640704\ttotal: 3m 40s\tremaining: 3m 50s\n",
            "383:\tlearn: 46.4044757\ttotal: 3m 41s\tremaining: 3m 49s\n",
            "384:\tlearn: 46.3648464\ttotal: 3m 41s\tremaining: 3m 49s\n",
            "385:\tlearn: 46.2763598\ttotal: 3m 42s\tremaining: 3m 48s\n",
            "386:\tlearn: 46.2088210\ttotal: 3m 43s\tremaining: 3m 48s\n",
            "387:\tlearn: 46.0588748\ttotal: 3m 43s\tremaining: 3m 47s\n",
            "388:\tlearn: 46.0083857\ttotal: 3m 44s\tremaining: 3m 47s\n",
            "389:\tlearn: 45.9343653\ttotal: 3m 45s\tremaining: 3m 46s\n",
            "390:\tlearn: 45.8487098\ttotal: 3m 45s\tremaining: 3m 46s\n",
            "391:\tlearn: 45.7250049\ttotal: 3m 46s\tremaining: 3m 45s\n",
            "392:\tlearn: 45.6572975\ttotal: 3m 46s\tremaining: 3m 45s\n",
            "393:\tlearn: 45.5282804\ttotal: 3m 47s\tremaining: 3m 44s\n",
            "394:\tlearn: 45.3669626\ttotal: 3m 48s\tremaining: 3m 44s\n",
            "395:\tlearn: 45.2690452\ttotal: 3m 48s\tremaining: 3m 43s\n",
            "396:\tlearn: 45.2105410\ttotal: 3m 49s\tremaining: 3m 43s\n",
            "397:\tlearn: 45.0429064\ttotal: 3m 50s\tremaining: 3m 42s\n",
            "398:\tlearn: 44.9213483\ttotal: 3m 50s\tremaining: 3m 42s\n",
            "399:\tlearn: 44.8035188\ttotal: 3m 51s\tremaining: 3m 41s\n",
            "400:\tlearn: 44.7307887\ttotal: 3m 52s\tremaining: 3m 41s\n",
            "401:\tlearn: 44.6810505\ttotal: 3m 52s\tremaining: 3m 40s\n",
            "402:\tlearn: 44.5430207\ttotal: 3m 53s\tremaining: 3m 40s\n",
            "403:\tlearn: 44.4599318\ttotal: 3m 54s\tremaining: 3m 39s\n",
            "404:\tlearn: 44.3213145\ttotal: 3m 54s\tremaining: 3m 39s\n",
            "405:\tlearn: 44.2535845\ttotal: 3m 55s\tremaining: 3m 38s\n",
            "406:\tlearn: 44.1545816\ttotal: 3m 56s\tremaining: 3m 38s\n",
            "407:\tlearn: 44.0807887\ttotal: 3m 56s\tremaining: 3m 37s\n",
            "408:\tlearn: 44.0806478\ttotal: 3m 57s\tremaining: 3m 37s\n",
            "409:\tlearn: 43.9417251\ttotal: 3m 58s\tremaining: 3m 36s\n",
            "410:\tlearn: 43.8672349\ttotal: 3m 58s\tremaining: 3m 36s\n",
            "411:\tlearn: 43.8032880\ttotal: 3m 59s\tremaining: 3m 35s\n",
            "412:\tlearn: 43.6621182\ttotal: 4m\tremaining: 3m 35s\n",
            "413:\tlearn: 43.5781662\ttotal: 4m\tremaining: 3m 34s\n",
            "414:\tlearn: 43.4735106\ttotal: 4m 1s\tremaining: 3m 33s\n",
            "415:\tlearn: 43.4279091\ttotal: 4m 1s\tremaining: 3m 33s\n",
            "416:\tlearn: 43.2831981\ttotal: 4m 2s\tremaining: 3m 32s\n",
            "417:\tlearn: 43.1698824\ttotal: 4m 3s\tremaining: 3m 32s\n",
            "418:\tlearn: 43.0427181\ttotal: 4m 3s\tremaining: 3m 31s\n",
            "419:\tlearn: 42.9692529\ttotal: 4m 4s\tremaining: 3m 31s\n",
            "420:\tlearn: 42.9076782\ttotal: 4m 5s\tremaining: 3m 30s\n",
            "421:\tlearn: 42.8338125\ttotal: 4m 5s\tremaining: 3m 30s\n",
            "422:\tlearn: 42.7113549\ttotal: 4m 6s\tremaining: 3m 29s\n",
            "423:\tlearn: 42.5986198\ttotal: 4m 6s\tremaining: 3m 29s\n",
            "424:\tlearn: 42.4949178\ttotal: 4m 7s\tremaining: 3m 28s\n",
            "425:\tlearn: 42.4240885\ttotal: 4m 8s\tremaining: 3m 28s\n",
            "426:\tlearn: 42.3600451\ttotal: 4m 8s\tremaining: 3m 27s\n",
            "427:\tlearn: 42.2683079\ttotal: 4m 9s\tremaining: 3m 26s\n",
            "428:\tlearn: 42.2203929\ttotal: 4m 10s\tremaining: 3m 26s\n",
            "429:\tlearn: 42.2164277\ttotal: 4m 10s\tremaining: 3m 26s\n",
            "430:\tlearn: 42.1493218\ttotal: 4m 11s\tremaining: 3m 25s\n",
            "431:\tlearn: 42.0615803\ttotal: 4m 12s\tremaining: 3m 25s\n",
            "432:\tlearn: 41.9716436\ttotal: 4m 12s\tremaining: 3m 24s\n",
            "433:\tlearn: 41.9012954\ttotal: 4m 13s\tremaining: 3m 23s\n",
            "434:\tlearn: 41.7518960\ttotal: 4m 14s\tremaining: 3m 23s\n",
            "435:\tlearn: 41.6578782\ttotal: 4m 14s\tremaining: 3m 22s\n",
            "436:\tlearn: 41.6022848\ttotal: 4m 15s\tremaining: 3m 22s\n",
            "437:\tlearn: 41.5130104\ttotal: 4m 16s\tremaining: 3m 21s\n",
            "438:\tlearn: 41.4398925\ttotal: 4m 16s\tremaining: 3m 21s\n",
            "439:\tlearn: 41.3700486\ttotal: 4m 17s\tremaining: 3m 20s\n",
            "440:\tlearn: 41.3047942\ttotal: 4m 18s\tremaining: 3m 20s\n",
            "441:\tlearn: 41.1716238\ttotal: 4m 18s\tremaining: 3m 19s\n",
            "442:\tlearn: 41.0419166\ttotal: 4m 19s\tremaining: 3m 19s\n",
            "443:\tlearn: 40.8775039\ttotal: 4m 19s\tremaining: 3m 18s\n",
            "444:\tlearn: 40.8219762\ttotal: 4m 20s\tremaining: 3m 18s\n",
            "445:\tlearn: 40.7192393\ttotal: 4m 21s\tremaining: 3m 17s\n",
            "446:\tlearn: 40.6470104\ttotal: 4m 22s\tremaining: 3m 17s\n",
            "447:\tlearn: 40.5789041\ttotal: 4m 22s\tremaining: 3m 16s\n",
            "448:\tlearn: 40.5262170\ttotal: 4m 23s\tremaining: 3m 15s\n",
            "449:\tlearn: 40.4490861\ttotal: 4m 24s\tremaining: 3m 15s\n",
            "450:\tlearn: 40.3303341\ttotal: 4m 24s\tremaining: 3m 14s\n",
            "451:\tlearn: 40.2283902\ttotal: 4m 25s\tremaining: 3m 14s\n",
            "452:\tlearn: 40.1928886\ttotal: 4m 26s\tremaining: 3m 13s\n",
            "453:\tlearn: 40.1314801\ttotal: 4m 26s\tremaining: 3m 13s\n",
            "454:\tlearn: 40.0598410\ttotal: 4m 27s\tremaining: 3m 12s\n",
            "455:\tlearn: 39.9895150\ttotal: 4m 27s\tremaining: 3m 12s\n",
            "456:\tlearn: 39.9231073\ttotal: 4m 28s\tremaining: 3m 11s\n",
            "457:\tlearn: 39.8794641\ttotal: 4m 29s\tremaining: 3m 11s\n",
            "458:\tlearn: 39.7499947\ttotal: 4m 29s\tremaining: 3m 10s\n",
            "459:\tlearn: 39.6524938\ttotal: 4m 30s\tremaining: 3m 9s\n",
            "460:\tlearn: 39.6267369\ttotal: 4m 31s\tremaining: 3m 9s\n",
            "461:\tlearn: 39.5550352\ttotal: 4m 31s\tremaining: 3m 8s\n",
            "462:\tlearn: 39.4948455\ttotal: 4m 32s\tremaining: 3m 8s\n",
            "463:\tlearn: 39.3900024\ttotal: 4m 33s\tremaining: 3m 7s\n",
            "464:\tlearn: 39.3131860\ttotal: 4m 33s\tremaining: 3m 7s\n",
            "465:\tlearn: 39.1974082\ttotal: 4m 34s\tremaining: 3m 6s\n",
            "466:\tlearn: 39.1845694\ttotal: 4m 35s\tremaining: 3m 6s\n",
            "467:\tlearn: 39.0647470\ttotal: 4m 35s\tremaining: 3m 5s\n",
            "468:\tlearn: 38.9687071\ttotal: 4m 36s\tremaining: 3m 5s\n",
            "469:\tlearn: 38.8618470\ttotal: 4m 37s\tremaining: 3m 4s\n",
            "470:\tlearn: 38.8236767\ttotal: 4m 37s\tremaining: 3m 4s\n",
            "471:\tlearn: 38.7595097\ttotal: 4m 38s\tremaining: 3m 3s\n",
            "472:\tlearn: 38.7112066\ttotal: 4m 39s\tremaining: 3m 2s\n",
            "473:\tlearn: 38.6366892\ttotal: 4m 39s\tremaining: 3m 2s\n",
            "474:\tlearn: 38.5550293\ttotal: 4m 40s\tremaining: 3m 1s\n",
            "475:\tlearn: 38.5236201\ttotal: 4m 41s\tremaining: 3m 1s\n",
            "476:\tlearn: 38.4369085\ttotal: 4m 41s\tremaining: 3m\n",
            "477:\tlearn: 38.3127263\ttotal: 4m 42s\tremaining: 3m\n",
            "478:\tlearn: 38.2602802\ttotal: 4m 43s\tremaining: 2m 59s\n",
            "479:\tlearn: 38.2109004\ttotal: 4m 43s\tremaining: 2m 59s\n",
            "480:\tlearn: 38.1836401\ttotal: 4m 44s\tremaining: 2m 58s\n",
            "481:\tlearn: 38.0733091\ttotal: 4m 45s\tremaining: 2m 58s\n",
            "482:\tlearn: 38.0084110\ttotal: 4m 46s\tremaining: 2m 57s\n",
            "483:\tlearn: 37.9694656\ttotal: 4m 46s\tremaining: 2m 57s\n",
            "484:\tlearn: 37.8434287\ttotal: 4m 47s\tremaining: 2m 56s\n",
            "485:\tlearn: 37.7545754\ttotal: 4m 47s\tremaining: 2m 55s\n",
            "486:\tlearn: 37.6702475\ttotal: 4m 48s\tremaining: 2m 55s\n",
            "487:\tlearn: 37.6446614\ttotal: 4m 49s\tremaining: 2m 54s\n",
            "488:\tlearn: 37.6031458\ttotal: 4m 50s\tremaining: 2m 54s\n",
            "489:\tlearn: 37.5568270\ttotal: 4m 50s\tremaining: 2m 53s\n",
            "490:\tlearn: 37.5026852\ttotal: 4m 51s\tremaining: 2m 53s\n",
            "491:\tlearn: 37.4944485\ttotal: 4m 52s\tremaining: 2m 52s\n",
            "492:\tlearn: 37.4100858\ttotal: 4m 53s\tremaining: 2m 52s\n",
            "493:\tlearn: 37.3456359\ttotal: 4m 53s\tremaining: 2m 51s\n",
            "494:\tlearn: 37.2823499\ttotal: 4m 54s\tremaining: 2m 51s\n",
            "495:\tlearn: 37.2544518\ttotal: 4m 54s\tremaining: 2m 50s\n",
            "496:\tlearn: 37.1596153\ttotal: 4m 55s\tremaining: 2m 50s\n",
            "497:\tlearn: 37.0483474\ttotal: 4m 56s\tremaining: 2m 49s\n",
            "498:\tlearn: 36.9454990\ttotal: 4m 57s\tremaining: 2m 49s\n",
            "499:\tlearn: 36.8792925\ttotal: 4m 57s\tremaining: 2m 48s\n",
            "500:\tlearn: 36.7496398\ttotal: 4m 58s\tremaining: 2m 47s\n",
            "501:\tlearn: 36.6640588\ttotal: 4m 59s\tremaining: 2m 47s\n",
            "502:\tlearn: 36.5939750\ttotal: 4m 59s\tremaining: 2m 46s\n",
            "503:\tlearn: 36.4915012\ttotal: 5m\tremaining: 2m 46s\n",
            "504:\tlearn: 36.4291494\ttotal: 5m 1s\tremaining: 2m 45s\n",
            "505:\tlearn: 36.3067385\ttotal: 5m 1s\tremaining: 2m 45s\n",
            "506:\tlearn: 36.2374870\ttotal: 5m 2s\tremaining: 2m 44s\n",
            "507:\tlearn: 36.1577782\ttotal: 5m 3s\tremaining: 2m 44s\n",
            "508:\tlearn: 36.1237946\ttotal: 5m 3s\tremaining: 2m 43s\n",
            "509:\tlearn: 36.1215783\ttotal: 5m 4s\tremaining: 2m 42s\n",
            "510:\tlearn: 36.0317195\ttotal: 5m 5s\tremaining: 2m 42s\n",
            "511:\tlearn: 35.9637740\ttotal: 5m 5s\tremaining: 2m 41s\n",
            "512:\tlearn: 35.8729784\ttotal: 5m 6s\tremaining: 2m 41s\n",
            "513:\tlearn: 35.8497473\ttotal: 5m 7s\tremaining: 2m 40s\n",
            "514:\tlearn: 35.7832044\ttotal: 5m 7s\tremaining: 2m 40s\n",
            "515:\tlearn: 35.7252686\ttotal: 5m 8s\tremaining: 2m 39s\n",
            "516:\tlearn: 35.6529445\ttotal: 5m 9s\tremaining: 2m 39s\n",
            "517:\tlearn: 35.5813248\ttotal: 5m 10s\tremaining: 2m 38s\n",
            "518:\tlearn: 35.5370199\ttotal: 5m 10s\tremaining: 2m 38s\n",
            "519:\tlearn: 35.4439827\ttotal: 5m 11s\tremaining: 2m 37s\n",
            "520:\tlearn: 35.3732398\ttotal: 5m 11s\tremaining: 2m 36s\n",
            "521:\tlearn: 35.3162877\ttotal: 5m 12s\tremaining: 2m 36s\n",
            "522:\tlearn: 35.2236736\ttotal: 5m 13s\tremaining: 2m 35s\n",
            "523:\tlearn: 35.1435291\ttotal: 5m 14s\tremaining: 2m 35s\n",
            "524:\tlearn: 35.1069832\ttotal: 5m 14s\tremaining: 2m 34s\n",
            "525:\tlearn: 35.0317151\ttotal: 5m 15s\tremaining: 2m 34s\n",
            "526:\tlearn: 34.9540830\ttotal: 5m 16s\tremaining: 2m 33s\n",
            "527:\tlearn: 34.8760399\ttotal: 5m 16s\tremaining: 2m 33s\n",
            "528:\tlearn: 34.7981303\ttotal: 5m 17s\tremaining: 2m 32s\n",
            "529:\tlearn: 34.7052307\ttotal: 5m 18s\tremaining: 2m 32s\n",
            "530:\tlearn: 34.6235952\ttotal: 5m 19s\tremaining: 2m 31s\n",
            "531:\tlearn: 34.5534420\ttotal: 5m 19s\tremaining: 2m 30s\n",
            "532:\tlearn: 34.5311090\ttotal: 5m 20s\tremaining: 2m 30s\n",
            "533:\tlearn: 34.4993615\ttotal: 5m 21s\tremaining: 2m 29s\n",
            "534:\tlearn: 34.4703900\ttotal: 5m 22s\tremaining: 2m 29s\n",
            "535:\tlearn: 34.3733517\ttotal: 5m 22s\tremaining: 2m 28s\n",
            "536:\tlearn: 34.2675512\ttotal: 5m 23s\tremaining: 2m 28s\n",
            "537:\tlearn: 34.2343214\ttotal: 5m 24s\tremaining: 2m 27s\n",
            "538:\tlearn: 34.1516129\ttotal: 5m 24s\tremaining: 2m 27s\n",
            "539:\tlearn: 34.1326695\ttotal: 5m 25s\tremaining: 2m 26s\n",
            "540:\tlearn: 34.1273696\ttotal: 5m 26s\tremaining: 2m 25s\n",
            "541:\tlearn: 34.1265038\ttotal: 5m 26s\tremaining: 2m 25s\n",
            "542:\tlearn: 34.0510366\ttotal: 5m 27s\tremaining: 2m 24s\n",
            "543:\tlearn: 33.9912175\ttotal: 5m 28s\tremaining: 2m 24s\n",
            "544:\tlearn: 33.9300318\ttotal: 5m 28s\tremaining: 2m 23s\n",
            "545:\tlearn: 33.8959197\ttotal: 5m 29s\tremaining: 2m 23s\n",
            "546:\tlearn: 33.7578031\ttotal: 5m 30s\tremaining: 2m 22s\n",
            "547:\tlearn: 33.7106387\ttotal: 5m 31s\tremaining: 2m 21s\n",
            "548:\tlearn: 33.6255912\ttotal: 5m 31s\tremaining: 2m 21s\n",
            "549:\tlearn: 33.5070920\ttotal: 5m 32s\tremaining: 2m 20s\n",
            "550:\tlearn: 33.4664563\ttotal: 5m 33s\tremaining: 2m 20s\n",
            "551:\tlearn: 33.4057851\ttotal: 5m 34s\tremaining: 2m 19s\n",
            "552:\tlearn: 33.3723317\ttotal: 5m 34s\tremaining: 2m 19s\n",
            "553:\tlearn: 33.3331699\ttotal: 5m 35s\tremaining: 2m 18s\n",
            "554:\tlearn: 33.2894393\ttotal: 5m 36s\tremaining: 2m 18s\n",
            "555:\tlearn: 33.1923993\ttotal: 5m 36s\tremaining: 2m 17s\n",
            "556:\tlearn: 33.1679980\ttotal: 5m 37s\tremaining: 2m 16s\n",
            "557:\tlearn: 33.1409175\ttotal: 5m 38s\tremaining: 2m 16s\n",
            "558:\tlearn: 33.0551936\ttotal: 5m 38s\tremaining: 2m 15s\n",
            "559:\tlearn: 33.0081328\ttotal: 5m 39s\tremaining: 2m 15s\n",
            "560:\tlearn: 33.0067679\ttotal: 5m 40s\tremaining: 2m 14s\n",
            "561:\tlearn: 32.9363316\ttotal: 5m 40s\tremaining: 2m 14s\n",
            "562:\tlearn: 32.8860785\ttotal: 5m 41s\tremaining: 2m 13s\n",
            "563:\tlearn: 32.8211626\ttotal: 5m 42s\tremaining: 2m 12s\n",
            "564:\tlearn: 32.7831504\ttotal: 5m 42s\tremaining: 2m 12s\n",
            "565:\tlearn: 32.6981085\ttotal: 5m 43s\tremaining: 2m 11s\n",
            "566:\tlearn: 32.6314434\ttotal: 5m 44s\tremaining: 2m 11s\n",
            "567:\tlearn: 32.5954473\ttotal: 5m 44s\tremaining: 2m 10s\n",
            "568:\tlearn: 32.5701924\ttotal: 5m 45s\tremaining: 2m 10s\n",
            "569:\tlearn: 32.5125557\ttotal: 5m 46s\tremaining: 2m 9s\n",
            "570:\tlearn: 32.4390040\ttotal: 5m 47s\tremaining: 2m 8s\n",
            "571:\tlearn: 32.3436592\ttotal: 5m 47s\tremaining: 2m 8s\n",
            "572:\tlearn: 32.2414275\ttotal: 5m 48s\tremaining: 2m 7s\n",
            "573:\tlearn: 32.2087389\ttotal: 5m 49s\tremaining: 2m 7s\n",
            "574:\tlearn: 32.1806600\ttotal: 5m 50s\tremaining: 2m 6s\n",
            "575:\tlearn: 32.1434660\ttotal: 5m 50s\tremaining: 2m 6s\n",
            "576:\tlearn: 32.0881138\ttotal: 5m 51s\tremaining: 2m 5s\n",
            "577:\tlearn: 32.0872759\ttotal: 5m 52s\tremaining: 2m 4s\n",
            "578:\tlearn: 32.0564369\ttotal: 5m 52s\tremaining: 2m 4s\n",
            "579:\tlearn: 32.0557588\ttotal: 5m 53s\tremaining: 2m 3s\n",
            "580:\tlearn: 32.0166934\ttotal: 5m 54s\tremaining: 2m 3s\n",
            "581:\tlearn: 31.9395649\ttotal: 5m 54s\tremaining: 2m 2s\n",
            "582:\tlearn: 31.8566844\ttotal: 5m 55s\tremaining: 2m 1s\n",
            "583:\tlearn: 31.8242276\ttotal: 5m 56s\tremaining: 2m 1s\n",
            "584:\tlearn: 31.7619381\ttotal: 5m 56s\tremaining: 2m\n",
            "585:\tlearn: 31.6755552\ttotal: 5m 57s\tremaining: 2m\n",
            "586:\tlearn: 31.6488802\ttotal: 5m 58s\tremaining: 1m 59s\n",
            "587:\tlearn: 31.5936902\ttotal: 5m 58s\tremaining: 1m 59s\n",
            "588:\tlearn: 31.5066106\ttotal: 5m 59s\tremaining: 1m 58s\n",
            "589:\tlearn: 31.4405114\ttotal: 6m\tremaining: 1m 57s\n",
            "590:\tlearn: 31.3993873\ttotal: 6m 1s\tremaining: 1m 57s\n",
            "591:\tlearn: 31.3227306\ttotal: 6m 1s\tremaining: 1m 56s\n",
            "592:\tlearn: 31.2526867\ttotal: 6m 2s\tremaining: 1m 56s\n",
            "593:\tlearn: 31.1618921\ttotal: 6m 3s\tremaining: 1m 55s\n",
            "594:\tlearn: 31.1188380\ttotal: 6m 4s\tremaining: 1m 55s\n",
            "595:\tlearn: 31.0505617\ttotal: 6m 4s\tremaining: 1m 54s\n",
            "596:\tlearn: 30.9824466\ttotal: 6m 5s\tremaining: 1m 53s\n",
            "597:\tlearn: 30.9449986\ttotal: 6m 6s\tremaining: 1m 53s\n",
            "598:\tlearn: 30.9034801\ttotal: 6m 6s\tremaining: 1m 52s\n",
            "599:\tlearn: 30.8813242\ttotal: 6m 7s\tremaining: 1m 52s\n",
            "600:\tlearn: 30.8431951\ttotal: 6m 8s\tremaining: 1m 51s\n",
            "601:\tlearn: 30.8059793\ttotal: 6m 8s\tremaining: 1m 50s\n",
            "602:\tlearn: 30.7064641\ttotal: 6m 9s\tremaining: 1m 50s\n",
            "603:\tlearn: 30.6266438\ttotal: 6m 10s\tremaining: 1m 49s\n",
            "604:\tlearn: 30.5667196\ttotal: 6m 10s\tremaining: 1m 49s\n",
            "605:\tlearn: 30.5297197\ttotal: 6m 11s\tremaining: 1m 48s\n",
            "606:\tlearn: 30.4183786\ttotal: 6m 12s\tremaining: 1m 47s\n",
            "607:\tlearn: 30.3981335\ttotal: 6m 12s\tremaining: 1m 47s\n",
            "608:\tlearn: 30.3428560\ttotal: 6m 13s\tremaining: 1m 46s\n",
            "609:\tlearn: 30.3302835\ttotal: 6m 14s\tremaining: 1m 46s\n",
            "610:\tlearn: 30.2912855\ttotal: 6m 14s\tremaining: 1m 45s\n",
            "611:\tlearn: 30.2306515\ttotal: 6m 15s\tremaining: 1m 44s\n",
            "612:\tlearn: 30.1659054\ttotal: 6m 16s\tremaining: 1m 44s\n",
            "613:\tlearn: 30.0994801\ttotal: 6m 17s\tremaining: 1m 43s\n",
            "614:\tlearn: 30.0783188\ttotal: 6m 17s\tremaining: 1m 43s\n",
            "615:\tlearn: 30.0749163\ttotal: 6m 18s\tremaining: 1m 42s\n",
            "616:\tlearn: 29.9942433\ttotal: 6m 19s\tremaining: 1m 42s\n",
            "617:\tlearn: 29.9483661\ttotal: 6m 20s\tremaining: 1m 41s\n",
            "618:\tlearn: 29.8950600\ttotal: 6m 20s\tremaining: 1m 40s\n",
            "619:\tlearn: 29.8707193\ttotal: 6m 21s\tremaining: 1m 40s\n",
            "620:\tlearn: 29.8183650\ttotal: 6m 22s\tremaining: 1m 39s\n",
            "621:\tlearn: 29.7422780\ttotal: 6m 22s\tremaining: 1m 39s\n",
            "622:\tlearn: 29.7130917\ttotal: 6m 23s\tremaining: 1m 38s\n",
            "623:\tlearn: 29.7129389\ttotal: 6m 24s\tremaining: 1m 37s\n",
            "624:\tlearn: 29.6631473\ttotal: 6m 24s\tremaining: 1m 37s\n",
            "625:\tlearn: 29.6336268\ttotal: 6m 25s\tremaining: 1m 36s\n",
            "626:\tlearn: 29.5883886\ttotal: 6m 26s\tremaining: 1m 36s\n",
            "627:\tlearn: 29.5146182\ttotal: 6m 27s\tremaining: 1m 35s\n",
            "628:\tlearn: 29.4710862\ttotal: 6m 28s\tremaining: 1m 35s\n",
            "629:\tlearn: 29.4186777\ttotal: 6m 28s\tremaining: 1m 34s\n",
            "630:\tlearn: 29.3774013\ttotal: 6m 29s\tremaining: 1m 33s\n",
            "631:\tlearn: 29.3121887\ttotal: 6m 30s\tremaining: 1m 33s\n",
            "632:\tlearn: 29.2568033\ttotal: 6m 31s\tremaining: 1m 32s\n",
            "633:\tlearn: 29.1673577\ttotal: 6m 31s\tremaining: 1m 32s\n",
            "634:\tlearn: 29.1347900\ttotal: 6m 32s\tremaining: 1m 31s\n",
            "635:\tlearn: 29.0934389\ttotal: 6m 33s\tremaining: 1m 30s\n",
            "636:\tlearn: 29.0266866\ttotal: 6m 34s\tremaining: 1m 30s\n",
            "637:\tlearn: 28.9840722\ttotal: 6m 34s\tremaining: 1m 29s\n",
            "638:\tlearn: 28.9551520\ttotal: 6m 35s\tremaining: 1m 29s\n",
            "639:\tlearn: 28.9363878\ttotal: 6m 36s\tremaining: 1m 28s\n",
            "640:\tlearn: 28.8731570\ttotal: 6m 36s\tremaining: 1m 27s\n",
            "641:\tlearn: 28.8238813\ttotal: 6m 37s\tremaining: 1m 27s\n",
            "642:\tlearn: 28.7910394\ttotal: 6m 38s\tremaining: 1m 26s\n",
            "643:\tlearn: 28.7758996\ttotal: 6m 38s\tremaining: 1m 26s\n",
            "644:\tlearn: 28.7725471\ttotal: 6m 39s\tremaining: 1m 25s\n",
            "645:\tlearn: 28.7091465\ttotal: 6m 40s\tremaining: 1m 24s\n",
            "646:\tlearn: 28.6770884\ttotal: 6m 41s\tremaining: 1m 24s\n",
            "647:\tlearn: 28.6032716\ttotal: 6m 41s\tremaining: 1m 23s\n",
            "648:\tlearn: 28.5536090\ttotal: 6m 42s\tremaining: 1m 23s\n",
            "649:\tlearn: 28.5160310\ttotal: 6m 43s\tremaining: 1m 22s\n",
            "650:\tlearn: 28.4596313\ttotal: 6m 43s\tremaining: 1m 21s\n",
            "651:\tlearn: 28.4076130\ttotal: 6m 44s\tremaining: 1m 21s\n",
            "652:\tlearn: 28.3237447\ttotal: 6m 45s\tremaining: 1m 20s\n",
            "653:\tlearn: 28.3091690\ttotal: 6m 46s\tremaining: 1m 20s\n",
            "654:\tlearn: 28.2539208\ttotal: 6m 46s\tremaining: 1m 19s\n",
            "655:\tlearn: 28.2270267\ttotal: 6m 47s\tremaining: 1m 18s\n",
            "656:\tlearn: 28.1733331\ttotal: 6m 48s\tremaining: 1m 18s\n",
            "657:\tlearn: 28.1112263\ttotal: 6m 49s\tremaining: 1m 17s\n",
            "658:\tlearn: 28.0901404\ttotal: 6m 49s\tremaining: 1m 17s\n",
            "659:\tlearn: 28.0306324\ttotal: 6m 50s\tremaining: 1m 16s\n",
            "660:\tlearn: 27.9853132\ttotal: 6m 51s\tremaining: 1m 15s\n",
            "661:\tlearn: 27.9563585\ttotal: 6m 52s\tremaining: 1m 15s\n",
            "662:\tlearn: 27.9095797\ttotal: 6m 52s\tremaining: 1m 14s\n",
            "663:\tlearn: 27.8986416\ttotal: 6m 53s\tremaining: 1m 14s\n",
            "664:\tlearn: 27.8123518\ttotal: 6m 54s\tremaining: 1m 13s\n",
            "665:\tlearn: 27.7958978\ttotal: 6m 55s\tremaining: 1m 12s\n",
            "666:\tlearn: 27.7690997\ttotal: 6m 55s\tremaining: 1m 12s\n",
            "667:\tlearn: 27.7071850\ttotal: 6m 56s\tremaining: 1m 11s\n",
            "668:\tlearn: 27.6673389\ttotal: 6m 57s\tremaining: 1m 11s\n",
            "669:\tlearn: 27.6141862\ttotal: 6m 57s\tremaining: 1m 10s\n",
            "670:\tlearn: 27.5635057\ttotal: 6m 58s\tremaining: 1m 9s\n",
            "671:\tlearn: 27.5194413\ttotal: 6m 59s\tremaining: 1m 9s\n",
            "672:\tlearn: 27.4810077\ttotal: 7m\tremaining: 1m 8s\n",
            "673:\tlearn: 27.4382050\ttotal: 7m\tremaining: 1m 8s\n",
            "674:\tlearn: 27.4133766\ttotal: 7m 1s\tremaining: 1m 7s\n",
            "675:\tlearn: 27.3575540\ttotal: 7m 2s\tremaining: 1m 6s\n",
            "676:\tlearn: 27.3065450\ttotal: 7m 2s\tremaining: 1m 6s\n",
            "677:\tlearn: 27.2648770\ttotal: 7m 3s\tremaining: 1m 5s\n",
            "678:\tlearn: 27.2245544\ttotal: 7m 4s\tremaining: 1m 5s\n",
            "679:\tlearn: 27.1775429\ttotal: 7m 5s\tremaining: 1m 4s\n",
            "680:\tlearn: 27.1223770\ttotal: 7m 5s\tremaining: 1m 3s\n",
            "681:\tlearn: 27.0968058\ttotal: 7m 6s\tremaining: 1m 3s\n",
            "682:\tlearn: 27.0596076\ttotal: 7m 7s\tremaining: 1m 2s\n",
            "683:\tlearn: 26.9882321\ttotal: 7m 8s\tremaining: 1m 1s\n",
            "684:\tlearn: 26.9343560\ttotal: 7m 8s\tremaining: 1m 1s\n",
            "685:\tlearn: 26.9064374\ttotal: 7m 9s\tremaining: 1m\n",
            "686:\tlearn: 26.8430050\ttotal: 7m 10s\tremaining: 1m\n",
            "687:\tlearn: 26.7723257\ttotal: 7m 11s\tremaining: 59.6s\n",
            "688:\tlearn: 26.7257104\ttotal: 7m 12s\tremaining: 59s\n",
            "689:\tlearn: 26.6948634\ttotal: 7m 13s\tremaining: 58.4s\n",
            "690:\tlearn: 26.6515101\ttotal: 7m 13s\tremaining: 57.7s\n",
            "691:\tlearn: 26.5950916\ttotal: 7m 14s\tremaining: 57.1s\n",
            "692:\tlearn: 26.5655182\ttotal: 7m 15s\tremaining: 56.5s\n",
            "693:\tlearn: 26.5368457\ttotal: 7m 15s\tremaining: 55.9s\n",
            "694:\tlearn: 26.4915474\ttotal: 7m 16s\tremaining: 55.3s\n",
            "695:\tlearn: 26.4787552\ttotal: 7m 17s\tremaining: 54.7s\n",
            "696:\tlearn: 26.4612887\ttotal: 7m 18s\tremaining: 54s\n",
            "697:\tlearn: 26.4119684\ttotal: 7m 18s\tremaining: 53.4s\n",
            "698:\tlearn: 26.3825775\ttotal: 7m 19s\tremaining: 52.8s\n",
            "699:\tlearn: 26.3383493\ttotal: 7m 20s\tremaining: 52.2s\n",
            "700:\tlearn: 26.3071177\ttotal: 7m 21s\tremaining: 51.6s\n",
            "701:\tlearn: 26.2508920\ttotal: 7m 21s\tremaining: 51s\n",
            "702:\tlearn: 26.1811858\ttotal: 7m 22s\tremaining: 50.4s\n",
            "703:\tlearn: 26.1398049\ttotal: 7m 23s\tremaining: 49.8s\n",
            "704:\tlearn: 26.1153039\ttotal: 7m 24s\tremaining: 49.1s\n",
            "705:\tlearn: 26.0784376\ttotal: 7m 24s\tremaining: 48.5s\n",
            "706:\tlearn: 26.0242395\ttotal: 7m 25s\tremaining: 47.9s\n",
            "707:\tlearn: 25.9799821\ttotal: 7m 26s\tremaining: 47.3s\n",
            "708:\tlearn: 25.9269684\ttotal: 7m 27s\tremaining: 46.7s\n",
            "709:\tlearn: 25.8699171\ttotal: 7m 28s\tremaining: 46.1s\n",
            "710:\tlearn: 25.8091634\ttotal: 7m 28s\tremaining: 45.5s\n",
            "711:\tlearn: 25.7592866\ttotal: 7m 29s\tremaining: 44.9s\n",
            "712:\tlearn: 25.7419036\ttotal: 7m 30s\tremaining: 44.2s\n",
            "713:\tlearn: 25.7078390\ttotal: 7m 31s\tremaining: 43.6s\n",
            "714:\tlearn: 25.6993550\ttotal: 7m 31s\tremaining: 43s\n",
            "715:\tlearn: 25.6660889\ttotal: 7m 32s\tremaining: 42.4s\n",
            "716:\tlearn: 25.6512568\ttotal: 7m 33s\tremaining: 41.7s\n",
            "717:\tlearn: 25.6068182\ttotal: 7m 34s\tremaining: 41.1s\n",
            "718:\tlearn: 25.5694045\ttotal: 7m 34s\tremaining: 40.5s\n",
            "719:\tlearn: 25.5302316\ttotal: 7m 35s\tremaining: 39.9s\n",
            "720:\tlearn: 25.4864266\ttotal: 7m 36s\tremaining: 39.2s\n",
            "721:\tlearn: 25.4151809\ttotal: 7m 37s\tremaining: 38.6s\n",
            "722:\tlearn: 25.3638286\ttotal: 7m 38s\tremaining: 38s\n",
            "723:\tlearn: 25.3452554\ttotal: 7m 38s\tremaining: 37.4s\n",
            "724:\tlearn: 25.2988918\ttotal: 7m 39s\tremaining: 36.8s\n",
            "725:\tlearn: 25.2566146\ttotal: 7m 40s\tremaining: 36.1s\n",
            "726:\tlearn: 25.1933471\ttotal: 7m 40s\tremaining: 35.5s\n",
            "727:\tlearn: 25.1586979\ttotal: 7m 41s\tremaining: 34.9s\n",
            "728:\tlearn: 25.1456920\ttotal: 7m 42s\tremaining: 34.3s\n",
            "729:\tlearn: 25.1334536\ttotal: 7m 43s\tremaining: 33.6s\n",
            "730:\tlearn: 25.0977638\ttotal: 7m 43s\tremaining: 33s\n",
            "731:\tlearn: 25.0696547\ttotal: 7m 44s\tremaining: 32.4s\n",
            "732:\tlearn: 25.0692003\ttotal: 7m 45s\tremaining: 31.8s\n",
            "733:\tlearn: 25.0320777\ttotal: 7m 46s\tremaining: 31.1s\n",
            "734:\tlearn: 25.0003507\ttotal: 7m 46s\tremaining: 30.5s\n",
            "735:\tlearn: 24.9381652\ttotal: 7m 47s\tremaining: 29.9s\n",
            "736:\tlearn: 24.8578044\ttotal: 7m 48s\tremaining: 29.2s\n",
            "737:\tlearn: 24.8146324\ttotal: 7m 49s\tremaining: 28.6s\n",
            "738:\tlearn: 24.7791451\ttotal: 7m 49s\tremaining: 28s\n",
            "739:\tlearn: 24.7607990\ttotal: 7m 50s\tremaining: 27.3s\n",
            "740:\tlearn: 24.7044878\ttotal: 7m 51s\tremaining: 26.7s\n",
            "741:\tlearn: 24.6798919\ttotal: 7m 52s\tremaining: 26.1s\n",
            "742:\tlearn: 24.6312263\ttotal: 7m 52s\tremaining: 25.5s\n",
            "743:\tlearn: 24.5940151\ttotal: 7m 53s\tremaining: 24.8s\n",
            "744:\tlearn: 24.5641002\ttotal: 7m 54s\tremaining: 24.2s\n",
            "745:\tlearn: 24.5390858\ttotal: 7m 55s\tremaining: 23.6s\n",
            "746:\tlearn: 24.4938179\ttotal: 7m 55s\tremaining: 22.9s\n",
            "747:\tlearn: 24.4372857\ttotal: 7m 56s\tremaining: 22.3s\n",
            "748:\tlearn: 24.4181623\ttotal: 7m 57s\tremaining: 21.7s\n",
            "749:\tlearn: 24.3574138\ttotal: 7m 58s\tremaining: 21s\n",
            "750:\tlearn: 24.3179462\ttotal: 7m 58s\tremaining: 20.4s\n",
            "751:\tlearn: 24.2822957\ttotal: 7m 59s\tremaining: 19.8s\n",
            "752:\tlearn: 24.2536273\ttotal: 8m\tremaining: 19.1s\n",
            "753:\tlearn: 24.2342335\ttotal: 8m 1s\tremaining: 18.5s\n",
            "754:\tlearn: 24.1757022\ttotal: 8m 2s\tremaining: 17.9s\n",
            "755:\tlearn: 24.1479433\ttotal: 8m 2s\tremaining: 17.2s\n",
            "756:\tlearn: 24.1043855\ttotal: 8m 3s\tremaining: 16.6s\n",
            "757:\tlearn: 24.0744841\ttotal: 8m 4s\tremaining: 16s\n",
            "758:\tlearn: 24.0205550\ttotal: 8m 5s\tremaining: 15.3s\n",
            "759:\tlearn: 23.9641633\ttotal: 8m 5s\tremaining: 14.7s\n",
            "760:\tlearn: 23.9171355\ttotal: 8m 6s\tremaining: 14.1s\n",
            "761:\tlearn: 23.8597951\ttotal: 8m 7s\tremaining: 13.4s\n",
            "762:\tlearn: 23.8061276\ttotal: 8m 8s\tremaining: 12.8s\n",
            "763:\tlearn: 23.7488596\ttotal: 8m 8s\tremaining: 12.2s\n",
            "764:\tlearn: 23.7075800\ttotal: 8m 9s\tremaining: 11.5s\n",
            "765:\tlearn: 23.6849651\ttotal: 8m 10s\tremaining: 10.9s\n",
            "766:\tlearn: 23.6318088\ttotal: 8m 11s\tremaining: 10.2s\n",
            "767:\tlearn: 23.6210006\ttotal: 8m 11s\tremaining: 9.61s\n",
            "768:\tlearn: 23.6038271\ttotal: 8m 12s\tremaining: 8.97s\n",
            "769:\tlearn: 23.5872972\ttotal: 8m 13s\tremaining: 8.33s\n",
            "770:\tlearn: 23.5330749\ttotal: 8m 14s\tremaining: 7.69s\n",
            "771:\tlearn: 23.4985593\ttotal: 8m 14s\tremaining: 7.05s\n",
            "772:\tlearn: 23.4654111\ttotal: 8m 15s\tremaining: 6.41s\n",
            "773:\tlearn: 23.4236482\ttotal: 8m 16s\tremaining: 5.77s\n",
            "774:\tlearn: 23.3738717\ttotal: 8m 17s\tremaining: 5.13s\n",
            "775:\tlearn: 23.3267175\ttotal: 8m 17s\tremaining: 4.49s\n",
            "776:\tlearn: 23.3018553\ttotal: 8m 18s\tremaining: 3.85s\n",
            "777:\tlearn: 23.2836821\ttotal: 8m 19s\tremaining: 3.21s\n",
            "778:\tlearn: 23.2656513\ttotal: 8m 20s\tremaining: 2.57s\n",
            "779:\tlearn: 23.2141769\ttotal: 8m 20s\tremaining: 1.93s\n",
            "780:\tlearn: 23.1692540\ttotal: 8m 21s\tremaining: 1.28s\n",
            "781:\tlearn: 23.1342051\ttotal: 8m 22s\tremaining: 643ms\n",
            "782:\tlearn: 23.0996775\ttotal: 8m 23s\tremaining: 0us\n",
            "Fold 1, Best MAE: 42.25324087037929, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 2, Best MAE: 44.239773522743256, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 3, Best MAE: 42.615525740274286, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 4, Best MAE: 42.99479569369386, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 5, Best MAE: 43.48887650381214, Best hyperparameters: {'iterations': 783, 'learning_rate': 0.2584880573013857, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 45.02231140060744, 'subsample': 0.6753500090427036, 'random_strength': 21.788829786258407, 'od_wait': 90, 'leaf_estimation_iterations': 16, 'bagging_temperature': 58.67357912276298, 'colsample_bylevel': 0.5379457040607726}\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=724)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "# 각 Fold에 대해서 Optuna로 하이퍼파라미터 튜닝\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index, :], train_y.iloc[val_index, :]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=724))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best MAE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = CatBoostRegressor(**best_params)\n",
        "    model.fit(X_train, y_train, cat_features=cat_features)\n",
        "    test_pred = model.predict(test)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "# 평균 앙상블\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "# 결과 출력\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")\n",
        "\n",
        "# 최종 예측 저장\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>85.724445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>290.797561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>57.593767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>69.481320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>418.025254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>897.068029</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID     CI_HOUR\n",
              "0       TEST_000000   85.724445\n",
              "1       TEST_000001  290.797561\n",
              "2       TEST_000002    0.000000\n",
              "3       TEST_000003    0.000000\n",
              "4       TEST_000004   57.593767\n",
              "...             ...         ...\n",
              "244984  TEST_244984   69.481320\n",
              "244985  TEST_244985  418.025254\n",
              "244986  TEST_244986    0.000000\n",
              "244987  TEST_244987    0.000000\n",
              "244988  TEST_244988  897.068029\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('CAT_v2_724.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bHexh3orVzUq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AlogP</th>\n",
              "      <th>Molecular_Weight</th>\n",
              "      <th>Num_H_Acceptors</th>\n",
              "      <th>Num_H_Donors</th>\n",
              "      <th>Num_RotatableBonds</th>\n",
              "      <th>LogD</th>\n",
              "      <th>Molecular_PolarSurfaceArea</th>\n",
              "      <th>nAcid</th>\n",
              "      <th>nBase</th>\n",
              "      <th>SpAbs_A</th>\n",
              "      <th>...</th>\n",
              "      <th>MACCS_key_158</th>\n",
              "      <th>MACCS_key_159</th>\n",
              "      <th>MACCS_key_160</th>\n",
              "      <th>MACCS_key_161</th>\n",
              "      <th>MACCS_key_162</th>\n",
              "      <th>MACCS_key_163</th>\n",
              "      <th>MACCS_key_164</th>\n",
              "      <th>MACCS_key_165</th>\n",
              "      <th>MLM</th>\n",
              "      <th>HLM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.259</td>\n",
              "      <td>400.495</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3.259</td>\n",
              "      <td>117.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.689316</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.010</td>\n",
              "      <td>50.680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.169</td>\n",
              "      <td>301.407</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.172</td>\n",
              "      <td>73.47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.575899</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>29.270</td>\n",
              "      <td>50.590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.593</td>\n",
              "      <td>297.358</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.585</td>\n",
              "      <td>62.45</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>29.802128</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.586</td>\n",
              "      <td>80.892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.771</td>\n",
              "      <td>494.652</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3.475</td>\n",
              "      <td>92.60</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>45.884166</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.710</td>\n",
              "      <td>2.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.335</td>\n",
              "      <td>268.310</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.337</td>\n",
              "      <td>42.43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.308663</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>93.270</td>\n",
              "      <td>99.990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3.409</td>\n",
              "      <td>396.195</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3.409</td>\n",
              "      <td>64.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.902711</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.556</td>\n",
              "      <td>3.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>1.912</td>\n",
              "      <td>359.381</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.844</td>\n",
              "      <td>77.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.887372</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.560</td>\n",
              "      <td>47.630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3468</th>\n",
              "      <td>1.941</td>\n",
              "      <td>261.320</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2.124</td>\n",
              "      <td>70.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.546531</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>56.150</td>\n",
              "      <td>1.790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3469</th>\n",
              "      <td>0.989</td>\n",
              "      <td>284.696</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.989</td>\n",
              "      <td>91.51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.936088</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030</td>\n",
              "      <td>2.770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3470</th>\n",
              "      <td>4.321</td>\n",
              "      <td>295.399</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4.321</td>\n",
              "      <td>50.36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.713581</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.450</td>\n",
              "      <td>2.650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3471 rows × 5313 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
              "0     3.259           400.495                5             2   \n",
              "1     2.169           301.407                2             1   \n",
              "2     1.593           297.358                5             0   \n",
              "3     4.771           494.652                6             0   \n",
              "4     2.335           268.310                3             0   \n",
              "...     ...               ...              ...           ...   \n",
              "3466  3.409           396.195                3             1   \n",
              "3467  1.912           359.381                4             1   \n",
              "3468  1.941           261.320                3             1   \n",
              "3469  0.989           284.696                5             1   \n",
              "3470  4.321           295.399                2             0   \n",
              "\n",
              "      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  nAcid  nBase  \\\n",
              "0                      8  3.259                      117.37      0      0   \n",
              "1                      2  2.172                       73.47      0      0   \n",
              "2                      3  1.585                       62.45      2      1   \n",
              "3                      5  3.475                       92.60      0      1   \n",
              "4                      1  2.337                       42.43      0      0   \n",
              "...                  ...    ...                         ...    ...    ...   \n",
              "3466                   5  3.409                       64.74      0      0   \n",
              "3467                   3  1.844                       77.37      0      0   \n",
              "3468                   6  2.124                       70.14      0      0   \n",
              "3469                   5  0.989                       91.51      0      0   \n",
              "3470                   4  4.321                       50.36      0      0   \n",
              "\n",
              "        SpAbs_A  ...  MACCS_key_158  MACCS_key_159  MACCS_key_160  \\\n",
              "0     35.689316  ...              1              1              1   \n",
              "1     26.575899  ...              1              0              1   \n",
              "2     29.802128  ...              1              0              1   \n",
              "3     45.884166  ...              1              1              1   \n",
              "4     26.308663  ...              1              1              1   \n",
              "...         ...  ...            ...            ...            ...   \n",
              "3466  30.902711  ...              1              0              1   \n",
              "3467  35.887372  ...              1              1              1   \n",
              "3468  23.546531  ...              1              1              1   \n",
              "3469  23.936088  ...              1              1              0   \n",
              "3470  27.713581  ...              0              0              1   \n",
              "\n",
              "      MACCS_key_161  MACCS_key_162  MACCS_key_163  MACCS_key_164  \\\n",
              "0                 1              1              1              1   \n",
              "1                 1              1              1              1   \n",
              "2                 1              1              1              0   \n",
              "3                 1              1              1              1   \n",
              "4                 1              1              1              1   \n",
              "...             ...            ...            ...            ...   \n",
              "3466              1              1              0              1   \n",
              "3467              1              1              1              1   \n",
              "3468              1              1              1              1   \n",
              "3469              1              1              1              1   \n",
              "3470              1              1              1              1   \n",
              "\n",
              "      MACCS_key_165     MLM     HLM  \n",
              "0                 1  26.010  50.680  \n",
              "1                 1  29.270  50.590  \n",
              "2                 1   5.586  80.892  \n",
              "3                 1   5.710   2.000  \n",
              "4                 1  93.270  99.990  \n",
              "...             ...     ...     ...  \n",
              "3466              1   1.556   3.079  \n",
              "3467              1  35.560  47.630  \n",
              "3468              1  56.150   1.790  \n",
              "3469              1   0.030   2.770  \n",
              "3470              1   0.450   2.650  \n",
              "\n",
              "[3471 rows x 5313 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Objective 함수 정의\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    n_d = trial.suggest_int(\"n_d\", 4, 64)\n",
        "    n_a = trial.suggest_int(\"n_a\", 4, 64)\n",
        "    n_steps = trial.suggest_int(\"n_steps\", 1, 10)\n",
        "    gamma = trial.suggest_float(\"gamma\", 1.0, 2.0)\n",
        "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-5, 1e-3, log=True)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    optimizer_params = {\"lr\": lr}\n",
        "    \n",
        "    model = TabNetRegressor(\n",
        "        n_d=n_d,\n",
        "        n_a=n_a,\n",
        "        n_steps=n_steps,\n",
        "        gamma=gamma,\n",
        "        lambda_sparse=lambda_sparse,\n",
        "        optimizer_params=optimizer_params\n",
        "    )\n",
        "    \n",
        "    y_train_2d = y_train.values.reshape(-1, 1)\n",
        "    y_val_2d = y_val.values.reshape(-1, 1)\n",
        "    \n",
        "    model.fit(\n",
        "        X_train=X_train.values, \n",
        "        y_train=y_train_2d,\n",
        "        eval_set=[(X_val.values, y_val_2d)],\n",
        "        eval_metric=['mae'], \n",
        "        max_epochs=50, \n",
        "        patience=10\n",
        "    )\n",
        "    \n",
        "    preds_val = model.predict(X_val.values)\n",
        "    mae = mean_absolute_error(y_val.values.ravel(), preds_val)\n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모든 카테고리형 변수를 문자열로 변환\n",
        "categorical_cols = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "train_x[categorical_cols] = train_x[categorical_cols].astype(str)\n",
        "test[categorical_cols] = test[categorical_cols].astype(str)\n",
        "# object 타입 피처만 레이블 인코딩\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoders = {}\n",
        "test_unknown = test.copy()\n",
        "\n",
        "for col in train_x.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    train_x[col] = le.fit_transform(train_x[col])\n",
        "    \n",
        "    known_mask = test[col].isin(le.classes_)\n",
        "    test_unknown = test_unknown[~known_mask]  # 새로운 레이블을 포함하는 데이터 포인트를 별도로 저장\n",
        "    test = test[known_mask]\n",
        "    \n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 01:57:59,512]\u001b[0m A new study created in memory with name: no-name-a4896c90-e6f1-42a6-8c8b-5140d4f7cb5f\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n",
            "epoch 0  | loss: 54667.76901| val_0_mae: 180.92095|  0:00:12s\n",
            "epoch 1  | loss: 54281.59527| val_0_mae: 96.3394 |  0:00:23s\n",
            "epoch 2  | loss: 53814.79642| val_0_mae: 95.458  |  0:00:35s\n",
            "epoch 3  | loss: 53561.90072| val_0_mae: 94.85518|  0:00:47s\n",
            "epoch 4  | loss: 53108.98061| val_0_mae: 93.88412|  0:00:59s\n",
            "epoch 5  | loss: 52801.55621| val_0_mae: 93.0834 |  0:01:10s\n",
            "epoch 6  | loss: 52390.89816| val_0_mae: 92.26418|  0:01:21s\n",
            "epoch 7  | loss: 52056.07431| val_0_mae: 91.56768|  0:01:32s\n",
            "epoch 8  | loss: 51702.23693| val_0_mae: 91.00958|  0:01:43s\n",
            "epoch 9  | loss: 51476.37852| val_0_mae: 90.44323|  0:01:54s\n",
            "epoch 10 | loss: 51061.01298| val_0_mae: 89.9735 |  0:02:06s\n",
            "epoch 11 | loss: 50774.96551| val_0_mae: 89.50916|  0:02:18s\n",
            "epoch 12 | loss: 50536.75444| val_0_mae: 89.09913|  0:02:29s\n",
            "epoch 13 | loss: 50189.4527| val_0_mae: 88.73736|  0:02:41s\n",
            "epoch 14 | loss: 49885.91626| val_0_mae: 88.31281|  0:02:52s\n",
            "epoch 15 | loss: 49535.19113| val_0_mae: 88.13942|  0:03:03s\n",
            "epoch 16 | loss: 49218.82092| val_0_mae: 87.85673|  0:03:14s\n",
            "epoch 17 | loss: 48891.80871| val_0_mae: 87.62802|  0:03:26s\n",
            "epoch 18 | loss: 48622.86031| val_0_mae: 87.55156|  0:03:37s\n",
            "epoch 19 | loss: 48249.8295| val_0_mae: 87.49303|  0:03:48s\n",
            "epoch 20 | loss: 47972.63406| val_0_mae: 87.45159|  0:03:59s\n",
            "epoch 21 | loss: 47524.58687| val_0_mae: 87.47925|  0:04:11s\n",
            "epoch 22 | loss: 47300.80867| val_0_mae: 87.52749|  0:04:22s\n",
            "epoch 23 | loss: 47002.73457| val_0_mae: 87.71097|  0:04:33s\n",
            "epoch 24 | loss: 46627.28957| val_0_mae: 88.02458|  0:04:44s\n",
            "epoch 25 | loss: 46483.77078| val_0_mae: 88.32544|  0:04:55s\n",
            "epoch 26 | loss: 46106.75601| val_0_mae: 88.46872|  0:05:06s\n",
            "epoch 27 | loss: 45925.19347| val_0_mae: 88.79751|  0:05:18s\n",
            "epoch 28 | loss: 45613.11767| val_0_mae: 89.24789|  0:05:29s\n",
            "epoch 29 | loss: 45403.36398| val_0_mae: 89.69284|  0:05:40s\n",
            "epoch 30 | loss: 45193.25525| val_0_mae: 90.25419|  0:05:51s\n",
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_mae = 87.45159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:04:09,494]\u001b[0m Trial 0 finished with value: 87.4515860027796 and parameters: {'n_d': 58, 'n_a': 46, 'n_steps': 5, 'gamma': 1.471768994831443, 'lambda_sparse': 1.0520131499846483e-05, 'lr': 9.551591485457036e-05}. Best is trial 0 with value: 87.4515860027796.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 55200.8212| val_0_mae: 140.91934|  0:00:18s\n",
            "epoch 1  | loss: 54819.6298| val_0_mae: 104.29145|  0:00:35s\n",
            "epoch 2  | loss: 54864.47222| val_0_mae: 99.48548|  0:00:54s\n",
            "epoch 3  | loss: 54627.99508| val_0_mae: 98.81957|  0:01:13s\n",
            "epoch 4  | loss: 54556.0473| val_0_mae: 98.27959|  0:01:32s\n",
            "epoch 5  | loss: 54470.91333| val_0_mae: 97.81953|  0:01:51s\n",
            "epoch 6  | loss: 54251.61036| val_0_mae: 97.32818|  0:02:10s\n",
            "epoch 7  | loss: 54020.38877| val_0_mae: 96.90192|  0:02:29s\n",
            "epoch 8  | loss: 53893.73273| val_0_mae: 96.4771 |  0:02:48s\n",
            "epoch 9  | loss: 53832.84439| val_0_mae: 96.04991|  0:03:07s\n",
            "epoch 10 | loss: 53692.07525| val_0_mae: 95.65555|  0:03:27s\n",
            "epoch 11 | loss: 53605.63461| val_0_mae: 95.25395|  0:03:47s\n",
            "epoch 12 | loss: 53461.50434| val_0_mae: 94.9103 |  0:04:06s\n",
            "epoch 13 | loss: 53462.24061| val_0_mae: 94.5333 |  0:04:24s\n",
            "epoch 14 | loss: 53168.84983| val_0_mae: 94.22555|  0:04:46s\n",
            "epoch 15 | loss: 53158.89853| val_0_mae: 93.87284|  0:05:05s\n",
            "epoch 16 | loss: 52942.24935| val_0_mae: 93.5218 |  0:05:25s\n",
            "epoch 17 | loss: 52825.89031| val_0_mae: 93.17162|  0:05:45s\n",
            "epoch 18 | loss: 52754.69376| val_0_mae: 92.89039|  0:06:06s\n",
            "epoch 19 | loss: 52631.00737| val_0_mae: 92.63238|  0:06:25s\n",
            "epoch 20 | loss: 52469.38619| val_0_mae: 92.36485|  0:06:45s\n",
            "epoch 21 | loss: 52294.18978| val_0_mae: 92.12832|  0:07:05s\n",
            "epoch 22 | loss: 52255.04525| val_0_mae: 91.82937|  0:07:25s\n",
            "epoch 23 | loss: 52053.08052| val_0_mae: 91.5739 |  0:07:46s\n",
            "epoch 24 | loss: 51953.61611| val_0_mae: 91.29751|  0:08:06s\n",
            "epoch 25 | loss: 51918.84507| val_0_mae: 91.00982|  0:08:26s\n",
            "epoch 26 | loss: 51725.628| val_0_mae: 90.77064|  0:08:46s\n",
            "epoch 27 | loss: 51625.24408| val_0_mae: 90.53137|  0:09:07s\n",
            "epoch 28 | loss: 51454.64762| val_0_mae: 90.3105 |  0:09:27s\n",
            "epoch 29 | loss: 51339.49012| val_0_mae: 90.08561|  0:09:47s\n",
            "epoch 30 | loss: 51197.78717| val_0_mae: 89.87873|  0:10:07s\n",
            "epoch 31 | loss: 51030.4484| val_0_mae: 89.6764 |  0:10:26s\n",
            "epoch 32 | loss: 50904.01982| val_0_mae: 89.50132|  0:10:46s\n",
            "epoch 33 | loss: 50863.99302| val_0_mae: 89.25047|  0:11:05s\n",
            "epoch 34 | loss: 50743.54548| val_0_mae: 89.10319|  0:11:25s\n",
            "epoch 35 | loss: 50527.48773| val_0_mae: 88.89607|  0:11:45s\n",
            "epoch 36 | loss: 50354.59492| val_0_mae: 88.7992 |  0:12:06s\n",
            "epoch 37 | loss: 50247.45655| val_0_mae: 88.61919|  0:12:26s\n",
            "epoch 38 | loss: 50181.84406| val_0_mae: 88.43468|  0:12:47s\n",
            "epoch 39 | loss: 50016.84902| val_0_mae: 88.31349|  0:13:07s\n",
            "epoch 40 | loss: 49898.67976| val_0_mae: 88.20293|  0:13:27s\n",
            "epoch 41 | loss: 49669.91382| val_0_mae: 88.10884|  0:13:47s\n",
            "epoch 42 | loss: 49613.27013| val_0_mae: 87.95766|  0:14:08s\n",
            "epoch 43 | loss: 49392.73704| val_0_mae: 87.84672|  0:14:28s\n",
            "epoch 44 | loss: 49385.67642| val_0_mae: 87.81537|  0:14:48s\n",
            "epoch 45 | loss: 49225.80657| val_0_mae: 87.77375|  0:15:09s\n",
            "epoch 46 | loss: 49032.92301| val_0_mae: 87.62799|  0:15:28s\n",
            "epoch 47 | loss: 48970.87919| val_0_mae: 87.63349|  0:15:47s\n",
            "epoch 48 | loss: 48722.12248| val_0_mae: 87.59522|  0:16:05s\n",
            "epoch 49 | loss: 48686.18037| val_0_mae: 87.50547|  0:16:24s\n",
            "Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_mae = 87.50547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:21:06,939]\u001b[0m Trial 1 finished with value: 87.50547043557528 and parameters: {'n_d': 34, 'n_a': 21, 'n_steps': 9, 'gamma': 1.0189881736525601, 'lambda_sparse': 5.433270488189108e-05, 'lr': 3.736575233292654e-05}. Best is trial 0 with value: 87.4515860027796.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 56165.53162| val_0_mae: 121.68962|  0:00:12s\n",
            "epoch 1  | loss: 56051.26562| val_0_mae: 109.10764|  0:00:25s\n",
            "epoch 2  | loss: 55978.33927| val_0_mae: 103.57621|  0:00:38s\n",
            "epoch 3  | loss: 55695.06884| val_0_mae: 102.77369|  0:00:50s\n",
            "epoch 4  | loss: 55616.79267| val_0_mae: 102.1243|  0:01:03s\n",
            "epoch 5  | loss: 55510.32342| val_0_mae: 101.53767|  0:01:15s\n",
            "epoch 6  | loss: 55288.81478| val_0_mae: 100.96353|  0:01:28s\n",
            "epoch 7  | loss: 55249.3629| val_0_mae: 100.46883|  0:01:40s\n",
            "epoch 8  | loss: 55079.78255| val_0_mae: 99.99958|  0:01:53s\n",
            "epoch 9  | loss: 54784.32521| val_0_mae: 99.51779|  0:02:06s\n",
            "epoch 10 | loss: 54825.70543| val_0_mae: 99.04981|  0:02:19s\n",
            "epoch 11 | loss: 54633.61721| val_0_mae: 98.63612|  0:02:31s\n",
            "epoch 12 | loss: 54618.31074| val_0_mae: 98.21637|  0:02:44s\n",
            "epoch 13 | loss: 54454.86712| val_0_mae: 97.90979|  0:02:57s\n",
            "epoch 14 | loss: 54431.58068| val_0_mae: 97.47347|  0:03:09s\n",
            "epoch 15 | loss: 54247.88097| val_0_mae: 97.1203 |  0:03:22s\n",
            "epoch 16 | loss: 54147.97119| val_0_mae: 96.76958|  0:03:34s\n",
            "epoch 17 | loss: 54003.98159| val_0_mae: 96.44835|  0:03:47s\n",
            "epoch 18 | loss: 54002.59247| val_0_mae: 96.11228|  0:03:59s\n",
            "epoch 19 | loss: 53747.74515| val_0_mae: 95.7125 |  0:04:11s\n",
            "epoch 20 | loss: 53769.22652| val_0_mae: 95.46807|  0:04:23s\n",
            "epoch 21 | loss: 53522.46586| val_0_mae: 95.15146|  0:04:35s\n",
            "epoch 22 | loss: 53522.64373| val_0_mae: 94.77945|  0:04:48s\n",
            "epoch 23 | loss: 53417.77508| val_0_mae: 94.50758|  0:05:01s\n",
            "epoch 24 | loss: 53313.73561| val_0_mae: 94.16229|  0:05:13s\n",
            "epoch 25 | loss: 53193.45372| val_0_mae: 93.90529|  0:05:26s\n",
            "epoch 26 | loss: 53017.81351| val_0_mae: 93.57826|  0:05:39s\n",
            "epoch 27 | loss: 52988.32237| val_0_mae: 93.2881 |  0:05:52s\n",
            "epoch 28 | loss: 52825.71117| val_0_mae: 92.98421|  0:06:05s\n",
            "epoch 29 | loss: 52751.09608| val_0_mae: 92.73828|  0:06:18s\n",
            "epoch 30 | loss: 52602.01453| val_0_mae: 92.39195|  0:06:31s\n",
            "epoch 31 | loss: 52537.04341| val_0_mae: 92.15194|  0:06:44s\n",
            "epoch 32 | loss: 52237.66404| val_0_mae: 91.90387|  0:06:57s\n",
            "epoch 33 | loss: 52244.4889| val_0_mae: 91.64851|  0:07:11s\n",
            "epoch 34 | loss: 52043.33313| val_0_mae: 91.38639|  0:07:24s\n",
            "epoch 35 | loss: 52000.18867| val_0_mae: 91.14008|  0:07:37s\n",
            "epoch 36 | loss: 51912.39029| val_0_mae: 90.91026|  0:07:50s\n",
            "epoch 37 | loss: 51584.5855| val_0_mae: 90.68667|  0:08:03s\n",
            "epoch 38 | loss: 51697.83444| val_0_mae: 90.41624|  0:08:16s\n",
            "epoch 39 | loss: 51515.92176| val_0_mae: 90.19822|  0:08:29s\n",
            "epoch 40 | loss: 51307.26882| val_0_mae: 89.9548 |  0:08:42s\n",
            "epoch 41 | loss: 51252.54224| val_0_mae: 89.71145|  0:08:55s\n",
            "epoch 42 | loss: 51077.99508| val_0_mae: 89.51197|  0:09:08s\n",
            "epoch 43 | loss: 50931.33946| val_0_mae: 89.3787 |  0:09:20s\n",
            "epoch 44 | loss: 50747.92701| val_0_mae: 89.11786|  0:09:32s\n",
            "epoch 45 | loss: 50704.44979| val_0_mae: 88.98439|  0:09:45s\n",
            "epoch 46 | loss: 50438.94086| val_0_mae: 88.77437|  0:09:57s\n",
            "epoch 47 | loss: 50418.75014| val_0_mae: 88.61294|  0:10:09s\n",
            "epoch 48 | loss: 50266.76641| val_0_mae: 88.5067 |  0:10:22s\n",
            "epoch 49 | loss: 49980.89624| val_0_mae: 88.38432|  0:10:36s\n",
            "Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_mae = 88.38432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:32:03,453]\u001b[0m Trial 2 finished with value: 88.38432475220883 and parameters: {'n_d': 37, 'n_a': 27, 'n_steps': 5, 'gamma': 1.4603327006353592, 'lambda_sparse': 0.0008534092206517997, 'lr': 4.844451042562636e-05}. Best is trial 0 with value: 87.4515860027796.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 55328.52405| val_0_mae: 102.12292|  0:00:15s\n",
            "epoch 1  | loss: 54551.50612| val_0_mae: 97.92952|  0:00:30s\n",
            "epoch 2  | loss: 53936.78102| val_0_mae: 94.77041|  0:00:45s\n",
            "epoch 3  | loss: 53417.31689| val_0_mae: 93.91873|  0:01:01s\n",
            "epoch 4  | loss: 53012.97798| val_0_mae: 92.61567|  0:01:16s\n",
            "epoch 5  | loss: 52525.03646| val_0_mae: 91.47999|  0:01:31s\n",
            "epoch 6  | loss: 51853.47927| val_0_mae: 90.42937|  0:01:46s\n",
            "epoch 7  | loss: 51589.36794| val_0_mae: 89.52483|  0:02:02s\n",
            "epoch 8  | loss: 50911.37336| val_0_mae: 88.7996 |  0:02:18s\n",
            "epoch 9  | loss: 50522.28703| val_0_mae: 88.26465|  0:02:33s\n",
            "epoch 10 | loss: 50039.44559| val_0_mae: 87.80993|  0:02:48s\n",
            "epoch 11 | loss: 49595.04656| val_0_mae: 87.50941|  0:03:03s\n",
            "epoch 12 | loss: 49176.06787| val_0_mae: 87.24937|  0:03:19s\n",
            "epoch 13 | loss: 48691.30337| val_0_mae: 87.22915|  0:03:34s\n",
            "epoch 14 | loss: 48293.57882| val_0_mae: 87.31586|  0:03:50s\n",
            "epoch 15 | loss: 47840.62217| val_0_mae: 87.55827|  0:04:05s\n",
            "epoch 16 | loss: 47497.36271| val_0_mae: 87.90539|  0:04:19s\n",
            "epoch 17 | loss: 47156.06365| val_0_mae: 88.33343|  0:04:33s\n",
            "epoch 18 | loss: 46791.07335| val_0_mae: 88.95642|  0:04:48s\n",
            "epoch 19 | loss: 46446.78789| val_0_mae: 89.57177|  0:05:03s\n",
            "epoch 20 | loss: 46030.12559| val_0_mae: 90.24093|  0:05:17s\n",
            "epoch 21 | loss: 45729.87791| val_0_mae: 91.17123|  0:05:31s\n",
            "epoch 22 | loss: 45423.31725| val_0_mae: 91.9555 |  0:05:45s\n",
            "epoch 23 | loss: 45337.8606| val_0_mae: 93.00282|  0:06:00s\n",
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_mae = 87.22915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:38:26,739]\u001b[0m Trial 3 finished with value: 87.22914795206387 and parameters: {'n_d': 53, 'n_a': 62, 'n_steps': 6, 'gamma': 1.8290967984097537, 'lambda_sparse': 2.0638315057558745e-05, 'lr': 0.00014753325905081117}. Best is trial 3 with value: 87.22914795206387.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 55057.72515| val_0_mae: 103.21543|  0:00:19s\n",
            "epoch 1  | loss: 54999.30688| val_0_mae: 97.88925|  0:00:39s\n",
            "epoch 2  | loss: 54923.15769| val_0_mae: 99.3274 |  0:00:59s\n",
            "epoch 3  | loss: 54882.61184| val_0_mae: 99.66273|  0:01:18s\n",
            "epoch 4  | loss: 54781.98376| val_0_mae: 99.4892 |  0:01:38s\n",
            "epoch 5  | loss: 54834.16983| val_0_mae: 99.36418|  0:01:57s\n",
            "epoch 6  | loss: 54784.86199| val_0_mae: 99.13258|  0:02:17s\n",
            "epoch 7  | loss: 54676.25836| val_0_mae: 98.90912|  0:02:36s\n",
            "epoch 8  | loss: 54621.33027| val_0_mae: 98.75119|  0:02:56s\n",
            "epoch 9  | loss: 54569.33982| val_0_mae: 98.64019|  0:03:16s\n",
            "epoch 10 | loss: 54499.69737| val_0_mae: 98.42384|  0:03:36s\n",
            "epoch 11 | loss: 54499.32717| val_0_mae: 98.35121|  0:03:56s\n",
            "\n",
            "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_mae = 97.88925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:42:55,642]\u001b[0m Trial 4 finished with value: 97.88925116629748 and parameters: {'n_d': 33, 'n_a': 8, 'n_steps': 9, 'gamma': 1.3954106897914667, 'lambda_sparse': 3.435899500927241e-05, 'lr': 1.4530706847817623e-05}. Best is trial 3 with value: 87.22914795206387.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 46095.39365| val_0_mae: 417.88197|  0:00:18s\n",
            "epoch 1  | loss: 43015.46217| val_0_mae: 136.88145|  0:00:36s\n",
            "epoch 2  | loss: 42736.49585| val_0_mae: 103.93545|  0:00:55s\n",
            "epoch 3  | loss: 42568.07205| val_0_mae: 100.54177|  0:01:13s\n",
            "epoch 4  | loss: 42609.04317| val_0_mae: 98.73855|  0:01:30s\n",
            "epoch 5  | loss: 42485.57384| val_0_mae: 106.69002|  0:01:48s\n",
            "epoch 6  | loss: 41993.5897| val_0_mae: 101.21619|  0:02:07s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\XGB_v2.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m724\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\XGB_v2.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m724\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\XGB_v2.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m y_train_2d \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m y_val_2d \u001b[39m=\u001b[39m y_val\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     X_train\u001b[39m=\u001b[39;49mX_train\u001b[39m.\u001b[39;49mvalues, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     y_train\u001b[39m=\u001b[39;49my_train_2d,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(X_val\u001b[39m.\u001b[39;49mvalues, y_val_2d)],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     eval_metric\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmae\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m preds_val \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m mae \u001b[39m=\u001b[39m mean_absolute_error(y_val\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel(), preds_val)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:245\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n\u001b[1;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_epoch(eval_name, valid_dataloader)\n\u001b[0;32m    247\u001b[0m \u001b[39m# Call method on_epoch_end for all callbacks\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_end(\n\u001b[0;32m    249\u001b[0m     epoch_idx, logs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mepoch_metrics\n\u001b[0;32m    250\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:530\u001b[0m, in \u001b[0;36mTabModel._predict_epoch\u001b[1;34m(self, name, loader)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39m# Main loop\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m--> 530\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_batch(X)\n\u001b[0;32m    531\u001b[0m     list_y_true\u001b[39m.\u001b[39mappend(y)\n\u001b[0;32m    532\u001b[0m     list_y_score\u001b[39m.\u001b[39mappend(scores)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:558\u001b[0m, in \u001b[0;36mTabModel._predict_batch\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    555\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m    557\u001b[0m \u001b[39m# compute model output\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m scores, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scores, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    561\u001b[0m     scores \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scores]\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:586\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[1;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:471\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    470\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 471\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    472\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[0;32m    475\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:168\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[1;34m(self, x, prior)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m# output\u001b[39;00m\n\u001b[0;32m    167\u001b[0m masked_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(M, x)\n\u001b[1;32m--> 168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeat_transformers[step](masked_x)\n\u001b[0;32m    169\u001b[0m d \u001b[39m=\u001b[39m ReLU()(out[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_d])\n\u001b[0;32m    170\u001b[0m steps_output\u001b[39m.\u001b[39mappend(d)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:707\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    706\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared(x)\n\u001b[1;32m--> 707\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspecifics(x)\n\u001b[0;32m    708\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:749\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    746\u001b[0m     layers_left \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_glu)\n\u001b[0;32m    748\u001b[0m \u001b[39mfor\u001b[39;00m glu_id \u001b[39min\u001b[39;00m layers_left:\n\u001b[1;32m--> 749\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39madd(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglu_layers[glu_id](x))\n\u001b[0;32m    750\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m scale\n\u001b[0;32m    751\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:773\u001b[0m, in \u001b[0;36mGLU_Layer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    772\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m--> 773\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[0;32m    774\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim], torch\u001b[39m.\u001b[39msigmoid(x[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim :]))\n\u001b[0;32m    775\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36mGBN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=724)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=724))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best MAE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = TabNetRegressor(**best_params)\n",
        "    model.fit(X_train.values, y_train.values.ravel(), max_epochs=100)\n",
        "    test_pred = model.predict(test.values)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
